<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 100vh;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 100vh;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#d502a6", "description": "\"The Supremo Tribunal de Justi\u00e7a appears as a tribunal in indexed legal documents, indicating its relevance in the corpus.\"\u003cSEP\u003e\"The Supremo Tribunal de Justi\u00e7a is a Portuguese Supreme Court of Justice that the Semantic Search System aims to assist in its decision-making process.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SUPREMO TRIBUNAL DE JUSTI\u00c7A\"", "label": "\"SUPREMO TRIBUNAL DE JUSTI\u00c7A\"", "shape": "dot", "size": 10, "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7\u003cSEP\u003echunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"The Supremo Tribunal de Justi\u00e7a appears as a tribunal in indexed legal documents, indicating its relevance in the corpus.\"\u003cSEP\u003e\"The Supremo Tribunal de Justi\u00e7a is a Portuguese Supreme Court of Justice that the Semantic Search System aims to assist in its decision-making process.\""}, {"color": "#cda45e", "description": "\"Rui Filipe Coimbra Pereira de Melo is the author of the thesis and has supervised by Professors Pedro Alexandre Sim\u00f5es dos Santos and Jo\u00e3o Miguel De Sousa de Assis Dias.\"", "entity_type": "\"PERSON\"", "id": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "label": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "shape": "dot", "size": 10, "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Rui Filipe Coimbra Pereira de Melo is the author of the thesis and has supervised by Professors Pedro Alexandre Sim\u00f5es dos Santos and Jo\u00e3o Miguel De Sousa de Assis Dias.\""}, {"color": "#00aa5b", "description": "\"This is Rui Filipe Coimbra Pereira de Melo\u0027s thesis work, which aims to develop a Semantic Search System for the Supremo Tribunal de Justi\u00e7a.\"", "entity_type": "\"EVENT\"", "id": "\"THESIS TO OBTAIN THE MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING\"", "label": "\"THESIS TO OBTAIN THE MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING\"", "shape": "dot", "size": 10, "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"This is Rui Filipe Coimbra Pereira de Melo\u0027s thesis work, which aims to develop a Semantic Search System for the Supremo Tribunal de Justi\u00e7a.\""}, {"color": "#f89421", "description": "\"Chapter 5 delves into Legal-BERTimbau\u0027s role in the overall search system and explores its inner workings.\"::\u003cSEP\u003e\"Chapter 5 provides detailed information on how the Legal-BERTimbau model works and its contribution to the semantic search system.\"", "entity_type": "\"EVENT\"", "id": "\"CHAPTER 5\"", "label": "\"CHAPTER 5\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6\u003cSEP\u003echunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Chapter 5 delves into Legal-BERTimbau\u0027s role in the overall search system and explores its inner workings.\"::\u003cSEP\u003e\"Chapter 5 provides detailed information on how the Legal-BERTimbau model works and its contribution to the semantic search system.\""}, {"color": "#2e382a", "description": "\"Pedro Alexandre Sim\u00f5es dos Santos is one of the supervisors who guided Rui Filipe Coimbra Pereira de Melo in his thesis work.\"", "entity_type": "\"PERSON\"", "id": "\"PROF. PEDRO ALEXANDRE SIM\u00d5ES DOS SANTOS\"", "label": "\"PROF. PEDRO ALEXANDRE SIM\u00d5ES DOS SANTOS\"", "shape": "dot", "size": 10, "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Pedro Alexandre Sim\u00f5es dos Santos is one of the supervisors who guided Rui Filipe Coimbra Pereira de Melo in his thesis work.\""}, {"color": "#d0db92", "description": "\"Jo\u00e3o Miguel De Sousa de Assis Dias is another supervisor involved in guiding Rui Filipe Coimbra Pereira de Melo\u0027s thesis work.\"", "entity_type": "\"PERSON\"", "id": "\"PROF. JO\u00c3O MIGUEL DE SOUSA DE ASSIS DIAS\"", "label": "\"PROF. JO\u00c3O MIGUEL DE SOUSA DE ASSIS DIAS\"", "shape": "dot", "size": 10, "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Jo\u00e3o Miguel De Sousa de Assis Dias is another supervisor involved in guiding Rui Filipe Coimbra Pereira de Melo\u0027s thesis work.\""}, {"color": "#9fca6f", "description": "\"Maria Lu\u00edsa Torres Ribeiro Marques da Silva Coheur is the Chairperson of the Examination Committee for Rui Filipe Coimbra Pereira de Melo\u0027s thesis.\"", "entity_type": "\"PERSON\"", "id": "\"PROF. MARIA LU\u00cdSA TORRES RIBEIRO MARQUES DA SILVA COHEUR\"", "label": "\"PROF. MARIA LU\u00cdSA TORRES RIBEIRO MARQUES DA SILVA COHEUR\"", "shape": "dot", "size": 10, "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Maria Lu\u00edsa Torres Ribeiro Marques da Silva Coheur is the Chairperson of the Examination Committee for Rui Filipe Coimbra Pereira de Melo\u0027s thesis.\""}, {"color": "#ab3ca3", "description": "\"Jos\u00e9 Lu\u00eds Brinquete Borbinha is a member of the Examination Committee who participated in reviewing Rui Filipe Coimbra Pereira de Melo\u0027s thesis.\"", "entity_type": "\"PERSON\"", "id": "\"PROF. JOS\u00c9 LU\u00cdS BRINQUETE BORBINHA\"", "label": "\"PROF. JOS\u00c9 LU\u00cdS BRINQUETE BORBINHA\"", "shape": "dot", "size": 10, "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Jos\u00e9 Lu\u00eds Brinquete Borbinha is a member of the Examination Committee who participated in reviewing Rui Filipe Coimbra Pereira de Melo\u0027s thesis.\""}, {"color": "#a05d07", "description": "\"Project IRIS is an organization where Rui Filipe Coimbra Pereira de Melo cooperated with other team members in the past months.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"PROJECT IRIS MEMBERS\"", "label": "\"PROJECT IRIS MEMBERS\"", "shape": "dot", "size": 10, "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Project IRIS is an organization where Rui Filipe Coimbra Pereira de Melo cooperated with other team members in the past months.\""}, {"color": "#e5f686", "description": "\"Metadata Knowledge Distillation is a new technique introduced in this work to enhance the training of Large Language Models adapted to Portuguese jurisprudence.\"\u003cSEP\u003e\"Metadata Knowledge Distillation is a process where knowledge extracted from metadata is transferred to a target model, improving its performance on relevant tasks without requiring extensive labeled data.\"\u003cSEP\u003e\"Metadata Knowledge Distillation is an event or section covering knowledge distillation methods involving metadata.\"", "entity_type": "\"EVENT\"", "id": "\"METADATA KNOWLEDGE DISTILLATION\"", "label": "\"METADATA KNOWLEDGE DISTILLATION\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa\u003cSEP\u003echunk-61af9c4cbc05bef525950862cfae5a86\u003cSEP\u003echunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Metadata Knowledge Distillation is a new technique introduced in this work to enhance the training of Large Language Models adapted to Portuguese jurisprudence.\"\u003cSEP\u003e\"Metadata Knowledge Distillation is a process where knowledge extracted from metadata is transferred to a target model, improving its performance on relevant tasks without requiring extensive labeled data.\"\u003cSEP\u003e\"Metadata Knowledge Distillation is an event or section covering knowledge distillation methods involving metadata.\""}, {"color": "#4622a3", "description": "\"Legal-BERTimbau variants are a series of models developed for Portuguese legal context, tailored to improve information retrieval processes in court settings.\"\u003cSEP\u003e\"Legal-BERTimbau variants are specialized BERT models for legal document processing.\"\u003cSEP\u003e\"Legal-BERTimbau variants are specifically trained BERT models used in the Semantic Search System developed by Rui Filipe Coimbra Pereira de Melo.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEGAL-BERTIMBAU VARIANTS\"", "label": "\"LEGAL-BERTIMBAU VARIANTS\"", "shape": "dot", "size": 10, "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e\u003cSEP\u003echunk-61af9c4cbc05bef525950862cfae5a86\u003cSEP\u003echunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"Legal-BERTimbau variants are a series of models developed for Portuguese legal context, tailored to improve information retrieval processes in court settings.\"\u003cSEP\u003e\"Legal-BERTimbau variants are specialized BERT models for legal document processing.\"\u003cSEP\u003e\"Legal-BERTimbau variants are specifically trained BERT models used in the Semantic Search System developed by Rui Filipe Coimbra Pereira de Melo.\""}, {"color": "#af906b", "description": "\"Hybrid Search Systems are a combination of lexical and semantic techniques used in the developed Semantic Search System.\"\u003cSEP\u003e\"Hybrid search systems combining lexical and semantic techniques were incorporated into the Systematic Search prototype.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"HYBRID SEARCH SYSTEMS\"", "label": "\"HYBRID SEARCH SYSTEMS\"", "shape": "dot", "size": 10, "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a\u003cSEP\u003echunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Hybrid Search Systems are a combination of lexical and semantic techniques used in the developed Semantic Search System.\"\u003cSEP\u003e\"Hybrid search systems combining lexical and semantic techniques were incorporated into the Systematic Search prototype.\""}, {"color": "#51e3dd", "description": "\"BM25 is a language model used as a baseline for comparison with the developed Semantic Search System, indicating it maintains similar abilities to identify query sources but performs worse in accuracy compared to the new system.\"\u003cSEP\u003e\"BM25 is a search metric that outperforms the original Semantic Search System in several evaluations.\"\u003cSEP\u003e\"BM25 is a search metric that outperforms the original Semantic Search System in various evaluations.\"\u003cSEP\u003e\"BM25 is a search system architecture that provides specific top results for different metrics.\"\u003cSEP\u003e\"BM25 is a search system architecture that provides specific top results for different metrics.\")\u003cSEP\u003e\"BM25 is a technique used for information retrieval, compared to Legal-BERTimbau in terms of performance improvement.\"\u003cSEP\u003e\"BM25 is a traditional technique used for comparison in Search System Evaluation.\"\u003cSEP\u003e\"BM25 is an older methodology used as a comparison for the Semantic Search System developed by Rui Filipe Coimbra Pereira de Melo.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BM25\"", "label": "\"BM25\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051\u003cSEP\u003echunk-61af9c4cbc05bef525950862cfae5a86\u003cSEP\u003echunk-23944cdf583b2c2b5fcf537fea3f8421\u003cSEP\u003echunk-87a104ccfd8b71d9a4e7ad0343692cac\u003cSEP\u003echunk-206731ccea8c9feb0ba9feef0468e18a\u003cSEP\u003echunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"BM25 is a language model used as a baseline for comparison with the developed Semantic Search System, indicating it maintains similar abilities to identify query sources but performs worse in accuracy compared to the new system.\"\u003cSEP\u003e\"BM25 is a search metric that outperforms the original Semantic Search System in several evaluations.\"\u003cSEP\u003e\"BM25 is a search metric that outperforms the original Semantic Search System in various evaluations.\"\u003cSEP\u003e\"BM25 is a search system architecture that provides specific top results for different metrics.\"\u003cSEP\u003e\"BM25 is a search system architecture that provides specific top results for different metrics.\")\u003cSEP\u003e\"BM25 is a technique used for information retrieval, compared to Legal-BERTimbau in terms of performance improvement.\"\u003cSEP\u003e\"BM25 is a traditional technique used for comparison in Search System Evaluation.\"\u003cSEP\u003e\"BM25 is an older methodology used as a comparison for the Semantic Search System developed by Rui Filipe Coimbra Pereira de Melo.\""}, {"color": "#076a8f", "description": "\"Multilingual Knowledge Distillation is a specific event or section in the document that discusses knowledge distillation techniques across multiple languages.\"", "entity_type": "\"EVENT\"", "id": "\"MULTILINGUAL KNOWLEDGE DISTILLATION\"", "label": "\"MULTILINGUAL KNOWLEDGE DISTILLATION\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Multilingual Knowledge Distillation is a specific event or section in the document that discusses knowledge distillation techniques across multiple languages.\""}, {"color": "#c3839f", "description": "\"System Evaluation covers various evaluation methods and results for different systems within the document.\"\u003cSEP\u003e\"System Evaluation refers to the process of assessing the performance and effectiveness of search systems in retrieving relevant documents for judges.\"", "entity_type": "\"EVENT\"", "id": "\"SYSTEM EVALUATION\"", "label": "\"SYSTEM EVALUATION\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f\u003cSEP\u003echunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"System Evaluation covers various evaluation methods and results for different systems within the document.\"\u003cSEP\u003e\"System Evaluation refers to the process of assessing the performance and effectiveness of search systems in retrieving relevant documents for judges.\""}, {"color": "#873f97", "description": "\"A Systematic Search prototype was developed to assist the Supremo Tribunal de Justi\u00e7a portugu\u00eas in its decision-making process.\"\u003cSEP\u003e\"A Systematic Search prototype was developed to assist the Supremo Tribunal de Justi\u00e7a portugu\u00eas in its decision-making, utilizing advanced language models and hybrid search systems.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SYSTEMATIC SEARCH PROTOTYPE\"", "label": "\"SYSTEMATIC SEARCH PROTOTYPE\"", "shape": "dot", "size": 10, "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"A Systematic Search prototype was developed to assist the Supremo Tribunal de Justi\u00e7a portugu\u00eas in its decision-making process.\"\u003cSEP\u003e\"A Systematic Search prototype was developed to assist the Supremo Tribunal de Justi\u00e7a portugu\u00eas in its decision-making, utilizing advanced language models and hybrid search systems.\""}, {"color": "#beb629", "description": "\"The Lexical-First Search System combines lexical search techniques with large language models, using BM25 for initial filtering followed by cosine similarity metrics for ranking the final results.\"\u003cSEP\u003e\"The Lexical-First Search System is another component of the Semantic Search System, prioritizing lexical search before applying semantic methods.\"", "entity_type": "\"SYSTEM\"", "id": "\"LEXICAL-FIRST SEARCH SYSTEM\"", "label": "\"LEXICAL-FIRST SEARCH SYSTEM\"", "shape": "dot", "size": 10, "source_id": "chunk-338cf6d446307fa476c0b025098bcd87\u003cSEP\u003echunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Lexical-First Search System combines lexical search techniques with large language models, using BM25 for initial filtering followed by cosine similarity metrics for ranking the final results.\"\u003cSEP\u003e\"The Lexical-First Search System is another component of the Semantic Search System, prioritizing lexical search before applying semantic methods.\""}, {"color": "#dbcbce", "description": "\"The Lexical + Semantic Search System combines both lexical and semantic approaches for more comprehensive search functionalities.\"\u003cSEP\u003e\"The Lexical + Semantic Search System combines the strengths of both lexical and semantic systems, providing specific top results for different metrics.\"\u003cSEP\u003e\"The Lexical + Semantic Search System combines the strengths of both lexical and semantic systems, providing specific top results for different metrics.\")\u003cSEP\u003e\"The Lexical + Semantic Search System integrates both lexical and semantic information retrieval methods, normalizing BM25 scores with cosine similarity values from Legal-BERTimbau to rank search results more flexibly.\"\u003cSEP\u003e\"The Lexical + Semantic Search System integrates both lexical and semantic information retrieval methods, normalizing BM25 scores with cosine similarity values to rank search results more flexibly.\"\u003cSEP\u003e\"The Lexical + Semantic Search System is the overall system used for retrieval methods, including semantic textual similarity and natural language inference.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "label": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac\u003cSEP\u003echunk-338cf6d446307fa476c0b025098bcd87\u003cSEP\u003echunk-ca0f485e268dd70b104be9c53d4b68fd\u003cSEP\u003echunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Lexical + Semantic Search System combines both lexical and semantic approaches for more comprehensive search functionalities.\"\u003cSEP\u003e\"The Lexical + Semantic Search System combines the strengths of both lexical and semantic systems, providing specific top results for different metrics.\"\u003cSEP\u003e\"The Lexical + Semantic Search System combines the strengths of both lexical and semantic systems, providing specific top results for different metrics.\")\u003cSEP\u003e\"The Lexical + Semantic Search System integrates both lexical and semantic information retrieval methods, normalizing BM25 scores with cosine similarity values from Legal-BERTimbau to rank search results more flexibly.\"\u003cSEP\u003e\"The Lexical + Semantic Search System integrates both lexical and semantic information retrieval methods, normalizing BM25 scores with cosine similarity values to rank search results more flexibly.\"\u003cSEP\u003e\"The Lexical + Semantic Search System is the overall system used for retrieval methods, including semantic textual similarity and natural language inference.\""}, {"color": "#7d0f2a", "description": "\"Search System Evaluation covers the evaluation process and results related to search systems within the document.\"\u003cSEP\u003e\"Search System Evaluation involves comparing the performance of the developed solution against traditional techniques like BM25 to assess its effectiveness in retrieving relevant information for judges.\"", "entity_type": "\"EVENT\"", "id": "\"SEARCH SYSTEM EVALUATION\"", "label": "\"SEARCH SYSTEM EVALUATION\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f\u003cSEP\u003echunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Search System Evaluation covers the evaluation process and results related to search systems within the document.\"\u003cSEP\u003e\"Search System Evaluation involves comparing the performance of the developed solution against traditional techniques like BM25 to assess its effectiveness in retrieving relevant information for judges.\""}, {"color": "#e306d5", "description": "\"The Semantic Search System is a system developed to suggest more relevant documents accurately than BM25 with significant improvements in Discovery performance.\"\u003cSEP\u003e\"The Semantic Search System is a system used for searching documents with semantic understanding, and it has been evaluated against BM25 and other models.\"\u003cSEP\u003e\"The Semantic Search System refers to a series of components designed for efficient and meaningful information retrieval through pure semantic and lexical-semantic approaches.\"\u003cSEP\u003e\"The Semantic Search System refers to the overall system being evaluated for performance, involving Legal-BERTimbau, ElasticSearch index, and other models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SEMANTIC SEARCH SYSTEM\"", "label": "\"SEMANTIC SEARCH SYSTEM\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051\u003cSEP\u003echunk-23944cdf583b2c2b5fcf537fea3f8421\u003cSEP\u003echunk-437fca5e9e86e40c8ca3cf7fc41a8c65\u003cSEP\u003echunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Semantic Search System is a system developed to suggest more relevant documents accurately than BM25 with significant improvements in Discovery performance.\"\u003cSEP\u003e\"The Semantic Search System is a system used for searching documents with semantic understanding, and it has been evaluated against BM25 and other models.\"\u003cSEP\u003e\"The Semantic Search System refers to a series of components designed for efficient and meaningful information retrieval through pure semantic and lexical-semantic approaches.\"\u003cSEP\u003e\"The Semantic Search System refers to the overall system being evaluated for performance, involving Legal-BERTimbau, ElasticSearch index, and other models.\""}, {"color": "#72fae9", "description": "\"The Lexical-First approach combines lexical and semantic techniques, showing closer performance to BM25 and sometimes surpassing it.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEXICAL-FIRST APPROACH\"", "label": "\"LEXICAL-FIRST APPROACH\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"The Lexical-First approach combines lexical and semantic techniques, showing closer performance to BM25 and sometimes surpassing it.\""}, {"color": "#5dbd14", "description": "\"The Lexical+Semantic approach also uses a combination of lexical and semantic techniques, which performs close to or better than BM25 in some evaluations.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEXICAL+SEMANTIC APPROACH\"", "label": "\"LEXICAL+SEMANTIC APPROACH\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"The Lexical+Semantic approach also uses a combination of lexical and semantic techniques, which performs close to or better than BM25 in some evaluations.\""}, {"color": "#20078c", "description": "\"Custom STS dataset (V1 models) are datasets used to fine-tune search models, showing slightly lower performance compared to V0 models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"CUSTOM STS DATASET (V1 MODELS)\"", "label": "\"CUSTOM STS DATASET (V1 MODELS)\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Custom STS dataset (V1 models) are datasets used to fine-tune search models, showing slightly lower performance compared to V0 models.\""}, {"color": "#c6a065", "description": "\"Pre-existing and manually annotated datasets (V0 models) are used to fine-tune search models, showing better performance than V1 models in certain metrics.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"PRE-EXISTING AND MANUALLY ANNOTATED DATASETS (V0 MODELS)\"", "label": "\"PRE-EXISTING AND MANUALLY ANNOTATED DATASETS (V0 MODELS)\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Pre-existing and manually annotated datasets (V0 models) are used to fine-tune search models, showing better performance than V1 models in certain metrics.\""}, {"color": "#a2f129", "description": "\"A Hybrid Search System combines different techniques and can match or outperform BM25 capabilities, indicating its advanced nature.\"\u003cSEP\u003e\"A hybrid system used in query expansion, which is part of the research.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"HYBRID SEARCH SYSTEM\"", "label": "\"HYBRID SEARCH SYSTEM\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38\u003cSEP\u003echunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"A Hybrid Search System combines different techniques and can match or outperform BM25 capabilities, indicating its advanced nature.\"\u003cSEP\u003e\"A hybrid system used in query expansion, which is part of the research.\""}, {"color": "#84b8c8", "description": "\"The BM25 model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The BM25 model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "entity_type": "\"UNKNOWN\"", "id": "\"TOP 1 TOP 2 TOP 3 TOP 5 TOP 10\"", "label": "\"TOP 1 TOP 2 TOP 3 TOP 5 TOP 10\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The BM25 model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The BM25 model provides specific top results for different metrics in Table 6.3 and 6.4.\"::"}, {"color": "#1282c1", "description": "\"Lexical + Semantic Search System combines the strengths of both lexical and semantic systems, providing specific top results for different metrics.\"\u003cSEP\u003e\"Lexical + Semantic Search System combines the strengths of both lexical and semantic systems, providing specific top results for different metrics.\")", "entity_type": "\"ORGANIZATION\"", "id": "\"LEXICAL + SEMANTIC\"", "label": "\"LEXICAL + SEMANTIC\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"Lexical + Semantic Search System combines the strengths of both lexical and semantic systems, providing specific top results for different metrics.\"\u003cSEP\u003e\"Lexical + Semantic Search System combines the strengths of both lexical and semantic systems, providing specific top results for different metrics.\")"}, {"color": "#27c5af", "description": "\"The Supremo Tribunal de Justi\u00e7a portugu\u00eas is the main subject of the research, needing assistance with decision-making.\"\u003cSEP\u003e\"The Supremo Tribunal de Justi\u00e7a portugu\u00eas is the subject of a Systematic Search prototype developed to assist in its decision-making process.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SUPREMO TRIBUNAL DE JUSTI\u00c7A PORTUGU\u00caS\"", "label": "\"SUPREMO TRIBUNAL DE JUSTI\u00c7A PORTUGU\u00caS\"", "shape": "dot", "size": 10, "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"The Supremo Tribunal de Justi\u00e7a portugu\u00eas is the main subject of the research, needing assistance with decision-making.\"\u003cSEP\u003e\"The Supremo Tribunal de Justi\u00e7a portugu\u00eas is the subject of a Systematic Search prototype developed to assist in its decision-making process.\""}, {"color": "#7d7420", "description": "\"Legal-BERTimbau is a large language model designed to achieve the project\u0027s goals in the context of legal applications.\"\u003cSEP\u003e\"Legal-BERTimbau is a model created for the legal domain in Portugal through adaptation and fine-tuning of BERTimbau. It\u0027s designed to find similar passages within a legal context.\"\u003cSEP\u003e\"Legal-BERTimbau is a multilingual model that shows worse performance compared to the original Semantic Search System in certain evaluations.\"\u003cSEP\u003e\"Legal-BERTimbau is a variant of BERT specifically trained for legal purposes, used in the Systematic Search prototype.\"\u003cSEP\u003e\"Legal-BERTimbau is a variant of BERTimbau, specifically fine-tuned for the Portuguese legal domain and surpassing state-of-the-art multilingual language models on certain datasets.\"\u003cSEP\u003e\"Legal-BERTimbau is a version of BERTimbau developed for legal applications, used to create sentence embeddings and search systems.\"\u003cSEP\u003e\"Legal-BERTimbau is an adaptation of BERTimbau specifically for legal domain tasks, fine-tuned to better understand legal documents.\"\u003cSEP\u003e\"The Legal-BERTimbau model is used in the Lexical + Semantic Search System to provide cosine similarity values from a legal embedding space.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LEGAL-BERTIMBAU\"", "label": "\"LEGAL-BERTIMBAU\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051\u003cSEP\u003echunk-ca0f485e268dd70b104be9c53d4b68fd\u003cSEP\u003echunk-23944cdf583b2c2b5fcf537fea3f8421\u003cSEP\u003echunk-813c546217d2864adf1fc0789841ad36\u003cSEP\u003echunk-206731ccea8c9feb0ba9feef0468e18a\u003cSEP\u003echunk-338cf6d446307fa476c0b025098bcd87\u003cSEP\u003echunk-a24ce5ad9bdfbbf32c946ee404c32fd6\u003cSEP\u003echunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"Legal-BERTimbau is a large language model designed to achieve the project\u0027s goals in the context of legal applications.\"\u003cSEP\u003e\"Legal-BERTimbau is a model created for the legal domain in Portugal through adaptation and fine-tuning of BERTimbau. It\u0027s designed to find similar passages within a legal context.\"\u003cSEP\u003e\"Legal-BERTimbau is a multilingual model that shows worse performance compared to the original Semantic Search System in certain evaluations.\"\u003cSEP\u003e\"Legal-BERTimbau is a variant of BERT specifically trained for legal purposes, used in the Systematic Search prototype.\"\u003cSEP\u003e\"Legal-BERTimbau is a variant of BERTimbau, specifically fine-tuned for the Portuguese legal domain and surpassing state-of-the-art multilingual language models on certain datasets.\"\u003cSEP\u003e\"Legal-BERTimbau is a version of BERTimbau developed for legal applications, used to create sentence embeddings and search systems.\"\u003cSEP\u003e\"Legal-BERTimbau is an adaptation of BERTimbau specifically for legal domain tasks, fine-tuned to better understand legal documents.\"\u003cSEP\u003e\"The Legal-BERTimbau model is used in the Lexical + Semantic Search System to provide cosine similarity values from a legal embedding space.\""}, {"color": "#cf592f", "description": "\"Alex is involved in developing a Semantic Search System as part of Project IRIS.\"\u003cSEP\u003e\"Alex is involved in developing the Systematic Search prototype for the Supremo Tribunal de Justi\u00e7a portugu\u00eas.\"\u003cSEP\u003e\"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\"\u003cSEP\u003e\"Alex is the leader or key developer of the Systematic Search prototype.\"", "entity_type": "\"PERSON\"", "id": "\"ALEX\"", "label": "\"ALEX\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051\u003cSEP\u003echunk-afd8ce7e1a7d61d34be9bdded6cff755\u003cSEP\u003echunk-206731ccea8c9feb0ba9feef0468e18a\u003cSEP\u003echunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Alex is involved in developing a Semantic Search System as part of Project IRIS.\"\u003cSEP\u003e\"Alex is involved in developing the Systematic Search prototype for the Supremo Tribunal de Justi\u00e7a portugu\u00eas.\"\u003cSEP\u003e\"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\"\u003cSEP\u003e\"Alex is the leader or key developer of the Systematic Search prototype.\""}, {"color": "#289825", "description": "\"BERTimbau is a language model derived from BERT that was pre-trained using specific techniques for Portuguese language processing.\"\u003cSEP\u003e\"BERTimbau is a language model trained with a large Portuguese corpus, BrWaC, and later adapted for use in Portuguese jurisprudence.\"\u003cSEP\u003e\"BERTimbau is a monolingual BERT model specifically trained for the Portuguese language, utilizing Brazilian Web as Corpus (BrWaC).\"\u003cSEP\u003e\"BERTimbau is an adaptation of BERT for the Brazilian Portuguese language, designed to handle the unique linguistic challenges of the language.\"\u003cSEP\u003e\"BERTimbau is the base model used for Legal-BERTimbau, specifically adapted for the Portuguese language.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BERTIMBAU\"", "label": "\"BERTIMBAU\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506\u003cSEP\u003echunk-ca0f485e268dd70b104be9c53d4b68fd\u003cSEP\u003echunk-cb545b2bab1e85e24c22c3bd238a556f\u003cSEP\u003echunk-813c546217d2864adf1fc0789841ad36\u003cSEP\u003echunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"BERTimbau is a language model derived from BERT that was pre-trained using specific techniques for Portuguese language processing.\"\u003cSEP\u003e\"BERTimbau is a language model trained with a large Portuguese corpus, BrWaC, and later adapted for use in Portuguese jurisprudence.\"\u003cSEP\u003e\"BERTimbau is a monolingual BERT model specifically trained for the Portuguese language, utilizing Brazilian Web as Corpus (BrWaC).\"\u003cSEP\u003e\"BERTimbau is an adaptation of BERT for the Brazilian Portuguese language, designed to handle the unique linguistic challenges of the language.\"\u003cSEP\u003e\"BERTimbau is the base model used for Legal-BERTimbau, specifically adapted for the Portuguese language.\""}, {"color": "#bd214e", "description": "\"Albertina PT-BR outperforms BERTimbau in certain STS tasks but falls short of its performance overall.\"::", "entity_type": "\"UNKNOWN\"", "id": "\"ALBERTINA PT -BR\"", "label": "\"ALBERTINA PT -BR\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"Albertina PT-BR outperforms BERTimbau in certain STS tasks but falls short of its performance overall.\"::"}, {"color": "#4a28d3", "description": "\"HuggingFace is a platform where datasets and models are publicly available.\"\u003cSEP\u003e\"HuggingFace provides access to embedding models and frameworks that are utilized by various search systems.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"HUGGINGFACE\"", "label": "\"HUGGINGFACE\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc\u003cSEP\u003echunk-338cf6d446307fa476c0b025098bcd87", "title": "\"HuggingFace is a platform where datasets and models are publicly available.\"\u003cSEP\u003e\"HuggingFace provides access to embedding models and frameworks that are utilized by various search systems.\""}, {"color": "#21c5e6", "description": "\"SBERT Model refers to a Sentence-BERT model designed for finding similar passages. It\u0027s part of the broader project of Legal-BERTimbau.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SBERT MODEL\"", "label": "\"SBERT MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"SBERT Model refers to a Sentence-BERT model designed for finding similar passages. It\u0027s part of the broader project of Legal-BERTimbau.\""}, {"color": "#080402", "description": "\"BrWaC (Brazilian Web Corpus) is the pre-training dataset used to train BERTimbau.\"", "entity_type": "\"LOCATION\"", "id": "\"BRWAC\"", "label": "\"BRWAC\"", "shape": "dot", "size": 10, "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"BrWaC (Brazilian Web Corpus) is the pre-training dataset used to train BERTimbau.\""}, {"color": "#c435d2", "description": "\"The Search metric evaluates the system\u0027s ability to find which document a certain query refers to, scoring based on whether the retrieved document matches the original query\u0027s source.\"", "entity_type": "\"EVENT\"", "id": "\"SEARCH METRIC\"", "label": "\"SEARCH METRIC\"", "shape": "dot", "size": 10, "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Search metric evaluates the system\u0027s ability to find which document a certain query refers to, scoring based on whether the retrieved document matches the original query\u0027s source.\""}, {"color": "#2100e1", "description": "\"The Discovery metric assesses the search system\u0027s capability to retrieve additional documents relevant to the user\u0027s query, even if not directly from the original document.\"", "entity_type": "\"EVENT\"", "id": "\"DISCOVERY METRIC\"", "label": "\"DISCOVERY METRIC\"", "shape": "dot", "size": 10, "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Discovery metric assesses the search system\u0027s capability to retrieve additional documents relevant to the user\u0027s query, even if not directly from the original document.\""}, {"color": "#c97474", "description": "\"First Contact is the potential initial communication between humanity and an unknown intelligence.\"", "entity_type": "\"EVENT\"", "id": "\"FIRST CONTACT\"", "label": "\"FIRST CONTACT\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"First Contact is the potential initial communication between humanity and an unknown intelligence.\""}, {"color": "#dc4f29", "description": "\"Humanity\u0027s Response is the collective action taken by Alex\u0027s team in response to a message from an unknown intelligence.\"", "entity_type": "\"EVENT\"", "id": "\"HUMANITY\u0027S RESPONSE\"", "label": "\"HUMANITY\u0027S RESPONSE\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Humanity\u0027s Response is the collective action taken by Alex\u0027s team in response to a message from an unknown intelligence.\""}, {"color": "#dd4a59", "description": "\"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety.\"\u003cSEP\u003e\"Sam Rivera is a member of the team working on the Semantic Search System and contributing to the development process.\"", "entity_type": "\"PERSON\"", "id": "\"SAM RIVERA\"", "label": "\"SAM RIVERA\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051\u003cSEP\u003echunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety.\"\u003cSEP\u003e\"Sam Rivera is a member of the team working on the Semantic Search System and contributing to the development process.\""}, {"color": "#804ed1", "description": "\"GloVe is a method used for learning word embeddings, specifically through co-occurrence matrix analysis and optimization functions.\"\u003cSEP\u003e\"GloVe is a method used for word embeddings, often employed in natural language processing tasks.\"\u003cSEP\u003e\"GloVe is an unsupervised learning algorithm developed for representing words as vectors and deriving semantic relationships between them.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"GLOVE\"", "label": "\"GLOVE\"", "shape": "dot", "size": 10, "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6\u003cSEP\u003echunk-073485a6071a38818c47ff7188ec860b\u003cSEP\u003echunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"GloVe is a method used for learning word embeddings, specifically through co-occurrence matrix analysis and optimization functions.\"\u003cSEP\u003e\"GloVe is a method used for word embeddings, often employed in natural language processing tasks.\"\u003cSEP\u003e\"GloVe is an unsupervised learning algorithm developed for representing words as vectors and deriving semantic relationships between them.\""}, {"color": "#efbb40", "description": "\"A text summarization technique based on eigenvector centrality of sentences within a document.\"\u003cSEP\u003e\"LexRank is a text summarization technique that uses a link analysis algorithm similar to PageRank to identify the most important sentences in a document.\"\u003cSEP\u003e\"LexRank is an extractive summarization technique utilized in the implementation of the Semantic Search System.\"\u003cSEP\u003e\"LexRank is an unsupervised extractive summarization technique that uses eigenvector centrality in a sentence\u0027s graph representation for scoring sentences based on intra-sentence cosine similarity.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEXRANK\"", "label": "\"LEXRANK\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051\u003cSEP\u003echunk-cb545b2bab1e85e24c22c3bd238a556f\u003cSEP\u003echunk-4438e56d1dbc938d09784326b42337ca\u003cSEP\u003echunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"A text summarization technique based on eigenvector centrality of sentences within a document.\"\u003cSEP\u003e\"LexRank is a text summarization technique that uses a link analysis algorithm similar to PageRank to identify the most important sentences in a document.\"\u003cSEP\u003e\"LexRank is an extractive summarization technique utilized in the implementation of the Semantic Search System.\"\u003cSEP\u003e\"LexRank is an unsupervised extractive summarization technique that uses eigenvector centrality in a sentence\u0027s graph representation for scoring sentences based on intra-sentence cosine similarity.\""}, {"color": "#a1b024", "description": "\"Stanford is an educational institution where Word2Vec and GloVe were developed.\"", "entity_type": "\"LOCATION\"", "id": "\"STANFORD\"", "label": "\"STANFORD\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"Stanford is an educational institution where Word2Vec and GloVe were developed.\""}, {"color": "#f86b77", "description": "\"A co-occurrence matrix represents the probability of a word appearing in the context of another word, used by GloVe to derive semantic relationships between them.\"\u003cSEP\u003e\"Co-occurrence matrix represents the probability of words appearing next to each other, central to GloVe\u0027s methodology.\"", "entity_type": "\"CONCEPT\"", "id": "\"CO-OCCURRENCE MATRIX\"", "label": "\"CO-OCCURRENCE MATRIX\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b\u003cSEP\u003echunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"A co-occurrence matrix represents the probability of a word appearing in the context of another word, used by GloVe to derive semantic relationships between them.\"\u003cSEP\u003e\"Co-occurrence matrix represents the probability of words appearing next to each other, central to GloVe\u0027s methodology.\""}, {"color": "#b7b17c", "description": "\"Loss function is a formal definition in GloVe that minimizes the difference between predicted and actual word vector distances through dot products and biases.\"", "entity_type": "\"EVENT\"", "id": "\"LOSS FUNCTION\"", "label": "\"LOSS FUNCTION\"", "shape": "dot", "size": 10, "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Loss function is a formal definition in GloVe that minimizes the difference between predicted and actual word vector distances through dot products and biases.\""}, {"color": "#213fd6", "description": "\"Recurrent Neural Networks are used for sequential data analysis and can be applied in various NLP tasks.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"RECURRENT NEURAL NETWORK (RNN)\"", "label": "\"RECURRENT NEURAL NETWORK (RNN)\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Recurrent Neural Networks are used for sequential data analysis and can be applied in various NLP tasks.\""}, {"color": "#0f01a0", "description": "\"Long Short-Term Memory is a type of RNN capable of learning long-term dependencies, useful in sequence modeling tasks.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LONG SHORT-TERM MEMORY (LSTM)\"", "label": "\"LONG SHORT-TERM MEMORY (LSTM)\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Long Short-Term Memory is a type of RNN capable of learning long-term dependencies, useful in sequence modeling tasks.\""}, {"color": "#209305", "description": "\"Transformers are modern neural network architectures designed to process and generate sequences more efficiently than previous models like RNNs and LSTMs.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TRANSFORMERS\"", "label": "\"TRANSFORMERS\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Transformers are modern neural network architectures designed to process and generate sequences more efficiently than previous models like RNNs and LSTMs.\""}, {"color": "#6b88a8", "description": "\"BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model designed for natural language processing tasks.\"\u003cSEP\u003e\"BERT (Bidirectional Encoder Representations from Transformers) is an advanced natural language processing model developed by Google, known for its effectiveness in various NLP tasks.\"\u003cSEP\u003e\"BERT is a natural language processing framework that uses bidirectional training to enhance understanding of text sequences.\"\u003cSEP\u003e\"BERT is an existing model used by SBERT as a basis, with improvements made in embedding speed and cosine similarity methods.\"\u003cSEP\u003e\"BERT refers to the BERT model, which is used in natural language processing tasks and is specifically mentioned for its performance on description queries compared to title queries.\"\u003cSEP\u003e\"BERT stands for Bidirectional Encoder Representations from Transformers, a model developed by Google AI Language in 2019. It is used for natural language processing tasks such as text classification and question answering.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BERT\"", "label": "\"BERT\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4\u003cSEP\u003echunk-6eeb6febf5ce46ec96655d84dc54cd2f\u003cSEP\u003echunk-b6270162d82d1fef624d494a11c5caca\u003cSEP\u003echunk-cb545b2bab1e85e24c22c3bd238a556f\u003cSEP\u003echunk-abe3791c0e495b4a72310f8a36c50056\u003cSEP\u003echunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model designed for natural language processing tasks.\"\u003cSEP\u003e\"BERT (Bidirectional Encoder Representations from Transformers) is an advanced natural language processing model developed by Google, known for its effectiveness in various NLP tasks.\"\u003cSEP\u003e\"BERT is a natural language processing framework that uses bidirectional training to enhance understanding of text sequences.\"\u003cSEP\u003e\"BERT is an existing model used by SBERT as a basis, with improvements made in embedding speed and cosine similarity methods.\"\u003cSEP\u003e\"BERT refers to the BERT model, which is used in natural language processing tasks and is specifically mentioned for its performance on description queries compared to title queries.\"\u003cSEP\u003e\"BERT stands for Bidirectional Encoder Representations from Transformers, a model developed by Google AI Language in 2019. It is used for natural language processing tasks such as text classification and question answering.\""}, {"color": "#2243bf", "description": "\"Text Summarization is a process of reducing text to its most important information while maintaining its meaning.\"\u003cSEP\u003e\"Text Summarization is a technique that involves creating concise summaries of longer texts, retaining the key points and overall meaning.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"TEXT SUMMARIZATION\"", "label": "\"TEXT SUMMARIZATION\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f\u003cSEP\u003echunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Text Summarization is a process of reducing text to its most important information while maintaining its meaning.\"\u003cSEP\u003e\"Text Summarization is a technique that involves creating concise summaries of longer texts, retaining the key points and overall meaning.\""}, {"color": "#d51a4e", "description": "\"Semantic Search Type refers to methods used for searching and retrieving information based on meaning rather than just keywords.\"\u003cSEP\u003e\"Semantic search aims to improve overall search quality by understanding the underlying query and sentence meaning through vectorial representations of words, paragraphs, or documents.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SEMANTIC SEARCH TYPE\"", "label": "\"SEMANTIC SEARCH TYPE\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f\u003cSEP\u003echunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Semantic Search Type refers to methods used for searching and retrieving information based on meaning rather than just keywords.\"\u003cSEP\u003e\"Semantic search aims to improve overall search quality by understanding the underlying query and sentence meaning through vectorial representations of words, paragraphs, or documents.\""}, {"color": "#ab106c", "description": "\"A technique that focuses on understanding contextual meaning and user intention beyond exact word matches.\"\u003cSEP\u003e\"Semantic Search refers to the process of improving search quality by understanding query and sentence meaning through embeddings.\"\u003cSEP\u003e\"Semantic search involves understanding the context and meaning behind query terms to retrieve more relevant documents, in contrast to lexical search which looks for exact matches.\"", "entity_type": "\"CONCEPT\"", "id": "\"SEMANTIC SEARCH\"", "label": "\"SEMANTIC SEARCH\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494\u003cSEP\u003echunk-8dc9bc0243bc2842874f8fe5a91e0b07\u003cSEP\u003echunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"A technique that focuses on understanding contextual meaning and user intention beyond exact word matches.\"\u003cSEP\u003e\"Semantic Search refers to the process of improving search quality by understanding query and sentence meaning through embeddings.\"\u003cSEP\u003e\"Semantic search involves understanding the context and meaning behind query terms to retrieve more relevant documents, in contrast to lexical search which looks for exact matches.\""}, {"color": "#741389", "description": "\"Eigenvector centrality refers to a measure in graph theory where nodes are ranked based on the importance of neighboring nodes. In the context of sentences, it is used as part of an algorithm for determining sentence relevance.\"", "entity_type": "\"CONCEPT\"", "id": "\"EIGENVECTOR CENTRALITY\"", "label": "\"EIGENVECTOR CENTRALITY\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Eigenvector centrality refers to a measure in graph theory where nodes are ranked based on the importance of neighboring nodes. In the context of sentences, it is used as part of an algorithm for determining sentence relevance.\""}, {"color": "#24ce1f", "description": "\"A method of training models where the model processes text sequences either from left to right or right to left, but not both.\"\u003cSEP\u003e\"Unidirectional training is a traditional method where models only look at text sequences from left to right or vice-versa.\"", "entity_type": "\"CONCEPT\"", "id": "\"UNIDIRECTIONAL TRAINING\"", "label": "\"UNIDIRECTIONAL TRAINING\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056\u003cSEP\u003echunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"A method of training models where the model processes text sequences either from left to right or right to left, but not both.\"\u003cSEP\u003e\"Unidirectional training is a traditional method where models only look at text sequences from left to right or vice-versa.\""}, {"color": "#0f37b3", "description": "\"MLM is a task used to train language models to understand technical jargon and structured documents.\"\u003cSEP\u003e\"Masked Language Modeling is a task originally introduced by BERT, where words are masked and the model predicts them.\"\u003cSEP\u003e\"Masked Language Modeling is a technique in BERT\u0027s pre-training phase where words are masked and the model predicts them based on context.\"", "entity_type": "\"EVENT\"", "id": "\"MASKED LANGUAGE MODELING (MLM)\"", "label": "\"MASKED LANGUAGE MODELING (MLM)\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95\u003cSEP\u003echunk-03104590ddb41a3cfeb2c96dccb5c2f1\u003cSEP\u003echunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"MLM is a task used to train language models to understand technical jargon and structured documents.\"\u003cSEP\u003e\"Masked Language Modeling is a task originally introduced by BERT, where words are masked and the model predicts them.\"\u003cSEP\u003e\"Masked Language Modeling is a technique in BERT\u0027s pre-training phase where words are masked and the model predicts them based on context.\""}, {"color": "#e97250", "description": "\"Next Sentence Prediction is a task that helps BERT understand sentence relationships by predicting if a given pair of sentences are consecutive or not.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"NEXT SENTENCE PREDICTION (NSP)\"", "label": "\"NEXT SENTENCE PREDICTION (NSP)\"", "shape": "dot", "size": 10, "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Next Sentence Prediction is a task that helps BERT understand sentence relationships by predicting if a given pair of sentences are consecutive or not.\""}, {"color": "#1e0ad4", "description": "\"A Cross-Encoder is a component in the Pseudo Labeling phase that calculates the score margin between positive and negative passages based on their similarity to a query.\"\u003cSEP\u003e\"A Cross-Encoder is used in the Pseudo Labeling step to calculate the score margin between positive and negative passages based on their similarity to a query, utilizing the Margin Mean Squared Error Loss function.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"CROSS-ENCODER\"", "label": "\"CROSS-ENCODER\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"A Cross-Encoder is a component in the Pseudo Labeling phase that calculates the score margin between positive and negative passages based on their similarity to a query.\"\u003cSEP\u003e\"A Cross-Encoder is used in the Pseudo Labeling step to calculate the score margin between positive and negative passages based on their similarity to a query, utilizing the Margin Mean Squared Error Loss function.\""}, {"color": "#deccbe", "description": "\"SBERT is a library or tool used for semantic textual similarity tasks, often built using SentenceTransformers variants of BERTimbau.\"\u003cSEP\u003e\"SBERT is a model or system for Sentence-Bert evaluation and Multilingual Knowledge Distillation.\"\u003cSEP\u003e\"SBERT is a modification of BERT that is used for embedding small passages effectively, as suggested by research results.\"\u003cSEP\u003e\"SBERT is a modification of the BERT network designed to create sentence embeddings that are semantically meaningful and used for tasks such as semantic similarity comparison and information retrieval.\"\u003cSEP\u003e\"SBERT is a semantic model that improves on BERT for tasks like sentence pair comparison, fine-tuning through the Softmax loss approach, and evaluating semantic textual similarity.\"\u003cSEP\u003e\"SBert is mentioned as another model that could be used for semantic search, though it\u2019s not directly implemented in this case.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SBERT\"", "label": "\"SBERT\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506\u003cSEP\u003echunk-4178cfa608054c267be41d058b830af4\u003cSEP\u003echunk-6eeb6febf5ce46ec96655d84dc54cd2f\u003cSEP\u003echunk-b6270162d82d1fef624d494a11c5caca\u003cSEP\u003echunk-338cf6d446307fa476c0b025098bcd87\u003cSEP\u003echunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"SBERT is a library or tool used for semantic textual similarity tasks, often built using SentenceTransformers variants of BERTimbau.\"\u003cSEP\u003e\"SBERT is a model or system for Sentence-Bert evaluation and Multilingual Knowledge Distillation.\"\u003cSEP\u003e\"SBERT is a modification of BERT that is used for embedding small passages effectively, as suggested by research results.\"\u003cSEP\u003e\"SBERT is a modification of the BERT network designed to create sentence embeddings that are semantically meaningful and used for tasks such as semantic similarity comparison and information retrieval.\"\u003cSEP\u003e\"SBERT is a semantic model that improves on BERT for tasks like sentence pair comparison, fine-tuning through the Softmax loss approach, and evaluating semantic textual similarity.\"\u003cSEP\u003e\"SBert is mentioned as another model that could be used for semantic search, though it\u2019s not directly implemented in this case.\""}, {"color": "#cbd473", "description": "\"Domain Adaptation is a crucial technique used to address model generalization issues in new and unseen data, especially when labeled data is scarce in the target domain.\"\u003cSEP\u003e\"Domain adaptation refers to the process of adapting a machine learning model to work well on data from a different domain or distribution than it was originally trained on.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"DOMAIN ADAPTATION\"", "label": "\"DOMAIN ADAPTATION\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2\u003cSEP\u003echunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Domain Adaptation is a crucial technique used to address model generalization issues in new and unseen data, especially when labeled data is scarce in the target domain.\"\u003cSEP\u003e\"Domain adaptation refers to the process of adapting a machine learning model to work well on data from a different domain or distribution than it was originally trained on.\""}, {"color": "#949bcf", "description": "\"Model Generalization refers to how well a model can perform on new, unseen data, which Domain Adaptation helps improve.\"", "entity_type": "\"CONCEPT\"", "id": "\"MODEL GENERALIZATION\"", "label": "\"MODEL GENERALIZATION\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Model Generalization refers to how well a model can perform on new, unseen data, which Domain Adaptation helps improve.\""}, {"color": "#869a71", "description": "\"neuralmind/bert-base-portuguese-cased is a BERT Base variant pre-trained for Portuguese with 12 layers and 110M parameters.\"", "entity_type": "\"MODEL\"", "id": "\"NEURALMIND/BERT-BASE-PORTUGUESE-CASED\"", "label": "\"NEURALMIND/BERT-BASE-PORTUGUESE-CASED\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"neuralmind/bert-base-portuguese-cased is a BERT Base variant pre-trained for Portuguese with 12 layers and 110M parameters.\""}, {"color": "#992d3a", "description": "\"BrWaC corpus is the source dataset containing over 2.68 billion tokens and 3.53 million documents, providing diverse text data.\"", "entity_type": "\"LOCATION\"", "id": "\"BRWAC CORPUS\"", "label": "\"BRWAC CORPUS\"", "shape": "dot", "size": 10, "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"BrWaC corpus is the source dataset containing over 2.68 billion tokens and 3.53 million documents, providing diverse text data.\""}, {"color": "#f67a6e", "description": "\"Robust04 is one of the datasets used in performance testing, known for its well-written text.\"", "entity_type": "\"EVENT\"", "id": "\"ROBUST04\"", "label": "\"ROBUST04\"", "shape": "dot", "size": 10, "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"Robust04 is one of the datasets used in performance testing, known for its well-written text.\""}, {"color": "#c09272", "description": "\"Zhuyun Dai and Jamie Callan are researchers who explored the use of contextual neural language models like BERT in information retrieval.\"", "entity_type": "\"PERSON\"", "id": "\"ZHUYUN DAI AND JAMIE CALLAN\"", "label": "\"ZHUYUN DAI AND JAMIE CALLAN\"", "shape": "dot", "size": 10, "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"Zhuyun Dai and Jamie Callan are researchers who explored the use of contextual neural language models like BERT in information retrieval.\""}, {"color": "#575132", "description": "\"HuggingFace platform is where the developed datasets and language models are published.\"\u003cSEP\u003e\"HuggingFace platform refers to a resource where the dataset was published for model reproducibility and future use.\"\u003cSEP\u003e\"The HuggingFace Platform is a hosting platform for models like BERTimbau, using the HuggingFace\u0027s Transformers library.\"", "entity_type": "\"LOCATION\"", "id": "\"HUGGINGFACE PLATFORM\"", "label": "\"HUGGINGFACE PLATFORM\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7\u003cSEP\u003echunk-ae090822f3d4769354cc463665e2df89\u003cSEP\u003echunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"HuggingFace platform is where the developed datasets and language models are published.\"\u003cSEP\u003e\"HuggingFace platform refers to a resource where the dataset was published for model reproducibility and future use.\"\u003cSEP\u003e\"The HuggingFace Platform is a hosting platform for models like BERTimbau, using the HuggingFace\u0027s Transformers library.\""}, {"color": "#952a57", "description": "\"TensorFlow is one of the platforms on which BERTimbau variants can be hosted, alongside HuggingFace Platform and JAX.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TENSORFLOW\"", "label": "\"TENSORFLOW\"", "shape": "dot", "size": 10, "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"TensorFlow is one of the platforms on which BERTimbau variants can be hosted, alongside HuggingFace Platform and JAX.\""}, {"color": "#b82c99", "description": "\"PyTorch is another platform on which BERTimbau variants can be hosted, alongside HuggingFace Platform and TensorFlow.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"PYTORCH\"", "label": "\"PYTORCH\"", "shape": "dot", "size": 10, "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"PyTorch is another platform on which BERTimbau variants can be hosted, alongside HuggingFace Platform and TensorFlow.\""}, {"color": "#a835ef", "description": "\"JAX is a library used to host BERTimbau variants, along with the HuggingFace Platform and PyTorch.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"JAX\"", "label": "\"JAX\"", "shape": "dot", "size": 10, "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"JAX is a library used to host BERTimbau variants, along with the HuggingFace Platform and PyTorch.\""}, {"color": "#d45a10", "description": "\"This is a specific model variant that was fine-tuned for MLM and tested on Portuguese legal documents.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"This is a specific model variant that was fine-tuned for MLM and tested on Portuguese legal documents.\""}, {"color": "#331ec4", "description": "\"This is another model variant subjected to TSDAE domain adaptation, also tested on Portuguese legal documents.\"\u003cSEP\u003e\"This is the name of a variant created for STS evaluation with BERT.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc\u003cSEP\u003echunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"This is another model variant subjected to TSDAE domain adaptation, also tested on Portuguese legal documents.\"\u003cSEP\u003e\"This is the name of a variant created for STS evaluation with BERT.\""}, {"color": "#fdfe51", "description": "\"TSDAE stands for Transformer-based Sequential Denoising Auto-Encoder, an unsupervised sentence embedding approach.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE\"", "label": "\"TSDAE\"", "shape": "dot", "size": 10, "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"TSDAE stands for Transformer-based Sequential Denoising Auto-Encoder, an unsupervised sentence embedding approach.\""}, {"color": "#9f2110", "description": "\"STS Evaluation refers to a regression task that determines how similar two text segments are on a numeric scale, ranging from 1 to 5.\"\u003cSEP\u003e\"STS is a task that evaluates the similarity between two text segments on a numeric scale, ranging from 1 to 5.\"", "entity_type": "\"EVENT\"", "id": "\"STS EVALUATION\"", "label": "\"STS EVALUATION\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc\u003cSEP\u003echunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"STS Evaluation refers to a regression task that determines how similar two text segments are on a numeric scale, ranging from 1 to 5.\"\u003cSEP\u003e\"STS is a task that evaluates the similarity between two text segments on a numeric scale, ranging from 1 to 5.\""}, {"color": "#407aba", "description": "\"NLP is applied to the field of consumer law in Portugal, aiming to analyze and understand legal texts related to consumer rights and protections.\"\u003cSEP\u003e\"The Master\u2019s thesis by N. Cordeiro focused on applying natural language processing techniques to Portuguese consumer law at Instituto Superior T\u00e9cnico, Universidade de Lisboa.\"", "entity_type": "\"APPLICATION\"", "id": "\"NLP APPLIED TO PORTUGUESE CONSUMER LAW\"", "label": "\"NLP APPLIED TO PORTUGUESE CONSUMER LAW\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f\u003cSEP\u003echunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"NLP is applied to the field of consumer law in Portugal, aiming to analyze and understand legal texts related to consumer rights and protections.\"\u003cSEP\u003e\"The Master\u2019s thesis by N. Cordeiro focused on applying natural language processing techniques to Portuguese consumer law at Instituto Superior T\u00e9cnico, Universidade de Lisboa.\""}, {"color": "#912b8d", "description": "\"The master\u2019s thesis focused on applying NLP techniques to Portuguese consumer law at Instituto Superior T\u00e9cnico, Universidade de Lisboa. This clearly relates the organization to this location.\"", "entity_type": "\"UNKNOWN\"", "id": "\"INSTITUTO SUPERIOR T\u00c9CNICO, UNIVERSIDADE DE LISBOA\"", "label": "\"INSTITUTO SUPERIOR T\u00c9CNICO, UNIVERSIDADE DE LISBOA\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The master\u2019s thesis focused on applying NLP techniques to Portuguese consumer law at Instituto Superior T\u00e9cnico, Universidade de Lisboa. This clearly relates the organization to this location.\""}, {"color": "#01fb45", "description": "\"Albertina PT appears to be a specific tool or dataset developed for Portuguese language processing, possibly related to NLP tasks.\"\u003cSEP\u003e\"Albertina PT is a reference to an organization, possibly a partner or client involved in the semantic search system development.\"\u003cSEP\u003e\"Albertina PT is a state-of-the-art BERT model for European Portuguese (PT-PT) developed by Faculdade de Ci\u00eancias da Universidade de Lisboa and Faculdade de Engenharia da Universidade do Porto.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"ALBERTINA PT\"", "label": "\"ALBERTINA PT\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6\u003cSEP\u003echunk-cb545b2bab1e85e24c22c3bd238a556f\u003cSEP\u003echunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"Albertina PT appears to be a specific tool or dataset developed for Portuguese language processing, possibly related to NLP tasks.\"\u003cSEP\u003e\"Albertina PT is a reference to an organization, possibly a partner or client involved in the semantic search system development.\"\u003cSEP\u003e\"Albertina PT is a state-of-the-art BERT model for European Portuguese (PT-PT) developed by Faculdade de Ci\u00eancias da Universidade de Lisboa and Faculdade de Engenharia da Universidade do Porto.\""}, {"color": "#6c8738", "description": "\"DeBERTa is the architecture that served as the starting point for developing the Albertina PT BERT model.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"DEBERTA [16]\"", "label": "\"DEBERTA [16]\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"DeBERTa is the architecture that served as the starting point for developing the Albertina PT BERT model.\""}, {"color": "#941924", "description": "\"Jo \u02dcao Rodrigues et al. is a group of researchers who shared the brand new state-of-the-art model, Albertina.\"\u003cSEP\u003e\"Jo \u02dc\u00e3o Rodrigues et al. is a group of researchers who shared the brand new state-of-the-art model, Albertina.\"", "entity_type": "\"PERSON\"", "id": "\"JO \u02dcAO RODRIGUES ET AL.\"", "label": "\"JO \u02dcAO RODRIGUES ET AL.\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"Jo \u02dcao Rodrigues et al. is a group of researchers who shared the brand new state-of-the-art model, Albertina.\"\u003cSEP\u003e\"Jo \u02dc\u00e3o Rodrigues et al. is a group of researchers who shared the brand new state-of-the-art model, Albertina.\""}, {"color": "#8161b0", "description": "\"FCUL is a university known for its scientific faculty in Lisbon, Portugal.\"\u003cSEP\u003e\"FCUL is one of the institutions that partnered to develop the Albertina PT BERT model.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"FACULDADE DE CI\u00caNCIAS DA UNIVERSIDADE DE LISBOA (FCUL)\"", "label": "\"FACULDADE DE CI\u00caNCIAS DA UNIVERSIDADE DE LISBOA (FCUL)\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6\u003cSEP\u003echunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"FCUL is a university known for its scientific faculty in Lisbon, Portugal.\"\u003cSEP\u003e\"FCUL is one of the institutions that partnered to develop the Albertina PT BERT model.\""}, {"color": "#aee1e9", "description": "\"FEUP is another institution that participated in developing the Albertina PT BERT model, specifically contributing through NLX and Laborat\u00f3rio de Intelig\u00eancia Artificial e Ci\u00eancia de Computadores.\"\u003cSEP\u003e\"FEUP is the engineering faculty of the University of Porto, Portugal.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"FACULDADEDE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)\"", "label": "\"FACULDADEDE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6\u003cSEP\u003echunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"FEUP is another institution that participated in developing the Albertina PT BERT model, specifically contributing through NLX and Laborat\u00f3rio de Intelig\u00eancia Artificial e Ci\u00eancia de Computadores.\"\u003cSEP\u003e\"FEUP is the engineering faculty of the University of Porto, Portugal.\""}, {"color": "#90ec4b", "description": "\"Jo \u02dc\u00e3o Rodrigues and his team shared Albertina PT after its development.\"::", "entity_type": "\"UNKNOWN\"", "id": "\"JO \u02dc\u00c3O RODRIGUES ET AL.\"", "label": "\"JO \u02dc\u00c3O RODRIGUES ET AL.\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"Jo \u02dc\u00e3o Rodrigues and his team shared Albertina PT after its development.\"::"}, {"color": "#ee2db9", "description": "\"A technology used for storing and querying sentence embeddings, facilitating efficient retrieval of relevant documents or passages.\"\u003cSEP\u003e\"ElasticSearch is a search engine used in implementing the semantic search system.\"\u003cSEP\u003e\"ElasticSearch is mentioned as one component within the Semantic Search System, likely providing core search functionalities.\"\u003cSEP\u003e\"Elasticsearch is a distributed open-source search and analytics engine used as a pre-defined constraint in Project IRIS, supporting NO-SQL JSON document-based data storage.\"\u003cSEP\u003e\"Elasticsearch is a popular open-source, distributed search and analytics engine developed by Elastic, Inc., used in information retrieval systems.\"\u003cSEP\u003e\"Elasticsearch is used by the Purely Semantic Search System for searching through embeddings.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"ELASTICSEARCH\"", "label": "\"ELASTICSEARCH\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494\u003cSEP\u003echunk-9c13271d2c9dfd85cce22bb863dd2aa7\u003cSEP\u003echunk-1721070d8e5848c74ff9604584ac59f8\u003cSEP\u003echunk-338cf6d446307fa476c0b025098bcd87\u003cSEP\u003echunk-a24ce5ad9bdfbbf32c946ee404c32fd6\u003cSEP\u003echunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"A technology used for storing and querying sentence embeddings, facilitating efficient retrieval of relevant documents or passages.\"\u003cSEP\u003e\"ElasticSearch is a search engine used in implementing the semantic search system.\"\u003cSEP\u003e\"ElasticSearch is mentioned as one component within the Semantic Search System, likely providing core search functionalities.\"\u003cSEP\u003e\"Elasticsearch is a distributed open-source search and analytics engine used as a pre-defined constraint in Project IRIS, supporting NO-SQL JSON document-based data storage.\"\u003cSEP\u003e\"Elasticsearch is a popular open-source, distributed search and analytics engine developed by Elastic, Inc., used in information retrieval systems.\"\u003cSEP\u003e\"Elasticsearch is used by the Purely Semantic Search System for searching through embeddings.\""}, {"color": "#bde7ae", "description": "\"The Corpus refers to a collection of data processed for use in the Semantic Search System.\"", "entity_type": "\"CONCEPT\"", "id": "\"THE CORPUS\"", "label": "\"THE CORPUS\"", "shape": "dot", "size": 10, "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Corpus refers to a collection of data processed for use in the Semantic Search System.\""}, {"color": "#b0cc52", "description": "\"Architecture refers to the overall design and structure of the Semantic Search System, including different components like ElasticSearch.\"", "entity_type": "\"CONCEPT\"", "id": "\"ARCHITECTURE\"", "label": "\"ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"Architecture refers to the overall design and structure of the Semantic Search System, including different components like ElasticSearch.\""}, {"color": "#426ee6", "description": "\"The Purely Semantic Search System is a component within the Semantic Search System that focuses on semantic-based search functionalities.\"\u003cSEP\u003e\"The Purely Semantic Search System uses only the semantic capabilities of an embedding model, leveraging Elasticsearch and cosine similarity functions to find relevant search results.\"", "entity_type": "\"SYSTEM\"", "id": "\"PURELY SEMANTIC SEARCH SYSTEM\"", "label": "\"PURELY SEMANTIC SEARCH SYSTEM\"", "shape": "dot", "size": 10, "source_id": "chunk-338cf6d446307fa476c0b025098bcd87\u003cSEP\u003echunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Purely Semantic Search System is a component within the Semantic Search System that focuses on semantic-based search functionalities.\"\u003cSEP\u003e\"The Purely Semantic Search System uses only the semantic capabilities of an embedding model, leveraging Elasticsearch and cosine similarity functions to find relevant search results.\""}, {"color": "#347860", "description": "\"BM25 is an information retrieval algorithm used in search engines, combining Term Frequency-Inverse Document Frequency (TF-IDF) to rank document relevance.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"OKAPI BM25 (BM25)\"", "label": "\"OKAPI BM25 (BM25)\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"BM25 is an information retrieval algorithm used in search engines, combining Term Frequency-Inverse Document Frequency (TF-IDF) to rank document relevance.\""}, {"color": "#17dd2d", "description": "\"Project IRIS defines some pre-defined aspects for the implementation of the semantic search system.\"\u003cSEP\u003e\"Project IRIS is a project focused on developing a semantic search system, particularly using Legal-BERTimbau and Elasticsearch technologies.\"\u003cSEP\u003e\"Project IRIS is the larger project that includes this thesis and various other tasks, providing an environment for developing soft and hard skills.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"PROJECT IRIS\"", "label": "\"PROJECT IRIS\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6\u003cSEP\u003echunk-9c13271d2c9dfd85cce22bb863dd2aa7\u003cSEP\u003echunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Project IRIS defines some pre-defined aspects for the implementation of the semantic search system.\"\u003cSEP\u003e\"Project IRIS is a project focused on developing a semantic search system, particularly using Legal-BERTimbau and Elasticsearch technologies.\"\u003cSEP\u003e\"Project IRIS is the larger project that includes this thesis and various other tasks, providing an environment for developing soft and hard skills.\""}, {"color": "#c30a76", "description": "\"dgsi.pt is a public database containing legal documents that were indexed by ecli-indexer6 as part of the data collection process.\"", "entity_type": "\"LOCATION\"", "id": "\"DGSI.PT\"", "label": "\"DGSI.PT\"", "shape": "dot", "size": 10, "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"dgsi.pt is a public database containing legal documents that were indexed by ecli-indexer6 as part of the data collection process.\""}, {"color": "#2d664e", "description": "\"BM25 is the default search algorithm used by Elasticsearch for document searching, which is mentioned as a constraint in Project IRIS.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BM25 ALGORITHM\"", "label": "\"BM25 ALGORITHM\"", "shape": "dot", "size": 10, "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"BM25 is the default search algorithm used by Elasticsearch for document searching, which is mentioned as a constraint in Project IRIS.\""}, {"color": "#52de26", "description": "\"A measure of similarity between two non-zero vectors by calculating the cosine of the angle between them, often used in text analysis for determining sentence similarity.\"\u003cSEP\u003e\"A measure of similarity between two non-zero vectors defined as the cosine of the angle between them, used in calculating distance in embedding spaces.\"\u003cSEP\u003e\"Cosine Similarity is an alternative search function supported by Elasticsearch, requiring initial mapping of indices to utilize it.\"\u003cSEP\u003e\"Cosine similarity is a measure used by SBERT and other models to calculate the semantic similarity between passages and queries.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"COSINE SIMILARITY\"", "label": "\"COSINE SIMILARITY\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f\u003cSEP\u003echunk-8dc9bc0243bc2842874f8fe5a91e0b07\u003cSEP\u003echunk-abe3791c0e495b4a72310f8a36c50056\u003cSEP\u003echunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"A measure of similarity between two non-zero vectors by calculating the cosine of the angle between them, often used in text analysis for determining sentence similarity.\"\u003cSEP\u003e\"A measure of similarity between two non-zero vectors defined as the cosine of the angle between them, used in calculating distance in embedding spaces.\"\u003cSEP\u003e\"Cosine Similarity is an alternative search function supported by Elasticsearch, requiring initial mapping of indices to utilize it.\"\u003cSEP\u003e\"Cosine similarity is a measure used by SBERT and other models to calculate the semantic similarity between passages and queries.\""}, {"color": "#849d4d", "description": "\"An event involving the creation of queries from document summaries to evaluate a search system\u0027s performance.\"\u003cSEP\u003e\"Automatic Query Generation is an event or section discussing the methods used for generating automatic queries as part of system evaluation.\"", "entity_type": "\"EVENT\"", "id": "\"AUTOMATIC QUERY GENERATION\"", "label": "\"AUTOMATIC QUERY GENERATION\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40\u003cSEP\u003echunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"An event involving the creation of queries from document summaries to evaluate a search system\u0027s performance.\"\u003cSEP\u003e\"Automatic Query Generation is an event or section discussing the methods used for generating automatic queries as part of system evaluation.\""}, {"color": "#aac9f0", "description": "\"Data Processing describes the steps taken on the corpus to make it suitable for the semantic search system.\"", "entity_type": "\"PROCESS\"", "id": "\"DATA PROCESSING\"", "label": "\"DATA PROCESSING\"", "shape": "dot", "size": 10, "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"Data Processing describes the steps taken on the corpus to make it suitable for the semantic search system.\""}, {"color": "#3e4d08", "description": "\"The Purely Semantic Search System uses the cosine similarity function for retrieving relevant embeddings from Elasticsearch.\"", "entity_type": "\"UNKNOWN\"", "id": "\"COSINE SIMILARITY FUNCTION\"", "label": "\"COSINE SIMILARITY FUNCTION\"", "shape": "dot", "size": 10, "source_id": "chunk-338cf6d446307fa476c0b025098bcd87", "title": "\"The Purely Semantic Search System uses the cosine similarity function for retrieving relevant embeddings from Elasticsearch.\""}, {"color": "#6d5b53", "description": "\"The process of calculating cosine similarity between dense vector and query embedding is a key step in determining relevance.\"", "entity_type": "\"EVENT\"", "id": "\"COSINE SIMILARITY CALCULATION\"", "label": "\"COSINE SIMILARITY CALCULATION\"", "shape": "dot", "size": 10, "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"The process of calculating cosine similarity between dense vector and query embedding is a key step in determining relevance.\""}, {"color": "#03f5c0", "description": "\"Transfer Learning is a technique used to fine-tune pre-trained models for specific tasks, allowing the use of BERTimbau for legal domain adaptation.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"TRANSFER LEARNING TECHNIQUE\"", "label": "\"TRANSFER LEARNING TECHNIQUE\"", "shape": "dot", "size": 10, "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"Transfer Learning is a technique used to fine-tune pre-trained models for specific tasks, allowing the use of BERTimbau for legal domain adaptation.\""}, {"color": "#ff04ae", "description": "\"Judges and legal authorities are the primary users of the search systems as described in the document.\"\u003cSEP\u003e\"Judges and legal authorities are the primary users of the search systems as described in the document.\")", "entity_type": "\"PERSON\"", "id": "\"JUDGES AND LEGAL AUTHORITIES\"", "label": "\"JUDGES AND LEGAL AUTHORITIES\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"Judges and legal authorities are the primary users of the search systems as described in the document.\"\u003cSEP\u003e\"Judges and legal authorities are the primary users of the search systems as described in the document.\")"}, {"color": "#862675", "description": "\"Generative Pseudo Labeling is a technique where synthetic labels are generated using models, often used in semi-supervised learning scenarios to augment datasets.\"\u003cSEP\u003e\"Generative Pseudo Labeling is an event or section in the document discussing methods of pseudo labeling for generative tasks.\"", "entity_type": "\"EVENT\"", "id": "\"GENERATIVE PSEUDO LABELING\"", "label": "\"GENERATIVE PSEUDO LABELING\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa\u003cSEP\u003echunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Generative Pseudo Labeling is a technique where synthetic labels are generated using models, often used in semi-supervised learning scenarios to augment datasets.\"\u003cSEP\u003e\"Generative Pseudo Labeling is an event or section in the document discussing methods of pseudo labeling for generative tasks.\""}, {"color": "#9c4466", "description": "\"Chapter 6 is a section within the thesis that provides detailed insights into the project\u2019s evaluation, including language model and search system evaluations.\"", "entity_type": "\"LOCATION\"", "id": "\"CHAPTER 6\"", "label": "\"CHAPTER 6\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"Chapter 6 is a section within the thesis that provides detailed insights into the project\u2019s evaluation, including language model and search system evaluations.\""}, {"color": "#86e666", "description": "\"Language Model Evaluation discusses specific evaluations of language models in detail, including domain adaptation and semantic textual similarity tasks.\"\u003cSEP\u003e\"Language Model Evaluation is a specific phase within system evaluation, focusing on the performance of Legal-BERTimbau variants in tasks such as STS (Semantic Textual Similarity).\"", "entity_type": "\"EVENT\"", "id": "\"LANGUAGE MODEL EVALUATION\"", "label": "\"LANGUAGE MODEL EVALUATION\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f\u003cSEP\u003echunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Language Model Evaluation discusses specific evaluations of language models in detail, including domain adaptation and semantic textual similarity tasks.\"\u003cSEP\u003e\"Language Model Evaluation is a specific phase within system evaluation, focusing on the performance of Legal-BERTimbau variants in tasks such as STS (Semantic Textual Similarity).\""}, {"color": "#6e2644", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MKD and NLI capabilities\"\u003cSEP\u003e\"tsdae-mkd-nli-sts-v1 is one of the variants of a language model used in this research.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"TSDAE-MKD-NLI-STS-V1\"", "label": "\"TSDAE-MKD-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f\u003cSEP\u003echunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MKD and NLI capabilities\"\u003cSEP\u003e\"tsdae-mkd-nli-sts-v1 is one of the variants of a language model used in this research.\""}, {"color": "#3d76ac", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MetaKD and NLI capabilities\"\u003cSEP\u003e\"mlm-gpl-nli-sts-MetaKD-v0 is another variant of a language model used in this research.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"MLM-GPL-NLI-STS-METAKD-V0\"", "label": "\"MLM-GPL-NLI-STS-METAKD-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f\u003cSEP\u003echunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MetaKD and NLI capabilities\"\u003cSEP\u003e\"mlm-gpl-nli-sts-MetaKD-v0 is another variant of a language model used in this research.\""}, {"color": "#a7b140", "description": "\"Another variant of the mlm models with MetaKD and NLI capabilities\"\u003cSEP\u003e\"mlm-gpl-nli-sts-MetaKD-v1 is yet another variant of a language model used in this research.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"MLM-GPL-NLI-STS-METAKD-V1\"", "label": "\"MLM-GPL-NLI-STS-METAKD-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f\u003cSEP\u003echunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Another variant of the mlm models with MetaKD and NLI capabilities\"\u003cSEP\u003e\"mlm-gpl-nli-sts-MetaKD-v1 is yet another variant of a language model used in this research.\""}, {"color": "#a5b798", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities and MetaKD\"\u003cSEP\u003e\"tsdae-gpl-nli-sts-MetaKD-v0 is one of the variants of a language model used in this research.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"TSDAE-GPL-NLI-STS-METAKD-V0\"", "label": "\"TSDAE-GPL-NLI-STS-METAKD-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f\u003cSEP\u003echunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities and MetaKD\"\u003cSEP\u003e\"tsdae-gpl-nli-sts-MetaKD-v0 is one of the variants of a language model used in this research.\""}, {"color": "#2730e5", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL, NLI, and MetaKD capabilities\"\u003cSEP\u003e\"tsdae-gpl-nli-sts-MetaKD-v1 is another variant of a language model used in this research.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"TSDAE-GPL-NLI-STS-METAKD-V1\"", "label": "\"TSDAE-GPL-NLI-STS-METAKD-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f\u003cSEP\u003echunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL, NLI, and MetaKD capabilities\"\u003cSEP\u003e\"tsdae-gpl-nli-sts-MetaKD-v1 is another variant of a language model used in this research.\""}, {"color": "#073752", "description": "\"This technology involves a model from Word2Vec that predicts a target word using its context, specifically focusing on one-word contexts.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WORD2VEC WITH COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE WORD CONTEXT\"", "label": "\"WORD2VEC WITH COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE WORD CONTEXT\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"This technology involves a model from Word2Vec that predicts a target word using its context, specifically focusing on one-word contexts.\""}, {"color": "#efb7db", "description": "\"Another aspect of the Word2Vec CBOW model, this time predicting a target word using multiple surrounding words as context.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WORD2VEC WITH CBOW MODEL BASED ON MULTIPLE WORDS CONTEXT\"", "label": "\"WORD2VEC WITH CBOW MODEL BASED ON MULTIPLE WORDS CONTEXT\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Another aspect of the Word2Vec CBOW model, this time predicting a target word using multiple surrounding words as context.\""}, {"color": "#c635b3", "description": "\"A different model from Word2Vec that predicts contexts given a target word, often based on single or multiple word contexts.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WORD2VEC WITH SKIP-GRAM MODEL\"", "label": "\"WORD2VEC WITH SKIP-GRAM MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"A different model from Word2Vec that predicts contexts given a target word, often based on single or multiple word contexts.\""}, {"color": "#401581", "description": "\"This technology involves a model from Word2Vec that predicts a target word using its context, specifically focusing on one-word contexts.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WORD2VEC WITH COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE-WORD CONTEXT\"", "label": "\"WORD2VEC WITH COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE-WORD CONTEXT\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"This technology involves a model from Word2Vec that predicts a target word using its context, specifically focusing on one-word contexts.\""}, {"color": "#e57d0e", "description": "\"This refers to a type of recurrent neural network where all nodes in one layer are connected to every node in the next layer, used for sequence data processing.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"RECURRENT NETWORK FULLY CONNECTED\"", "label": "\"RECURRENT NETWORK FULLY CONNECTED\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"This refers to a type of recurrent neural network where all nodes in one layer are connected to every node in the next layer, used for sequence data processing.\""}, {"color": "#21168b", "description": "\"The structure diagram of Recurrent Neural Networks (RNNs), illustrating the flow and connections within such networks.\"", "entity_type": "\"EVENT\"", "id": "\"RNN STRUCTURE\"", "label": "\"RNN STRUCTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"The structure diagram of Recurrent Neural Networks (RNNs), illustrating the flow and connections within such networks.\""}, {"color": "#e1f83c", "description": "\"Long Short-Term Memory network\u0027s architecture, a specific type of RNN designed to handle long-term dependencies in sequence data.\"", "entity_type": "\"EVENT\"", "id": "\"LSTM STRUCTURE\"", "label": "\"LSTM STRUCTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Long Short-Term Memory network\u0027s architecture, a specific type of RNN designed to handle long-term dependencies in sequence data.\""}, {"color": "#e2b8d3", "description": "\"A novel model for sequence prediction that relies on self-attention mechanisms instead of recurrent layers, often used in natural language processing tasks.\"\u003cSEP\u003e\"The Transformer Model is a neural network architecture that utilizes Positional Encoding, Multi-Head Attention Layer, and Feed-Forward Neural Network for processing input sequences.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TRANSFORMER MODEL\"", "label": "\"TRANSFORMER MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca\u003cSEP\u003echunk-e27d93fef9843514dd8dbc524160d663", "title": "\"A novel model for sequence prediction that relies on self-attention mechanisms instead of recurrent layers, often used in natural language processing tasks.\"\u003cSEP\u003e\"The Transformer Model is a neural network architecture that utilizes Positional Encoding, Multi-Head Attention Layer, and Feed-Forward Neural Network for processing input sequences.\""}, {"color": "#14418c", "description": "\"Positional Encoding is a method to provide context based on the position of words in a sentence, using sine and cosine functions with different frequencies.\"\u003cSEP\u003e\"Positional Encoding provides context to the model about the position of each word in the sentence, using sine and cosine functions with different frequencies.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"POSITIONAL ENCODING\"", "label": "\"POSITIONAL ENCODING\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663\u003cSEP\u003echunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"Positional Encoding is a method to provide context based on the position of words in a sentence, using sine and cosine functions with different frequencies.\"\u003cSEP\u003e\"Positional Encoding provides context to the model about the position of each word in the sentence, using sine and cosine functions with different frequencies.\""}, {"color": "#9deb5b", "description": "\"The Multi-Head Attention Layer is a component of the Transformer architecture designed for handling long-range dependencies.\"\u003cSEP\u003e\"The Multi-Head Attention Layer uses an attention mechanism to assign weights to vectors of words in a sentence, capturing the importance between words.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"MULTI-HEAD ATTENTION LAYER\"", "label": "\"MULTI-HEAD ATTENTION LAYER\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663\u003cSEP\u003echunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"The Multi-Head Attention Layer is a component of the Transformer architecture designed for handling long-range dependencies.\"\u003cSEP\u003e\"The Multi-Head Attention Layer uses an attention mechanism to assign weights to vectors of words in a sentence, capturing the importance between words.\""}, {"color": "#92a632", "description": "\"A Feed-Forward Neural Network layer in the Transformer model processes inputs sequentially, producing outputs with 512 dimensions.\"\u003cSEP\u003e\"The Feed-Forward Neural Network is part of the Transformer Model and processes the input after it has been encoded by Positional Encoding and Multi-Head Attention Layer.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"FEED-FORWARD NEURAL NETWORK\"", "label": "\"FEED-FORWARD NEURAL NETWORK\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663\u003cSEP\u003echunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"A Feed-Forward Neural Network layer in the Transformer model processes inputs sequentially, producing outputs with 512 dimensions.\"\u003cSEP\u003e\"The Feed-Forward Neural Network is part of the Transformer Model and processes the input after it has been encoded by Positional Encoding and Multi-Head Attention Layer.\""}, {"color": "#8bef49", "description": "\"A generative question answering system, likely involving advanced models capable of generating contextually relevant questions from given documents or data sets.\"\u003cSEP\u003e\"GenQ is an unsupervised domain adaptation method for dense retrieval models developed by the Ubiquitous Knowledge Processing Lab team in October 2021, focusing on query generation from given passages and supporting semantic search systems through synthetic data training.\"\u003cSEP\u003e\"GenQ likely refers to a generative query system or method used for generating questions or text based on given inputs or contexts.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"GENQ\"", "label": "\"GENQ\"", "shape": "dot", "size": 10, "source_id": "chunk-dece8100a2db817f460c5edaaa852208\u003cSEP\u003echunk-86921910d0aba929bef9ae955431a4fa\u003cSEP\u003echunk-4438e56d1dbc938d09784326b42337ca", "title": "\"A generative question answering system, likely involving advanced models capable of generating contextually relevant questions from given documents or data sets.\"\u003cSEP\u003e\"GenQ is an unsupervised domain adaptation method for dense retrieval models developed by the Ubiquitous Knowledge Processing Lab team in October 2021, focusing on query generation from given passages and supporting semantic search systems through synthetic data training.\"\u003cSEP\u003e\"GenQ likely refers to a generative query system or method used for generating questions or text based on given inputs or contexts.\""}, {"color": "#48d1f7", "description": "\"GenQ was developed by the Ubiquitous Knowledge Processing Lab team, showing an affiliation and development relationship.\"", "entity_type": "\"UNKNOWN\"", "id": "\"UBIQUITOUS KNOWLEDGE PROCESSING LAB TEAM\"", "label": "\"UBIQUITOUS KNOWLEDGE PROCESSING LAB TEAM\"", "shape": "dot", "size": 10, "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"GenQ was developed by the Ubiquitous Knowledge Processing Lab team, showing an affiliation and development relationship.\""}, {"color": "#7abefa", "description": "\"T5 is a transformer-based architecture used in training the GenQ model, known for its ability to handle various NLP tasks with large amounts of data.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TEXT-TO-TEXT TRANSFER TRANSFORMER (T5)\"", "label": "\"TEXT-TO-TEXT TRANSFER TRANSFORMER (T5)\"", "shape": "dot", "size": 10, "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"T5 is a transformer-based architecture used in training the GenQ model, known for its ability to handle various NLP tasks with large amounts of data.\""}, {"color": "#6d7ffe", "description": "\"The TSDAE Training Loss is a metric related to the training process of the variant.\"\u003cSEP\u003e\"The TSDAE Training Loss refers to the loss function associated with the TSDAE architecture, likely measuring the difference between original and reconstructed data during training.\"", "entity_type": "\"LOSS FUNCTION\"", "id": "\"TSDAE TRAINING LOSS\"", "label": "\"TSDAE TRAINING LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc\u003cSEP\u003echunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The TSDAE Training Loss is a metric related to the training process of the variant.\"\u003cSEP\u003e\"The TSDAE Training Loss refers to the loss function associated with the TSDAE architecture, likely measuring the difference between original and reconstructed data during training.\""}, {"color": "#1a4054", "description": "\"This figure represents the training loss of the TSDAE variant.\"", "entity_type": "\"LOCATION\"", "id": "\"FIGURE 5.3: TSDAE TRAINING LOSS\"", "label": "\"FIGURE 5.3: TSDAE TRAINING LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"This figure represents the training loss of the TSDAE variant.\""}, {"color": "#4ad11e", "description": "\"A structured approach to evaluate the search system\u0027s performance through query generation and retrieval tests.\"\u003cSEP\u003e\"Evaluation Architecture refers to the overall structure and components involved in evaluating the system\u0027s performance, including various metrics and methods for assessing accuracy and effectiveness.\"", "entity_type": "\"ARCHITECTURE\"", "id": "\"EVALUATION ARCHITECTURE\"", "label": "\"EVALUATION ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40\u003cSEP\u003echunk-86921910d0aba929bef9ae955431a4fa", "title": "\"A structured approach to evaluate the search system\u0027s performance through query generation and retrieval tests.\"\u003cSEP\u003e\"Evaluation Architecture refers to the overall structure and components involved in evaluating the system\u0027s performance, including various metrics and methods for assessing accuracy and effectiveness.\""}, {"color": "#56bce7", "description": "\"A Portuguese language dataset used for evaluating text similarity.\"", "entity_type": "\"DATASET\"", "id": "\"ASSIN DATASET\"", "label": "\"ASSIN DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"A Portuguese language dataset used for evaluating text similarity.\""}, {"color": "#0b1222", "description": "\"The original dataset from which the STSB multi mt dataset is derived.\"", "entity_type": "\"DATASET\"", "id": "\"STSBENCHMARK DATASET\"", "label": "\"STSBENCHMARK DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"The original dataset from which the STSB multi mt dataset is derived.\""}, {"color": "#768aa3", "description": "\"The event is focused on evaluating the best model for the search metric in a search system evaluation.\"", "entity_type": "\"EVENT\"", "id": "\"SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC \u2013 BEST MODEL\"", "label": "\"SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC \u2013 BEST MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"The event is focused on evaluating the best model for the search metric in a search system evaluation.\""}, {"color": "#d15797", "description": "\"This event focuses on assessing the best model for the discovery metric within a search system evaluation.\"", "entity_type": "\"EVENT\"", "id": "\"SEARCH SYSTEM EVALUATION \u2013 DISCOVERY METRIC \u2013 BEST MODEL\"", "label": "\"SEARCH SYSTEM EVALUATION \u2013 DISCOVERY METRIC \u2013 BEST MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"This event focuses on assessing the best model for the discovery metric within a search system evaluation.\""}, {"color": "#289fde", "description": "\"SBERT Spearman correlation and Sentence-BERT are both used for evaluating models\u0027 performance.\"\u003cSEP\u003e\"SBERT Spearman correlation is a metric used for evaluating the performance of models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SBERT SPEARMAN CORRELATION\"", "label": "\"SBERT SPEARMAN CORRELATION\"", "shape": "dot", "size": 10, "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"SBERT Spearman correlation and Sentence-BERT are both used for evaluating models\u0027 performance.\"\u003cSEP\u003e\"SBERT Spearman correlation is a metric used for evaluating the performance of models.\""}, {"color": "#dcaefe", "description": "\"Sentence-BERT (SBERT) is a model used for semantic text similarity evaluation.\"\u003cSEP\u003e\"Sentence-BERT is a specific implementation of SBERT designed for representing entire sentences in an embedding rather than individual words or tokens, and it is more accurate compared to BERT for certain tasks like semantic similarity comparison.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SENTENCE-BERT (SBERT)\"", "label": "\"SENTENCE-BERT (SBERT)\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f\u003cSEP\u003echunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"Sentence-BERT (SBERT) is a model used for semantic text similarity evaluation.\"\u003cSEP\u003e\"Sentence-BERT is a specific implementation of SBERT designed for representing entire sentences in an embedding rather than individual words or tokens, and it is more accurate compared to BERT for certain tasks like semantic similarity comparison.\""}, {"color": "#de9171", "description": "\"Adapted versions of BERTimbau, a Portuguese-specific language model that performed well in the STS task on specific datasets but faced challenges with other multilingual models.\"\u003cSEP\u003e\"BERTimbau variants are different versions of BERT models tailored for specific tasks.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BERTIMBAU VARIANTS\"", "label": "\"BERTIMBAU VARIANTS\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40\u003cSEP\u003echunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"Adapted versions of BERTimbau, a Portuguese-specific language model that performed well in the STS task on specific datasets but faced challenges with other multilingual models.\"\u003cSEP\u003e\"BERTimbau variants are different versions of BERT models tailored for specific tasks.\""}, {"color": "#e054e6", "description": "\"SBERT refers to Sentence-BERT models which are fine-tuned on various datasets to improve sentence embedding quality. This includes custom and model-specific variations like sts-v0 and sts-v1.\"\u003cSEP\u003e\"SBERT variants are language models that performed better than state-of-the-art multilingual models on certain Portuguese datasets.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SBERT VARIANTS\"", "label": "\"SBERT VARIANTS\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40\u003cSEP\u003echunk-de764363085fa944c487f5f0db894d4d", "title": "\"SBERT refers to Sentence-BERT models which are fine-tuned on various datasets to improve sentence embedding quality. This includes custom and model-specific variations like sts-v0 and sts-v1.\"\u003cSEP\u003e\"SBERT variants are language models that performed better than state-of-the-art multilingual models on certain Portuguese datasets.\""}, {"color": "#c6177e", "description": "\"STJ serves as Portugal\u2019s highest judiciary court, also known as the Supreme Court of Justice and plays a crucial role in making well-informed, lawful, and ethical decisions that have significant impact on legal cases.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SUPREMO TRIBUNAL DE JUSTIC \u00b8A (STJ)\"", "label": "\"SUPREMO TRIBUNAL DE JUSTIC \u00b8A (STJ)\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"STJ serves as Portugal\u2019s highest judiciary court, also known as the Supreme Court of Justice and plays a crucial role in making well-informed, lawful, and ethical decisions that have significant impact on legal cases.\""}, {"color": "#4c27b9", "description": "\"INESC-ID Lisboa is a research institute involved in the development of the IRIS project, focusing on summarization approaches for court decisions.\"\u003cSEP\u003e\"INESC-ID Lisboa is involved in developing the IRIS project for STJ, which aims to develop summarization approaches for court decisions.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"INESC-ID LISBOA\"", "label": "\"INESC-ID LISBOA\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288\u003cSEP\u003echunk-ae090822f3d4769354cc463665e2df89", "title": "\"INESC-ID Lisboa is a research institute involved in the development of the IRIS project, focusing on summarization approaches for court decisions.\"\u003cSEP\u003e\"INESC-ID Lisboa is involved in developing the IRIS project for STJ, which aims to develop summarization approaches for court decisions.\""}, {"color": "#1cbfe6", "description": "\"The IR system plays a critical role in assisting STJ\u0027s decision-making by ensuring efficient access to necessary legal information.\"", "entity_type": "\"UNKNOWN\"", "id": "\"INFORMATION RETRIEVAL\"", "label": "\"INFORMATION RETRIEVAL\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"The IR system plays a critical role in assisting STJ\u0027s decision-making by ensuring efficient access to necessary legal information.\""}, {"color": "#fd1108", "description": "\"Semantic techniques are essential for retrieving broader and more relevant documents than traditional lexical methods when searching for specific information.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SEMANTIC TECHNIQUES\"", "label": "\"SEMANTIC TECHNIQUES\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Semantic techniques are essential for retrieving broader and more relevant documents than traditional lexical methods when searching for specific information.\""}, {"color": "#80a65e", "description": "\"Lexical techniques are used for specific word matches but might not retrieve all relevant information needed in complex cases.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEXICAL TECHNIQUES\"", "label": "\"LEXICAL TECHNIQUES\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Lexical techniques are used for specific word matches but might not retrieve all relevant information needed in complex cases.\""}, {"color": "#dcd06f", "description": "\"Term Frequency is a measure used in information retrieval to determine how frequently a word appears in a text relative to other words.\"", "entity_type": "\"CONCEPT\"", "id": "\"TF TERM FREQUENCY\"", "label": "\"TF TERM FREQUENCY\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Term Frequency is a measure used in information retrieval to determine how frequently a word appears in a text relative to other words.\""}, {"color": "#a15dc7", "description": "\"Consistency is crucial for the justice system as it promotes fairness and prevents imbalances through accurate legal application.\"", "entity_type": "\"CONCEPT\"", "id": "\"CONSISTENCY IN APPLYING THE LAW\"", "label": "\"CONSISTENCY IN APPLYING THE LAW\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Consistency is crucial for the justice system as it promotes fairness and prevents imbalances through accurate legal application.\""}, {"color": "#0675c0", "description": "\"Unjust precedents can lead to imbalances and weaken the justice system if not prevented through accurate information retrieval and summarization techniques.\"", "entity_type": "\"EVENT\"", "id": "\"UNJUST PRECEDENTS\"", "label": "\"UNJUST PRECEDENTS\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Unjust precedents can lead to imbalances and weaken the justice system if not prevented through accurate information retrieval and summarization techniques.\""}, {"color": "#8f2fa8", "description": "\"T5 is a transformer model designed for text generation and translation, often used in NLP tasks.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"T5 TEXT-TO-TEXT TRANSFER TRANSFORMER\"", "label": "\"T5 TEXT-TO-TEXT TRANSFER TRANSFORMER\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"T5 is a transformer model designed for text generation and translation, often used in NLP tasks.\""}, {"color": "#b0c07a", "description": "\"SNLI is a dataset for natural language inference tasks where the goal is to determine if the hypothesis logically follows from the premise.\"", "entity_type": "\"EVENT\"", "id": "\"SNLI STANFORD NATURAL LANGUAGE INFERENCE\"", "label": "\"SNLI STANFORD NATURAL LANGUAGE INFERENCE\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"SNLI is a dataset for natural language inference tasks where the goal is to determine if the hypothesis logically follows from the premise.\""}, {"color": "#ab596f", "description": "\"The IRIS project aims to develop summarization methods for legal documents and create representations useful in the court decision process.\"\u003cSEP\u003e\"The IRIS project is aimed at developing summarization approaches for court decisions and creating a representation that can be browsed during the decision-making process.\"", "entity_type": "\"PROJECT\"", "id": "\"IRIS PROJECT\"", "label": "\"IRIS PROJECT\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288\u003cSEP\u003echunk-ae090822f3d4769354cc463665e2df89", "title": "\"The IRIS project aims to develop summarization methods for legal documents and create representations useful in the court decision process.\"\u003cSEP\u003e\"The IRIS project is aimed at developing summarization approaches for court decisions and creating a representation that can be browsed during the decision-making process.\""}, {"color": "#cb694d", "description": "\"The Search System refers to the overall system that processes queries and retrieves relevant documents.\"\u003cSEP\u003e\"The thesis aims to develop a search system that uses semantic strategies for STJ, helping in the court decision process.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SEARCH SYSTEM\"", "label": "\"SEARCH SYSTEM\"", "shape": "dot", "size": 10, "source_id": "chunk-d23192c04e35b99777b833e26dafed9f\u003cSEP\u003echunk-ae090822f3d4769354cc463665e2df89", "title": "\"The Search System refers to the overall system that processes queries and retrieves relevant documents.\"\u003cSEP\u003e\"The thesis aims to develop a search system that uses semantic strategies for STJ, helping in the court decision process.\""}, {"color": "#2f836a", "description": "\"STJ is an organization that may benefit from the search system developed as part of the thesis, intended to assist judges in their work.\"\u003cSEP\u003e\"STJ likely refers to the Superior Tribunal de Justi\u00e7a of Brazil, where the Semantic Search System was developed for.\"\u003cSEP\u003e\"STJ likely refers to the Superior Tribunal de Justi\u00e7a of Brazil, where the Semantic Search System was developed for.\")\u003cSEP\u003e\"STJ likely stands for Supreme Tribunal de Justica, a legal entity involved in the research and development of the Semantic Search System.\"\u003cSEP\u003e\"STJ stands for Supremo Tribunal de Justica, Portugal\u0027s highest court that rendered this decision.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJ\"", "label": "\"STJ\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac\u003cSEP\u003echunk-a8661700ca5b769e561d0c13fe452ae7\u003cSEP\u003echunk-fc9187ad3fd00c96d11f9934e91f7051\u003cSEP\u003echunk-ae090822f3d4769354cc463665e2df89", "title": "\"STJ is an organization that may benefit from the search system developed as part of the thesis, intended to assist judges in their work.\"\u003cSEP\u003e\"STJ likely refers to the Superior Tribunal de Justi\u00e7a of Brazil, where the Semantic Search System was developed for.\"\u003cSEP\u003e\"STJ likely refers to the Superior Tribunal de Justi\u00e7a of Brazil, where the Semantic Search System was developed for.\")\u003cSEP\u003e\"STJ likely stands for Supreme Tribunal de Justica, a legal entity involved in the research and development of the Semantic Search System.\"\u003cSEP\u003e\"STJ stands for Supremo Tribunal de Justica, Portugal\u0027s highest court that rendered this decision.\""}, {"color": "#2c80b3", "description": "\"Information Retrieval is a process of searching for information within documents, typically involving techniques like lexical search and semantic search.\"\u003cSEP\u003e\"Information Retrieval is an indispensable tool for accessing legal resources efficiently and effectively in the court decision process.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"INFORMATION RETRIEVAL (IR)\"", "label": "\"INFORMATION RETRIEVAL (IR)\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494\u003cSEP\u003echunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Information Retrieval is a process of searching for information within documents, typically involving techniques like lexical search and semantic search.\"\u003cSEP\u003e\"Information Retrieval is an indispensable tool for accessing legal resources efficiently and effectively in the court decision process.\""}, {"color": "#28e5e4", "description": "\"Classical approaches refer to traditional information retrieval methods that rely on searching for exact query words within documents.\"", "entity_type": "\"APPROACH\"", "id": "\"CLASSICAL APPROACHES\"", "label": "\"CLASSICAL APPROACHES\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Classical approaches refer to traditional information retrieval methods that rely on searching for exact query words within documents.\""}, {"color": "#07ed48", "description": "\"Recent approaches in Information Retrieval involve using semantic search techniques, which go beyond simple keyword matching to understand context and meaning.\"", "entity_type": "\"APPROACH\"", "id": "\"RECENT APPROACHES\"", "label": "\"RECENT APPROACHES\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Recent approaches in Information Retrieval involve using semantic search techniques, which go beyond simple keyword matching to understand context and meaning.\""}, {"color": "#9ca66c", "description": "\"An Acord\u00e3o is a type of judicial decision or judgment in legal proceedings, which outlines the ruling and reasoning behind the court\u0027s decision.\"", "entity_type": "\"EVENT\"", "id": "\"ACORD\u00c3O\"", "label": "\"ACORD\u00c3O\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"An Acord\u00e3o is a type of judicial decision or judgment in legal proceedings, which outlines the ruling and reasoning behind the court\u0027s decision.\""}, {"color": "#75e05d", "description": "\"Court decisions are legal documents that form a significant part of the IRIS project\u0027s focus.\"", "entity_type": "\"LOCATION\"", "id": "\"COURT DECISIONS\"", "label": "\"COURT DECISIONS\"", "shape": "dot", "size": 10, "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"Court decisions are legal documents that form a significant part of the IRIS project\u0027s focus.\""}, {"color": "#4c0d94", "description": "\"Judges are key users who may use the search system developed as part of the thesis for their decision-making process.\"", "entity_type": "\"PERSON\"", "id": "\"JUDGES\"", "label": "\"JUDGES\"", "shape": "dot", "size": 10, "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"Judges are key users who may use the search system developed as part of the thesis for their decision-making process.\""}, {"color": "#6dabca", "description": "\"Natural Language systems refer to techniques used in information retrieval and semantic text analysis, crucial for accessing legal documents efficiently.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"NATURAL LANGUAGE (NL) SYSTEMS\"", "label": "\"NATURAL LANGUAGE (NL) SYSTEMS\"", "shape": "dot", "size": 10, "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"Natural Language systems refer to techniques used in information retrieval and semantic text analysis, crucial for accessing legal documents efficiently.\""}, {"color": "#f5def1", "description": "\"STS is a concept related to comparing the meaning of texts, important for developing effective search strategies.\"", "entity_type": "\"CONCEPT\"", "id": "\"SEMANTIC TEXTUAL SIMILARITY (STS)\"", "label": "\"SEMANTIC TEXTUAL SIMILARITY (STS)\"", "shape": "dot", "size": 10, "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"STS is a concept related to comparing the meaning of texts, important for developing effective search strategies.\""}, {"color": "#dcd0b8", "description": "\"Training dataset refers to a set of 26952 documents used for training models.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"TRAINING DATASET\"", "label": "\"TRAINING DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"Training dataset refers to a set of 26952 documents used for training models.\""}, {"color": "#340636", "description": "\"Portuguese legal sentences v0 is the name of the dataset published on the HuggingFace platform.\"", "entity_type": "\"LOCATION\"", "id": "\"PORTUGUESE LEGAL SENTENCES V0\"", "label": "\"PORTUGUESE LEGAL SENTENCES V0\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"Portuguese legal sentences v0 is the name of the dataset published on the HuggingFace platform.\""}, {"color": "#d1d769", "description": "\"Entity recognition is a technique that identifies named entities such as people, organizations, and locations in a given text.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"ENTITY RECOGNITION\"", "label": "\"ENTITY RECOGNITION\"", "shape": "dot", "size": 10, "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"Entity recognition is a technique that identifies named entities such as people, organizations, and locations in a given text.\""}, {"color": "#cb3917", "description": "\"Embeddings are numerical representations used in machine learning models to capture semantic and syntactic information from text documents.\"\u003cSEP\u003e\"Embeddings are used to convert textual data into numerical vectors for processing by machine learning models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"EMBEDDINGS\"", "label": "\"EMBEDDINGS\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae\u003cSEP\u003echunk-d23192c04e35b99777b833e26dafed9f", "title": "\"Embeddings are numerical representations used in machine learning models to capture semantic and syntactic information from text documents.\"\u003cSEP\u003e\"Embeddings are used to convert textual data into numerical vectors for processing by machine learning models.\""}, {"color": "#821ec1", "description": "\"Artificial Intelligence refers to advanced technologies and methodologies designed to simulate human intelligence in machines, playing a crucial role in modern society.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"ARTIFICIAL INTELLIGENCE (AI)\"", "label": "\"ARTIFICIAL INTELLIGENCE (AI)\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Artificial Intelligence refers to advanced technologies and methodologies designed to simulate human intelligence in machines, playing a crucial role in modern society.\""}, {"color": "#221cd3", "description": "\"Machine Learning is a subset of AI that focuses on developing algorithms enabling systems to learn from data without being explicitly programmed.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"MACHINE LEARNING (ML)\"", "label": "\"MACHINE LEARNING (ML)\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Machine Learning is a subset of AI that focuses on developing algorithms enabling systems to learn from data without being explicitly programmed.\""}, {"color": "#fa9dee", "description": "\"TF-IDF is a ranking function for document search and information retrieval that evaluates term relevancy within documents relative to a set of documents.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF)\"", "label": "\"TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF)\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"TF-IDF is a ranking function for document search and information retrieval that evaluates term relevancy within documents relative to a set of documents.\""}, {"color": "#67ecd3", "description": "\"A mathematical technique to represent words based on the distributional hypothesis, where similar contexts imply similar meanings.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WORD EMBEDDING\"", "label": "\"WORD EMBEDDING\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"A mathematical technique to represent words based on the distributional hypothesis, where similar contexts imply similar meanings.\""}, {"color": "#99f1fe", "description": "\"Bi-Encoders are a type of semantic search approach that generates sentence embeddings independently for comparison using cosine similarity.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BI-ENCODERS\"", "label": "\"BI-ENCODERS\"", "shape": "dot", "size": 10, "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Bi-Encoders are a type of semantic search approach that generates sentence embeddings independently for comparison using cosine similarity.\""}, {"color": "#bcd0b2", "description": "\"Extractive Summarization is a technique in text summarization where the most important sentences from a document are selected without considering their meaning.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"EXTRACTIVE SUMMARIZATION\"", "label": "\"EXTRACTIVE SUMMARIZATION\"", "shape": "dot", "size": 10, "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Extractive Summarization is a technique in text summarization where the most important sentences from a document are selected without considering their meaning.\""}, {"color": "#cdf0f9", "description": "\"Lexical search is a traditional information retrieval method that searches for exact query words within documents, often using algorithms like Okapi BM25.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"LEXICAL SEARCH (TRADITIONAL)\"", "label": "\"LEXICAL SEARCH (TRADITIONAL)\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Lexical search is a traditional information retrieval method that searches for exact query words within documents, often using algorithms like Okapi BM25.\""}, {"color": "#3b9aa4", "description": "\"A method to evaluate shared information between sets.\"\u003cSEP\u003e\"A method to evaluate the amount of shared information or content between two sets by dividing the intersection size by the union size.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"JACCARD SIMILARITY MEASURE\"", "label": "\"JACCARD SIMILARITY MEASURE\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"A method to evaluate shared information between sets.\"\u003cSEP\u003e\"A method to evaluate the amount of shared information or content between two sets by dividing the intersection size by the union size.\""}, {"color": "#22da11", "description": "\"Refers to searching for documents based on literal word matches, highlighting its limitations in retrieving important passages.\"", "entity_type": "\"CONCEPT\"", "id": "\"LEXICAL SEARCHES\"", "label": "\"LEXICAL SEARCHES\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Refers to searching for documents based on literal word matches, highlighting its limitations in retrieving important passages.\""}, {"color": "#b5d3d6", "description": "\"Sets are used in Jaccard Similarity measure to evaluate shared information.\"", "entity_type": "\"CONCEPT\"", "id": "\"SETS\"", "label": "\"SETS\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Sets are used in Jaccard Similarity measure to evaluate shared information.\""}, {"color": "#b03eda", "description": "\"A neural network model proposed by a Google team led by Tomas Mikolov in 2013 for word embeddings, emphasizing co-occurrence context to infer meaning.\"\u003cSEP\u003e\"Word2Vec is a two-layer neural network that uses context to predict words, based on the proximity of similar meaning words in text corpora.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"WORD2VEC\"", "label": "\"WORD2VEC\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07\u003cSEP\u003echunk-073485a6071a38818c47ff7188ec860b", "title": "\"A neural network model proposed by a Google team led by Tomas Mikolov in 2013 for word embeddings, emphasizing co-occurrence context to infer meaning.\"\u003cSEP\u003e\"Word2Vec is a two-layer neural network that uses context to predict words, based on the proximity of similar meaning words in text corpora.\""}, {"color": "#fdb0a4", "description": "\"Connectivity matrix is a matrix that represents connections or relationships between elements (in this case, sentences) based on cosine similarity scores.\"", "entity_type": "\"CONCEPT\"", "id": "\"CONNECTIVITY MATRIX\"", "label": "\"CONNECTIVITY MATRIX\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Connectivity matrix is a matrix that represents connections or relationships between elements (in this case, sentences) based on cosine similarity scores.\""}, {"color": "#cd55b4", "description": "\"Tomas Mikolov et al. are the creators of Word2Vec, a two-layer neural network that revolves around the idea of context-based word prediction.\"", "entity_type": "\"PERSON\"", "id": "\"TOMAS MIKOLOV ET AL.\"", "label": "\"TOMAS MIKOLOV ET AL.\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"Tomas Mikolov et al. are the creators of Word2Vec, a two-layer neural network that revolves around the idea of context-based word prediction.\""}, {"color": "#ef5ac2", "description": "\"The CBOW (Continuous Bag-of-Words) model is a part of Word2Vec that aims to predict a target word based on surrounding words as input.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"CBOW MODEL\"", "label": "\"CBOW MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The CBOW (Continuous Bag-of-Words) model is a part of Word2Vec that aims to predict a target word based on surrounding words as input.\""}, {"color": "#a0d029", "description": "\"The Skip-Gram model, also part of Word2Vec, takes a word as an input and predicts its context using surrounding words as output.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SKIP-GRAM MODEL\"", "label": "\"SKIP-GRAM MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The Skip-Gram model, also part of Word2Vec, takes a word as an input and predicts its context using surrounding words as output.\""}, {"color": "#9e99ac", "description": "\"The output layer of the Word2Vec model represents the predicted word as a vector of size V, where V is the vocabulary size.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"OUTPUT LAYER OF WORD2VEC\"", "label": "\"OUTPUT LAYER OF WORD2VEC\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The output layer of the Word2Vec model represents the predicted word as a vector of size V, where V is the vocabulary size.\""}, {"color": "#bfe055", "description": "\"qifrequency is a component of the BM25 function.\"", "entity_type": "\"CONCEPT\"", "id": "\"QIFREQUENCY\"", "label": "\"QIFREQUENCY\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"qifrequency is a component of the BM25 function.\""}, {"color": "#9c28ac", "description": "\"BM25 function is an optimization technique for information retrieval.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BM25 FUNCTION\"", "label": "\"BM25 FUNCTION\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"BM25 function is an optimization technique for information retrieval.\""}, {"color": "#c70d9c", "description": "\"k1 is a parameter in the BM25 function.\"", "entity_type": "\"CONCEPT\"", "id": "\"K1\"", "label": "\"K1\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"k1 is a parameter in the BM25 function.\""}, {"color": "#c9877c", "description": "\"bterms are used to optimize the BM25 function.\"", "entity_type": "\"CONCEPT\"", "id": "\"BTERMS\"", "label": "\"BTERMS\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"bterms are used to optimize the BM25 function.\""}, {"color": "#a4b5b4", "description": "\"A query is a term or phrase used in information retrieval processes.\"\u003cSEP\u003e\"The input to a search system, which is recognized for expansion by entity recognition techniques.\"", "entity_type": "\"CONCEPT\"", "id": "\"QUERY\"", "label": "\"QUERY\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07\u003cSEP\u003echunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"A query is a term or phrase used in information retrieval processes.\"\u003cSEP\u003e\"The input to a search system, which is recognized for expansion by entity recognition techniques.\""}, {"color": "#680e41", "description": "\"Exact words are terms used directly in queries for precise matches.\"", "entity_type": "\"CONCEPT\"", "id": "\"EXACT WORD\"", "label": "\"EXACT WORD\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Exact words are terms used directly in queries for precise matches.\""}, {"color": "#9cee99", "description": "\"An advanced approach that focuses on retrieving information based on intrinsic meaning rather than literal word matches.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"ADVANCE AND ROBUST APPROACH\"", "label": "\"ADVANCE AND ROBUST APPROACH\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"An advanced approach that focuses on retrieving information based on intrinsic meaning rather than literal word matches.\""}, {"color": "#ed4815", "description": "\"Intrinsic meaning refers to the deeper understanding of user intention beyond exact word matches.\"", "entity_type": "\"CONCEPT\"", "id": "\"INTRINSIC MEANING\"", "label": "\"INTRINSIC MEANING\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Intrinsic meaning refers to the deeper understanding of user intention beyond exact word matches.\""}, {"color": "#f1e194", "description": "\"The CBOW model uses a two-layer neural network to predict words based on their context, represented by surrounding words.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"CBOW MODEL ARCHITECTURE\"", "label": "\"CBOW MODEL ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The CBOW model uses a two-layer neural network to predict words based on their context, represented by surrounding words.\""}, {"color": "#b4371a", "description": "\"The hidden layer in the Word2Vec model consists of N neurons, which help in predicting words based on their context.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"HIDDEN LAYER OF WORD2VEC\"", "label": "\"HIDDEN LAYER OF WORD2VEC\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The hidden layer in the Word2Vec model consists of N neurons, which help in predicting words based on their context.\""}, {"color": "#dd03a9", "description": "\"The weight matrix W in the Word2Vec model has a size V\u00d7N, where V is the vocabulary size and N is the number of hidden layer neurons.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WEIGHT MATRIX W OF WORD2VEC\"", "label": "\"WEIGHT MATRIX W OF WORD2VEC\"", "shape": "dot", "size": 10, "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The weight matrix W in the Word2Vec model has a size V\u00d7N, where V is the vocabulary size and N is the number of hidden layer neurons.\""}, {"color": "#cfcca3", "description": "\"Ice refers to a specific state of water, which has lower co-occurrence with the word gas compared to solid.\"", "entity_type": "\"PERSON_OR_CONCEPT\"", "id": "\"ICE\"", "label": "\"ICE\"", "shape": "dot", "size": 10, "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Ice refers to a specific state of water, which has lower co-occurrence with the word gas compared to solid.\""}, {"color": "#78156a", "description": "\"Solid is mentioned as the more frequent co-occurring term with ice.\"", "entity_type": "\"CONCEPT\"", "id": "\"SOLID\"", "label": "\"SOLID\"", "shape": "dot", "size": 10, "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Solid is mentioned as the more frequent co-occurring term with ice.\""}, {"color": "#7e825c", "description": "\"Water is a concept closely related to both ice and steam, showing moderate co-occurrence but not strong enough to be considered a high probability pair in terms of GloVe\u0027s loss function.\"", "entity_type": "\"CONCEPT\"", "id": "\"WATER\"", "label": "\"WATER\"", "shape": "dot", "size": 10, "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Water is a concept closely related to both ice and steam, showing moderate co-occurrence but not strong enough to be considered a high probability pair in terms of GloVe\u0027s loss function.\""}, {"color": "#257d9f", "description": "\"Steam refers to a specific state of water, which has higher co-occurrence with the word gas compared to ice.\"", "entity_type": "\"PERSON_OR_CONCEPT\"", "id": "\"STEAM\"", "label": "\"STEAM\"", "shape": "dot", "size": 10, "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Steam refers to a specific state of water, which has higher co-occurrence with the word gas compared to ice.\""}, {"color": "#2e2ade", "description": "\"Steam has higher co-occurrence with gas compared to ice, suggesting a strong relationship in the context of states of matter.\"", "entity_type": "\"UNKNOWN\"", "id": "\"GAS\"", "label": "\"GAS\"", "shape": "dot", "size": 10, "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Steam has higher co-occurrence with gas compared to ice, suggesting a strong relationship in the context of states of matter.\""}, {"color": "#f3decd", "description": "\"Recurrent Neural Network is a type of neural network used for processing sequential data.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"RNN\"", "label": "\"RNN\"", "shape": "dot", "size": 10, "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Recurrent Neural Network is a type of neural network used for processing sequential data.\""}, {"color": "#c3004f", "description": "\"Long Short-Term Memory (LSTM) is a variant of RNN designed to handle long-term dependencies.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LSTM\"", "label": "\"LSTM\"", "shape": "dot", "size": 10, "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Long Short-Term Memory (LSTM) is a variant of RNN designed to handle long-term dependencies.\""}, {"color": "#14fe5d", "description": "\"Backpropagation Through Time is a training process used in RNNs and LSTM networks.\"", "entity_type": "\"EVENT\"", "id": "\"BACKPROPAGATION THROUGH TIME\"", "label": "\"BACKPROPAGATION THROUGH TIME\"", "shape": "dot", "size": 10, "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Backpropagation Through Time is a training process used in RNNs and LSTM networks.\""}, {"color": "#5e233c", "description": "\"Forget Gate is a component in LSTM responsible for deciding which information should be retained or discarded.\"\u003cSEP\u003e\"Forget Gate is a component of the neural network architecture responsible for deciding which information to retain or discard in the cell state.\"", "entity_type": "\"ENTITY\"", "id": "\"FORGET GATE\"", "label": "\"FORGET GATE\"", "shape": "dot", "size": 10, "source_id": "chunk-3595353bc78e782128ef8148dfaf1357\u003cSEP\u003echunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"Forget Gate is a component in LSTM responsible for deciding which information should be retained or discarded.\"\u003cSEP\u003e\"Forget Gate is a component of the neural network architecture responsible for deciding which information to retain or discard in the cell state.\""}, {"color": "#2c28b6", "description": "\"Input Gate is a component in LSTM responsible for adding new information to the cell state.\"", "entity_type": "\"ENTITY\"", "id": "\"INPUT GATE\"", "label": "\"INPUT GATE\"", "shape": "dot", "size": 10, "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Input Gate is a component in LSTM responsible for adding new information to the cell state.\""}, {"color": "#855ef5", "description": "\"Output Gate is a component in LSTM responsible for controlling which part of the cell state should be outputted.\"", "entity_type": "\"ENTITY\"", "id": "\"OUTPUT GATE\"", "label": "\"OUTPUT GATE\"", "shape": "dot", "size": 10, "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Output Gate is a component in LSTM responsible for controlling which part of the cell state should be outputted.\""}, {"color": "#cbb990", "description": "\"\u03c3 is a sigmoid function used in various gates of neural networks, such as Forget Gate, Input Gate, and Output Gate.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"\u03a3\"", "label": "\"\u03a3\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"\u03c3 is a sigmoid function used in various gates of neural networks, such as Forget Gate, Input Gate, and Output Gate.\""}, {"color": "#f17fad", "description": "\"tanh is a hyperbolic tangent activation function used to compute the new cell state candidate vector.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TANH\"", "label": "\"TANH\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"tanh is a hyperbolic tangent activation function used to compute the new cell state candidate vector.\""}, {"color": "#35bdf7", "description": "\"\u03c3t is the output of the Output Gate, deciding what information will be outputted and passed to the next layer.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"\u03a3T\"", "label": "\"\u03a3T\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"\u03c3t is the output of the Output Gate, deciding what information will be outputted and passed to the next layer.\""}, {"color": "#3ccec3", "description": "\"The Multi-Head Attention Layer is a key component of the Transformer architecture designed for efficient long-range dependency handling.\" \"", "entity_type": "\"UNKNOWN\"", "id": "\"TRANSFORMER\"", "label": "\"TRANSFORMER\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"The Multi-Head Attention Layer is a key component of the Transformer architecture designed for efficient long-range dependency handling.\" \""}, {"color": "#41f07a", "description": "\"Attention Mechanism is a key component of Multi-Head Attention Layer, which assigns different weights to vectors for every word in the sentence.\"", "entity_type": "\"CONCEPT\"", "id": "\"ATTENTION MECHANISM\"", "label": "\"ATTENTION MECHANISM\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Attention Mechanism is a key component of Multi-Head Attention Layer, which assigns different weights to vectors for every word in the sentence.\""}, {"color": "#6e317e", "description": "\"Sine and Cosine functions are used in Positional Encoding to provide context based on word positions.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SINE AND COSINE FUNCTIONS\"", "label": "\"SINE AND COSINE FUNCTIONS\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Sine and Cosine functions are used in Positional Encoding to provide context based on word positions.\""}, {"color": "#d0f32a", "description": "\"The Masked Multi-Head Attention layer in the Decoder Block applies a mask to ensure that the model only attends to positions before the current position, using past information for predictions.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MASKED MULTI-HEAD ATTENTION LAYER\"", "label": "\"MASKED MULTI-HEAD ATTENTION LAYER\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"The Masked Multi-Head Attention layer in the Decoder Block applies a mask to ensure that the model only attends to positions before the current position, using past information for predictions.\""}, {"color": "#177f46", "description": "\"The Masked Multi-Head Attention layer is part of the Decoder Block in the Transformer Model, ensuring predictions are made based on past information.\"", "entity_type": "\"UNKNOWN\"", "id": "\"DECODER BLOCK\"", "label": "\"DECODER BLOCK\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"The Masked Multi-Head Attention layer is part of the Decoder Block in the Transformer Model, ensuring predictions are made based on past information.\""}, {"color": "#e77d7c", "description": "\"Vectorial Representations are used in Semantic Search to convert words, paragraphs, or documents into vector spaces for better search quality.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"VECTORIAL REPRESENTATIONS\"", "label": "\"VECTORIAL REPRESENTATIONS\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Vectorial Representations are used in Semantic Search to convert words, paragraphs, or documents into vector spaces for better search quality.\""}, {"color": "#f6b587", "description": "\"Query Embedding is a vectorial representation used in Semantic Search to understand and process user queries in the same space as document embeddings.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"QUERY EMBEDDING\"", "label": "\"QUERY EMBEDDING\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Query Embedding is a vectorial representation used in Semantic Search to understand and process user queries in the same space as document embeddings.\""}, {"color": "#6a9cfa", "description": "\"Sentence Embeddings are vectorial representations of sentences, which are created using Semantic Search methods for better understanding and processing.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SENTENCE EMBEDDINGS\"", "label": "\"SENTENCE EMBEDDINGS\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Sentence Embeddings are vectorial representations of sentences, which are created using Semantic Search methods for better understanding and processing.\""}, {"color": "#383daa", "description": "\"Cross-Encoders are another type of semantic search method that compare sentences simultaneously to output their similarity score between 0 and 1, without generating separate embeddings.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"CROSS-ENCODERS\"", "label": "\"CROSS-ENCODERS\"", "shape": "dot", "size": 10, "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Cross-Encoders are another type of semantic search method that compare sentences simultaneously to output their similarity score between 0 and 1, without generating separate embeddings.\""}, {"color": "#5d2c6f", "description": "\"Abstractive Summarization involves complex models to understand the semantics of the document and create a proper summary, making it more challenging but flexible compared to extractive methods.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"ABSTRACTIVE SUMMARIZATION\"", "label": "\"ABSTRACTIVE SUMMARIZATION\"", "shape": "dot", "size": 10, "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Abstractive Summarization involves complex models to understand the semantics of the document and create a proper summary, making it more challenging but flexible compared to extractive methods.\""}, {"color": "#b8ea5a", "description": "\"Scores indicating the similarity between sentences based on their text content.\"", "entity_type": "\"CONCEPT\"", "id": "\"SENTENCE SIMILARITY SCORES\"", "label": "\"SENTENCE SIMILARITY SCORES\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Scores indicating the similarity between sentences based on their text content.\""}, {"color": "#f6be38", "description": "\"Advanced methods and algorithms in natural language processing, including BERT, which are used to improve text understanding and processing.\"", "entity_type": "\"CONCEPT\"", "id": "\"STATE-OF-THE-ART MODELS AND TECHNIQUES\"", "label": "\"STATE-OF-THE-ART MODELS AND TECHNIQUES\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Advanced methods and algorithms in natural language processing, including BERT, which are used to improve text understanding and processing.\""}, {"color": "#fb4390", "description": "\"PageRank is an algorithm developed by Google to rank web pages in their search engine results. It calculates the importance of nodes in a graph, which is used here for generating LexRank scores of phrases.\"", "entity_type": "\"ALGORITHM\"", "id": "\"PAGERANK METHOD\"", "label": "\"PAGERANK METHOD\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"PageRank is an algorithm developed by Google to rank web pages in their search engine results. It calculates the importance of nodes in a graph, which is used here for generating LexRank scores of phrases.\""}, {"color": "#967181", "description": "\"LexRank score is a measure derived from PageRank that assigns a score to each phrase based on its relevance and connectivity within a document\u2019s sentence graph.\"", "entity_type": "\"CONCEPT\"", "id": "\"LEXRANK SCORE\"", "label": "\"LEXRANK SCORE\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"LexRank score is a measure derived from PageRank that assigns a score to each phrase based on its relevance and connectivity within a document\u2019s sentence graph.\""}, {"color": "#6f14be", "description": "\"A parameter in the PageRank algorithm that ensures convergence by controlling the probability of a random walk continuing at any node.\"", "entity_type": "\"CONCEPT\"", "id": "\"DAMPING FACTOR (D)\"", "label": "\"DAMPING FACTOR (D)\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"A parameter in the PageRank algorithm that ensures convergence by controlling the probability of a random walk continuing at any node.\""}, {"color": "#18f5aa", "description": "\"Values derived from the decomposition of a similarity matrix that can be used to calculate sentence relevance in LexRank scoring.\"", "entity_type": "\"CONCEPT\"", "id": "\"EIGENVALUES\"", "label": "\"EIGENVALUES\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Values derived from the decomposition of a similarity matrix that can be used to calculate sentence relevance in LexRank scoring.\""}, {"color": "#3a6c8d", "description": "\"Bidirectional training, used by BERT, allows the model to understand words based on both their left and right context.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BIDIRECTIONAL TRAINING\"", "label": "\"BIDIRECTIONAL TRAINING\"", "shape": "dot", "size": 10, "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Bidirectional training, used by BERT, allows the model to understand words based on both their left and right context.\""}, {"color": "#8d6da8", "description": "\"Adam Optimizer is a method used in the fine-tuning phase of BERT for training the entire network.\"\u003cSEP\u003e\"Adam Optimizer is used in fine-tuning, playing a role in the training process of the network.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"ADAM OPTIMIZER\"", "label": "\"ADAM OPTIMIZER\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95\u003cSEP\u003echunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Adam Optimizer is a method used in the fine-tuning phase of BERT for training the entire network.\"\u003cSEP\u003e\"Adam Optimizer is used in fine-tuning, playing a role in the training process of the network.\""}, {"color": "#7c786b", "description": "\"Transformer blocks are components of the architecture used in BERT to process input sequences.\"\u003cSEP\u003e\"Transformer blocks are denoted by L and refer to the number of layers in the BERT architecture.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TRANSFORMER BLOCKS\"", "label": "\"TRANSFORMER BLOCKS\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95\u003cSEP\u003echunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Transformer blocks are components of the architecture used in BERT to process input sequences.\"\u003cSEP\u003e\"Transformer blocks are denoted by L and refer to the number of layers in the BERT architecture.\""}, {"color": "#b1bdd3", "description": "\"The Token Embedding Layer assigns a value to each word based on vocabulary IDs during the input embedding process.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TOKEN EMBEDDING LAYER\"", "label": "\"TOKEN EMBEDDING LAYER\"", "shape": "dot", "size": 10, "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"The Token Embedding Layer assigns a value to each word based on vocabulary IDs during the input embedding process.\""}, {"color": "#e788bc", "description": "\"The Segmentation Embedding Layer distinguishes whether a word belongs to sentence A or B during the input embedding process.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SEGMENTATION EMBEDDING LAYER\"", "label": "\"SEGMENTATION EMBEDDING LAYER\"", "shape": "dot", "size": 10, "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"The Segmentation Embedding Layer distinguishes whether a word belongs to sentence A or B during the input embedding process.\""}, {"color": "#6368e2", "description": "\"The Position Embedding Layer indicates the position of each word in a sentence during the input embedding process.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"POSITION EMBEDDING LAYER\"", "label": "\"POSITION EMBEDDING LAYER\"", "shape": "dot", "size": 10, "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"The Position Embedding Layer indicates the position of each word in a sentence during the input embedding process.\""}, {"color": "#01c266", "description": "\"BERT BASE is a reported model size with specific architectural details, including L= 12, H= 768, A= 12 and total parameters = 110M.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BERT BASE\"", "label": "\"BERT BASE\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"BERT BASE is a reported model size with specific architectural details, including L= 12, H= 768, A= 12 and total parameters = 110M.\""}, {"color": "#54c50f", "description": "\"Fine-tuning refers to the process of adding an additional layer after the BERT final layer and training for a few epochs with Adam Optimizer.\"", "entity_type": "\"EVENT\"", "id": "\"FINE-TUNING\"", "label": "\"FINE-TUNING\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Fine-tuning refers to the process of adding an additional layer after the BERT final layer and training for a few epochs with Adam Optimizer.\""}, {"color": "#5fbf26", "description": "\"BERT architecture is described with specific details such as L (Transformer blocks), H (hidden size), A (self-attention heads), and total parameters.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BERT ARCHITECTURE\"", "label": "\"BERT ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"BERT architecture is described with specific details such as L (Transformer blocks), H (hidden size), A (self-attention heads), and total parameters.\""}, {"color": "#b6296d", "description": "\"Hidden size H is a parameter specified in the BERT architecture.\"", "entity_type": "\"UNKNOWN\"", "id": "\"HIDDEN SIZE H\"", "label": "\"HIDDEN SIZE H\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Hidden size H is a parameter specified in the BERT architecture.\""}, {"color": "#ae9849", "description": "\"Nils Reimers is the author of TSDAE, first published on April 14th, 2021.\"", "entity_type": "\"PERSON\"", "id": "\"NILS REIMERS\"", "label": "\"NILS REIMERS\"", "shape": "dot", "size": 10, "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"Nils Reimers is the author of TSDAE, first published on April 14th, 2021.\""}, {"color": "#e46014", "description": "\"MLM is a method for masked language modeling, which is used as a comparison to TSDAE.\"", "entity_type": "\"CONCEPT\"", "id": "\"MLM\"", "label": "\"MLM\"", "shape": "dot", "size": 10, "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"MLM is a method for masked language modeling, which is used as a comparison to TSDAE.\""}, {"color": "#1adb1c", "description": "\"STS task refers to the Semantic Textual Similarity (STS) evaluation for assessing text similarity.\"\u003cSEP\u003e\"The STS (Semantic Textual Similarity) task involves evaluating the similarity between two sentences, which TSDAE aims to improve upon for specific domains.\"\u003cSEP\u003e\"The STS (Semantic Textual Similarity) task involves evaluating the similarity of sentence pairs using various metrics including cosine similarity and negative Manhattan or Euclidean distances.\"\u003cSEP\u003e\"The STS task involves training models to understand sentence relatedness and entailment based on given datasets.\"\u003cSEP\u003e\"The STS task refers to the evaluation of how semantically similar two sentences are, which is a main task within the search system architecture.\"", "entity_type": "\"EVENT\"", "id": "\"STS TASK\"", "label": "\"STS TASK\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2\u003cSEP\u003echunk-de764363085fa944c487f5f0db894d4d\u003cSEP\u003echunk-b6270162d82d1fef624d494a11c5caca\u003cSEP\u003echunk-dc1fa210fa8cf3125e9c46996f5dbd40\u003cSEP\u003echunk-dece8100a2db817f460c5edaaa852208", "title": "\"STS task refers to the Semantic Textual Similarity (STS) evaluation for assessing text similarity.\"\u003cSEP\u003e\"The STS (Semantic Textual Similarity) task involves evaluating the similarity between two sentences, which TSDAE aims to improve upon for specific domains.\"\u003cSEP\u003e\"The STS (Semantic Textual Similarity) task involves evaluating the similarity of sentence pairs using various metrics including cosine similarity and negative Manhattan or Euclidean distances.\"\u003cSEP\u003e\"The STS task involves training models to understand sentence relatedness and entailment based on given datasets.\"\u003cSEP\u003e\"The STS task refers to the evaluation of how semantically similar two sentences are, which is a main task within the search system architecture.\""}, {"color": "#7afd87", "description": "\"The NLI concept involves determining sentence relationships, which can be extended to tasks like STS where similar principles are applied for semantic textual similarity evaluation.\"", "entity_type": "\"UNKNOWN\"", "id": "\"NLI\"", "label": "\"NLI\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The NLI concept involves determining sentence relationships, which can be extended to tasks like STS where similar principles are applied for semantic textual similarity evaluation.\""}, {"color": "#25349e", "description": "\"This refers to a specific dataset used in natural language processing tasks, particularly related to sentence textual similarity (STS) and entailment. It includes translations from English and is unique for its legal domain.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STS DATASETS: ASSIN, ASSIN2 AND STSB MULTI MT PORTUGUESE SUB-DATASET\"", "label": "\"STS DATASETS: ASSIN, ASSIN2 AND STSB MULTI MT PORTUGUESE SUB-DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"This refers to a specific dataset used in natural language processing tasks, particularly related to sentence textual similarity (STS) and entailment. It includes translations from English and is unique for its legal domain.\""}, {"color": "#675ec7", "description": "\"Semantic textual similarity is a concept related to evaluating how similar two pieces of text are semantically.\"", "entity_type": "\"CONCEPT\"", "id": "\"SEMANTIC TEXTUAL SIMILARITY\"", "label": "\"SEMANTIC TEXTUAL SIMILARITY\"", "shape": "dot", "size": 10, "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"Semantic textual similarity is a concept related to evaluating how similar two pieces of text are semantically.\""}, {"color": "#272a73", "description": "\"MS MARCO is a dataset used in the fine-tuning process of T5 for question-answering tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"MS MARCO [26]\"", "label": "\"MS MARCO [26]\"", "shape": "dot", "size": 10, "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"MS MARCO is a dataset used in the fine-tuning process of T5 for question-answering tasks.\""}, {"color": "#c13fcd", "description": "\"T5 refers to the model used in Query Generation, known for its use in generating questions from text summaries.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"T5\"", "label": "\"T5\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"T5 refers to the model used in Query Generation, known for its use in generating questions from text summaries.\""}, {"color": "#ce09e9", "description": "\"A model utilized to generate queries with reduced bias compared to GPT3, though still using some keywords from summaries.\"\u003cSEP\u003e\"T5 model is a general-purpose machine learning model used for text-to-text tasks, trained on extensive datasets.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"T5 MODEL\"", "label": "\"T5 MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f\u003cSEP\u003echunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"A model utilized to generate queries with reduced bias compared to GPT3, though still using some keywords from summaries.\"\u003cSEP\u003e\"T5 model is a general-purpose machine learning model used for text-to-text tasks, trained on extensive datasets.\""}, {"color": "#8e7f4a", "description": "\"Query Generation (GenQ) is an approach used by T5 models to generate multiple questions that a passage may answer, which can be far from ideal due to the model\u0027s general nature.\"", "entity_type": "\"EVENT\"", "id": "\"QUERY GENERATION (GENQ)\"", "label": "\"QUERY GENERATION (GENQ)\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"Query Generation (GenQ) is an approach used by T5 models to generate multiple questions that a passage may answer, which can be far from ideal due to the model\u0027s general nature.\""}, {"color": "#61c5a9", "description": "\"Passages refer to text documents or segments used for training models like T5 and SBERT.\"", "entity_type": "\"LOCATION\"", "id": "\"PASSAGES\"", "label": "\"PASSAGES\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"Passages refer to text documents or segments used for training models like T5 and SBERT.\""}, {"color": "#662ddb", "description": "\"Margin Mean Squared Error Loss is a loss function used in the training of models like Cross-Encoder to identify whether passages are relevant to given queries based on sim(Query, Pos) - sim(Query, Neg).\"", "entity_type": "\"CONCEPT\"", "id": "\"MARGIN MEAN SQUARED ERROR LOSS\"", "label": "\"MARGIN MEAN SQUARED ERROR LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"Margin Mean Squared Error Loss is a loss function used in the training of models like Cross-Encoder to identify whether passages are relevant to given queries based on sim(Query, Pos) - sim(Query, Neg).\""}, {"color": "#77fba0", "description": "\"An advanced language model used for rewriting sentences while maintaining the same meaning, but did not yield satisfactory query results.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"GPT3 MODEL PROVIDED BY OPEN AI\"", "label": "\"GPT3 MODEL PROVIDED BY OPEN AI\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"An advanced language model used for rewriting sentences while maintaining the same meaning, but did not yield satisfactory query results.\""}, {"color": "#8c9e67", "description": "\"Multiple Negatives Ranking (MNR) loss is a technique for fine-tuning dense models such as SBERT with passages and synthetically generated query pairs, based on finding the closest passages in vector space to generate negative samples.\"", "entity_type": "\"CONCEPT\"", "id": "\"MULTIPLE NEGATIVES RANKING (MNR) LOSS\"", "label": "\"MULTIPLE NEGATIVES RANKING (MNR) LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"Multiple Negatives Ranking (MNR) loss is a technique for fine-tuning dense models such as SBERT with passages and synthetically generated query pairs, based on finding the closest passages in vector space to generate negative samples.\""}, {"color": "#c7724a", "description": "\"Reimers and Gurevych are researchers who demonstrated the efficiency of SBERT compared to BERT.\"", "entity_type": "\"PERSON\"", "id": "\"REIMERS AND GUREVYCH\"", "label": "\"REIMERS AND GUREVYCH\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"Reimers and Gurevych are researchers who demonstrated the efficiency of SBERT compared to BERT.\""}, {"color": "#018017", "description": "\"The SNLI dataset is used for fine-tuning SBERT through a softmax classifier approach in natural language inference tasks.\"", "entity_type": "\"UNKNOWN\"", "id": "\"SNLI\"", "label": "\"SNLI\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The SNLI dataset is used for fine-tuning SBERT through a softmax classifier approach in natural language inference tasks.\""}, {"color": "#c7c15c", "description": "\"Multi-Genre NLI is another dataset source used for natural language inference tasks, expanding the scope of the training data.\"", "entity_type": "\"EVENT\"", "id": "\"MULTI-GENRE NLI\"", "label": "\"MULTI-GENRE NLI\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"Multi-Genre NLI is another dataset source used for natural language inference tasks, expanding the scope of the training data.\""}, {"color": "#7e06c4", "description": "\"The Siamese architecture is a model structure used to compare sentence pairs by feeding both sentences into two BERT models with entangled weights.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SIAMESE ARCHITECTURE\"", "label": "\"SIAMESE ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The Siamese architecture is a model structure used to compare sentence pairs by feeding both sentences into two BERT models with entangled weights.\""}, {"color": "#0e198c", "description": "\"A pooling operation transforms token embeddings into a fixed-size vector, crucial for comparing sentence pairs in the siamese architecture.\"", "entity_type": "\"CONCEPT\"", "id": "\"POOLING OPERATION\"", "label": "\"POOLING OPERATION\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"A pooling operation transforms token embeddings into a fixed-size vector, crucial for comparing sentence pairs in the siamese architecture.\""}, {"color": "#757e48", "description": "\"The Softmax loss approach is used to fine-tune SBERT by applying a softmax classifier on top of a siamese network.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SOFTMAX LOSS APPROACH\"", "label": "\"SOFTMAX LOSS APPROACH\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The Softmax loss approach is used to fine-tune SBERT by applying a softmax classifier on top of a siamese network.\""}, {"color": "#60e2d9", "description": "\"MKD is a technique for extending a model\u0027s knowledge across languages, particularly emphasized in the context of Portuguese.\"\u003cSEP\u003e\"MKD is a technique introduced in 2020 to enable multilingual sentence embeddings, involving teacher and student models.\"\u003cSEP\u003e\"Multilingual Knowledge Distillation (MKD) is a technique introduced to address the issue of monolingual pre-trained models, aiming to improve their performance across multiple languages.\"", "entity_type": "\"CONCEPT\"", "id": "\"MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)\"", "label": "\"MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81\u003cSEP\u003echunk-ce4847f54b29367988561206721bdbb7\u003cSEP\u003echunk-493aab369fd44529a838be55e938c506", "title": "\"MKD is a technique for extending a model\u0027s knowledge across languages, particularly emphasized in the context of Portuguese.\"\u003cSEP\u003e\"MKD is a technique introduced in 2020 to enable multilingual sentence embeddings, involving teacher and student models.\"\u003cSEP\u003e\"Multilingual Knowledge Distillation (MKD) is a technique introduced to address the issue of monolingual pre-trained models, aiming to improve their performance across multiple languages.\""}, {"color": "#7b6194", "description": "\"JNLP team is a group that participated in COLIEE 2020 and worked on deep learning for legal processing.\"\u003cSEP\u003e\"The JNLP team participated in the COLIEE 2021 competition and focused on dealing with large articles by conducting text chunking on the training data.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"JNLP TEAM\"", "label": "\"JNLP TEAM\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c\u003cSEP\u003echunk-4178cfa608054c267be41d058b830af4", "title": "\"JNLP team is a group that participated in COLIEE 2020 and worked on deep learning for legal processing.\"\u003cSEP\u003e\"The JNLP team participated in the COLIEE 2021 competition and focused on dealing with large articles by conducting text chunking on the training data.\""}, {"color": "#eec525", "description": "\"The OvGU team achieved top scores on the first task of the COLIEE 2021 competition by combining lexical and semantic techniques, using a two-stage TF-IDF vectorization combined with Sentence-BERT embeddings for Task 3.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"OVGU TEAM\"", "label": "\"OVGU TEAM\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The OvGU team achieved top scores on the first task of the COLIEE 2021 competition by combining lexical and semantic techniques, using a two-stage TF-IDF vectorization combined with Sentence-BERT embeddings for Task 3.\""}, {"color": "#450e6b", "description": "\"The nigam team proposed an approach combining transformer-based and traditional IR techniques in the first task of COLIEE 2021, using SBERT and Sent2Vec for the semantic component and combined scores with BM25.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"NIGAM TEAM\"", "label": "\"NIGAM TEAM\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The nigam team proposed an approach combining transformer-based and traditional IR techniques in the first task of COLIEE 2021, using SBERT and Sent2Vec for the semantic component and combined scores with BM25.\""}, {"color": "#08d782", "description": "\"Generative Pseudo Labeling (GPL) is an improved state-of-the-art technique for domain adaptation of dense models, involving three phases: Query Generation, Negative Mining, and Pseudo Labeling, which helps in identifying relevant passages for a given query.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"GENERATIVE PSEUDO LABELING (GPL)\"", "label": "\"GENERATIVE PSEUDO LABELING (GPL)\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"Generative Pseudo Labeling (GPL) is an improved state-of-the-art technique for domain adaptation of dense models, involving three phases: Query Generation, Negative Mining, and Pseudo Labeling, which helps in identifying relevant passages for a given query.\""}, {"color": "#981ee2", "description": "\"Generative Pseudo Labeling involves three phases: Query Generation, Negative Mining, and Pseudo Labeling, with the latter step using a Cross-Encoder to calculate score margins between positive and negative passages.\"", "entity_type": "\"UNKNOWN\"", "id": "\"PSEUDO LABELING\"", "label": "\"PSEUDO LABELING\"", "shape": "dot", "size": 10, "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"Generative Pseudo Labeling involves three phases: Query Generation, Negative Mining, and Pseudo Labeling, with the latter step using a Cross-Encoder to calculate score margins between positive and negative passages.\""}, {"color": "#f3974b", "description": "\"The STS12-16 datasets are a series of benchmark datasets used for evaluating semantic textual similarity tasks, containing gold labels between 0 and 5.\"", "entity_type": "\"LOCATION\"", "id": "\"STS12-16 DATASETS\"", "label": "\"STS12-16 DATASETS\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The STS12-16 datasets are a series of benchmark datasets used for evaluating semantic textual similarity tasks, containing gold labels between 0 and 5.\""}, {"color": "#fb1516", "description": "\"The STS benchmark (STSb) is another dataset used to evaluate supervised STS systems, consisting of pairs of sentences labeled with entailment, neutral, or contradiction.\"", "entity_type": "\"LOCATION\"", "id": "\"STS BENCHMARK (STSB)\"", "label": "\"STS BENCHMARK (STSB)\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The STS benchmark (STSb) is another dataset used to evaluate supervised STS systems, consisting of pairs of sentences labeled with entailment, neutral, or contradiction.\""}, {"color": "#b97f7f", "description": "\"STSb dataset is a benchmark for evaluating sentence embeddings and other natural language processing models.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STSB DATASET\"", "label": "\"STSB DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"STSb dataset is a benchmark for evaluating sentence embeddings and other natural language processing models.\""}, {"color": "#d5704f", "description": "\"SBERT -STSb-base is another model specifically trained on the STSb benchmark dataset, showing excellent results compared to other models.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SBERT -STSB-BASE\"", "label": "\"SBERT -STSB-BASE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SBERT -STSb-base is another model specifically trained on the STSb benchmark dataset, showing excellent results compared to other models.\""}, {"color": "#35cd43", "description": "\"SRoBERTa-STSb-base is a version of RoBERTa tailored for the STSb dataset and performs competitively in sentence embedding tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SROBERTA-STSB-BASE\"", "label": "\"SROBERTA-STSB-BASE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SRoBERTa-STSb-base is a version of RoBERTa tailored for the STSb dataset and performs competitively in sentence embedding tasks.\""}, {"color": "#57b3fa", "description": "\"InferSent - GloVe is a model trained with GloVe embeddings that shows good performance in sentence embedding tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"INFERSENT - GLOVE\"", "label": "\"INFERSENT - GLOVE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"InferSent - GloVe is a model trained with GloVe embeddings that shows good performance in sentence embedding tasks.\""}, {"color": "#9682d2", "description": "\"Universal Sentence Encoder is another strong performer, known for its comprehensive sentence understanding and embedding capabilities.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"UNIVERSAL SENTENCE ENCODER\"", "label": "\"UNIVERSAL SENTENCE ENCODER\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"Universal Sentence Encoder is another strong performer, known for its comprehensive sentence understanding and embedding capabilities.\""}, {"color": "#ac759f", "description": "\"BERT -STSb-large is a large version of BERT specifically trained on the STS benchmark dataset and performs very well in sentence embedding tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BERT -STSB-LARGE\"", "label": "\"BERT -STSB-LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"BERT -STSb-large is a large version of BERT specifically trained on the STS benchmark dataset and performs very well in sentence embedding tasks.\""}, {"color": "#8ad92b", "description": "\"SBERT -STSb-large is also a large version of SBERT, trained on the STS benchmark dataset for high performance in sentence embedding.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SBERT -STSB-LARGE\"", "label": "\"SBERT -STSB-LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SBERT -STSb-large is also a large version of SBERT, trained on the STS benchmark dataset for high performance in sentence embedding.\""}, {"color": "#ec9f2f", "description": "\"SRoBERTa-STSb-large is a large version of RoBERTa tailored for the STS benchmark dataset and performs well in sentence embedding tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SROBERTA-STSB-LARGE\"", "label": "\"SROBERTA-STSB-LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SRoBERTa-STSb-large is a large version of RoBERTa tailored for the STS benchmark dataset and performs well in sentence embedding tasks.\""}, {"color": "#0bdd58", "description": "\"BERT -NLI-STSb-base is a model trained on both NLI data and the STS benchmark dataset, showing strong performance in sentence embedding.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BERT -NLI-STSB-BASE\"", "label": "\"BERT -NLI-STSB-BASE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"BERT -NLI-STSb-base is a model trained on both NLI data and the STS benchmark dataset, showing strong performance in sentence embedding.\""}, {"color": "#76421e", "description": "\"SBERT -NLI-STSb-base is another model trained on both NLI and STS benchmark datasets for robust sentence embedding tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SBERT -NLI-STSB-BASE\"", "label": "\"SBERT -NLI-STSB-BASE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SBERT -NLI-STSb-base is another model trained on both NLI and STS benchmark datasets for robust sentence embedding tasks.\""}, {"color": "#d207dc", "description": "\"SRoBERTa-NLI-STSb-base is a RoBERTa model trained on both NLI and STS benchmark datasets, performing well in sentence embedding.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SROBERTA-NLI-STSB-BASE\"", "label": "\"SROBERTA-NLI-STSB-BASE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SRoBERTa-NLI-STSb-base is a RoBERTa model trained on both NLI and STS benchmark datasets, performing well in sentence embedding.\""}, {"color": "#4411c2", "description": "\"Neil Reimers is a person who developed MKD, as stated in Subsection 3.1.2.B.\"\u003cSEP\u003e\"Neil Reimers is credited for developing the MKD technique which is central to this work.\"", "entity_type": "\"PERSON\"", "id": "\"NEIL REIMERS\"", "label": "\"NEIL REIMERS\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7\u003cSEP\u003echunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"Neil Reimers is a person who developed MKD, as stated in Subsection 3.1.2.B.\"\u003cSEP\u003e\"Neil Reimers is credited for developing the MKD technique which is central to this work.\""}, {"color": "#458339", "description": "\"Both techniques are part of the broader NLP knowledge transfer methods but serve different purposes. \"", "entity_type": "\"UNKNOWN\"", "id": "\"METADATA KNOWLEDGE DISTILLATION (METAKD)\"", "label": "\"METADATA KNOWLEDGE DISTILLATION (METAKD)\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Both techniques are part of the broader NLP knowledge transfer methods but serve different purposes. \""}, {"color": "#1cb9c5", "description": "\"The Teacher Model M is the pre-trained model that serves as the reference for embedding vectors of parallel sentences.\"", "entity_type": "\"MODEL\"", "id": "\"TEACHER MODEL M\"", "label": "\"TEACHER MODEL M\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"The Teacher Model M is the pre-trained model that serves as the reference for embedding vectors of parallel sentences.\""}, {"color": "#8abe94", "description": "\"The Student Model \u02c6M is the model being trained to produce embeddings similar to those of the teacher model for multilingual sentences.\"", "entity_type": "\"MODEL\"", "id": "\"STUDENT MODEL \u02c6M\"", "label": "\"STUDENT MODEL \u02c6M\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"The Student Model \u02c6M is the model being trained to produce embeddings similar to those of the teacher model for multilingual sentences.\""}, {"color": "#97d553", "description": "\"Multilingual SBERT versions include paraphrase-multilingual-mpnet-base and paraphrase-multilingual-MiniLM-L12, providing embeddings in multiple languages.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MULTILINGUAL SBERT VERSIONS\"", "label": "\"MULTILINGUAL SBERT VERSIONS\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"Multilingual SBERT versions include paraphrase-multilingual-mpnet-base and paraphrase-multilingual-MiniLM-L12, providing embeddings in multiple languages.\""}, {"color": "#1301b1", "description": "\"neuralmind/bert-large-portuguese-cased is a BERT Large variant pre-trained for Portuguese with 24 layers and 335M parameters.\"", "entity_type": "\"MODEL\"", "id": "\"NEURALMIND/BERT-LARGE-PORTUGUESE-CASED\"", "label": "\"NEURALMIND/BERT-LARGE-PORTUGUESE-CASED\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"neuralmind/bert-large-portuguese-cased is a BERT Large variant pre-trained for Portuguese with 24 layers and 335M parameters.\""}, {"color": "#70a758", "description": "\"ClueWeb09-B is another dataset used alongside Robust04 for performance testing and comparison.\"", "entity_type": "\"EVENT\"", "id": "\"CLUEWEB09-B\"", "label": "\"CLUEWEB09-B\"", "shape": "dot", "size": 10, "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"ClueWeb09-B is another dataset used alongside Robust04 for performance testing and comparison.\""}, {"color": "#9a3b53", "description": "\"This task involved case law retrieval and entailment, with a training set of 4415 case law files and approximately 4.9 noticed cases per query case.\"", "entity_type": "\"EVENT\"", "id": "\"TASK 1 OF COLIEE 2021\"", "label": "\"TASK 1 OF COLIEE 2021\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"This task involved case law retrieval and entailment, with a training set of 4415 case law files and approximately 4.9 noticed cases per query case.\""}, {"color": "#c2fcda", "description": "\"T. Nguyen et al. developed the MS MARCO dataset.\"", "entity_type": "\"UNKNOWN\"", "id": "\"T. NGUYEN ET AL.\"", "label": "\"T. NGUYEN ET AL.\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"T. Nguyen et al. developed the MS MARCO dataset.\""}, {"color": "#b3862e", "description": "\"COLIEE 2020 is an event or competition where the JNLP team presented their work.\"", "entity_type": "\"EVENT\"", "id": "\"COLIEE 2020\"", "label": "\"COLIEE 2020\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"COLIEE 2020 is an event or competition where the JNLP team presented their work.\""}, {"color": "#ebdf29", "description": "\"This event refers to Task 3 of the COLIEE 2021 competition focusing on extracting Japanese Civil Code Articles.\"\u003cSEP\u003e\"This task aimed to extract articles from the Japanese Civil Code appropriate for answering legal bar exam questions.\"", "entity_type": "\"EVENT\"", "id": "\"TASK 3 OF COLIEE 2021 - STATUTE LAW RETRIEVAL\"", "label": "\"TASK 3 OF COLIEE 2021 - STATUTE LAW RETRIEVAL\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"This event refers to Task 3 of the COLIEE 2021 competition focusing on extracting Japanese Civil Code Articles.\"\u003cSEP\u003e\"This task aimed to extract articles from the Japanese Civil Code appropriate for answering legal bar exam questions.\""}, {"color": "#89ce74", "description": "\"The COLIEE 2021 edition focused on four specific challenges in the legal domain: case law retrieval, case law entailment, statute law retrieval, and statute law entailment.\"", "entity_type": "\"EVENT\"", "id": "\"COLIEE 2021\"", "label": "\"COLIEE 2021\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The COLIEE 2021 edition focused on four specific challenges in the legal domain: case law retrieval, case law entailment, statute law retrieval, and statute law entailment.\""}, {"color": "#e0a3c1", "description": "\"Mi-Young Kim et al. are researchers who discussed the use of deep learning techniques for legal information retrieval in the COLIEE 2021 competition.\"", "entity_type": "\"PERSON\"", "id": "\"MI-YOUNG KIM ET AL.\"", "label": "\"MI-YOUNG KIM ET AL.\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"Mi-Young Kim et al. are researchers who discussed the use of deep learning techniques for legal information retrieval in the COLIEE 2021 competition.\""}, {"color": "#5244e9", "description": "\"Nuno Cordeiro is a researcher mentioned as part of his master\u2019s thesis work, creating the Legal Semantic Search Engine (LeSSE).\"\u003cSEP\u003e\"Nuno Cordeiro is the author of LeSSE and a key figure in developing systems for Portuguese consumer law.\"", "entity_type": "\"PERSON\"", "id": "\"NUNO CORDEIRO\"", "label": "\"NUNO CORDEIRO\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4\u003cSEP\u003echunk-afd8ce7e1a7d61d34be9bdded6cff755", "title": "\"Nuno Cordeiro is a researcher mentioned as part of his master\u2019s thesis work, creating the Legal Semantic Search Engine (LeSSE).\"\u003cSEP\u003e\"Nuno Cordeiro is the author of LeSSE and a key figure in developing systems for Portuguese consumer law.\""}, {"color": "#7e5d5f", "description": "\"LeSSE is a system created by Nuno Cordeiro that merges common document retrieval techniques with semantic search abilities as part of his master\u2019s thesis work.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LESSE\"", "label": "\"LESSE\"", "shape": "dot", "size": 10, "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"LeSSE is a system created by Nuno Cordeiro that merges common document retrieval techniques with semantic search abilities as part of his master\u2019s thesis work.\""}, {"color": "#451874", "description": "\"Table 3.4 is a reference to a table showing Task 1 results.\"", "entity_type": "\"LOCATION\"", "id": "\"TABLE 3.4\"", "label": "\"TABLE 3.4\"", "shape": "dot", "size": 10, "source_id": "chunk-afd8ce7e1a7d61d34be9bdded6cff755", "title": "\"Table 3.4 is a reference to a table showing Task 1 results.\""}, {"color": "#f5ca68", "description": "\"Table 3.4 references the results of Task 1, indicating a direct relationship between the table and task results.\"", "entity_type": "\"UNKNOWN\"", "id": "\"TASK 1 RESULTS\"", "label": "\"TASK 1 RESULTS\"", "shape": "dot", "size": 10, "source_id": "chunk-afd8ce7e1a7d61d34be9bdded6cff755", "title": "\"Table 3.4 references the results of Task 1, indicating a direct relationship between the table and task results.\""}, {"color": "#fc649e", "description": "\"The Legal-BERTimbau model is the primary subject of Chapter 5, designed to enhance the effectiveness of the semantic search system.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEGAL-BERTIMBAU MODEL\"", "label": "\"LEGAL-BERTIMBAU MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"The Legal-BERTimbau model is the primary subject of Chapter 5, designed to enhance the effectiveness of the semantic search system.\""}, {"color": "#88e60e", "description": "\"ecli-indexer6 is a tool used in the data collection process for Project IRIS to extract documents from dgsi.pt and index them into Elasticsearch.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"ECLI-INDEXER6\"", "label": "\"ECLI-INDEXER6\"", "shape": "dot", "size": 10, "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"ecli-indexer6 is a tool used in the data collection process for Project IRIS to extract documents from dgsi.pt and index them into Elasticsearch.\""}, {"color": "#503bb4", "description": "\"The work presented originates from Project IRIS, which is a more significant project involving multiple members and tasks.\"::", "entity_type": "\"UNKNOWN\"", "id": "\"WORK PRESENTED IN THIS DOCUMENT\"", "label": "\"WORK PRESENTED IN THIS DOCUMENT\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The work presented originates from Project IRIS, which is a more significant project involving multiple members and tasks.\"::"}, {"color": "#bf9928", "description": "\"Developing a prototype of the Semantic Search System was a significant activity in the project.\"", "entity_type": "\"ACTIVITY\"", "id": "\"DEVELOPING A PROTOTYPE\"", "label": "\"DEVELOPING A PROTOTYPE\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Developing a prototype of the Semantic Search System was a significant activity in the project.\""}, {"color": "#6f4213", "description": "\"Relator 1 is mentioned in an indexed document, representing a person involved in legal proceedings.\"", "entity_type": "\"PERSON\"", "id": "\"RELATOR 1\"", "label": "\"RELATOR 1\"", "shape": "dot", "size": 10, "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Relator 1 is mentioned in an indexed document, representing a person involved in legal proceedings.\""}, {"color": "#e25882", "description": "\"The data pre-processing step involves cleaning and splitting the text into sentences, removing HTML tags, unexpected characters, and references to other sections.\"", "entity_type": "\"EVENT\"", "id": "\"DATA PRE-PROCESSING STEP\"", "label": "\"DATA PRE-PROCESSING STEP\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The data pre-processing step involves cleaning and splitting the text into sentences, removing HTML tags, unexpected characters, and references to other sections.\""}, {"color": "#cc3268", "description": "\"Testing dataset refers to a set of 3169 documents used for testing model performance.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"TESTING DATASET\"", "label": "\"TESTING DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"Testing dataset refers to a set of 3169 documents used for testing model performance.\""}, {"color": "#3073e4", "description": "\"Validation dataset refers to a set of 3169 documents utilized for validating the models.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"VALIDATION DATASET\"", "label": "\"VALIDATION DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"Validation dataset refers to a set of 3169 documents utilized for validating the models.\""}, {"color": "#bb8c16", "description": "\"A Bi-Encoder is a solution used for creating embeddings independently to improve search system performance.\"\u003cSEP\u003e\"Bi-Encoder is a technique used to create embeddings for documents independently, allowing for more effective searching of sentences within the text.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BI-ENCODER\"", "label": "\"BI-ENCODER\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7\u003cSEP\u003echunk-486e9fdbc67025b64b42032778600c9c", "title": "\"A Bi-Encoder is a solution used for creating embeddings independently to improve search system performance.\"\u003cSEP\u003e\"Bi-Encoder is a technique used to create embeddings for documents independently, allowing for more effective searching of sentences within the text.\""}, {"color": "#842c6e", "description": "\"Re refers to the defendant in a legal dispute involving an action brought by Autor.\"", "entity_type": "\"PERSON\"", "id": "\"RE\"", "label": "\"RE\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"Re refers to the defendant in a legal dispute involving an action brought by Autor.\""}, {"color": "#64ded6", "description": "\"Autor denotes the plaintiff or claimant in a legal case, who initiates the lawsuit against Re.\"", "entity_type": "\"PERSON\"", "id": "\"AUTOR\"", "label": "\"AUTOR\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"Autor denotes the plaintiff or claimant in a legal case, who initiates the lawsuit against Re.\""}, {"color": "#d86848", "description": "\"DGSI refers to the Directorate-General of Security and Information, which is an entity mentioned in the URL path indicating its origin.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"DGSI\"", "label": "\"DGSI\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"DGSI refers to the Directorate-General of Security and Information, which is an entity mentioned in the URL path indicating its origin.\""}, {"color": "#95416f", "description": "\"Jurisprud\u00eancia refers to the body of case law or legal precedents, indicating the historical and current rulings that influence legal decisions.\"", "entity_type": "\"CONCEPT\"", "id": "\"JURISPRUD\u00caNCIA\"", "label": "\"JURISPRUD\u00caNCIA\"", "shape": "dot", "size": 10, "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"Jurisprud\u00eancia refers to the body of case law or legal precedents, indicating the historical and current rulings that influence legal decisions.\""}, {"color": "#8898b5", "description": "\"The HuggingFace platform is where the stjiris/IRIS sts dataset is publicly available.\"", "entity_type": "\"UNKNOWN\"", "id": "\"STJIRIS/IRIS STS DATASET\"", "label": "\"STJIRIS/IRIS STS DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"The HuggingFace platform is where the stjiris/IRIS sts dataset is publicly available.\""}, {"color": "#18e9a0", "description": "\"Neural Networks are used in creating embeddings that can be adapted to specific domains like legal texts.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"NEURAL NETWORKS\"", "label": "\"NEURAL NETWORKS\"", "shape": "dot", "size": 10, "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"Neural Networks are used in creating embeddings that can be adapted to specific domains like legal texts.\""}, {"color": "#e13e3d", "description": "\"Large pre-trained language models are fundamental in transfer learning and provide learned internal states that can be adapted.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LARGE PRE-TRAINED LANGUAGE MODELS\"", "label": "\"LARGE PRE-TRAINED LANGUAGE MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"Large pre-trained language models are fundamental in transfer learning and provide learned internal states that can be adapted.\""}, {"color": "#d16c81", "description": "\"The training process of Legal-BERTimbau involves combining multiple tasks to generate different model versions.\"", "entity_type": "\"EVENT\"", "id": "\"LEGAL-BERTIMBAU TRAINING\"", "label": "\"LEGAL-BERTIMBAU TRAINING\"", "shape": "dot", "size": 10, "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"The training process of Legal-BERTimbau involves combining multiple tasks to generate different model versions.\""}, {"color": "#98144c", "description": "\"Assin is one of the Portuguese STS datasets used for training.\"", "entity_type": "\"EVENT\"", "id": "\"ASSINS\"", "label": "\"ASSINS\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"Assin is one of the Portuguese STS datasets used for training.\""}, {"color": "#c0d219", "description": "\"Assin2 is another Portuguese STS dataset used for training.\"", "entity_type": "\"EVENT\"", "id": "\"ASSIN2\"", "label": "\"ASSIN2\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"Assin2 is another Portuguese STS dataset used for training.\""}, {"color": "#5febd7", "description": "\"The stsb multi mt Portuguese sub-dataset is one of the datasets used for training.\"", "entity_type": "\"DATASET\"", "id": "\"STSB MULTI MT PORTUGUESE SUB-DATASET\"", "label": "\"STSB MULTI MT PORTUGUESE SUB-DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"The stsb multi mt Portuguese sub-dataset is one of the datasets used for training.\""}, {"color": "#617052", "description": "\"The name of the BERT variant used as a base for creating other variants, specifically for STS tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"RUFIMELO/LEGAL-BERTIMBAU-LARGE\"", "label": "\"RUFIMELO/LEGAL-BERTIMBAU-LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"The name of the BERT variant used as a base for creating other variants, specifically for STS tasks.\""}, {"color": "#b5b09d", "description": "\"Learning rate refers to the step size at each iteration while moving toward a minimum of a loss function during training. Here, it was set to 10\u207b\u2076.\"\u003cSEP\u003e\"The learning rate is a hyperparameter used in the model training process.\"", "entity_type": "\"CONCEPT\"", "id": "\"LEARNING RATE\"", "label": "\"LEARNING RATE\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc\u003cSEP\u003echunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"Learning rate refers to the step size at each iteration while moving toward a minimum of a loss function during training. Here, it was set to 10\u207b\u2076.\"\u003cSEP\u003e\"The learning rate is a hyperparameter used in the model training process.\""}, {"color": "#7f4881", "description": "\"Adam is an optimization algorithm used in the training of the large models.\"\u003cSEP\u003e\"The Adam optimization algorithm was used during the training of the BERT variant.\"\u003cSEP\u003e\"The Adam optimizer is a key part of the training process for the models mentioned.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"ADAM OPTIMIZATION ALGORITHM\"", "label": "\"ADAM OPTIMIZATION ALGORITHM\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc\u003cSEP\u003echunk-ce4847f54b29367988561206721bdbb7\u003cSEP\u003echunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"Adam is an optimization algorithm used in the training of the large models.\"\u003cSEP\u003e\"The Adam optimization algorithm was used during the training of the BERT variant.\"\u003cSEP\u003e\"The Adam optimizer is a key part of the training process for the models mentioned.\""}, {"color": "#6505ef", "description": "\"TSDAE (Target Domain Self-Adversarial Training) is a specific domain adaptation technique that was applied to the Legal-BERTimbau models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE TECHNIQUE\"", "label": "\"TSDAE TECHNIQUE\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"TSDAE (Target Domain Self-Adversarial Training) is a specific domain adaptation technique that was applied to the Legal-BERTimbau models.\""}, {"color": "#bd83bc", "description": "\"Table 6.1.1 presents the AVG loss values for different models evaluated using the MLM task.\"", "entity_type": "\"UNKNOWN\"", "id": "\"AVG LOSS\"", "label": "\"AVG LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Table 6.1.1 presents the AVG loss values for different models evaluated using the MLM task.\""}, {"color": "#7b15b7", "description": "\"Another state-of-the-art model used as a benchmark for STS task performance evaluations.\"\u003cSEP\u003e\"Another technology name, possibly another variant of mpnet base version\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"ALL-MPNET-BASE-V2\"", "label": "\"ALL-MPNET-BASE-V2\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2\u003cSEP\u003echunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Another state-of-the-art model used as a benchmark for STS task performance evaluations.\"\u003cSEP\u003e\"Another technology name, possibly another variant of mpnet base version\""}, {"color": "#e88c61", "description": "\"SentenceTransformer is a library utilized to create SBERT models from pre-trained BERT models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SENTENCETRANSFORMER LIBRARY\"", "label": "\"SENTENCETRANSFORMER LIBRARY\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"SentenceTransformer is a library utilized to create SBERT models from pre-trained BERT models.\""}, {"color": "#79e2f0", "description": "\"Legal-BERTimbau-large is the model being used as the student model, designed for legal purposes in Portuguese.\"\u003cSEP\u003e\"The name of the BERT variant used as a base for creating other variants, specifically for STS tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LEGAL-BERTIMBAU-LARGE\"", "label": "\"LEGAL-BERTIMBAU-LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc\u003cSEP\u003echunk-ce4847f54b29367988561206721bdbb7", "title": "\"Legal-BERTimbau-large is the model being used as the student model, designed for legal purposes in Portuguese.\"\u003cSEP\u003e\"The name of the BERT variant used as a base for creating other variants, specifically for STS tasks.\""}, {"color": "#470686", "description": "\"This is the teacher model used to transfer knowledge to the student model Legal-BERTimbau-large.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SENTENCE-TRANSFORMERS/STSB-ROBERTA-LARGE\"", "label": "\"SENTENCE-TRANSFORMERS/STSB-ROBERTA-LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"This is the teacher model used to transfer knowledge to the student model Legal-BERTimbau-large.\""}, {"color": "#7b10c9", "description": "\"This is a model name indicating it was trained using NLI, STS, and GPL data, with specific hyperparameters such as batch size, epochs, and learning rate.\"\u003cSEP\u003e\"This is a model name, likely associated with a research or development entity within the field of Natural Language Processing.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7\u003cSEP\u003echunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This is a model name indicating it was trained using NLI, STS, and GPL data, with specific hyperparameters such as batch size, epochs, and learning rate.\"\u003cSEP\u003e\"This is a model name, likely associated with a research or development entity within the field of Natural Language Processing.\""}, {"color": "#cfa5ef", "description": "\"The learning rate of 10\u207b\u2076 influences how quickly or slowly the model learns during training by controlling the size of updates to weights.\"", "entity_type": "\"UNKNOWN\"", "id": "\"TRAINING MODEL\"", "label": "\"TRAINING MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"The learning rate of 10\u207b\u2076 influences how quickly or slowly the model learns during training by controlling the size of updates to weights.\""}, {"color": "#1290c8", "description": "\"STJIRIS is a specific organization involved in generating and training models.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS\"", "label": "\"STJIRIS\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"STJIRIS is a specific organization involved in generating and training models.\""}, {"color": "#31280f", "description": "\"Refers to BERT-based models used for fine-tuning on various datasets. These are specifically large and cased versions.\"\u003cSEP\u003e\"The large models are used for training with specific hyperparameters such as batch size, epochs, and learning rate.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LARGE MODELS\"", "label": "\"LARGE MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d\u003cSEP\u003echunk-de764363085fa944c487f5f0db894d4d", "title": "\"Refers to BERT-based models used for fine-tuning on various datasets. These are specifically large and cased versions.\"\u003cSEP\u003e\"The large models are used for training with specific hyperparameters such as batch size, epochs, and learning rate.\""}, {"color": "#67570a", "description": "\"A specific learning rate used in the Adam optimization algorithm during model training.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEARNING RATE 10\u22125\"", "label": "\"LEARNING RATE 10\u22125\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"A specific learning rate used in the Adam optimization algorithm during model training.\""}, {"color": "#53ecc9", "description": "\"HuggingFace is a platform where the STS dataset can be accessed, while \u0027stjiris/IRIS sts\u0027 specifies the particular dataset available on this platform.\"", "entity_type": "\"LOCATION\"", "id": "\"HUGGINGFACE: STJIRIS/IRIS STS\"", "label": "\"HUGGINGFACE: STJIRIS/IRIS STS\"", "shape": "dot", "size": 10, "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"HuggingFace is a platform where the STS dataset can be accessed, while \u0027stjiris/IRIS sts\u0027 specifies the particular dataset available on this platform.\""}, {"color": "#335124", "description": "\"BERTimbau is a research work that discusses fine-tuning techniques for BERT models. It provides context on the STS performance improvement.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BERTIMBAU\u2019S PAPER\"", "label": "\"BERTIMBAU\u2019S PAPER\"", "shape": "dot", "size": 10, "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"BERTimbau is a research work that discusses fine-tuning techniques for BERT models. It provides context on the STS performance improvement.\""}, {"color": "#5f6cfa", "description": "\"NLI is a sub-task in natural language processing where models predict the logical relationship between two sentences.\"", "entity_type": "\"CONCEPT\"", "id": "\"NATURAL LANGUAGE INFERENCE (NLI)\"", "label": "\"NATURAL LANGUAGE INFERENCE (NLI)\"", "shape": "dot", "size": 10, "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"NLI is a sub-task in natural language processing where models predict the logical relationship between two sentences.\""}, {"color": "#88dbcb", "description": "\"These are specific model variants fine-tuned on custom datasets.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SBERT VARIANTS: STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-STS-V1, STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-STS-V1\"", "label": "\"SBERT VARIANTS: STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-STS-V1, STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"These are specific model variants fine-tuned on custom datasets.\""}, {"color": "#6be151", "description": "\"These refer to the current best-performing models in handling multilingual tasks, which were outperformed by SBERT variants on some datasets but not all.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"STATE-OF-THE-ART MULTILINGUAL MODELS\"", "label": "\"STATE-OF-THE-ART MULTILINGUAL MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"These refer to the current best-performing models in handling multilingual tasks, which were outperformed by SBERT variants on some datasets but not all.\""}, {"color": "#a4b9f8", "description": "\"SNLI is a dataset used in Natural Language Inference tasks. It contains sentence pairs and their entailment relationships.\"", "entity_type": "\"LOCATION\"", "id": "\"SNLI DATASET\"", "label": "\"SNLI DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"SNLI is a dataset used in Natural Language Inference tasks. It contains sentence pairs and their entailment relationships.\""}, {"color": "#aacd4f", "description": "\"These are parameters used in the model training process to optimize performance.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEARNING RATE OF 10\u22125, ADAM OPTIMIZATION ALGORITHM\"", "label": "\"LEARNING RATE OF 10\u22125, ADAM OPTIMIZATION ALGORITHM\"", "shape": "dot", "size": 10, "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"These are parameters used in the model training process to optimize performance.\""}, {"color": "#72d9b2", "description": "\"This refers to the batch size used for training the large models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"8 BATCH SIZE\"", "label": "\"8 BATCH SIZE\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This refers to the batch size used for training the large models.\""}, {"color": "#3286af", "description": "\"The number of epochs used for training the large models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"5 EPOCHS\"", "label": "\"5 EPOCHS\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The number of epochs used for training the large models.\""}, {"color": "#712035", "description": "\"These data sets were used for training the large models.\"", "entity_type": "\"DATA SET\"", "id": "\"ASSIM AND ASSIN2 NLI INFORMATION\"", "label": "\"ASSIM AND ASSIN2 NLI INFORMATION\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"These data sets were used for training the large models.\""}, {"color": "#66a8f8", "description": "\"Stjiris is an organization that developed or trained these models.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/", "label": "\"STJIRIS/", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"Stjiris is an organization that developed or trained these models.\""}, {"color": "#d447a4", "description": "\"This is a model name indicating it was trained using NLI and STS data, with specific hyperparameters such as batch size, epochs, and learning rate.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V0\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This is a model name indicating it was trained using NLI and STS data, with specific hyperparameters such as batch size, epochs, and learning rate.\""}, {"color": "#c7bf6e", "description": "\"Another model name, possibly developed by another entity in NLP research.\"\u003cSEP\u003e\"This is a model name indicating it was fine-tuned for the Portuguese Language to generate queries from document summaries.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "label": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7\u003cSEP\u003echunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"Another model name, possibly developed by another entity in NLP research.\"\u003cSEP\u003e\"This is a model name indicating it was fine-tuned for the Portuguese Language to generate queries from document summaries.\""}, {"color": "#aaf129", "description": "\"This is a model name indicating it was trained using NLI and STS data, with specific hyperparameters such as batch size, epochs, and learning rate.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This is a model name indicating it was trained using NLI and STS data, with specific hyperparameters such as batch size, epochs, and learning rate.\""}, {"color": "#0a6e80", "description": "\"This is a model name indicating it was trained using NLI and STS data, with specific hyperparameters such as batch size, epochs, and learning rate.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V1\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This is a model name indicating it was trained using NLI and STS data, with specific hyperparameters such as batch size, epochs, and learning rate.\""}, {"color": "#9b6efc", "description": "\"This is a model name indicating it was trained using NLI and STS data, with specific hyperparameters such as batch size, epochs, and learning rate.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V1\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This is a model name indicating it was trained using NLI and STS data, with specific hyperparameters such as batch size, epochs, and learning rate.\""}, {"color": "#f81cf5", "description": "\"This is a model name indicating it was trained using NLI, STS, and GPL data, with specific hyperparameters such as batch size, epochs, and learning rate.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V0\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This is a model name indicating it was trained using NLI, STS, and GPL data, with specific hyperparameters such as batch size, epochs, and learning rate.\""}, {"color": "#9f6a9d", "description": "\"This is a model name indicating it was trained using NLI, STS, and GPL data, with specific hyperparameters such as batch size, epochs, and learning rate.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V0\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This is a model name indicating it was trained using NLI, STS, and GPL data, with specific hyperparameters such as batch size, epochs, and learning rate.\""}, {"color": "#e3550b", "description": "\"This is a model name indicating it was trained using NLI, STS, and GPL data, with specific hyperparameters such as batch size, epochs, and learning rate.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V1\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"This is a model name indicating it was trained using NLI, STS, and GPL data, with specific hyperparameters such as batch size, epochs, and learning rate.\""}, {"color": "#45572e", "description": "\"This refers to a specific dataset containing TED13 and TED-X transcripts translated into multiple languages.\"", "entity_type": "\"LOCATION\"", "id": "\"TED 2020 \u2013 PARALLEL SENTENCES CORPUS\"", "label": "\"TED 2020 \u2013 PARALLEL SENTENCES CORPUS\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"This refers to a specific dataset containing TED13 and TED-X transcripts translated into multiple languages.\""}, {"color": "#6cbb99", "description": "\"This is another model name, likely developed by the same entity or a similar research group.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V0\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"This is another model name, likely developed by the same entity or a similar research group.\""}, {"color": "#ca8131", "description": "\"Yet another model variant developed as part of this research or project.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V0\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Yet another model variant developed as part of this research or project.\""}, {"color": "#bb01a1", "description": "\"The Query Generation step used a T5 model to generate queries from document summaries.\"", "entity_type": "\"UNKNOWN\"", "id": "\"QUERY GENERATION\"", "label": "\"QUERY GENERATION\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The Query Generation step used a T5 model to generate queries from document summaries.\""}, {"color": "#8e75fa", "description": "\"Multilingual Knowledge Distillation (MKD) is a technique developed by Neil Reimers for training models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MKD\"", "label": "\"MKD\"", "shape": "dot", "size": 10, "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"Multilingual Knowledge Distillation (MKD) is a technique developed by Neil Reimers for training models.\""}, {"color": "#8d01e5", "description": "\"Documents refer to a collection of texts or data sets that share some level of relatedness or entailment centered on the topic of COVID-19.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"DOCUMENTS\"", "label": "\"DOCUMENTS\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"Documents refer to a collection of texts or data sets that share some level of relatedness or entailment centered on the topic of COVID-19.\""}, {"color": "#f61e71", "description": "\"The centroid calculation is an event where the embeddings of related sentences are adjusted towards a central point, representing their collective meaning.\"", "entity_type": "\"EVENT\"", "id": "\"CENTROID CALCULATION\"", "label": "\"CENTROID CALCULATION\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"The centroid calculation is an event where the embeddings of related sentences are adjusted towards a central point, representing their collective meaning.\""}, {"color": "#dbdce8", "description": "\"The batch size is the number of samples that are propagated through the model as one learning step. It was set to 3 sentences per epoch.\"", "entity_type": "\"CONCEPT\"", "id": "\"BATCH SIZE\"", "label": "\"BATCH SIZE\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"The batch size is the number of samples that are propagated through the model as one learning step. It was set to 3 sentences per epoch.\""}, {"color": "#178ea6", "description": "\"This model variant indicates a fine-tuned version of imbau model for STS tasks with performance metrics.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "label": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"This model variant indicates a fine-tuned version of imbau model for STS tasks with performance metrics.\""}, {"color": "#b894e8", "description": "\"BERTimbau large refers to a model variant used in the evaluation, which served as a baseline for comparison.\"\u003cSEP\u003e\"BERTimbau large refers to a specific model in the BERTimbau series that was utilized for evaluating different sentences using cosine similarity.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BERTIMBAU LARGE\"", "label": "\"BERTIMBAU LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2\u003cSEP\u003echunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"BERTimbau large refers to a model variant used in the evaluation, which served as a baseline for comparison.\"\u003cSEP\u003e\"BERTimbau large refers to a specific model in the BERTimbau series that was utilized for evaluating different sentences using cosine similarity.\""}, {"color": "#3e9a31", "description": "\"An ElasticSearch index where sentence embeddings are stored, enabling efficient document retrieval.\"\u003cSEP\u003e\"ElasticSearch index is a type of data storage used to store sentence embeddings created by Legal-BERTimbau and other models.\"", "entity_type": "\"LOCATION\"", "id": "\"ELASTICSEARCH INDEX\"", "label": "\"ELASTICSEARCH INDEX\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40\u003cSEP\u003echunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"An ElasticSearch index where sentence embeddings are stored, enabling efficient document retrieval.\"\u003cSEP\u003e\"ElasticSearch index is a type of data storage used to store sentence embeddings created by Legal-BERTimbau and other models.\""}, {"color": "#60c75f", "description": "\"A technology name, likely referring to a specific model or algorithm used in natural language processing\"\u003cSEP\u003e\"Paraphrase-multilingual-mpnet-base-v2 is another multilingual model used for comparison with Legal-BERTimbau and other models.\"\u003cSEP\u003e\"This is one of the state-of-the-art multilingual models used as a baseline for comparison in STS evaluations.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2\"", "label": "\"PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2\u003cSEP\u003echunk-437fca5e9e86e40c8ca3cf7fc41a8c65\u003cSEP\u003echunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in natural language processing\"\u003cSEP\u003e\"Paraphrase-multilingual-mpnet-base-v2 is another multilingual model used for comparison with Legal-BERTimbau and other models.\"\u003cSEP\u003e\"This is one of the state-of-the-art multilingual models used as a baseline for comparison in STS evaluations.\""}, {"color": "#b01e5d", "description": "\"Subsection 4.2 refers to a specific part of the document where certain splits or datasets were generated.\"", "entity_type": "\"LOCATION\"", "id": "\"SUBSECTION 4.2\"", "label": "\"SUBSECTION 4.2\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Subsection 4.2 refers to a specific part of the document where certain splits or datasets were generated.\""}, {"color": "#927f0f", "description": "\"Table 6.1.1 is a table containing average loss values for different models on the MLM task.\"", "entity_type": "\"LOCATION\"", "id": "\"TABLE 6.1.1\"", "label": "\"TABLE 6.1.1\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Table 6.1.1 is a table containing average loss values for different models on the MLM task.\""}, {"color": "#c2b8f2", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MLM-STS-V0\"", "label": "\"MLM-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks\""}, {"color": "#21ae3b", "description": "\"Another variant of the mlm-sts models\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MLM-STS-V1\"", "label": "\"MLM-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Another variant of the mlm-sts models\""}, {"color": "#674382", "description": "\"A technology name, likely referring to another variant of the previous model with NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MLM-NLI-STS-V0\"", "label": "\"MLM-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to another variant of the previous model with NLI capabilities\""}, {"color": "#8de2fc", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MLM-GPL-NLI-STS-V0\"", "label": "\"MLM-GPL-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities\""}, {"color": "#19104a", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MKD and NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MLM-MKD-NLI-STS-V0\"", "label": "\"MLM-MKD-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MKD and NLI capabilities\""}, {"color": "#3da3ca", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE-STS-V0\"", "label": "\"TSDAE-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks\""}, {"color": "#2e714c", "description": "\"A technology name, likely referring to another variant of the previous model with NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE-NLI-STS-V0\"", "label": "\"TSDAE-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to another variant of the previous model with NLI capabilities\""}, {"color": "#007466", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE-GPL-NLI-STS-V0\"", "label": "\"TSDAE-GPL-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities\""}, {"color": "#b3d7fb", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MKD and NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE-MKD-NLI-STS-V0\"", "label": "\"TSDAE-MKD-NLI-STS-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MKD and NLI capabilities\""}, {"color": "#fb3a48", "description": "\"A technology name, likely referring to another variant of the previous model with NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MLM-NLI-STS-V1\"", "label": "\"MLM-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to another variant of the previous model with NLI capabilities\""}, {"color": "#598996", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MLM-GPL-NLI-STS-V1\"", "label": "\"MLM-GPL-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities\""}, {"color": "#418058", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MKD and NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MLM-MKD-NLI-STS-V1\"", "label": "\"MLM-MKD-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with MKD and NLI capabilities\""}, {"color": "#75fa4f", "description": "\"Another variant of the tsdae-sts models\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE-STS-V1\"", "label": "\"TSDAE-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Another variant of the tsdae-sts models\""}, {"color": "#66eb1a", "description": "\"A technology name, likely referring to another variant of the previous model with NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE-NLI-STS-V1\"", "label": "\"TSDAE-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to another variant of the previous model with NLI capabilities\""}, {"color": "#c06c7c", "description": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE-GPL-NLI-STS-V1\"", "label": "\"TSDAE-GPL-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"A technology name, likely referring to a specific model or algorithm used in STS tasks with GPL and NLI capabilities\""}, {"color": "#4e550a", "description": "\"A dataset composed of different multilingual translations from the original STSbenchmark dataset, used to train and test multilingual models.\"", "entity_type": "\"DATASET\"", "id": "\"STSB MULTI MT DATASET\"", "label": "\"STSB MULTI MT DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"A dataset composed of different multilingual translations from the original STSbenchmark dataset, used to train and test multilingual models.\""}, {"color": "#d90d29", "description": "\"Models specifically designed for handling multiple languages during both training and testing phases.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MULTILINGUAL MODELS\"", "label": "\"MULTILINGUAL MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"Models specifically designed for handling multiple languages during both training and testing phases.\""}, {"color": "#9931e0", "description": "\"A method used to retrieve the most important sentences from document summaries by calculating their centrality.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEXRANK SUMMARIZATION TECHNIQUE\"", "label": "\"LEXRANK SUMMARIZATION TECHNIQUE\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"A method used to retrieve the most important sentences from document summaries by calculating their centrality.\""}, {"color": "#0e127b", "description": "\"Collection of legal documents used for generating embeddings and creating queries.\"", "entity_type": "\"LOCATION\"", "id": "\"LEGAL DOCUMENTS COLLECTION\"", "label": "\"LEGAL DOCUMENTS COLLECTION\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"Collection of legal documents used for generating embeddings and creating queries.\""}, {"color": "#e92add", "description": "\"Top results sizes refer to different result sets used for evaluating the Semantic Search System\u0027s performance, such as Top 1, Top 2, and so forth.\"", "entity_type": "\"CONCEPT\"", "id": "\"TOP RESULTS SIZES\"", "label": "\"TOP RESULTS SIZES\"", "shape": "dot", "size": 10, "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"Top results sizes refer to different result sets used for evaluating the Semantic Search System\u0027s performance, such as Top 1, Top 2, and so forth.\""}, {"color": "#8942bd", "description": "\"BM25 technique is a method for scoring documents in information retrieval systems, which was combined with cosine similarity metric for the search system evaluation.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BM25 TECHNIQUE\"", "label": "\"BM25 TECHNIQUE\"", "shape": "dot", "size": 10, "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"BM25 technique is a method for scoring documents in information retrieval systems, which was combined with cosine similarity metric for the search system evaluation.\""}, {"color": "#28f681", "description": "\"Sentence-transformers/all-mpnet-base-v2 is a multilingual model used as a comparison baseline in the evaluation of the Semantic Search System.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SENTENCE-TRANSFORMERS/ALL-MPNET-BASE-V2\"", "label": "\"SENTENCE-TRANSFORMERS/ALL-MPNET-BASE-V2\"", "shape": "dot", "size": 10, "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"Sentence-transformers/all-mpnet-base-v2 is a multilingual model used as a comparison baseline in the evaluation of the Semantic Search System.\""}, {"color": "#8bcb9c", "description": "\"Cosine similarity metric is a method used to evaluate the similarity between sentences in the context of semantic search.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"COSINE SIMILARITY METRIC\"", "label": "\"COSINE SIMILARITY METRIC\"", "shape": "dot", "size": 10, "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"Cosine similarity metric is a method used to evaluate the similarity between sentences in the context of semantic search.\""}, {"color": "#fc8ffa", "description": "\"These refer to different result sizes evaluated for the Semantic Search System\u0027s performance.\"\u003cSEP\u003e\"Various top result sizes (Top 1 to Top 20) are evaluated in the search system performance, indicating a focus on different levels of document retrieval.\"", "entity_type": "\"EVENT\"", "id": "\"TOP 1, TOP 2, TOP 3, TOP 5, TOP 10, AND TOP 20\"", "label": "\"TOP 1, TOP 2, TOP 3, TOP 5, TOP 10, AND TOP 20\"", "shape": "dot", "size": 10, "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65\u003cSEP\u003echunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"These refer to different result sizes evaluated for the Semantic Search System\u0027s performance.\"\u003cSEP\u003e\"Various top result sizes (Top 1 to Top 20) are evaluated in the search system performance, indicating a focus on different levels of document retrieval.\""}, {"color": "#654cd0", "description": "\"STS (SemEval Task 4) dataset refers to a custom dataset used for fine-tuning models, which was utilized in evaluating Semantic Search System performance.\"", "entity_type": "\"LOCATION\"", "id": "\"STS DATASET\"", "label": "\"STS DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"STS (SemEval Task 4) dataset refers to a custom dataset used for fine-tuning models, which was utilized in evaluating Semantic Search System performance.\""}, {"color": "#a506bb", "description": "\"The process by which the Hybrid Search System improves its understanding and retrieval capabilities.\"", "entity_type": "\"CONCEPT\"", "id": "\"QUERY EXPANSION\"", "label": "\"QUERY EXPANSION\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The process by which the Hybrid Search System improves its understanding and retrieval capabilities.\""}, {"color": "#3c1ded", "description": "\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\"", "entity_type": "\"CONCEPT\"", "id": "\"INTELLIGENCE\"", "label": "\"INTELLIGENCE\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\""}, {"color": "#3c6def", "description": "\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\"", "entity_type": "\"CONCEPT\"", "id": "\"CONTROL\"", "label": "\"CONTROL\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\""}, {"color": "#226a24", "description": "\"Purely Semantic refers to a search system architecture that focuses solely on semantic capabilities without lexical ones, providing specific top results for different metrics.\"\u003cSEP\u003e\"Purely Semantic refers to a search system architecture that focuses solely on semantic capabilities without lexical ones, providing specific top results for different metrics.\")", "entity_type": "\"ORGANIZATION\"", "id": "\"PURELY SEMANTIC\"", "label": "\"PURELY SEMANTIC\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"Purely Semantic refers to a search system architecture that focuses solely on semantic capabilities without lexical ones, providing specific top results for different metrics.\"\u003cSEP\u003e\"Purely Semantic refers to a search system architecture that focuses solely on semantic capabilities without lexical ones, providing specific top results for different metrics.\")"}, {"color": "#5e3e54", "description": "\"The Lexical-First model shows improvement percentages compared to other models in Table 6.4.\"::", "entity_type": "\"UNKNOWN\"", "id": "\"IMPROVEMENT\"", "label": "\"IMPROVEMENT\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The Lexical-First model shows improvement percentages compared to other models in Table 6.4.\"::"}, {"color": "#b4a613", "description": "\"Lexical-First is a search system architecture that prioritizes lexical capabilities before integrating semantic aspects, providing specific top results for different metrics.\"\u003cSEP\u003e\"Lexical-First is a search system architecture that prioritizes lexical capabilities before integrating semantic aspects, providing specific top results for different metrics.\")", "entity_type": "\"ORGANIZATION\"", "id": "\"LEXICAL-FIRST\"", "label": "\"LEXICAL-FIRST\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"Lexical-First is a search system architecture that prioritizes lexical capabilities before integrating semantic aspects, providing specific top results for different metrics.\"\u003cSEP\u003e\"Lexical-First is a search system architecture that prioritizes lexical capabilities before integrating semantic aspects, providing specific top results for different metrics.\")"}, {"color": "#556305", "description": "\"Legal-BERTimbau models are research outputs used to achieve specific scores on semantic search tasks.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEGAL-BERTIMBAU MODELS\"", "label": "\"LEGAL-BERTIMBAU MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"Legal-BERTimbau models are research outputs used to achieve specific scores on semantic search tasks.\""}, {"color": "#62f6cd", "description": "\"These are the performance metrics achieved by Legal-BERTimbau models in semantic search tasks, indicating their effectiveness.\"", "entity_type": "\"EVENT\"", "id": "\"ROUGE-1 SCORE OF 47.92 AND A ROUGE-2 SCORE OF 22.50\"", "label": "\"ROUGE-1 SCORE OF 47.92 AND A ROUGE-2 SCORE OF 22.50\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"These are the performance metrics achieved by Legal-BERTimbau models in semantic search tasks, indicating their effectiveness.\""}, {"color": "#1aa481", "description": "\"This is a location where aspects of the research were applied or tested within the thesis context.\"", "entity_type": "\"LOCATION\"", "id": "\"SUPREMO TRIBUNAL DE JUSTICA\"", "label": "\"SUPREMO TRIBUNAL DE JUSTICA\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"This is a location where aspects of the research were applied or tested within the thesis context.\""}, {"color": "#7a6b6f", "description": "\"This organization hosts an important European conference in the field of AI, accepting a paper from the thesis work.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE\"", "label": "\"EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"This organization hosts an important European conference in the field of AI, accepting a paper from the thesis work.\""}, {"color": "#012997", "description": "\"This is an approach to improve query expansion in Hybrid Search Systems by incorporating structured information into the system\u2019s understanding of queries.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"KNOWLEDGE GRAPH EMBEDDINGS\"", "label": "\"KNOWLEDGE GRAPH EMBEDDINGS\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"This is an approach to improve query expansion in Hybrid Search Systems by incorporating structured information into the system\u2019s understanding of queries.\""}, {"color": "#b5eacf", "description": "\"This is the title of a paper that discusses ethical challenges faced during dataset creation and the importance of legal data filtering, providing context for future research.\"", "entity_type": "\"EVENT\"", "id": "\"PILE OF LAW: LEARNING RESPONSIBLE DATA FILTERING FROM THE LAW AND A 256 GB OPEN-SOURCE LEGAL DATASET\"", "label": "\"PILE OF LAW: LEARNING RESPONSIBLE DATA FILTERING FROM THE LAW AND A 256 GB OPEN-SOURCE LEGAL DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"This is the title of a paper that discusses ethical challenges faced during dataset creation and the importance of legal data filtering, providing context for future research.\""}, {"color": "#6fdbb0", "description": "\"The publication of this model represents a significant advancement for training Portuguese legal domain models specifically for European Portuguese.\"", "entity_type": "\"EVENT\"", "id": "\"ALBERTINA PT-PT\"", "label": "\"ALBERTINA PT-PT\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The publication of this model represents a significant advancement for training Portuguese legal domain models specifically for European Portuguese.\""}, {"color": "#f1eb70", "description": "\"This concept refers to legal decisions and rulings that can be used for training models, highlighting its importance in legal domain adaptation.\"", "entity_type": "\"CONCEPT\"", "id": "\"PORTUGUESE JURISPRUDENCE\"", "label": "\"PORTUGUESE JURISPRUDENCE\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"This concept refers to legal decisions and rulings that can be used for training models, highlighting its importance in legal domain adaptation.\""}, {"color": "#275142", "description": "\"Ethical challenges faced during the development of legal datasets, highlighting potential issues with data quality and security.\"", "entity_type": "\"CONCEPT\"", "id": "\"BIAS, OBSCENE, COPYRIGHTED, AND PRIVATE INFORMATION\"", "label": "\"BIAS, OBSCENE, COPYRIGHTED, AND PRIVATE INFORMATION\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"Ethical challenges faced during the development of legal datasets, highlighting potential issues with data quality and security.\""}, {"color": "#f51dd0", "description": "\"The effort required to manually annotate datasets for training language models, emphasizing the challenges and limitations.\"", "entity_type": "\"CONCEPT\"", "id": "\"MANUAL WORK\"", "label": "\"MANUAL WORK\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The effort required to manually annotate datasets for training language models, emphasizing the challenges and limitations.\""}, {"color": "#b09f9e", "description": "\"Tools or methods used in dataset annotation, indicating the initial steps taken in preparing data for research.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"ASSASSIN AND ASSASSIN2\"", "label": "\"ASSASSIN AND ASSASSIN2\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"Tools or methods used in dataset annotation, indicating the initial steps taken in preparing data for research.\""}, {"color": "#c33929", "description": "\"Active learning is a machine learning technique that enables the system to continuously learn from user\u2019s feedback.\"", "entity_type": "\"CONCEPT\"", "id": "\"ACTIVE LEARNING\"", "label": "\"ACTIVE LEARNING\"", "shape": "dot", "size": 10, "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"Active learning is a machine learning technique that enables the system to continuously learn from user\u2019s feedback.\""}, {"color": "#86a9d7", "description": "\"GLM refers to generative language models, such as GPT-3, used in various improvements and interactions.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"GLM (GENERATIVE LANGUAGE MODEL)\"", "label": "\"GLM (GENERATIVE LANGUAGE MODEL)\"", "shape": "dot", "size": 10, "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"GLM refers to generative language models, such as GPT-3, used in various improvements and interactions.\""}, {"color": "#d97ed1", "description": "\"This event refers to the second task in the 9th International Workshop on Semantic Evaluation focusing on semantic textual similarity.\"", "entity_type": "\"EVENT\"", "id": "\"SEMEVAL-2015 TASK 2: SEMANTIC TEXTUAL SIMILARITY, ENGLISH, SPANISH AND PILOT ON INTERPRETABILITY\"", "label": "\"SEMEVAL-2015 TASK 2: SEMANTIC TEXTUAL SIMILARITY, ENGLISH, SPANISH AND PILOT ON INTERPRETABILITY\"", "shape": "dot", "size": 10, "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"This event refers to the second task in the 9th International Workshop on Semantic Evaluation focusing on semantic textual similarity.\""}, {"color": "#cf43bb", "description": "\"GPT-3 is a specific type of GLM that can be improved through tailored prompts for better model responses.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"GPT-3\"", "label": "\"GPT-3\"", "shape": "dot", "size": 10, "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"GPT-3 is a specific type of GLM that can be improved through tailored prompts for better model responses.\""}, {"color": "#7c979b", "description": "\"SemEval-2016 is an international workshop on semantic evaluation held in San Diego, California in 2016.\"", "entity_type": "\"EVENT\"", "id": "\"SEMEVAL-2016\"", "label": "\"SEMEVAL-2016\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"SemEval-2016 is an international workshop on semantic evaluation held in San Diego, California in 2016.\""}, {"color": "#2116f4", "description": "\"San Diego, California is the location where SemEval-2016 took place.\"", "entity_type": "\"LOCATION\"", "id": "\"SAN DIEGO, CALIFORNIA\"", "label": "\"SAN DIEGO, CALIFORNIA\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"San Diego, California is the location where SemEval-2016 took place.\""}, {"color": "#998d3a", "description": "\"The 2013 shared task focused on semantic textual similarity as part of the SemEval conference in Atlanta, Georgia.\"", "entity_type": "\"EVENT\"", "id": "\"SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY\"", "label": "\"SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2013 shared task focused on semantic textual similarity as part of the SemEval conference in Atlanta, Georgia.\""}, {"color": "#97f7c6", "description": "\"Atlanta, Georgia, USA is where the 2013 shared task of Semantic Textual Similarity took place.\"", "entity_type": "\"LOCATION\"", "id": "\"ATLANTA, GEORGIA, USA\"", "label": "\"ATLANTA, GEORGIA, USA\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"Atlanta, Georgia, USA is where the 2013 shared task of Semantic Textual Similarity took place.\""}, {"color": "#bc9564", "description": "\"SemEval-2012 Task 6 was a pilot study for semantic textual similarity, held at the First Joint Conference on Lexical and Computational Semantics in USA.\"", "entity_type": "\"EVENT\"", "id": "\"SEMEVAL-2012 TASK 6: A PILOT ON SEMANTIC TEXTUAL SIMILARITY\"", "label": "\"SEMEVAL-2012 TASK 6: A PILOT ON SEMANTIC TEXTUAL SIMILARITY\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"SemEval-2012 Task 6 was a pilot study for semantic textual similarity, held at the First Joint Conference on Lexical and Computational Semantics in USA.\""}, {"color": "#454799", "description": "\"The 2012 task was a part of the first joint conference held in USA. This clearly relates the event to this location.\"", "entity_type": "\"UNKNOWN\"", "id": "\"FIRST JOINT CONFERENCE ON LEXICAL AND COMPUTATIONAL SEMANTICS\"", "label": "\"FIRST JOINT CONFERENCE ON LEXICAL AND COMPUTATIONAL SEMANTICS\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2012 task was a part of the first joint conference held in USA. This clearly relates the event to this location.\""}, {"color": "#16c126", "description": "\"The 2014 Computational Processing of the Portuguese Language conference was held in Cham, Switzerland.\"", "entity_type": "\"EVENT\"", "id": "\"COMPUTATIONAL PROCESSING OF THE PORTUGUESE LANGUAGE\"", "label": "\"COMPUTATIONAL PROCESSING OF THE PORTUGUESE LANGUAGE\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2014 Computational Processing of the Portuguese Language conference was held in Cham, Switzerland.\""}, {"color": "#30a638", "description": "\"Cham, Switzerland is the location where the 2014 Computational Processing of the Portuguese Language conference took place.\"", "entity_type": "\"LOCATION\"", "id": "\"CHAM, SWITZERLAND\"", "label": "\"CHAM, SWITZERLAND\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"Cham, Switzerland is the location where the 2014 Computational Processing of the Portuguese Language conference took place.\""}, {"color": "#f09cb8", "description": "\"The 2015 Conference on Empirical Methods in Natural Language Processing featured a large annotated corpus for learning natural language inference in Lisbon, Portugal.\"", "entity_type": "\"EVENT\"", "id": "\"A LARGE ANNOTATED CORPUS FOR LEARNING NATURAL LANGUAGE INFERENCE\"", "label": "\"A LARGE ANNOTATED CORPUS FOR LEARNING NATURAL LANGUAGE INFERENCE\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2015 Conference on Empirical Methods in Natural Language Processing featured a large annotated corpus for learning natural language inference in Lisbon, Portugal.\""}, {"color": "#249473", "description": "\"Lisbon, Portugal is the location where the 2015 Conference on Empirical Methods in Natural Language Processing was held.\"", "entity_type": "\"LOCATION\"", "id": "\"LISBON, PORTUGAL\"", "label": "\"LISBON, PORTUGAL\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"Lisbon, Portugal is the location where the 2015 Conference on Empirical Methods in Natural Language Processing was held.\""}, {"color": "#e4b079", "description": "\"The 2017 SemEval task 1 evaluated semantic textual similarity across multiple languages and was part of the 11th International Workshop on Semantic Evaluation in Vancouver, Canada.\"", "entity_type": "\"EVENT\"", "id": "\"SEMEVAL-2017 TASK 1: SEMANTIC TEXTUAL SIMILARITY MULTILINGUAL AND CROSSLINGUAL FOCUSED EVALUATION\"", "label": "\"SEMEVAL-2017 TASK 1: SEMANTIC TEXTUAL SIMILARITY MULTILINGUAL AND CROSSLINGUAL FOCUSED EVALUATION\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2017 SemEval task 1 evaluated semantic textual similarity across multiple languages and was part of the 11th International Workshop on Semantic Evaluation in Vancouver, Canada.\""}, {"color": "#773444", "description": "\"Vancouver, Canada is where the 2017 SemEval-2017 task took place.\"", "entity_type": "\"LOCATION\"", "id": "\"VANCOUVER, CANADA\"", "label": "\"VANCOUVER, CANADA\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"Vancouver, Canada is where the 2017 SemEval-2017 task took place.\""}, {"color": "#7dd8d2", "description": "\"The 2017 Conference on Empirical Methods in Natural Language Processing featured a study on supervised learning of universal sentence representations using natural language inference data in Copenhagen, Denmark.\"", "entity_type": "\"EVENT\"", "id": "\"SUPERVISED LEARNING OF UNIVERSAL SENTENCE REPRESENTATIONS FROM NATURAL LANGUAGE INFERENCE DATA\"", "label": "\"SUPERVISED LEARNING OF UNIVERSAL SENTENCE REPRESENTATIONS FROM NATURAL LANGUAGE INFERENCE DATA\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2017 Conference on Empirical Methods in Natural Language Processing featured a study on supervised learning of universal sentence representations using natural language inference data in Copenhagen, Denmark.\""}, {"color": "#09c390", "description": "\"Copenhagen, Denmark is the location where the 2017 Conference on Empirical Methods in Natural Language Processing was held.\"", "entity_type": "\"LOCATION\"", "id": "\"COPENHAGEN, DENMARK\"", "label": "\"COPENHAGEN, DENMARK\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"Copenhagen, Denmark is the location where the 2017 Conference on Empirical Methods in Natural Language Processing was held.\""}, {"color": "#b87ad5", "description": "\"The 2019 ACM SIGIR Conference featured a study on deeper text understanding using contextual neural language modeling in Dublin, Ireland.\"", "entity_type": "\"EVENT\"", "id": "\"DEEPER TEXT UNDERSTANDING FOR IR WITH CONTEXTUAL NEURAL LANGUAGE MODELING\"", "label": "\"DEEPER TEXT UNDERSTANDING FOR IR WITH CONTEXTUAL NEURAL LANGUAGE MODELING\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2019 ACM SIGIR Conference featured a study on deeper text understanding using contextual neural language modeling in Dublin, Ireland.\""}, {"color": "#55458c", "description": "\"Dublin, Ireland is the location where the 2019 ACM SIGIR Conference took place.\"", "entity_type": "\"LOCATION\"", "id": "\"DUBLIN, IRELAND\"", "label": "\"DUBLIN, IRELAND\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"Dublin, Ireland is the location where the 2019 ACM SIGIR Conference took place.\""}, {"color": "#7072ff", "description": "\"Callan is an author involved in text understanding for IR with a contextual neural language modeling approach.\"", "entity_type": "\"PERSON\"", "id": "\"CALLAN, J.\"", "label": "\"CALLAN, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Callan is an author involved in text understanding for IR with a contextual neural language modeling approach.\""}, {"color": "#ac30c7", "description": "\"Devlin is the co-author of BERT: Pre-training of deep bidirectional transformers for language understanding.\"", "entity_type": "\"PERSON\"", "id": "\"DEVLIN, J.\"", "label": "\"DEVLIN, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Devlin is the co-author of BERT: Pre-training of deep bidirectional transformers for language understanding.\""}, {"color": "#967011", "description": "\"Erkan is an author who developed Lexrank with Raevad for text summarization.\"", "entity_type": "\"PERSON\"", "id": "\"ERKAN, G.\"", "label": "\"ERKAN, G.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Erkan is an author who developed Lexrank with Raevad for text summarization.\""}, {"color": "#ce765e", "description": "\"FONSECA, E. is involved in the ASSIN 2 shared task with REAL, L.\"\u003cSEP\u003e\"Fonseca is an author involved in the ASSIN project for evaluating semantic similarity and textual inference.\"", "entity_type": "\"PERSON\"", "id": "\"FONSECA, E.\"", "label": "\"FONSECA, E.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2\u003cSEP\u003echunk-2016e530048d82125b70492619ae1cd8", "title": "\"FONSECA, E. is involved in the ASSIN 2 shared task with REAL, L.\"\u003cSEP\u003e\"Fonseca is an author involved in the ASSIN project for evaluating semantic similarity and textual inference.\""}, {"color": "#3f15e3", "description": "\"Kim is one of the authors who developed BM25 and transformer-based legal information extraction and entailment.\"", "entity_type": "\"PERSON\"", "id": "\"KIM, M.\"", "label": "\"KIM, M.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Kim is one of the authors who developed BM25 and transformer-based legal information extraction and entailment.\""}, {"color": "#0b2a6d", "description": "\"OLIVEIRA, H. G. is involved in the ASSIN 2 shared task with REAL, L and E. FONSECA.\"\u003cSEP\u003e\"OLIVEIRE, H. G. is involved in the ASSIN 2 shared task with REAL, L and FONSECA, E.\"", "entity_type": "\"PERSON\"", "id": "\"OLIVEIRA, H. G.\"", "label": "\"OLIVEIRA, H. G.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"OLIVEIRA, H. G. is involved in the ASSIN 2 shared task with REAL, L and E. FONSECA.\"\u003cSEP\u003e\"OLIVEIRE, H. G. is involved in the ASSIN 2 shared task with REAL, L and FONSECA, E.\""}, {"color": "#227413", "description": "\"ICLR 2015 is an event held in San Diego, CA, USA from May 7-9, 2015.\"", "entity_type": "\"EVENT\"", "id": "\"ICLR 2015\"", "label": "\"ICLR 2015\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"ICLR 2015 is an event held in San Diego, CA, USA from May 7-9, 2015.\""}, {"color": "#8d937a", "description": "\"San Diego, CA, USA is the location where ICLR 2015 was held.\"", "entity_type": "\"LOCATION\"", "id": "\"SAN DIEGO, CA, USA\"", "label": "\"SAN DIEGO, CA, USA\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"San Diego, CA, USA is the location where ICLR 2015 was held.\""}, {"color": "#38c130", "description": "\"This is a technology or dataset created by P. May for machine translation evaluation.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MACHINE TRANSLATED MULTILINGUAL STS BENCHMARK DATASET\"", "label": "\"MACHINE TRANSLATED MULTILINGUAL STS BENCHMARK DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"This is a technology or dataset created by P. May for machine translation evaluation.\""}, {"color": "#da6cd1", "description": "\"P. May developed the Machine translated multilingual STS benchmark dataset.\"", "entity_type": "\"PERSON\"", "id": "\"P. MAY\"", "label": "\"P. MAY\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"P. May developed the Machine translated multilingual STS benchmark dataset.\""}, {"color": "#f31d99", "description": "\"Scottsdale, Arizona, USA is the location where ICLR 2013 was held.\"", "entity_type": "\"LOCATION\"", "id": "\"SCOTTSDALE, ARIZONA, USA\"", "label": "\"SCOTTSDALE, ARIZONA, USA\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"Scottsdale, Arizona, USA is the location where ICLR 2013 was held.\""}, {"color": "#a16781", "description": "\"ICLR 2013 took place in Scottsdale, Arizona, USA.\"", "entity_type": "\"UNKNOWN\"", "id": "\"ICLR 2013\"", "label": "\"ICLR 2013\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"ICLR 2013 took place in Scottsdale, Arizona, USA.\""}, {"color": "#a5e7c4", "description": "\"MS MARCO is a technology or dataset created by T. Nguyen et al. for machine reading comprehension evaluation.\"\u003cSEP\u003e\"This is a technology or dataset created by T. Nguyen et al. for machine reading comprehension evaluation.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MS MARCO: A HUMAN GENERATED MACHINE READING COMPREHENSION DATASET\"", "label": "\"MS MARCO: A HUMAN GENERATED MACHINE READING COMPREHENSION DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"MS MARCO is a technology or dataset created by T. Nguyen et al. for machine reading comprehension evaluation.\"\u003cSEP\u003e\"This is a technology or dataset created by T. Nguyen et al. for machine reading comprehension evaluation.\""}, {"color": "#75e21a", "description": "\"This is an event or research work by S. K. Nigam et al.\"", "entity_type": "\"EVENT\"", "id": "\"NIGAM@COLIEE-22: LEGAL CASE RETRIEVAL AND ENTAILMENT USING CASCADING OF LEXICAL AND SEMANTIC-BASED MODELS\"", "label": "\"NIGAM@COLIEE-22: LEGAL CASE RETRIEVAL AND ENTAILMENT USING CASCADING OF LEXICAL AND SEMANTIC-BASED MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"This is an event or research work by S. K. Nigam et al.\""}, {"color": "#188546", "description": "\"The research by S. K. Nigam et al. is included in the book published by New Frontiers in Artificial Intelligence.\"", "entity_type": "\"UNKNOWN\"", "id": "\"S. K. NIGAM, K. YADA, K. SATOH, AND S. ARAI\"", "label": "\"S. K. NIGAM, K. YADA, K. SATOH, AND S. ARAI\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"The research by S. K. Nigam et al. is included in the book published by New Frontiers in Artificial Intelligence.\""}, {"color": "#2c5780", "description": "\"GloVe is a technology or dataset for global vector word representation developed by J. Pennington et al.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"GLOVE: GLOBAL VECTORS FOR WORD REPRESENTATION\"", "label": "\"GLOVE: GLOBAL VECTORS FOR WORD REPRESENTATION\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"GloVe is a technology or dataset for global vector word representation developed by J. Pennington et al.\""}, {"color": "#5e7b93", "description": "\"J. Pennington, R. Socher, and C. Manning developed the GloVe technology.\"", "entity_type": "\"UNKNOWN\"", "id": "\"J. PENNINGTON, R. SOCHER, AND C. MANNING\"", "label": "\"J. PENNINGTON, R. SOCHER, AND C. MANNING\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"J. Pennington, R. Socher, and C. Manning developed the GloVe technology.\""}, {"color": "#0162b2", "description": "\"This is an event or research work by L. Real et al.\"", "entity_type": "\"EVENT\"", "id": "\"THE ASSIN 2 SHARED TASK: A QUICK OVERVIEW\"", "label": "\"THE ASSIN 2 SHARED TASK: A QUICK OVERVIEW\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"This is an event or research work by L. Real et al.\""}, {"color": "#2c2934", "description": "\"The research by L. Real et al. is about the ASIN 2 shared task.\"", "entity_type": "\"UNKNOWN\"", "id": "\"L. REAL, E. FONSECA, AND H. G. OLIVEIRA\"", "label": "\"L. REAL, E. FONSECA, AND H. G. OLIVEIRA\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"The research by L. Real et al. is about the ASIN 2 shared task.\""}, {"color": "#58f3b7", "description": "\"ANDLIU, P. J. is an author of a paper on exploring transfer learning with a unified text-to-text transformer.\"", "entity_type": "\"PERSON\"", "id": "\"ANDLIU, P. J.\"", "label": "\"ANDLIU, P. J.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"ANDLIU, P. J. is an author of a paper on exploring transfer learning with a unified text-to-text transformer.\""}, {"color": "#16f504", "description": "\"REIMERS, N. is an author of papers on Sentence-BERT and making monolingual sentence embeddings multilingual using knowledge distillation, and collaborates with Thakur, N., R\u00fcckle, A., Srivastava, A., and Gurevych, I.\"\u003cSEP\u003e\"REIMERS, N. is an author of papers on Sentence-BERT and making monolingual sentence embeddings multilingual using knowledge distillation.\"", "entity_type": "\"PERSON\"", "id": "\"REIMERS, N.\"", "label": "\"REIMERS, N.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"REIMERS, N. is an author of papers on Sentence-BERT and making monolingual sentence embeddings multilingual using knowledge distillation, and collaborates with Thakur, N., R\u00fcckle, A., Srivastava, A., and Gurevych, I.\"\u003cSEP\u003e\"REIMERS, N. is an author of papers on Sentence-BERT and making monolingual sentence embeddings multilingual using knowledge distillation.\""}, {"color": "#e3f75d", "description": "\"REAL, L. is involved in the ASSIN 2 shared task and provides a quick overview.\"", "entity_type": "\"PERSON\"", "id": "\"REAL, L.\"", "label": "\"REAL, L.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"REAL, L. is involved in the ASSIN 2 shared task and provides a quick overview.\""}, {"color": "#63a160", "description": "\"GUREVYCH, I. collaborates with REIMERS, N. on multiple works including Sentence-BERT and making monolingual sentence embeddings multilingual.\"\u003cSEP\u003e\"GUREVYCH, I. collaborates with Reimers, N., Thakur, N., R\u00fcckle, A., Srivastava, A., and co-authors the BEIR benchmark.\"\u003cSEP\u003e\"Gurevych, I. collaborates with Reimers, N., Thakur, N., R\u00fcckle, A., Srivastava, A., and co-authors the BEIR benchmark.\"", "entity_type": "\"PERSON\"", "id": "\"GUREVYCH, I.\"", "label": "\"GUREVYCH, I.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"GUREVYCH, I. collaborates with REIMERS, N. on multiple works including Sentence-BERT and making monolingual sentence embeddings multilingual.\"\u003cSEP\u003e\"GUREVYCH, I. collaborates with Reimers, N., Thakur, N., R\u00fcckle, A., Srivastava, A., and co-authors the BEIR benchmark.\"\u003cSEP\u003e\"Gurevych, I. collaborates with Reimers, N., Thakur, N., R\u00fcckle, A., Srivastava, A., and co-authors the BEIR benchmark.\""}, {"color": "#f0ec5d", "description": "\"SOUZA, F. collaborates with Nogueira, R., and de Alencar Lotufo, R. on BERTimbau pre-trained models for Brazilian Portuguese.\"\u003cSEP\u003e\"Souza, F. collaborates with Nogueira, R., and de Alencar Lotufo, R. on BERTimbau pre-trained models for Brazilian Portuguese.\"", "entity_type": "\"PERSON\"", "id": "\"SOUZA, F.\"", "label": "\"SOUZA, F.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"SOUZA, F. collaborates with Nogueira, R., and de Alencar Lotufo, R. on BERTimbau pre-trained models for Brazilian Portuguese.\"\u003cSEP\u003e\"Souza, F. collaborates with Nogueira, R., and de Alencar Lotufo, R. on BERTimbau pre-trained models for Brazilian Portuguese.\""}, {"color": "#2f8e81", "description": "\"NOGUEIRA, R. collaborates with Souza, F. and de Alencar Lotufo, R. on BERTimbau pre-trained models for Brazilian Portuguese.\"\u003cSEP\u003e\"Nogueira, R. collaborates with Souza, F. and de Alencar Lotufo, R. on BERTimbau pre-trained models for Brazilian Portuguese.\"", "entity_type": "\"PERSON\"", "id": "\"NOGUEIRA, R.\"", "label": "\"NOGUEIRA, R.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"NOGUEIRA, R. collaborates with Souza, F. and de Alencar Lotufo, R. on BERTimbau pre-trained models for Brazilian Portuguese.\"\u003cSEP\u003e\"Nogueira, R. collaborates with Souza, F. and de Alencar Lotufo, R. on BERTimbau pre-trained models for Brazilian Portuguese.\""}, {"color": "#6c211c", "description": "\"DE ALENCAR LOTUFO, R. collaborates with Souza, F. and Nogueira, R. on BERTimbau pre-trained models for Brazilian Portuguese.\"", "entity_type": "\"PERSON\"", "id": "\"DE ALENCAR LOTUFO, R.\"", "label": "\"DE ALENCAR LOTUFO, R.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"DE ALENCAR LOTUFO, R. collaborates with Souza, F. and Nogueira, R. on BERTimbau pre-trained models for Brazilian Portuguese.\""}, {"color": "#17c91b", "description": "\"THAKUR, N. co-authors the BEIR benchmark with Reimers, N., R\u00fcckle, A., Srivastava, A., and Gurevych, I.\"", "entity_type": "\"PERSON\"", "id": "\"THAKUR, N.\"", "label": "\"THAKUR, N.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"THAKUR, N. co-authors the BEIR benchmark with Reimers, N., R\u00fcckle, A., Srivastava, A., and Gurevych, I.\""}, {"color": "#4f106e", "description": "\"R\u00fcckle, A. co-authors the BEIR benchmark with Thakur, N., Reimers, N., Srivastava, A., and Gurevych, I.\"", "entity_type": "\"PERSON\"", "id": "\"R\u00dcCKLE, A.\"", "label": "\"R\u00dcCKLE, A.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"R\u00fcckle, A. co-authors the BEIR benchmark with Thakur, N., Reimers, N., Srivastava, A., and Gurevych, I.\""}, {"color": "#ea9bdb", "description": "\"Srivastava, A. co-authors the BEIR benchmark with Thakur, N., R\u00fcckle, A., Reimers, N., and Gurevych, I.\"", "entity_type": "\"PERSON\"", "id": "\"SRIVASTAVA, A.\"", "label": "\"SRIVASTAVA, A.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Srivastava, A. co-authors the BEIR benchmark with Thakur, N., R\u00fcckle, A., Reimers, N., and Gurevych, I.\""}, {"color": "#786160", "description": "\"Vaswani, A. is an author of the Attention is All You Need paper.\"", "entity_type": "\"PERSON\"", "id": "\"VASWANI, A.\"", "label": "\"VASWANI, A.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Vaswani, A. is an author of the Attention is All You Need paper.\""}, {"color": "#004334", "description": "\"SHAZEER, N. co-authors the Attention is All You Need paper with Vaswani, A.\"", "entity_type": "\"PERSON\"", "id": "\"SHAZEER, N.\"", "label": "\"SHAZEER, N.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"SHAZEER, N. co-authors the Attention is All You Need paper with Vaswani, A.\""}, {"color": "#9f2d40", "description": "\"PARMAR, N. co-authors the Attention is All You Need paper with Vaswani, A. and Shazeer, N.\"\u003cSEP\u003e\"PARMAR, N. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., and U Szkoreit, J.\"", "entity_type": "\"PERSON\"", "id": "\"PARMAR, N.\"", "label": "\"PARMAR, N.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"PARMAR, N. co-authors the Attention is All You Need paper with Vaswani, A. and Shazeer, N.\"\u003cSEP\u003e\"PARMAR, N. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., and U Szkoreit, J.\""}, {"color": "#b607a1", "description": "\"U SZKOREIT, J. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., and Jones, L.\"\u003cSEP\u003e\"U SZKOREIT, J. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., and Parmar, N.\"", "entity_type": "\"PERSON\"", "id": "\"U SZKOREIT, J.\"", "label": "\"U SZKOREIT, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"U SZKOREIT, J. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., and Jones, L.\"\u003cSEP\u003e\"U SZKOREIT, J. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., and Parmar, N.\""}, {"color": "#2b9576", "description": "\"J ONES, L. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., U Szkoreit, J.\"", "entity_type": "\"PERSON\"", "id": "\"JONES, L.\"", "label": "\"JONES, L.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"J ONES, L. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., U Szkoreit, J.\""}, {"color": "#d7186c", "description": "\"G OMEZ, A. N. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., and U Szkoreit, J.\"", "entity_type": "\"PERSON\"", "id": "\"GOMEZ, A. N.\"", "label": "\"GOMEZ, A. N.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"G OMEZ, A. N. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., and U Szkoreit, J.\""}, {"color": "#b27614", "description": "\"K AISER, L.U. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., and U Szkoreit, J.\"", "entity_type": "\"PERSON\"", "id": "\"KAISER, L.U.\"", "label": "\"KAISER, L.U.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"K AISER, L.U. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., and U Szkoreit, J.\""}, {"color": "#beff3b", "description": "\"POLOSUKHIN, I. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., Kaiser, L.U.\"", "entity_type": "\"PERSON\"", "id": "\"POLOSUKHIN, I.\"", "label": "\"POLOSUKHIN, I.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"POLOSUKHIN, I. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., Kaiser, L.U.\""}, {"color": "#2424c9", "description": "\"N., Kaiser is an author of the paper \u0027Attention is all you need\u0027.\"", "entity_type": "\"PERSON\"", "id": "\"N., K AISER \"", "label": "\"N., K AISER \"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"N., Kaiser is an author of the paper \u0027Attention is all you need\u0027.\""}, {"color": "#31b7c8", "description": "\"The contributors to the SciPy 1.0 project are a collective group of individuals who worked on developing and contributing to the SciPy software library.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "label": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"The contributors to the SciPy 1.0 project are a collective group of individuals who worked on developing and contributing to the SciPy software library.\""}, {"color": "#8d8f6e", "description": "\"L. U. and Polosukhin, I. are authors of the paper \u0027Attention is all you need\u0027.\"", "entity_type": "\"PERSON\"", "id": "\"L. U. AND POLOSUKHIN , I.\"", "label": "\"L. U. AND POLOSUKHIN , I.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"L. U. and Polosukhin, I. are authors of the paper \u0027Attention is all you need\u0027.\""}, {"color": "#ef0e70", "description": "\"P. Virtanen is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"V IRTANEN , P.\"", "label": "\"V IRTANEN , P.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"P. Virtanen is an author of the SciPy 1.0 paper.\""}, {"color": "#2a831f", "description": "\"R. Gommers is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"G OMMERS , R.\"", "label": "\"G OMMERS , R.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"R. Gommers is an author of the SciPy 1.0 paper.\""}, {"color": "#3369e6", "description": "\"T. E. Oliphant is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"O LIPHANT , T. E.\"", "label": "\"O LIPHANT , T. E.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"T. E. Oliphant is an author of the SciPy 1.0 paper.\""}, {"color": "#e09619", "description": "\"M. Haberland is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"H ABERLAND , M.\"", "label": "\"H ABERLAND , M.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"M. Haberland is an author of the SciPy 1.0 paper.\""}, {"color": "#10a0e6", "description": "\"T. Reddy is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"R EDDY , T.\"", "label": "\"R EDDY , T.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"T. Reddy is an author of the SciPy 1.0 paper.\""}, {"color": "#76ef5e", "description": "\"D. Cornuelle is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"C OURNAPEAU , D.\"", "label": "\"C OURNAPEAU , D.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"D. Cornuelle is an author of the SciPy 1.0 paper.\""}, {"color": "#1ea314", "description": "\"E. Burovski is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"B UROVSKI , E.\"", "label": "\"B UROVSKI , E.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. Burovski is an author of the SciPy 1.0 paper.\""}, {"color": "#b198b7", "description": "\"P. Peterson is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"P ETERSON , P.\"", "label": "\"P ETERSON , P.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"P. Peterson is an author of the SciPy 1.0 paper.\""}, {"color": "#c304ae", "description": "\"W. Eckesser is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"W ECKESSER , W.\"", "label": "\"W ECKESSER , W.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"W. Eckesser is an author of the SciPy 1.0 paper.\""}, {"color": "#f2a55b", "description": "\"J. Bright is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"B RIGHT , J.\"", "label": "\"B RIGHT , J.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"J. Bright is an author of the SciPy 1.0 paper.\""}, {"color": "#9986b8", "description": "\"S. J. Van der Walt is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"V AN DER WALT, S. J.\"", "label": "\"V AN DER WALT, S. J.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"S. J. Van der Walt is an author of the SciPy 1.0 paper.\""}, {"color": "#382d26", "description": "\"M. Brett is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"B RETT , M.\"", "label": "\"B RETT , M.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"M. Brett is an author of the SciPy 1.0 paper.\""}, {"color": "#5d0f4d", "description": "\"J. Wilson is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"WILSON , J.\"", "label": "\"WILSON , J.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"J. Wilson is an author of the SciPy 1.0 paper.\""}, {"color": "#551bee", "description": "\"K. J. Millman is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"M ILLMAN , K. J.\"", "label": "\"M ILLMAN , K. J.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"K. J. Millman is an author of the SciPy 1.0 paper.\""}, {"color": "#a91453", "description": "\"N. Mayorov is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"M AYOROV , N.\"", "label": "\"M AYOROV , N.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"N. Mayorov is an author of the SciPy 1.0 paper.\""}, {"color": "#99d851", "description": "\"A. R. J. Nelson is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"N ELSON , A. R. J.\"", "label": "\"N ELSON , A. R. J.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"A. R. J. Nelson is an author of the SciPy 1.0 paper.\""}, {"color": "#aee0d6", "description": "\"E. Jones is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"J ONES , E.\"", "label": "\"J ONES , E.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. Jones is an author of the SciPy 1.0 paper.\""}, {"color": "#036d23", "description": "\"R. Kern is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"K ERN, R.\"", "label": "\"K ERN, R.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"R. Kern is an author of the SciPy 1.0 paper.\""}, {"color": "#04b2ec", "description": "\"E. Larson is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"L ARSON , E.\"", "label": "\"L ARSON , E.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. Larson is an author of the SciPy 1.0 paper.\""}, {"color": "#4955d5", "description": "\"C. J. Carey is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"C AREY , C. J.\"", "label": "\"C AREY , C. J.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"C. J. Carey is an author of the SciPy 1.0 paper.\""}, {"color": "#cb08c1", "description": "\"I. Polat is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"P OLAT,\u02d9I.\"", "label": "\"P OLAT,\u02d9I.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"I. Polat is an author of the SciPy 1.0 paper.\""}, {"color": "#a237a0", "description": "\"Y. Feng is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"F ENG, Y.\"", "label": "\"F ENG, Y.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"Y. Feng is an author of the SciPy 1.0 paper.\""}, {"color": "#0fe07d", "description": "\"E. W. Moore is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"M OORE , E. W.\"", "label": "\"M OORE , E. W.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. W. Moore is an author of the SciPy 1.0 paper.\""}, {"color": "#d36bbe", "description": "\"J. Vander Plas is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"V ANDER PLAS, J.\"", "label": "\"V ANDER PLAS, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"J. Vander Plas is an author of the SciPy 1.0 paper.\""}, {"color": "#a9ae0c", "description": "\"D. Laxalde is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"L AXALDE , D.\"", "label": "\"L AXALDE , D.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"D. Laxalde is an author of the SciPy 1.0 paper.\""}, {"color": "#12c701", "description": "\"J. Perktold is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"P ERKTOLD, J.\"", "label": "\"P ERKTOLD, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"J. Perktold is an author of the SciPy 1.0 paper.\""}, {"color": "#e80177", "description": "\"R. Cimrman is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"C IMRMAN , R.\"", "label": "\"C IMRMAN , R.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"R. Cimrman is an author of the SciPy 1.0 paper.\""}, {"color": "#6fe354", "description": "\"I. Henriksen is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"H ENRIKSEN , I.\"", "label": "\"H ENRIKSEN , I.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"I. Henriksen is an author of the SciPy 1.0 paper.\""}, {"color": "#fb4d46", "description": "\"E. A. Quintero is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"Q UINTERO , E. A.\"", "label": "\"Q UINTERO , E. A.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. A. Quintero is an author of the SciPy 1.0 paper.\""}, {"color": "#441f4b", "description": "\"C. R. Harris is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"H ARRIS, C. R.\"", "label": "\"H ARRIS, C. R.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"C. R. Harris is an author of the SciPy 1.0 paper.\""}, {"color": "#697cdc", "description": "\"A. M. Archibald is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"A RCHIBALD , A. M.\"", "label": "\"A RCHIBALD , A. M.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"A. M. Archibald is an author of the SciPy 1.0 paper.\""}, {"color": "#ce2bd4", "description": "\"A. H. Ribeiro is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"RIBEIRO , A. H.\"", "label": "\"RIBEIRO , A. H.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"A. H. Ribeiro is an author of the SciPy 1.0 paper.\""}, {"color": "#c56610", "description": "\"F. Pedregosa is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"P EDREGOSA , F.\"", "label": "\"P EDREGOSA , F.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"F. Pedregosa is an author of the SciPy 1.0 paper.\""}, {"color": "#8a9d40", "description": "\"P. Van Mulbregt is an author of the SciPy 1.0 paper.\"", "entity_type": "\"PERSON\"", "id": "\"V ANMULBREGT, P.\"", "label": "\"V ANMULBREGT, P.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"P. Van Mulbregt is an author of the SciPy 1.0 paper.\""}, {"color": "#adfaf9", "description": "\"K. Wang is an author of the papers \u0027TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning\u0027 and \u0027GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval\u0027.\"", "entity_type": "\"PERSON\"", "id": "\"W ANG, K.\"", "label": "\"W ANG, K.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"K. Wang is an author of the papers \u0027TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning\u0027 and \u0027GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval\u0027.\""}, {"color": "#57290b", "description": "\"N. Reimers is an author of the papers \u0027TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning\u0027 and \u0027GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval\u0027.\"", "entity_type": "\"PERSON\"", "id": "\"R EIMERS , N.\"", "label": "\"R EIMERS , N.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"N. Reimers is an author of the papers \u0027TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning\u0027 and \u0027GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval\u0027.\""}, {"color": "#ac0618", "description": "\"I. Gurevych is an author of the papers \u0027TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning\u0027 and \u0027GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval\u0027.\"", "entity_type": "\"PERSON\"", "id": "\"GUREVYCH , I.\"", "label": "\"GUREVYCH , I.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"I. Gurevych is an author of the papers \u0027TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning\u0027 and \u0027GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval\u0027.\""}, {"color": "#63a991", "description": "\"A. Williams is an author of the paper \u0027A broad-coverage challenge corpus for sentence understanding through inference\u0027.\"", "entity_type": "\"PERSON\"", "id": "\"W ILLIAMS , A.\"", "label": "\"W ILLIAMS , A.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"A. Williams is an author of the paper \u0027A broad-coverage challenge corpus for sentence understanding through inference\u0027.\""}, {"color": "#60a890", "description": "\"N. Angia is an author of the paper \u0027A broad-coverage challenge corpus for sentence understanding through inference\u0027.\"", "entity_type": "\"PERSON\"", "id": "\"N ANGIA , N.\"", "label": "\"N ANGIA , N.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"N. Angia is an author of the paper \u0027A broad-coverage challenge corpus for sentence understanding through inference\u0027.\""}, {"color": "#ec027b", "description": "\"S. Bowman is an author of the paper \u0027A broad-coverage challenge corpus for sentence understanding through inference\u0027.\"", "entity_type": "\"PERSON\"", "id": "\"BOWMAN , S.\"", "label": "\"BOWMAN , S.\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"S. Bowman is an author of the paper \u0027A broad-coverage challenge corpus for sentence understanding through inference\u0027.\""}, {"color": "#134660", "description": "\"June 2023 marks the date when Rui Filipe Coimbra Pereira de Melo submitted his thesis to obtain a Master of Science Degree in Computer Science and Engineering.\"", "entity_type": "\"EVENT\"", "id": "\"JUNE 2023\"", "label": "\"JUNE 2023\"", "shape": "dot", "size": 10, "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"June 2023 marks the date when Rui Filipe Coimbra Pereira de Melo submitted his thesis to obtain a Master of Science Degree in Computer Science and Engineering.\""}, {"color": "#1f3501", "description": "\"Hybrid search systems combining lexical and semantic techniques were incorporated into the Systematic Search prototype.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SISTEMAS DE PESQUISA H\u00cdBRIDA\"", "label": "\"SISTEMAS DE PESQUISA H\u00cdBRIDA\"", "shape": "dot", "size": 10, "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"Hybrid search systems combining lexical and semantic techniques were incorporated into the Systematic Search prototype.\""}, {"color": "#930264", "description": "\"Alex is involved in developing the Systematic Search prototype for the Supremo Tribunal de Justi\u00e7a portugu\u00eas.\"\u003cSEP\u003e\"Alex is the leader or key developer of the Systematic Search prototype.\"", "entity_type": "\"PERSON\"", "id": "\"ALEX", "label": "\"ALEX", "shape": "dot", "size": 10, "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"Alex is involved in developing the Systematic Search prototype for the Supremo Tribunal de Justi\u00e7a portugu\u00eas.\"\u003cSEP\u003e\"Alex is the leader or key developer of the Systematic Search prototype.\""}, {"color": "#5db624", "description": "\"The development and testing of the Systematic Search prototype represent a significant research effort.\"", "entity_type": "\"EVENT\"", "id": "\"RESEARCH\"", "label": "\"RESEARCH\"", "shape": "dot", "size": 10, "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"The development and testing of the Systematic Search prototype represent a significant research effort.\""}, {"color": "#ca7bf4", "description": "\"Fine-tuning involves taking a pre-trained model and further training it on specific tasks, allowing it to perform better in those contexts.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"FINE-TUNING ON DOWNSTREAM TASKS\"", "label": "\"FINE-TUNING ON DOWNSTREAM TASKS\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Fine-tuning involves taking a pre-trained model and further training it on specific tasks, allowing it to perform better in those contexts.\""}, {"color": "#2fab82", "description": "\"Deeper Text Understanding refers to advancements in NLP that allow for more nuanced and comprehensive analysis of text data.\"", "entity_type": "\"CONCEPT\"", "id": "\"DEEPER TEXT UNDERSTANDING\"", "label": "\"DEEPER TEXT UNDERSTANDING\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Deeper Text Understanding refers to advancements in NLP that allow for more nuanced and comprehensive analysis of text data.\""}, {"color": "#cf83ad", "description": "\"Legal Information Retrieval involves using NLP techniques to search and retrieve relevant legal documents or information from large datasets.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"LEGAL INFORMATION RETRIEVAL\"", "label": "\"LEGAL INFORMATION RETRIEVAL\"", "shape": "dot", "size": 10, "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Legal Information Retrieval involves using NLP techniques to search and retrieve relevant legal documents or information from large datasets.\""}, {"color": "#8b07d2", "description": "\"The Corpus refers to a collection of data processed for use in the Semantic Search System.\"|", "entity_type": "\"CONCEPT\"", "id": "\"CORPUS\"", "label": "\"CORPUS\"", "shape": "dot", "size": 10, "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Corpus refers to a collection of data processed for use in the Semantic Search System.\"|"}, {"color": "#640a7c", "description": "\"A Legal Language Model is introduced as a separate entity from the Semantic Search System, possibly used in conjunction or with specific adaptations.\"|\u003cSEP\u003e\"Legal Language Model is an overarching concept or framework that encompasses models like Legal-BERTimbau specifically tailored for legal applications in Portuguese.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LEGAL LANGUAGE MODEL\"", "label": "\"LEGAL LANGUAGE MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd\u003cSEP\u003echunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"A Legal Language Model is introduced as a separate entity from the Semantic Search System, possibly used in conjunction or with specific adaptations.\"|\u003cSEP\u003e\"Legal Language Model is an overarching concept or framework that encompasses models like Legal-BERTimbau specifically tailored for legal applications in Portuguese.\""}, {"color": "#39cf85", "description": "\"Overview provides a summary of the sections covered in the document, setting up context for subsequent discussions.\"", "entity_type": "\"EVENT\"", "id": "\"OVERVIEW\"", "label": "\"OVERVIEW\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Overview provides a summary of the sections covered in the document, setting up context for subsequent discussions.\""}, {"color": "#53b6e3", "description": "\"Results provide outcomes and findings from various evaluations conducted throughout the document.\"", "entity_type": "\"EVENT\"", "id": "\"RESULTS\"", "label": "\"RESULTS\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Results provide outcomes and findings from various evaluations conducted throughout the document.\""}, {"color": "#0df117", "description": "\"Conclusion summarizes the contributions of the research and outlines future work, providing a final wrap-up of the document\u2019s main points.\"", "entity_type": "\"EVENT\"", "id": "\"CONCLUSION\"", "label": "\"CONCLUSION\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Conclusion summarizes the contributions of the research and outlines future work, providing a final wrap-up of the document\u2019s main points.\""}, {"color": "#af20aa", "description": "\"Contributions detail the key findings and advancements made by the authors through their research.\"", "entity_type": "\"EVENT\"", "id": "\"CONTRIBUTIONS\"", "label": "\"CONTRIBUTIONS\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Contributions detail the key findings and advancements made by the authors through their research.\""}, {"color": "#28e73f", "description": "\"Publications section lists the publications resulting from the research covered in the document.\"", "entity_type": "\"EVENT\"", "id": "\"PUBLICATIONS\"", "label": "\"PUBLICATIONS\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Publications section lists the publications resulting from the research covered in the document.\""}, {"color": "#79d8fc", "description": "\"Future Work outlines potential areas of further research and development based on the conclusions drawn from the current work.\"", "entity_type": "\"EVENT\"", "id": "\"FUTURE WORK\"", "label": "\"FUTURE WORK\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Future Work outlines potential areas of further research and development based on the conclusions drawn from the current work.\""}, {"color": "#b37d6b", "description": "\"Albertina appears as a named entity, possibly referring to an organization or project involved in the document\u2019s research.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"ALBERTINA\"", "label": "\"ALBERTINA\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Albertina appears as a named entity, possibly referring to an organization or project involved in the document\u2019s research.\""}, {"color": "#e633ac", "description": "\"Dataset Annotation is part of future work that discusses potential improvements involving data annotation processes.\"", "entity_type": "\"EVENT\"", "id": "\"DATASET ANNOTATION\"", "label": "\"DATASET ANNOTATION\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Dataset Annotation is part of future work that discusses potential improvements involving data annotation processes.\""}, {"color": "#2fc008", "description": "\"Architecture Improvements are mentioned as a part of future work, indicating plans for enhancing the system\u0027s architecture.\"", "entity_type": "\"EVENT\"", "id": "\"ARCHITECTURE IMPROVEMENTS\"", "label": "\"ARCHITECTURE IMPROVEMENTS\"", "shape": "dot", "size": 10, "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Architecture Improvements are mentioned as a part of future work, indicating plans for enhancing the system\u0027s architecture.\""}, {"color": "#dab05d", "description": "\"This is an illustrative event showing the concept of word embeddings in three-dimensional space, useful for visual understanding but not a specific activity or event within the text.\"", "entity_type": "\"EVENT\"", "id": "\"EXAMPLE OF WORD EMBEDDINGS IN A 3D VECTOR SPACE\"", "label": "\"EXAMPLE OF WORD EMBEDDINGS IN A 3D VECTOR SPACE\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"This is an illustrative event showing the concept of word embeddings in three-dimensional space, useful for visual understanding but not a specific activity or event within the text.\""}, {"color": "#e9fc18", "description": "\"These are key components within the Transformer architecture, focusing on how multiple attention heads process information across sequences.\"", "entity_type": "\"EVENT\"", "id": "\"SCALED DOT-PRODUCT ATTENTION AND MULTI-HEAD ATTENTION\"", "label": "\"SCALED DOT-PRODUCT ATTENTION AND MULTI-HEAD ATTENTION\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"These are key components within the Transformer architecture, focusing on how multiple attention heads process information across sequences.\""}, {"color": "#d681be", "description": "\"Illustrates a vector space where a single query is compared against multiple sentences through their embeddings, useful in similarity or relevance ranking tasks.\"", "entity_type": "\"EVENT\"", "id": "\"VECTOR SPACE WITH A QUERY EMBEDDING AND MULTIPLE SENTENCE EMBEDDINGS\"", "label": "\"VECTOR SPACE WITH A QUERY EMBEDDING AND MULTIPLE SENTENCE EMBEDDINGS\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Illustrates a vector space where a single query is compared against multiple sentences through their embeddings, useful in similarity or relevance ranking tasks.\""}, {"color": "#d55819", "description": "\"These are models that encode inputs (like questions and documents) into vectors for similarity search, with cross-encoder being more focused on predicting pairwise similarities.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BI-ENCODER AND CROSS-ENCODER\"", "label": "\"BI-ENCODER AND CROSS-ENCODER\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"These are models that encode inputs (like questions and documents) into vectors for similarity search, with cross-encoder being more focused on predicting pairwise similarities.\""}, {"color": "#8d4979", "description": "\"Illustrates how BERT processes inputs, involving tokenization and embedding techniques for contextual understanding.\"", "entity_type": "\"EVENT\"", "id": "\"BERT INPUT REPRESENTATION\"", "label": "\"BERT INPUT REPRESENTATION\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Illustrates how BERT processes inputs, involving tokenization and embedding techniques for contextual understanding.\""}, {"color": "#060917", "description": "\"Visual representations or statistical analysis of data used in machine learning models to understand the distribution of features or labels.\"", "entity_type": "\"EVENT\"", "id": "\"DATA DISTRIBUTIONS\"", "label": "\"DATA DISTRIBUTIONS\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Visual representations or statistical analysis of data used in machine learning models to understand the distribution of features or labels.\""}, {"color": "#e8450d", "description": "\"A technique where parts of a text are masked and the model is trained to predict those masked tokens, enhancing its ability to understand context within sentences.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MASKED LANGUAGE MODELING\"", "label": "\"MASKED LANGUAGE MODELING\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"A technique where parts of a text are masked and the model is trained to predict those masked tokens, enhancing its ability to understand context within sentences.\""}, {"color": "#904e4f", "description": "\"An advanced architecture combining elements of Transformer models with denoising autoencoders for sequence data, used in tasks like text generation or correction.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE\"", "label": "\"TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"An advanced architecture combining elements of Transformer models with denoising autoencoders for sequence data, used in tasks like text generation or correction.\""}, {"color": "#84911c", "description": "\"A diagram illustrating the structure and workings of the T5 (Text-to-Text Transfer Transformer) model, known for its versatility in handling a wide range of natural language processing tasks.\"\u003cSEP\u003e\"T5 diagram refers to the visual representation of the Transformer model, which is fundamental in many natural language processing tasks.\"", "entity_type": "\"EVENT\"", "id": "\"T5 DIAGRAM\"", "label": "\"T5 DIAGRAM\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa\u003cSEP\u003echunk-4438e56d1dbc938d09784326b42337ca", "title": "\"A diagram illustrating the structure and workings of the T5 (Text-to-Text Transfer Transformer) model, known for its versatility in handling a wide range of natural language processing tasks.\"\u003cSEP\u003e\"T5 diagram refers to the visual representation of the Transformer model, which is fundamental in many natural language processing tasks.\""}, {"color": "#2fe4b2", "description": "\"The TSDAE Architecture is a detailed sequential denoising auto-encoder architecture based on transformers, used for training and fine-tuning models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE ARCHITECTURE\"", "label": "\"TSDAE ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The TSDAE Architecture is a detailed sequential denoising auto-encoder architecture based on transformers, used for training and fine-tuning models.\""}, {"color": "#737cbe", "description": "\"Fine-Tuning SBERT refers to the process of fine-tuning Sentence-BERT models for specific tasks, enhancing their performance on particular language processing challenges.\"", "entity_type": "\"PROCESS\"", "id": "\"FINE-TUNING SBERT\"", "label": "\"FINE-TUNING SBERT\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Fine-Tuning SBERT refers to the process of fine-tuning Sentence-BERT models for specific tasks, enhancing their performance on particular language processing challenges.\""}, {"color": "#fac5ff", "description": "\"The Multilingual Knowledge Distillation Process involves transferring knowledge from a multilingual model to a smaller or less complex model, aiming to maintain or improve performance in multiple languages.\"", "entity_type": "\"PROCESS\"", "id": "\"MULTILINGUAL KNOWLEDGE DISTILLATION PROCESS\"", "label": "\"MULTILINGUAL KNOWLEDGE DISTILLATION PROCESS\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Multilingual Knowledge Distillation Process involves transferring knowledge from a multilingual model to a smaller or less complex model, aiming to maintain or improve performance in multiple languages.\""}, {"color": "#fe97ea", "description": "\"System Architecture refers to the overall design and structure of the system being discussed, likely involving various components and their interactions.\"", "entity_type": "\"ARCHITECTURE\"", "id": "\"SYSTEM ARCHITECTURE\"", "label": "\"SYSTEM ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"System Architecture refers to the overall design and structure of the system being discussed, likely involving various components and their interactions.\""}, {"color": "#fffb01", "description": "\"The Lexical-First Search System Retrieval Method is a retrieval method that prioritizes lexical analysis in its search process, focusing on word-level matching for initial results.\"", "entity_type": "\"METHOD\"", "id": "\"LEXICAL-FIRST SEARCH SYSTEM RETRIEVAL METHOD\"", "label": "\"LEXICAL-FIRST SEARCH SYSTEM RETRIEVAL METHOD\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Lexical-First Search System Retrieval Method is a retrieval method that prioritizes lexical analysis in its search process, focusing on word-level matching for initial results.\""}, {"color": "#f40994", "description": "\"The Lexical + Semantic Search System Retrieval Method combines both lexical and semantic analysis in the retrieval process, aiming to provide more contextually relevant results.\"", "entity_type": "\"METHOD\"", "id": "\"LEXICAL + SEMANTIC SEARCH SYSTEM RETRIEVAL METHOD\"", "label": "\"LEXICAL + SEMANTIC SEARCH SYSTEM RETRIEVAL METHOD\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Lexical + Semantic Search System Retrieval Method combines both lexical and semantic analysis in the retrieval process, aiming to provide more contextually relevant results.\""}, {"color": "#b5ee95", "description": "\"Training Tasks Overview provides an overview of various training tasks involved in the model\u0027s development, including different types of loss functions and data augmentation techniques.\"", "entity_type": "\"TASK\"", "id": "\"TRAINING TASKS OVERVIEW\"", "label": "\"TRAINING TASKS OVERVIEW\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Training Tasks Overview provides an overview of various training tasks involved in the model\u0027s development, including different types of loss functions and data augmentation techniques.\""}, {"color": "#8d2a70", "description": "\"The Masked Language Modeling (MLM) Training Loss is a specific type of loss function used during training to measure the accuracy of predicting masked tokens in sentences, crucial for natural language understanding tasks.\"", "entity_type": "\"LOSS FUNCTION\"", "id": "\"MASKED LANGUAGE MODELING (MLM) TRAINING LOSS\"", "label": "\"MASKED LANGUAGE MODELING (MLM) TRAINING LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Masked Language Modeling (MLM) Training Loss is a specific type of loss function used during training to measure the accuracy of predicting masked tokens in sentences, crucial for natural language understanding tasks.\""}, {"color": "#bf4c35", "description": "\"Metadata Knowledge Distillation Ideology describes the principles behind distilling knowledge from metadata into a target model, optimizing for specific tasks or datasets.\"", "entity_type": "\"CONCEPT\"", "id": "\"METADATA KNOWLEDGE DISTILLATION IDEOLOGY\"", "label": "\"METADATA KNOWLEDGE DISTILLATION IDEOLOGY\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Metadata Knowledge Distillation Ideology describes the principles behind distilling knowledge from metadata into a target model, optimizing for specific tasks or datasets.\""}, {"color": "#a9491f", "description": "\"The Search System Evaluation \u2013 Search metric - Models V0 is an evaluation of the search system\u0027s performance using a specific version (V0) of the model, focusing on search-related metrics.\"", "entity_type": "\"EVALUATION\"", "id": "\"SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC - MODELS V0\"", "label": "\"SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC - MODELS V0\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Search System Evaluation \u2013 Search metric - Models V0 is an evaluation of the search system\u0027s performance using a specific version (V0) of the model, focusing on search-related metrics.\""}, {"color": "#748569", "description": "\"The Search System Evaluation \u2013 Search metric - Models V1 evaluates the search system\u0027s performance with another version (V1) of the model, continuing to focus on search-related metrics.\"", "entity_type": "\"EVALUATION\"", "id": "\"SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC - MODELS V1\"", "label": "\"SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC - MODELS V1\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Search System Evaluation \u2013 Search metric - Models V1 evaluates the search system\u0027s performance with another version (V1) of the model, continuing to focus on search-related metrics.\""}, {"color": "#5a0ece", "description": "\"The Search System Evaluation - Discovery metric - Models V0 assesses the discovery capabilities of the system using a specific version (V0) of the model, measuring its ability to find relevant information.\"", "entity_type": "\"EVALUATION\"", "id": "\"SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V0\"", "label": "\"SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V0\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Search System Evaluation - Discovery metric - Models V0 assesses the discovery capabilities of the system using a specific version (V0) of the model, measuring its ability to find relevant information.\""}, {"color": "#dd7991", "description": "\"The Search System Evaluation - Discovery metric - Models V1 evaluates the discovery performance with another version (V1) of the model, focusing on its ability to uncover relevant information.\"", "entity_type": "\"EVALUATION\"", "id": "\"SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V1\"", "label": "\"SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V1\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Search System Evaluation - Discovery metric - Models V1 evaluates the discovery performance with another version (V1) of the model, focusing on its ability to uncover relevant information.\""}, {"color": "#3bb899", "description": "\"The TSDAE Architecture is a detailed sequential denoising auto-encoder architecture based on transformers, used for training and fine-tuning models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"3.4 TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE\"", "label": "\"3.4 TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The TSDAE Architecture is a detailed sequential denoising auto-encoder architecture based on transformers, used for training and fine-tuning models.\""}, {"color": "#256234", "description": "\"T5 diagram refers to the visual representation of the Transformer model, which is fundamental in many natural language processing tasks.\"", "entity_type": "\"DIAGRAM\"", "id": "\"3.5 T5 DIAGRAM\"", "label": "\"3.5 T5 DIAGRAM\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"T5 diagram refers to the visual representation of the Transformer model, which is fundamental in many natural language processing tasks.\""}, {"color": "#6dda4a", "description": "\"GenQ likely refers to a generative query system or method used for generating questions or text based on given inputs or contexts.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"3.6 GENQ\"", "label": "\"3.6 GENQ\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"GenQ likely refers to a generative query system or method used for generating questions or text based on given inputs or contexts.\""}, {"color": "#df4ad2", "description": "\"Generative Pseudo Labeling is a technique where synthetic labels are generated using models, often used in semi-supervised learning scenarios to augment datasets.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"3.7 GENERATIVE PSEUDO LABELING\"", "label": "\"3.7 GENERATIVE PSEUDO LABELING\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Generative Pseudo Labeling is a technique where synthetic labels are generated using models, often used in semi-supervised learning scenarios to augment datasets.\""}, {"color": "#f3cdf9", "description": "\"Fine-Tuning SBERT refers to the process of fine-tuning Sentence-BERT models for specific tasks, enhancing their performance on particular language processing challenges.\"", "entity_type": "\"PROCESS\"", "id": "\"3.8 FINE-TUNING SBERT\"", "label": "\"3.8 FINE-TUNING SBERT\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Fine-Tuning SBERT refers to the process of fine-tuning Sentence-BERT models for specific tasks, enhancing their performance on particular language processing challenges.\""}, {"color": "#8747ad", "description": "\"The Multilingual Knowledge Distillation Process involves transferring knowledge from a multilingual model to a smaller or less complex model, aiming to maintain or improve performance in multiple languages.\"", "entity_type": "\"PROCESS\"", "id": "\"3.9 MULTILINGUAL KNOWLEDGE DISTILLATION PROCESS\"", "label": "\"3.9 MULTILINGUAL KNOWLEDGE DISTILLATION PROCESS\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Multilingual Knowledge Distillation Process involves transferring knowledge from a multilingual model to a smaller or less complex model, aiming to maintain or improve performance in multiple languages.\""}, {"color": "#8b9b5f", "description": "\"System Architecture refers to the overall design and structure of the system being discussed, likely involving various components and their interactions.\"", "entity_type": "\"ARCHITECTURE\"", "id": "\"4.1 SYSTEM ARCHITECTURE\"", "label": "\"4.1 SYSTEM ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"System Architecture refers to the overall design and structure of the system being discussed, likely involving various components and their interactions.\""}, {"color": "#54b6cd", "description": "\"The Lexical-First Search System Retrieval Method is a retrieval method that prioritizes lexical analysis in its search process, focusing on word-level matching for initial results.\"", "entity_type": "\"METHOD\"", "id": "\"4.2 LEXICAL-FIRST SEARCH SYSTEM RETRIEVAL METHOD\"", "label": "\"4.2 LEXICAL-FIRST SEARCH SYSTEM RETRIEVAL METHOD\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Lexical-First Search System Retrieval Method is a retrieval method that prioritizes lexical analysis in its search process, focusing on word-level matching for initial results.\""}, {"color": "#6f0a93", "description": "\"The Lexical + Semantic Search System Retrieval Method combines both lexical and semantic analysis in the retrieval process, aiming to provide more contextually relevant results.\"", "entity_type": "\"METHOD\"", "id": "\"4.3 LEXICAL + SEMANTIC SEARCH SYSTEM RETRIEVAL METHOD\"", "label": "\"4.3 LEXICAL + SEMANTIC SEARCH SYSTEM RETRIEVAL METHOD\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Lexical + Semantic Search System Retrieval Method combines both lexical and semantic analysis in the retrieval process, aiming to provide more contextually relevant results.\""}, {"color": "#1c1d89", "description": "\"Training Tasks Overview provides an overview of various training tasks involved in the model\u0027s development, including different types of loss functions and data augmentation techniques.\"", "entity_type": "\"TASK\"", "id": "\"5.1 TRAINING TASKS OVERVIEW\"", "label": "\"5.1 TRAINING TASKS OVERVIEW\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Training Tasks Overview provides an overview of various training tasks involved in the model\u0027s development, including different types of loss functions and data augmentation techniques.\""}, {"color": "#365cf2", "description": "\"The Masked Language Modeling (MLM) Training Loss is a specific type of loss function used during training to measure the accuracy of predicting masked tokens in sentences, crucial for natural language understanding tasks.\"", "entity_type": "\"LOSS FUNCTION\"", "id": "\"5.2 MASKED LANGUAGE MODELING (MLM) TRAINING LOSS\"", "label": "\"5.2 MASKED LANGUAGE MODELING (MLM) TRAINING LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Masked Language Modeling (MLM) Training Loss is a specific type of loss function used during training to measure the accuracy of predicting masked tokens in sentences, crucial for natural language understanding tasks.\""}, {"color": "#4d8405", "description": "\"The TSDAE Training Loss refers to the loss function associated with the TSDAE architecture, likely measuring the difference between original and reconstructed data during training.\"", "entity_type": "\"LOSS FUNCTION\"", "id": "\"5.3 TSDAE TRAINING LOSS\"", "label": "\"5.3 TSDAE TRAINING LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The TSDAE Training Loss refers to the loss function associated with the TSDAE architecture, likely measuring the difference between original and reconstructed data during training.\""}, {"color": "#1d2912", "description": "\"Metadata Knowledge Distillation Ideology describes the principles behind distilling knowledge from metadata into a target model, optimizing for specific tasks or datasets.\"", "entity_type": "\"CONCEPT\"", "id": "\"5.4 METADATA KNOWLEDGE DISTILLATION IDEOLOGY\"", "label": "\"5.4 METADATA KNOWLEDGE DISTILLATION IDEOLOGY\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Metadata Knowledge Distillation Ideology describes the principles behind distilling knowledge from metadata into a target model, optimizing for specific tasks or datasets.\""}, {"color": "#a28f26", "description": "\"Metadata Knowledge Distillation is a process where knowledge extracted from metadata is transferred to a target model, improving its performance on relevant tasks without requiring extensive labeled data.\"", "entity_type": "\"PROCESS\"", "id": "\"5.5 METADATA KNOWLEDGE DISTILLATION\"", "label": "\"5.5 METADATA KNOWLEDGE DISTILLATION\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Metadata Knowledge Distillation is a process where knowledge extracted from metadata is transferred to a target model, improving its performance on relevant tasks without requiring extensive labeled data.\""}, {"color": "#7c4a4b", "description": "\"Evaluation Architecture refers to the overall structure and components involved in evaluating the system\u0027s performance, including various metrics and methods for assessing accuracy and effectiveness.\"", "entity_type": "\"ARCHITECTURE\"", "id": "\"6.1 EVALUATION ARCHITECTURE\"", "label": "\"6.1 EVALUATION ARCHITECTURE\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"Evaluation Architecture refers to the overall structure and components involved in evaluating the system\u0027s performance, including various metrics and methods for assessing accuracy and effectiveness.\""}, {"color": "#b231bc", "description": "\"The Search System Evaluation \u2013 Search metric - Models V0 is an evaluation of the search system\u0027s performance using a specific version (V0) of the model, focusing on search-related metrics.\"", "entity_type": "\"EVALUATION\"", "id": "\"6.2 SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC - MODELS V0\"", "label": "\"6.2 SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC - MODELS V0\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Search System Evaluation \u2013 Search metric - Models V0 is an evaluation of the search system\u0027s performance using a specific version (V0) of the model, focusing on search-related metrics.\""}, {"color": "#97ecde", "description": "\"The Search System Evaluation \u2013 Search metric - Models V1 evaluates the search system\u0027s performance with another version (V0) of the model, continuing to focus on search-related metrics.\"", "entity_type": "\"EVALUATION\"", "id": "\"6.3 SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC - MODELS V1\"", "label": "\"6.3 SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC - MODELS V1\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Search System Evaluation \u2013 Search metric - Models V1 evaluates the search system\u0027s performance with another version (V0) of the model, continuing to focus on search-related metrics.\""}, {"color": "#7fff5a", "description": "\"The Search System Evaluation - Discovery metric - Models V0 assesses the discovery capabilities of the system using a specific version (V0) of the model, measuring its ability to find relevant information.\"", "entity_type": "\"EVALUATION\"", "id": "\"6.4 SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V0\"", "label": "\"6.4 SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V0\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Search System Evaluation - Discovery metric - Models V0 assesses the discovery capabilities of the system using a specific version (V0) of the model, measuring its ability to find relevant information.\""}, {"color": "#4e9d0c", "description": "\"The Search System Evaluation - Discovery metric - Models V1 evaluates the discovery performance with another version (V0) of the model, focusing on its ability to uncover relevant information.\"", "entity_type": "\"EVALUATION\"", "id": "\"6.5 SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V1\"", "label": "\"6.5 SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V1\"", "shape": "dot", "size": 10, "source_id": "chunk-86921910d0aba929bef9ae955431a4fa", "title": "\"The Search System Evaluation - Discovery metric - Models V1 evaluates the discovery performance with another version (V0) of the model, focusing on its ability to uncover relevant information.\""}, {"color": "#922013", "description": "\"The event refers to the outcomes of Task 1 in COLIEE 2021, a competition on legal information extraction and entailment.\"", "entity_type": "\"EVENT\"", "id": "\"COLIEE 2021 - TASK 1 RESULTS\"", "label": "\"COLIEE 2021 - TASK 1 RESULTS\"", "shape": "dot", "size": 10, "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"The event refers to the outcomes of Task 1 in COLIEE 2021, a competition on legal information extraction and entailment.\""}, {"color": "#58357d", "description": "\"This event is about the results of Task 3 in COLIEE 2021, another task within the same competition.\"", "entity_type": "\"EVENT\"", "id": "\"COLIEE 2021 - TASK 3 RESULTS\"", "label": "\"COLIEE 2021 - TASK 3 RESULTS\"", "shape": "dot", "size": 10, "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"This event is about the results of Task 3 in COLIEE 2021, another task within the same competition.\""}, {"color": "#db4602", "description": "\"This is an event where the MLM average loss for legal documents in a specific test set was calculated.\"", "entity_type": "\"EVENT\"", "id": "\"MLM AVERAGE LOSS FOR LEGAL DOCUMENTS IN THE TEST SET\"", "label": "\"MLM AVERAGE LOSS FOR LEGAL DOCUMENTS IN THE TEST SET\"", "shape": "dot", "size": 10, "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"This is an event where the MLM average loss for legal documents in a specific test set was calculated.\""}, {"color": "#eecba1", "description": "\"The event involves evaluating models using Semantic Textual Similarity (STS) benchmarks on Portuguese datasets.\"", "entity_type": "\"EVENT\"", "id": "\"STS EVALUATION ON PORTUGUESE DATASETS\"", "label": "\"STS EVALUATION ON PORTUGUESE DATASETS\"", "shape": "dot", "size": 10, "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"The event involves evaluating models using Semantic Textual Similarity (STS) benchmarks on Portuguese datasets.\""}, {"color": "#64b767", "description": "\"NLP is a field within AI that deals with the interaction between human language and computers.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"NLP NATURAL LANGUAGE PROCESSING\"", "label": "\"NLP NATURAL LANGUAGE PROCESSING\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"NLP is a field within AI that deals with the interaction between human language and computers.\""}, {"color": "#555906", "description": "\"NL refers to natural language, which involves understanding and processing of human languages by machines.\"", "entity_type": "\"CONCEPT\"", "id": "\"NL NATURAL LANGUAGE\"", "label": "\"NL NATURAL LANGUAGE\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"NL refers to natural language, which involves understanding and processing of human languages by machines.\""}, {"color": "#174f5d", "description": "\"Neural Networks are a type of machine learning model that can be used in NLP tasks such as text classification and generation.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"NN NEURAL NETWORK\"", "label": "\"NN NEURAL NETWORK\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Neural Networks are a type of machine learning model that can be used in NLP tasks such as text classification and generation.\""}, {"color": "#1d0d9b", "description": "\"Next Sentence Prediction is an NLP task where models predict the next sentence given the current context.\"", "entity_type": "\"EVENT\"", "id": "\"NSP NEXT SENTENCE PREDICTION\"", "label": "\"NSP NEXT SENTENCE PREDICTION\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Next Sentence Prediction is an NLP task where models predict the next sentence given the current context.\""}, {"color": "#7dedee", "description": "\"Question and Answer systems are used to generate answers based on input questions, often in the context of a knowledge base or document corpus.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"QA QUESTION AND ANSWER\"", "label": "\"QA QUESTION AND ANSWER\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Question and Answer systems are used to generate answers based on input questions, often in the context of a knowledge base or document corpus.\""}, {"color": "#112e48", "description": "\"Recurrent Neural Networks are a type of neural network that can process sequences of data such as text for tasks like language modeling and translation.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"RNN RECURRENT NEURAL NETWORK\"", "label": "\"RNN RECURRENT NEURAL NETWORK\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Recurrent Neural Networks are a type of neural network that can process sequences of data such as text for tasks like language modeling and translation.\""}, {"color": "#4b5e92", "description": "\"Sentence-BERT is a model used for sentence embeddings in NLP, enabling more accurate semantic comparisons between sentences.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SBERT SENTENCE-BERT\"", "label": "\"SBERT SENTENCE-BERT\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"Sentence-BERT is a model used for sentence embeddings in NLP, enabling more accurate semantic comparisons between sentences.\""}, {"color": "#e7b59d", "description": "\"TF-IDF measures the importance of a term within a document or corpus based on its frequency and rarity across documents.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TF-IDF TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY\"", "label": "\"TF-IDF TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"TF-IDF measures the importance of a term within a document or corpus based on its frequency and rarity across documents.\""}, {"color": "#624367", "description": "\"TSDAE is a model that uses transformers for denoising auto-encoding tasks, often applied in data cleaning and preprocessing.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TSDAE TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER\"", "label": "\"TSDAE TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"TSDAE is a model that uses transformers for denoising auto-encoding tasks, often applied in data cleaning and preprocessing.\""}, {"color": "#0e3b91", "description": "\"UKP is an organization focused on research related to natural language processing and information retrieval.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"UKP UBIQUITOUS KNOWLEDGE PROCESSING\"", "label": "\"UKP UBIQUITOUS KNOWLEDGE PROCESSING\"", "shape": "dot", "size": 10, "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"UKP is an organization focused on research related to natural language processing and information retrieval.\""}, {"color": "#d45229", "description": "\"Lexical search involves searching for exact query terms in documents, often using algorithms like Okapi BM25 which incorporates TF-IDF.\"", "entity_type": "\"TECHNIQUE\"", "id": "\"LEXICAL SEARCH\"", "label": "\"LEXICAL SEARCH\"", "shape": "dot", "size": 10, "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Lexical search involves searching for exact query terms in documents, often using algorithms like Okapi BM25 which incorporates TF-IDF.\""}, {"color": "#53cc48", "description": "\"Describes the traditional way of searching for similar documents through Jaccard Similarity measure, evaluating shared information between sets.\"", "entity_type": "\"CONCEPT\"", "id": "\"DISTANCE METRICS FOR LEXICAL APPROACHES\"", "label": "\"DISTANCE METRICS FOR LEXICAL APPROACHES\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Describes the traditional way of searching for similar documents through Jaccard Similarity measure, evaluating shared information between sets.\""}, {"color": "#a94359", "description": "\"d refers to the length in the document.\"", "entity_type": "\"CONCEPT\"", "id": "\"D\"", "label": "\"D\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"d refers to the length in the document.\""}, {"color": "#85efa3", "description": "\"Public garden is an example of a location that can be mentioned in text.\"", "entity_type": "\"LOCATION\"", "id": "\"PUBLIC GARDEN\"", "label": "\"PUBLIC GARDEN\"", "shape": "dot", "size": 10, "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Public garden is an example of a location that can be mentioned in text.\""}, {"color": "#3aaca8", "description": "\"Fashion is mentioned as a concept that does not show strong co-occurrence with ice or steam.\"", "entity_type": "\"CONCEPT\"", "id": "\"FASHION\"", "label": "\"FASHION\"", "shape": "dot", "size": 10, "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Fashion is mentioned as a concept that does not show strong co-occurrence with ice or steam.\""}, {"color": "#e7a06e", "description": "\"Gates refer to the components within LSTM that manage information flow, including forget gate, input gate, and output gate.\"", "entity_type": "\"CONCEPT\"", "id": "\"GATES\"", "label": "\"GATES\"", "shape": "dot", "size": 10, "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Gates refer to the components within LSTM that manage information flow, including forget gate, input gate, and output gate.\""}, {"color": "#fba77c", "description": "\"Wf represents the weight matrix for the Forget Gate layer in the LSTM architecture.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WF\"", "label": "\"WF\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"Wf represents the weight matrix for the Forget Gate layer in the LSTM architecture.\""}, {"color": "#5d1a54", "description": "\"Wi is the weight matrix for the Input Gate layer in the LSTM architecture.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WI\"", "label": "\"WI\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"Wi is the weight matrix for the Input Gate layer in the LSTM architecture.\""}, {"color": "#0f99bd", "description": "\"WC denotes the weight matrix for the candidate cell state vector in the Input Gate layer of the LSTM architecture.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"WC\"", "label": "\"WC\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"WC denotes the weight matrix for the candidate cell state vector in the Input Gate layer of the LSTM architecture.\""}, {"color": "#023f9f", "description": "\"bf is a bias term used in the Forget Gate layer to adjust the output of the sigmoid function.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BF\"", "label": "\"BF\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"bf is a bias term used in the Forget Gate layer to adjust the output of the sigmoid function.\""}, {"color": "#eb55be", "description": "\"bi represents a bias term for the Input Gate layer, helping in adjusting its output.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BI\"", "label": "\"BI\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"bi represents a bias term for the Input Gate layer, helping in adjusting its output.\""}, {"color": "#63bc85", "description": "\"bC is the bias term used in the candidate cell state vector computation of the Input Gate layer.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BC\"", "label": "\"BC\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"bC is the bias term used in the candidate cell state vector computation of the Input Gate layer.\""}, {"color": "#801b9f", "description": "\"ft stands for the output of the Forget Gate, which determines how much information to keep or discard from the previous cell state.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"FT\"", "label": "\"FT\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"ft stands for the output of the Forget Gate, which determines how much information to keep or discard from the previous cell state.\""}, {"color": "#c3420c", "description": "\"it is the output of the Input Gate layer, determining what values will be updated in the cell state.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"IT\"", "label": "\"IT\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"it is the output of the Input Gate layer, determining what values will be updated in the cell state.\""}, {"color": "#680ce0", "description": "\"\u02dcCt represents the candidate cell state vector, computed using a tanh function in the Input Gate layer.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"\u02dcCT\"", "label": "\"\u02dcCT\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"\u02dcCt represents the candidate cell state vector, computed using a tanh function in the Input Gate layer.\""}, {"color": "#ca8f90", "description": "\"Ct is the updated cell state, resulting from the combination of retained and updated information from the previous step.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"CT\"", "label": "\"CT\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"Ct is the updated cell state, resulting from the combination of retained and updated information from the previous step.\""}, {"color": "#5ff694", "description": "\"This formula represents the updated cell state computation in the LSTM architecture.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"FT\u2217CT\u22121+IT\u2217\u02dcCT\"", "label": "\"FT\u2217CT\u22121+IT\u2217\u02dcCT\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"This formula represents the updated cell state computation in the LSTM architecture.\""}, {"color": "#34c885", "description": "\"This expression computes the hidden vector ht by applying a tanh function to the updated cell state Ct.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"TANH(CT)\"", "label": "\"TANH(CT)\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"This expression computes the hidden vector ht by applying a tanh function to the updated cell state Ct.\""}, {"color": "#b9cf7a", "description": "\"ht is the hidden vector that will be transmitted to the next cell in the sequence.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"HT\"", "label": "\"HT\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"ht is the hidden vector that will be transmitted to the next cell in the sequence.\""}, {"color": "#e68c14", "description": "\"N equals 6 refers to the number of identical layers in the Transformer Encoder.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"N=6\"", "label": "\"N=6\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"N equals 6 refers to the number of identical layers in the Transformer Encoder.\""}, {"color": "#e760b2", "description": "\"Each layer in the Transformer\u0027s Encoder contains two sub-layers: a Multi-Head Attention Layer and a Feed-Forward Neural Network, working together to process input sequences.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MULTI-HEAD ATTENTION LAYER AND FEED-FORWARD NEURAL NETWORK\"", "label": "\"MULTI-HEAD ATTENTION LAYER AND FEED-FORWARD NEURAL NETWORK\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"Each layer in the Transformer\u0027s Encoder contains two sub-layers: a Multi-Head Attention Layer and a Feed-Forward Neural Network, working together to process input sequences.\""}, {"color": "#0e6eee", "description": "\"The Input Embedding is responsible for transforming inputs into scalar vectors and mapping them into a space where similar words are close based on their meanings.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"INPUT EMBEDDING\"", "label": "\"INPUT EMBEDDING\"", "shape": "dot", "size": 10, "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"The Input Embedding is responsible for transforming inputs into scalar vectors and mapping them into a space where similar words are close based on their meanings.\""}, {"color": "#58539b", "description": "\"Word Positions refer to the specific order of words in a sentence, which is crucial for understanding context using Positional Encoding.\"", "entity_type": "\"CONCEPT\"", "id": "\"WORD POSITIONS\"", "label": "\"WORD POSITIONS\"", "shape": "dot", "size": 10, "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Word Positions refer to the specific order of words in a sentence, which is crucial for understanding context using Positional Encoding.\""}, {"color": "#1c3dd7", "description": "\"Figure 2.10 refers to a vector space with query embedding and multiple sentence embeddings.\"", "entity_type": "\"LOCATION\"", "id": "\"FIGURE 2.10\"", "label": "\"FIGURE 2.10\"", "shape": "dot", "size": 10, "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Figure 2.10 refers to a vector space with query embedding and multiple sentence embeddings.\""}, {"color": "#6761de", "description": "\"Figure 2.11 depicts the architecture of Bi-Encoder (left) and Cross-Encoder (right).\"", "entity_type": "\"LOCATION\"", "id": "\"FIGURE 2.11\"", "label": "\"FIGURE 2.11\"", "shape": "dot", "size": 10, "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Figure 2.11 depicts the architecture of Bi-Encoder (left) and Cross-Encoder (right).\""}, {"color": "#ff7dee", "description": "\"Natural Language Processing (NLP) is the field that deals with processing and understanding human language by machines.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"NATURAL LANGUAGE PROCESSING (NLP)\"", "label": "\"NATURAL LANGUAGE PROCESSING (NLP)\"", "shape": "dot", "size": 10, "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Natural Language Processing (NLP) is the field that deals with processing and understanding human language by machines.\""}, {"color": "#a9ab40", "description": "\"A structure representing sentences as vertices in a graph, with edge weights calculated using cosine similarity or Jaccard similarity.\"", "entity_type": "\"CONCEPT\"", "id": "\"GRAPH REPRESENTATION OF SENTENCES\"", "label": "\"GRAPH REPRESENTATION OF SENTENCES\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"A structure representing sentences as vertices in a graph, with edge weights calculated using cosine similarity or Jaccard similarity.\""}, {"color": "#2b5bd6", "description": "\"A unit of text within a sentence that can be represented as a node in a graph.\"", "entity_type": "\"CONCEPT\"", "id": "\"PHRASE\"", "label": "\"PHRASE\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"A unit of text within a sentence that can be represented as a node in a graph.\""}, {"color": "#d484ad", "description": "\"A graph where sentences are nodes and edges denote the similarity score between respective sentences.\"", "entity_type": "\"CONCEPT\"", "id": "\"SENTENCE SIMILARITY GRAPH\"", "label": "\"SENTENCE SIMILARITY GRAPH\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"A graph where sentences are nodes and edges denote the similarity score between respective sentences.\""}, {"color": "#d3d5a6", "description": "\"A statistic used for comparing the similarity and diversity of sample sets, often used to measure sentence similarity.\"", "entity_type": "\"CONCEPT\"", "id": "\"JACCARD SIMILARITY\"", "label": "\"JACCARD SIMILARITY\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"A statistic used for comparing the similarity and diversity of sample sets, often used to measure sentence similarity.\""}, {"color": "#4ffe6e", "description": "\"A matrix used to represent the pairwise similarity between sentences, often decomposed into eigenvalues for generating LexRank scores.\"", "entity_type": "\"CONCEPT\"", "id": "\"SIMILARITY MATRIX\"", "label": "\"SIMILARITY MATRIX\"", "shape": "dot", "size": 10, "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"A matrix used to represent the pairwise similarity between sentences, often decomposed into eigenvalues for generating LexRank scores.\""}, {"color": "#4f48e2", "description": "\"BooksCorpus refers to the dataset used for pre-training BERT, consisting of 800 million words.\"", "entity_type": "\"LOCATION\"", "id": "\"BOOKSCORPUS (800M WORDS)\"", "label": "\"BOOKSCORPUS (800M WORDS)\"", "shape": "dot", "size": 10, "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"BooksCorpus refers to the dataset used for pre-training BERT, consisting of 800 million words.\""}, {"color": "#041576", "description": "\"English Wikipedia is another dataset used for BERT\u0027s pre-training phase, containing about 2.5 billion words.\"", "entity_type": "\"LOCATION\"", "id": "\"ENGLISH WIKIPEDIA (2500M WORDS)\"", "label": "\"ENGLISH WIKIPEDIA (2500M WORDS)\"", "shape": "dot", "size": 10, "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"English Wikipedia is another dataset used for BERT\u0027s pre-training phase, containing about 2.5 billion words.\""}, {"color": "#e085c4", "description": "\"BERT LARGE is another reported model size with distinct architectural details, featuring L= 24, H= 1024, A= 16 and total parameters = 340M.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BERT LARGE\"", "label": "\"BERT LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"BERT LARGE is another reported model size with distinct architectural details, featuring L= 24, H= 1024, A= 16 and total parameters = 340M.\""}, {"color": "#05cba7", "description": "\"Hidden size H is a parameter in the BERT architecture, with values specified for both BASE and LARGE models.\"", "entity_type": "\"CONCEPT\"", "id": "\"HIDDEN SIZE (H)\"", "label": "\"HIDDEN SIZE (H)\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Hidden size H is a parameter in the BERT architecture, with values specified for both BASE and LARGE models.\""}, {"color": "#36f653", "description": "\"Self-attention heads A is another architectural concept denoted by the authors, specifying the number of attention heads in each block.\"", "entity_type": "\"CONCEPT\"", "id": "\"SELF-ATTENTION HEADS (A)\"", "label": "\"SELF-ATTENTION HEADS (A)\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Self-attention heads A is another architectural concept denoted by the authors, specifying the number of attention heads in each block.\""}, {"color": "#dd1d2e", "description": "\"Domain Adaptation is a technique aimed at improving model performance on datasets from different domains through re-training with unsupervised learning tasks.\"", "entity_type": "\"EVENT\"", "id": "\"DOMAIN ADAPTATION (DA)\"", "label": "\"DOMAIN ADAPTATION (DA)\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Domain Adaptation is a technique aimed at improving model performance on datasets from different domains through re-training with unsupervised learning tasks.\""}, {"color": "#8c8b5c", "description": "\"Task data refers to observable task distribution information used for specific training purposes, sampled non-randomly from a larger target domain.\"", "entity_type": "\"CONCEPT\"", "id": "\"TASK DATA\"", "label": "\"TASK DATA\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Task data refers to observable task distribution information used for specific training purposes, sampled non-randomly from a larger target domain.\""}, {"color": "#59187b", "description": "\"Large Language Model is the context in which DA techniques are applied, particularly with respect to BERT models.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LARGE LANGUAGE MODEL\"", "label": "\"LARGE LANGUAGE MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Large Language Model is the context in which DA techniques are applied, particularly with respect to BERT models.\""}, {"color": "#da0550", "description": "\"Authors are the individuals who denoted the number of layers, hidden size, self-attention heads, and total parameters in the BERT models.\"", "entity_type": "\"PERSON\"", "id": "\"AUTHORS\"", "label": "\"AUTHORS\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Authors are the individuals who denoted the number of layers, hidden size, self-attention heads, and total parameters in the BERT models.\""}, {"color": "#8147ca", "description": "\"Sequential Denoising Auto-Encoder is another technique mentioned as part of domain adaptation techniques for BERT models.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SEQUENTIAL DENOISING AUTO-ENCODER\"", "label": "\"SEQUENTIAL DENOISING AUTO-ENCODER\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Sequential Denoising Auto-Encoder is another technique mentioned as part of domain adaptation techniques for BERT models.\""}, {"color": "#28f3c8", "description": "\"Unsupervised learning methods are used in domain adaptation to re-train models on new datasets without labeled data.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"UNSUPERVISED LEARNING METHODS\"", "label": "\"UNSUPERVISED LEARNING METHODS\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Unsupervised learning methods are used in domain adaptation to re-train models on new datasets without labeled data.\""}, {"color": "#bfa4cc", "description": "\"Adam Optimizer has specific parameters and settings used during the fine-tuning process of BERT models.\"", "entity_type": "\"CONCEPT\"", "id": "\"ADAM OPTIMIZER PARAMETERS\"", "label": "\"ADAM OPTIMIZER PARAMETERS\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Adam Optimizer has specific parameters and settings used during the fine-tuning process of BERT models.\""}, {"color": "#9aedff", "description": "\"The weight \\( w \\) is part of the loss function LMLM, which is minimized to predict correct tokens for masked positions.\"", "entity_type": "\"CONCEPT\"", "id": "\"WEIGHT \\( W \\) IN THE LOSS FUNCTION\"", "label": "\"WEIGHT \\( W \\) IN THE LOSS FUNCTION\"", "shape": "dot", "size": 10, "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"The weight \\( w \\) is part of the loss function LMLM, which is minimized to predict correct tokens for masked positions.\""}, {"color": "#c95f19", "description": "\"TSDAE is an unsupervised state-of-the-art sentence embedding method introduced in 2021 by Nils Reimers, aimed at improving domain knowledge and filling the gap between models that usually only perform STS tasks on a certain domain.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE)\"", "label": "\"TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE)\"", "shape": "dot", "size": 10, "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"TSDAE is an unsupervised state-of-the-art sentence embedding method introduced in 2021 by Nils Reimers, aimed at improving domain knowledge and filling the gap between models that usually only perform STS tasks on a certain domain.\""}, {"color": "#1d50ae", "description": "\"The SNLI event involves datasets used for Natural Language Inference, which helped in fine-tuning the model through its premise and hypothesis pairs.\"", "entity_type": "\"EVENT\"", "id": "\"STANFORD NATURAL LANGUAGE INFERENCE (SNLI)\"", "label": "\"STANFORD NATURAL LANGUAGE INFERENCE (SNLI)\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The SNLI event involves datasets used for Natural Language Inference, which helped in fine-tuning the model through its premise and hypothesis pairs.\""}, {"color": "#44629c", "description": "\"NLI, or Natural Language Inference, involves determining the relationship between a premise and a hypothesis, being trained using datasets like SNLI and Multi-Genre NLI.\"", "entity_type": "\"CONCEPT\"", "id": "\"NLI (NATURAL LANGUAGE INFERENCE)\"", "label": "\"NLI (NATURAL LANGUAGE INFERENCE)\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"NLI, or Natural Language Inference, involves determining the relationship between a premise and a hypothesis, being trained using datasets like SNLI and Multi-Genre NLI.\""}, {"color": "#84dd13", "description": "\"NLP tasks in 2019 refer to various natural language processing and semantic similarity evaluation tasks where SBERT was fine-tuned and evaluated.\"", "entity_type": "\"EVENT\"", "id": "\"NLP TASKS 2019\"", "label": "\"NLP TASKS 2019\"", "shape": "dot", "size": 10, "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"NLP tasks in 2019 refer to various natural language processing and semantic similarity evaluation tasks where SBERT was fine-tuned and evaluated.\""}, {"color": "#c2d604", "description": "\"BERT -STSb-base refers to a version of BERT trained on the STSb benchmark dataset, achieving high performance in sentence embedding evaluation.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BERT -STSB-BASE\"", "label": "\"BERT -STSB-BASE\"", "shape": "dot", "size": 10, "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"BERT -STSb-base refers to a version of BERT trained on the STSb benchmark dataset, achieving high performance in sentence embedding evaluation.\""}, {"color": "#b5f68b", "description": "\"The mpnet model is a larger model than MiniLM, capable of better understanding texts it has not explicitly seen before.\"", "entity_type": "\"MODEL\"", "id": "\"MPNET MODEL\"", "label": "\"MPNET MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"The mpnet model is a larger model than MiniLM, capable of better understanding texts it has not explicitly seen before.\""}, {"color": "#231dba", "description": "\"The MiniLM model is a smaller version compared to the mpnet model, used for generating embeddings with 384 dimensions.\"", "entity_type": "\"MODEL\"", "id": "\"MINILM MODEL\"", "label": "\"MINILM MODEL\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"The MiniLM model is a smaller version compared to the mpnet model, used for generating embeddings with 384 dimensions.\""}, {"color": "#71ff2e", "description": "\"Multilingual BERT models are pre-trained on multiple languages but can be limited in their understanding of specific contexts.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"MULTILINGUAL BERT MODELS\"", "label": "\"MULTILINGUAL BERT MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"Multilingual BERT models are pre-trained on multiple languages but can be limited in their understanding of specific contexts.\""}, {"color": "#5f6df1", "description": "\"BrWaC is a large Portuguese corpus used in the training of BERTimbau, containing over 2.68 billion tokens from 3.53 million documents.\"\u003cSEP\u003e\"BrWaC is a large corpus used in training BERTimbau, containing Portuguese text data.\"", "entity_type": "\"LOCATION\"", "id": "\"BRAZILIAN WEB AS CORPUS (BRWAC)\"", "label": "\"BRAZILIAN WEB AS CORPUS (BRWAC)\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"BrWaC is a large Portuguese corpus used in the training of BERTimbau, containing over 2.68 billion tokens from 3.53 million documents.\"\u003cSEP\u003e\"BrWaC is a large corpus used in training BERTimbau, containing Portuguese text data.\""}, {"color": "#bd9d76", "description": "\"The STS benchmark test set is used for evaluating models like SBERT.\"", "entity_type": "\"LOCATION\"", "id": "\"STS BENCHMARK TEST SET\"", "label": "\"STS BENCHMARK TEST SET\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"The STS benchmark test set is used for evaluating models like SBERT.\""}, {"color": "#443a6f", "description": "\"These are pairs of sentences with the same meaning but in different languages used for training multilingual models.\"", "entity_type": "\"CONCEPT\"", "id": "\"PARALLEL SENTENCES ((S1, T1), ...,(SN, TN))\"", "label": "\"PARALLEL SENTENCES ((S1, T1), ...,(SN, TN))\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"These are pairs of sentences with the same meaning but in different languages used for training multilingual models.\""}, {"color": "#1883e2", "description": "\"Mean-squared loss is used in minimizing the difference between the embeddings produced by the student and teacher models.\"", "entity_type": "\"CONCEPT\"", "id": "\"MEAN-SQUARED LOSS\"", "label": "\"MEAN-SQUARED LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"Mean-squared loss is used in minimizing the difference between the embeddings produced by the student and teacher models.\""}, {"color": "#fd6981", "description": "\"The learning rate of 1e-4 is used during the training process of BERTimbau models.\"", "entity_type": "\"CONCEPT\"", "id": "\"1E\u22124 LEARNING RATE\"", "label": "\"1E\u22124 LEARNING RATE\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"The learning rate of 1e-4 is used during the training process of BERTimbau models.\""}, {"color": "#d0d076", "description": "\"The maximum sentence length is 512 tokens, which defines how long a sentence can be in BERTimbau models.\"", "entity_type": "\"CONCEPT\"", "id": "\"512 TOKENS MAXIMUM SENTENCE LENGTH\"", "label": "\"512 TOKENS MAXIMUM SENTENCE LENGTH\"", "shape": "dot", "size": 10, "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"The maximum sentence length is 512 tokens, which defines how long a sentence can be in BERTimbau models.\""}, {"color": "#c39f14", "description": "\"ERTimbau variants refer to a set of language model variations derived from BERT, utilized for specific linguistic tasks.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"ERTIMBAU VARIANTS\"", "label": "\"ERTIMBAU VARIANTS\"", "shape": "dot", "size": 10, "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"ERTimbau variants refer to a set of language model variations derived from BERT, utilized for specific linguistic tasks.\""}, {"color": "#b9f61b", "description": "\"Page number 38 refers to a section within the text that discusses fine-tuning of BERTimbau for legal domain tasks.\"", "entity_type": "\"LOCATION\"", "id": "\"38\"", "label": "\"38\"", "shape": "dot", "size": 10, "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"Page number 38 refers to a section within the text that discusses fine-tuning of BERTimbau for legal domain tasks.\""}, {"color": "#89452b", "description": "\"FEUP is another institution that participated in developing the Albertina PT BERT model, specifically contributing through NLX and Laborat\u00f3rio de Intelig\u00eancia Artificial e Ci\u00eancia de Computadores.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"FACULDADE DE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)\"", "label": "\"FACULDADE DE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"FEUP is another institution that participated in developing the Albertina PT BERT model, specifically contributing through NLX and Laborat\u00f3rio de Intelig\u00eancia Artificial e Ci\u00eancia de Computadores.\""}, {"color": "#bcc682", "description": "\"NLX is a group at FEUP involved in the development of the Albertina PT BERT model.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"NLX\u2013NATURAL LANGUAGE AND SPEECH GROUP\"", "label": "\"NLX\u2013NATURAL LANGUAGE AND SPEECH GROUP\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"NLX is a group at FEUP involved in the development of the Albertina PT BERT model.\""}, {"color": "#4e2ca2", "description": "\"This lab at FEUP contributed to the development of the Albertina PT BERT model.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"LABORAT \u00b4ORIO DE INTELIG \u02c6ENCIA ARTIFICIAL E CI \u02c6ENCIA DE COMPUTADORES\"", "label": "\"LABORAT \u00b4ORIO DE INTELIG \u02c6ENCIA ARTIFICIAL E CI \u02c6ENCIA DE COMPUTADORES\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"This lab at FEUP contributed to the development of the Albertina PT BERT model.\""}, {"color": "#04728d", "description": "\"Chapter 4 provides an overview of the implemented search system architecture and requirements.\"::", "entity_type": "\"EVENT\"", "id": "\"CHAPTER 4\"", "label": "\"CHAPTER 4\"", "shape": "dot", "size": 10, "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"Chapter 4 provides an overview of the implemented search system architecture and requirements.\"::"}, {"color": "#7edb63", "description": "\"Chapter 1 provides background information on the project context, including the segment of Project IRIS it covers.\"", "entity_type": "\"EVENT\"", "id": "\"CHAPTER 1\"", "label": "\"CHAPTER 1\"", "shape": "dot", "size": 10, "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Chapter 1 provides background information on the project context, including the segment of Project IRIS it covers.\""}, {"color": "#ae62af", "description": "\"Refers to a large dataset of documents being processed.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"30000 DOCUMENTS\"", "label": "\"30000 DOCUMENTS\"", "shape": "dot", "size": 10, "source_id": "chunk-486e9fdbc67025b64b42032778600c9c", "title": "\"Refers to a large dataset of documents being processed.\""}, {"color": "#c0d951", "description": "\"Portuguese language corpus BrWaC is a large dataset used to pre-train BERTimbau. It forms the basis for further model adaptations and fine-tuning.\"", "entity_type": "\"LOCATION\"", "id": "\"PORTUGUESE LANGUAGE CORPUS (BRWAC)\"", "label": "\"PORTUGUESE LANGUAGE CORPUS (BRWAC)\"", "shape": "dot", "size": 10, "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"Portuguese language corpus BrWaC is a large dataset used to pre-train BERTimbau. It forms the basis for further model adaptations and fine-tuning.\""}, {"color": "#27561e", "description": "\"This technology was used during the domain adaptation stages of BERTimbau.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"NVIDIA GEFORCE RTX 3090 24 GB GPU\"", "label": "\"NVIDIA GEFORCE RTX 3090 24 GB GPU\"", "shape": "dot", "size": 10, "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"This technology was used during the domain adaptation stages of BERTimbau.\""}, {"color": "#4c9e53", "description": "\"This is a library that can be used to work with SentenceTransformers variants of BERTimbau.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"SENTENCETRANSFORMERS PYTHON LIBRARY\"", "label": "\"SENTENCETRANSFORMERS PYTHON LIBRARY\"", "shape": "dot", "size": 10, "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"This is a library that can be used to work with SentenceTransformers variants of BERTimbau.\""}, {"color": "#8d417c", "description": "\"This is an SBERT model version created using the Legal-BERTimbau-large foundation.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"SBERT VERSION OF LEGAL-BERTIMBAU-LARGE\"", "label": "\"SBERT VERSION OF LEGAL-BERTIMBAU-LARGE\"", "shape": "dot", "size": 10, "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"This is an SBERT model version created using the Legal-BERTimbau-large foundation.\""}, {"color": "#c69172", "description": "\"GPT3 is a powerful language generation tool used to generate sentence pairs for the STS task, providing relatedness scores from 4 to 5.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"OPENAI\u2019S GPT3 TEXT-DAVINCI-003 MODEL API\"", "label": "\"OPENAI\u2019S GPT3 TEXT-DAVINCI-003 MODEL API\"", "shape": "dot", "size": 10, "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"GPT3 is a powerful language generation tool used to generate sentence pairs for the STS task, providing relatedness scores from 4 to 5.\""}, {"color": "#a3e6dd", "description": "\"This refers to the process of further training a model on specific tasks like STS regression.\"", "entity_type": "\"EVENT\"", "id": "\"ST(S) FINE-TUNING TASK\"", "label": "\"ST(S) FINE-TUNING TASK\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"This refers to the process of further training a model on specific tasks like STS regression.\""}, {"color": "#b4d34d", "description": "\"Another variant of the Legal-BERTimbau-large model, indicating ongoing development and improvements.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V1\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Another variant of the Legal-BERTimbau-large model, indicating ongoing development and improvements.\""}, {"color": "#6e3800", "description": "\"A further refinement or version of the Legal-BERTimbau-large model.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1\"", "label": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"A further refinement or version of the Legal-BERTimbau-large model.\""}, {"color": "#032a2d", "description": "\"The event of COVID-19 is the subject around which the documents are related, influencing the content and focus of the text.\"", "entity_type": "\"EVENT\"", "id": "\"COVID-19\"", "label": "\"COVID-19\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"The event of COVID-19 is the subject around which the documents are related, influencing the content and focus of the text.\""}, {"color": "#3626a9", "description": "\"This is a model name generated by STJIRIS, focusing on multilingual knowledge distillation for legal tasks.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-METAKD-V0\"", "label": "\"BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-METAKD-V0\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"This is a model name generated by STJIRIS, focusing on multilingual knowledge distillation for legal tasks.\""}, {"color": "#801eb7", "description": "\"Another model name from the same organization, involving TSDAE and STS-NLI tasks with MetaKD.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V1\"", "label": "\"BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V1\"", "shape": "dot", "size": 10, "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"Another model name from the same organization, involving TSDAE and STS-NLI tasks with MetaKD.\""}, {"color": "#4fee16", "description": "\"The MLM task involves predicting masked words in a sentence, used as a benchmark for evaluating domain adaptation techniques.\"", "entity_type": "\"EVENT\"", "id": "\"MLM TASK\"", "label": "\"MLM TASK\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"The MLM task involves predicting masked words in a sentence, used as a benchmark for evaluating domain adaptation techniques.\""}, {"color": "#68de95", "description": "\"Negative Log-Likelihood Loss is the loss function used to evaluate model performance in tasks such as MLM and TSDAE.\"", "entity_type": "\"CONCEPT\"", "id": "\"NEGATIVE LOG-LIKELIHOOD LOSS\"", "label": "\"NEGATIVE LOG-LIKELIHOOD LOSS\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Negative Log-Likelihood Loss is the loss function used to evaluate model performance in tasks such as MLM and TSDAE.\""}, {"color": "#577e63", "description": "\"Table 6.1 contains average loss values for legal documents in the test set across different models.\"", "entity_type": "\"LOCATION\"", "id": "\"TABLE 6.1\"", "label": "\"TABLE 6.1\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Table 6.1 contains average loss values for legal documents in the test set across different models.\""}, {"color": "#fbc44d", "description": "\"Pearson correlation is a statistical measure used to evaluate model performance on STS tasks by comparing expected and projected similarity scores.\"", "entity_type": "\"CONCEPT\"", "id": "\"PEARSON CORRELATION\"", "label": "\"PEARSON CORRELATION\"", "shape": "dot", "size": 10, "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Pearson correlation is a statistical measure used to evaluate model performance on STS tasks by comparing expected and projected similarity scores.\""}, {"color": "#fa5214", "description": "\"The target language for the evaluation and performance comparison between SBERT variants and state-of-the-art multilingual models.\"", "entity_type": "\"LANGUAGE\"", "id": "\"PORTUGUESE LANGUAGE\"", "label": "\"PORTUGUESE LANGUAGE\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"The target language for the evaluation and performance comparison between SBERT variants and state-of-the-art multilingual models.\""}, {"color": "#f6582a", "description": "\"Another dataset, similar to ASSIN but with different characteristics or additional content, used for evaluation purposes.\"", "entity_type": "\"DATASET\"", "id": "\"ASSIN2 DATASET\"", "label": "\"ASSIN2 DATASET\"", "shape": "dot", "size": 10, "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"Another dataset, similar to ASSIN but with different characteristics or additional content, used for evaluation purposes.\""}, {"color": "#ad572b", "description": "\"NLI training is a technique that improves model performance by incorporating natural language inference, showing a positive impact on both Search and Discovery metrics.\"", "entity_type": "\"CONCEPT\"", "id": "\"NLI TRAINING\"", "label": "\"NLI TRAINING\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"NLI training is a technique that improves model performance by incorporating natural language inference, showing a positive impact on both Search and Discovery metrics.\""}, {"color": "#7ca0f4", "description": "\"GPL (Generalized Pooling Layer) training approach is another method used to fine-tune models, improving performance in the Search metric but not as much in the Discovery metric.\"", "entity_type": "\"CONCEPT\"", "id": "\"GPL TRAINING APPROACH\"", "label": "\"GPL TRAINING APPROACH\"", "shape": "dot", "size": 10, "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"GPL (Generalized Pooling Layer) training approach is another method used to fine-tune models, improving performance in the Search metric but not as much in the Discovery metric.\""}, {"color": "#3304bd", "description": "\"Elastic-Search is mentioned as a tool used for hosting semantic search systems, developing the necessary language models along the way.\"\u003cSEP\u003e\"Elastic-Search is mentioned as a tool used for hosting semantic search systems, developing the necessary language models along the way.\")", "entity_type": "\"ORGANIZATION\"", "id": "\"ELASTIC-SEARCH\"", "label": "\"ELASTIC-SEARCH\"", "shape": "dot", "size": 10, "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"Elastic-Search is mentioned as a tool used for hosting semantic search systems, developing the necessary language models along the way.\"\u003cSEP\u003e\"Elastic-Search is mentioned as a tool used for hosting semantic search systems, developing the necessary language models along the way.\")"}, {"color": "#6b1b3e", "description": "\"The event of developing necessary language models was part of the project\u0027s tasks, contributing to the overall system.\"", "entity_type": "\"EVENT\"", "id": "\"DEVELOPING THE NECESSARY LANGUAGE MODELS\"", "label": "\"DEVELOPING THE NECESSARY LANGUAGE MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"The event of developing necessary language models was part of the project\u0027s tasks, contributing to the overall system.\""}, {"color": "#597b14", "description": "\"Pre-processing text is a crucial step in handling raw data before performing further tasks.\"", "entity_type": "\"PROCESS\"", "id": "\"PRE-PROCESSING TEXT\"", "label": "\"PRE-PROCESSING TEXT\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Pre-processing text is a crucial step in handling raw data before performing further tasks.\""}, {"color": "#964e9f", "description": "\"The team tackled various problems when pre-processing raw text data, indicating continuous challenges and progress.\"", "entity_type": "\"EVENT\"", "id": "\"TACKLING PROBLEMS\"", "label": "\"TACKLING PROBLEMS\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"The team tackled various problems when pre-processing raw text data, indicating continuous challenges and progress.\""}, {"color": "#e41526", "description": "\"Working with multiple Project IRIS members was an ongoing activity, contributing to the development process.\"", "entity_type": "\"ACTIVITY\"", "id": "\"WORKING WITH MULTIPLE PROJECT IRIS MEMBERS\"", "label": "\"WORKING WITH MULTIPLE PROJECT IRIS MEMBERS\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Working with multiple Project IRIS members was an ongoing activity, contributing to the development process.\""}, {"color": "#6f6900", "description": "\"Developing soft and hard skills by interacting with professionals and doctoral students was a key outcome of the project.\"", "entity_type": "\"EVENT\"", "id": "\"SOFT AND HARD SKILLS DEVELOPMENT\"", "label": "\"SOFT AND HARD SKILLS DEVELOPMENT\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Developing soft and hard skills by interacting with professionals and doctoral students was a key outcome of the project.\""}, {"color": "#51d26d", "description": "\"Effectively researching and collaborating with multiple members on a large project were important activities in the project.\"", "entity_type": "\"ACTIVITY\"", "id": "\"EFFECTIVELY RESEARCHING DAILY AND COOPERATING ON A LARGE PROJECT\"", "label": "\"EFFECTIVELY RESEARCHING DAILY AND COOPERATING ON A LARGE PROJECT\"", "shape": "dot", "size": 10, "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Effectively researching and collaborating with multiple members on a large project were important activities in the project.\""}, {"color": "#8ac682", "description": "\"This is the specific language variant targeted by the model in Albertina, indicating a regional focus.\"", "entity_type": "\"LOCATION\"", "id": "\"EUROPEAN PORTUGUESE\"", "label": "\"EUROPEAN PORTUGUESE\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"This is the specific language variant targeted by the model in Albertina, indicating a regional focus.\""}, {"color": "#c0a389", "description": "\"These are manually annotated data sets crucial for training language models specifically for the Portuguese legal domain, emphasizing their value in ensuring accuracy and relevance.\"", "entity_type": "\"TECHNOLOGY\"", "id": "\"PORTUGUESE LEGAL DOMAIN DATASETS\"", "label": "\"PORTUGUESE LEGAL DOMAIN DATASETS\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"These are manually annotated data sets crucial for training language models specifically for the Portuguese legal domain, emphasizing their value in ensuring accuracy and relevance.\""}, {"color": "#d146be", "description": "\"Models trained on general data versus specific domain data, highlighting their differences in performance.\"", "entity_type": "\"CONCEPT\"", "id": "\"DOMAIN-AGNOSTIC OR DOMAIN-ADAPTED MODELS\"", "label": "\"DOMAIN-AGNOSTIC OR DOMAIN-ADAPTED MODELS\"", "shape": "dot", "size": 10, "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"Models trained on general data versus specific domain data, highlighting their differences in performance.\""}, {"color": "#32694a", "description": "\"brWaC is a corpus created by researchers from Brazil focusing on Brazilian Portuguese language processing.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"BRWAC: A WACKY CORPUS FOR BRAZILIAN PORTUGUESE\"", "label": "\"BRWAC: A WACKY CORPUS FOR BRAZILIAN PORTUGUESE\"", "shape": "dot", "size": 10, "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"brWaC is a corpus created by researchers from Brazil focusing on Brazilian Portuguese language processing.\""}, {"color": "#d35258", "description": "\"Chang is a co-author of BERT alongside Devlin and Lee.\"", "entity_type": "\"PERSON\"", "id": "\"CHANG, M.-W.\"", "label": "\"CHANG, M.-W.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Chang is a co-author of BERT alongside Devlin and Lee.\""}, {"color": "#4017f8", "description": "\"Lee is one of the authors who developed BERT with Devlin and Chang.\"", "entity_type": "\"PERSON\"", "id": "\"LEE, K.\"", "label": "\"LEE, K.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Lee is one of the authors who developed BERT with Devlin and Chang.\""}, {"color": "#63c5bb", "description": "\"Toutanova is a co-author of BERT alongside Devlin, Chang, and Lee.\"", "entity_type": "\"PERSON\"", "id": "\" TOUTANOVA, K.\"", "label": "\" TOUTANOVA, K.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Toutanova is a co-author of BERT alongside Devlin, Chang, and Lee.\""}, {"color": "#a074c8", "description": "\"Raevad is the co-author of Lexrank alongside Erkan.\"", "entity_type": "\"PERSON\"", "id": "\"RAEVAD, D. R.\"", "label": "\"RAEVAD, D. R.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Raevad is the co-author of Lexrank alongside Erkan.\""}, {"color": "#870492", "description": "\"Santos is a co-author of the ASSIN project with Fonseca.\"", "entity_type": "\"PERSON\"", "id": "\"SANTOS, L.\"", "label": "\"SANTOS, L.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Santos is a co-author of the ASSIN project with Fonseca.\""}, {"color": "#c778d5", "description": "\"Criscuolo is an author involved in the ASSIN project for evaluating semantic similarity and textual inference.\"", "entity_type": "\"PERSON\"", "id": "\"CRISCUOLO, M.\"", "label": "\"CRISCUOLO, M.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Criscuolo is an author involved in the ASSIN project for evaluating semantic similarity and textual inference.\""}, {"color": "#d846e6", "description": "\"Aluisio is a co-author of the ASSIN project with Fonseca and Santos.\"", "entity_type": "\"PERSON\"", "id": "\"ALUISIO, S.\"", "label": "\"ALUISIO, S.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Aluisio is a co-author of the ASSIN project with Fonseca and Santos.\""}, {"color": "#387b66", "description": "\"Gururangan is an author who developed Don\u0027t stop pretraining: Adapt language models to domains and tasks.\"\u003cSEP\u003e\"Gururangan is an author who developed Don\u2019t stop pretraining: Adapt language models to domains and tasks.\"", "entity_type": "\"PERSON\"", "id": "\"GURURANGAN, S.\"", "label": "\"GURURANGAN, S.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Gururangan is an author who developed Don\u0027t stop pretraining: Adapt language models to domains and tasks.\"\u003cSEP\u003e\"Gururangan is an author who developed Don\u2019t stop pretraining: Adapt language models to domains and tasks.\""}, {"color": "#9f1744", "description": "\"Marasovic is a co-author of Don\u0027t stop pretraining with Gururangan.\"\u003cSEP\u003e\"Marasovic is a co-author of Don\u2019t stop pretraining with Gururangan.\"", "entity_type": "\"PERSON\"", "id": "\"MARASOVIC, A.\"", "label": "\"MARASOVIC, A.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Marasovic is a co-author of Don\u0027t stop pretraining with Gururangan.\"\u003cSEP\u003e\"Marasovic is a co-author of Don\u2019t stop pretraining with Gururangan.\""}, {"color": "#7772a6", "description": "\"Swayambipta is a co-author of Don\u0027t stop pretraining with Gururangan and Marasovic.\"\u003cSEP\u003e\"Swayambipta is a co-author of Don\u2019t stop pretraining with Gururangan and Marasovic.\"", "entity_type": "\"PERSON\"", "id": "\"SWAYAMBIPTA, S.\"", "label": "\"SWAYAMBIPTA, S.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Swayambipta is a co-author of Don\u0027t stop pretraining with Gururangan and Marasovic.\"\u003cSEP\u003e\"Swayambipta is a co-author of Don\u2019t stop pretraining with Gururangan and Marasovic.\""}, {"color": "#27a1c7", "description": "\"Lo is a co-author of Don\u0027t stop pretraining with Gururangan, Marasovic, and Swayambipta.\"\u003cSEP\u003e\"Lo is a co-author of Don\u2019t stop pretraining with Gururangan, Marasovic, and Swayambipta.\"", "entity_type": "\"PERSON\"", "id": "\"LO, K.\"", "label": "\"LO, K.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Lo is a co-author of Don\u0027t stop pretraining with Gururangan, Marasovic, and Swayambipta.\"\u003cSEP\u003e\"Lo is a co-author of Don\u2019t stop pretraining with Gururangan, Marasovic, and Swayambipta.\""}, {"color": "#7017e1", "description": "\"Beltagy is a co-author of Don\u0027t stop pretraining with Gururangan, Lo, and Swayambipta.\"\u003cSEP\u003e\"Beltagy is a co-author of Don\u2019t stop pretraining with Gururangan, Lo, and Swayambipta.\"", "entity_type": "\"PERSON\"", "id": "\"BELTAGY, I.\"", "label": "\"BELTAGY, I.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Beltagy is a co-author of Don\u0027t stop pretraining with Gururangan, Lo, and Swayambipta.\"\u003cSEP\u003e\"Beltagy is a co-author of Don\u2019t stop pretraining with Gururangan, Lo, and Swayambipta.\""}, {"color": "#39aedf", "description": "\"Downey is a co-author of Don\u0027t stop pretraining with Gururangan, Gururangan, Lo, Marasovic, and Beltagy.\"\u003cSEP\u003e\"Downey is a co-author of Don\u2019t stop pretraining with Gururangan, Lo, Marasovic, and Beltagy.\"", "entity_type": "\"PERSON\"", "id": "\"DOWNEY, D.\"", "label": "\"DOWNEY, D.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Downey is a co-author of Don\u0027t stop pretraining with Gururangan, Gururangan, Lo, Marasovic, and Beltagy.\"\u003cSEP\u003e\"Downey is a co-author of Don\u2019t stop pretraining with Gururangan, Lo, Marasovic, and Beltagy.\""}, {"color": "#5c0914", "description": "\"Smith is the last co-author of Don\u0027t stop pretraining with Downey, Gururangan, Lo, Marasovic, and Beltagy.\"\u003cSEP\u003e\"Smith is the last co-author of Don\u2019t stop pretraining with Downey, Gururangan, Lo, Marasovic, and Beltagy.\"", "entity_type": "\"PERSON\"", "id": "\"SMITH, N. A. D.\"", "label": "\"SMITH, N. A. D.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Smith is the last co-author of Don\u0027t stop pretraining with Downey, Gururangan, Lo, Marasovic, and Beltagy.\"\u003cSEP\u003e\"Smith is the last co-author of Don\u2019t stop pretraining with Downey, Gururangan, Lo, Marasovic, and Beltagy.\""}, {"color": "#49fb21", "description": "\"He is an author involved in DeBERTa: Decoding-enhanced BERT with disentangled attention.\"", "entity_type": "\"PERSON\"", "id": "\"HE, P.\"", "label": "\"HE, P.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"He is an author involved in DeBERTa: Decoding-enhanced BERT with disentangled attention.\""}, {"color": "#90018b", "description": "\"Liu is the co-author of DeBERTa alongside He.\"", "entity_type": "\"PERSON\"", "id": "\"LIU, X.\"", "label": "\"LIU, X.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Liu is the co-author of DeBERTa alongside He.\""}, {"color": "#87ccd0", "description": "\"Gao is a co-author of DeBERTa with He and Liu.\"", "entity_type": "\"PERSON\"", "id": "\"GAO, J.\"", "label": "\"GAO, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Gao is a co-author of DeBERTa with He and Liu.\""}, {"color": "#0f4ce0", "description": "\"Chen is the last co-author of DeBERTa with Gao, He, and Liu.\"", "entity_type": "\"PERSON\"", "id": "\"CHEN, W.\"", "label": "\"CHEN, W.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Chen is the last co-author of DeBERTa with Gao, He, and Liu.\""}, {"color": "#1eecf7", "description": "\"Henderson is an author involved in learning responsible data filtering from a 256GB open-source legal dataset.\"", "entity_type": "\"PERSON\"", "id": "\"HENDERSON, P.\"", "label": "\"HENDERSON, P.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Henderson is an author involved in learning responsible data filtering from a 256GB open-source legal dataset.\""}, {"color": "#ef8ef7", "description": "\"Kras is the co-author of Pile of Law with Henderson, Zheng, Guha, Manning, Jurafsky, and Ho.\"", "entity_type": "\"PERSON\"", "id": "\"KRAS, M. S.\"", "label": "\"KRAS, M. S.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Kras is the co-author of Pile of Law with Henderson, Zheng, Guha, Manning, Jurafsky, and Ho.\""}, {"color": "#910cce", "description": "\"Zheng is a co-author of Pile of Law alongside Henderson, Kras, Manning, Jurafsky, and Ho.\"", "entity_type": "\"PERSON\"", "id": "\"ZHENG, L.\"", "label": "\"ZHENG, L.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Zheng is a co-author of Pile of Law alongside Henderson, Kras, Manning, Jurafsky, and Ho.\""}, {"color": "#67a65b", "description": "\"Guha is the co-author of Pile of Law with Henderson, Kras, Zheng, Manning, and Jurafsky.\"", "entity_type": "\"PERSON\"", "id": "\"GUHA, N.\"", "label": "\"GUHA, N.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Guha is the co-author of Pile of Law with Henderson, Kras, Zheng, Manning, and Jurafsky.\""}, {"color": "#1de59c", "description": "\"Manning is a co-author of Pile of Law with Henderson, Guha, Kras, Zheng, and Jurafsky.\"", "entity_type": "\"PERSON\"", "id": "\"MANNING, C. D.\"", "label": "\"MANNING, C. D.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Manning is a co-author of Pile of Law with Henderson, Guha, Kras, Zheng, and Jurafsky.\""}, {"color": "#e69fb3", "description": "\"Jurafsky is the co-author of Pile of Law with Henderson, Guha, Manning, Kras, and Ho.\"", "entity_type": "\"PERSON\"", "id": "\"JURAFSKY, D.\"", "label": "\"JURAFSKY, D.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Jurafsky is the co-author of Pile of Law with Henderson, Guha, Manning, Kras, and Ho.\""}, {"color": "#9ad93a", "description": "\"Ho is a co-author of Pile of Law alongside Henderson, Guha, Jurafsky, Kras, and Manning.\"", "entity_type": "\"PERSON\"", "id": "\"HO, D. E.\"", "label": "\"HO, D. E.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Ho is a co-author of Pile of Law alongside Henderson, Guha, Jurafsky, Kras, and Manning.\""}, {"color": "#fc594b", "description": "\"IJITEE is an international journal for innovative technology exploration.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"IJITEE (INTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY AND EXPLORING ENGINEERING)\"", "label": "\"IJITEE (INTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY AND EXPLORING ENGINEERING)\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"IJITEE is an international journal for innovative technology exploration.\""}, {"color": "#a3f0c5", "description": "\"Vaishnav is an author who contributed to an AI-based analysis in the legal domain with Deepalakshmi.\"", "entity_type": "\"PERSON\"", "id": "\"VAISHNAV, V.\"", "label": "\"VAISHNAV, V.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Vaishnav is an author who contributed to an AI-based analysis in the legal domain with Deepalakshmi.\""}, {"color": "#516630", "description": "\"Deepalakshmi is a co-author of an AI-based analysis in the legal domain with Vaishnav.\"", "entity_type": "\"PERSON\"", "id": "\"DEEPALAKSHMI, P.\"", "label": "\"DEEPALAKSHMI, P.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Deepalakshmi is a co-author of an AI-based analysis in the legal domain with Vaishnav.\""}, {"color": "#bd0f6b", "description": "\"Rabelo is a co-author with Kim for their work on BM25 and transformer-based legal information extraction and entailment.\"", "entity_type": "\"PERSON\"", "id": "\"RABELO, J.\"", "label": "\"RABELO, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Rabelo is a co-author with Kim for their work on BM25 and transformer-based legal information extraction and entailment.\""}, {"color": "#ca1710", "description": "\"Goebel is the third author of BM25 and transformer-based legal information extraction and entailment alongside Kim and Rabelo.\"", "entity_type": "\"PERSON\"", "id": "\"GOEBEL, R.\"", "label": "\"GOEBEL, R.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Goebel is the third author of BM25 and transformer-based legal information extraction and entailment alongside Kim and Rabelo.\""}, {"color": "#e4d695", "description": "\"Kim is an author who worked on research paper classification systems based on TF-IDF and LDA schemes.\"", "entity_type": "\"PERSON\"", "id": "\"KIM, S.-W.\"", "label": "\"KIM, S.-W.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Kim is an author who worked on research paper classification systems based on TF-IDF and LDA schemes.\""}, {"color": "#462ec0", "description": "\"Gil is the co-author of Kim for their work on research paper classification systems based on TF-IDF and LDA schemes.\"", "entity_type": "\"PERSON\"", "id": "\"GIL, J.-M.\"", "label": "\"GIL, J.-M.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Gil is the co-author of Kim for their work on research paper classification systems based on TF-IDF and LDA schemes.\""}, {"color": "#fac98a", "description": "\"Kingma is an author who developed Adam: A method for stochastic optimization.\"", "entity_type": "\"PERSON\"", "id": "\"KINGMA, D. P.\"", "label": "\"KINGMA, D. P.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Kingma is an author who developed Adam: A method for stochastic optimization.\""}, {"color": "#2ab52f", "description": "\"Ba is the co-author of Kingma for their work on Adam: A method for stochastic optimization.\"", "entity_type": "\"PERSON\"", "id": "\"BA, J.\"", "label": "\"BA, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Ba is the co-author of Kingma for their work on Adam: A method for stochastic optimization.\""}, {"color": "#e08a39", "description": "\"Marelli is an author involved in the SICK cure project.\"", "entity_type": "\"PERSON\"", "id": "\"MARELLI, M.\"", "label": "\"MARELLI, M.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Marelli is an author involved in the SICK cure project.\""}, {"color": "#c1d708", "description": "\"Menini is a co-author of Marelli for their work on the SICK cure project.\"", "entity_type": "\"PERSON\"", "id": "\"MENINI, S.\"", "label": "\"MENINI, S.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Menini is a co-author of Marelli for their work on the SICK cure project.\""}, {"color": "#9f41ef", "description": "\"Baroni is an author involved in the SICK cure project with Marelli and Menini.\"", "entity_type": "\"PERSON\"", "id": "\"BARONI, M.\"", "label": "\"BARONI, M.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Baroni is an author involved in the SICK cure project with Marelli and Menini.\""}, {"color": "#9e9a55", "description": "\"Bentivogli is a co-author of the SICK cure project with Baroni, Marelli, and Menini.\"", "entity_type": "\"PERSON\"", "id": "\"BENTIVOGLI, L.\"", "label": "\"BENTIVOGLI, L.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Bentivogli is a co-author of the SICK cure project with Baroni, Marelli, and Menini.\""}, {"color": "#721dac", "description": "\"Bernardi is a co-author of the SICK cure project with Bentivogli, Baroni, Marelli, and Menini.\"", "entity_type": "\"PERSON\"", "id": "\"BERNARDI, R.\"", "label": "\"BERNARDI, R.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Bernardi is a co-author of the SICK cure project with Bentivogli, Baroni, Marelli, and Menini.\""}, {"color": "#6d377b", "description": "\"Zamparelli is a co-author of the SICK cure project with Bernardi, Bentivogli, Baroni, Marelli, and Menini.\"", "entity_type": "\"PERSON\"", "id": "\"ZAMPARELLI, R.\"", "label": "\"ZAMPARELLI, R.\"", "shape": "dot", "size": 10, "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Zamparelli is a co-author of the SICK cure project with Bernardi, Bentivogli, Baroni, Marelli, and Menini.\""}, {"color": "#cfed4b", "description": "\"Y. Bengio and Y. LeCun are editors of conference proceedings from ICLR 2013.\"\u003cSEP\u003e\"Y. Bengio and Y. LeCun are editors of conference proceedings from ICLR 2015.\"", "entity_type": "\"PERSON\"", "id": "\"Y. BENGIO AND Y. LECUN\"", "label": "\"Y. BENGIO AND Y. LECUN\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"Y. Bengio and Y. LeCun are editors of conference proceedings from ICLR 2013.\"\u003cSEP\u003e\"Y. Bengio and Y. LeCun are editors of conference proceedings from ICLR 2015.\""}, {"color": "#f2ad6a", "description": "\"ICLR 2013 is an event held in Scottsdale, Arizona from May 2-4, 2013.\"", "entity_type": "\"EVENT\"", "id": "\"1ST INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS, ICLR 2013\"", "label": "\"1ST INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS, ICLR 2013\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"ICLR 2013 is an event held in Scottsdale, Arizona from May 2-4, 2013.\""}, {"color": "#289607", "description": "\"New Frontiers in Artificial Intelligence is an organization that published a book containing the research by S. K. Nigam et al.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"NEW FRONTIERS IN ARTIFICIAL INTELLIGENCE (CHAM, 2023)\"", "label": "\"NEW FRONTIERS IN ARTIFICIAL INTELLIGENCE (CHAM, 2023)\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"New Frontiers in Artificial Intelligence is an organization that published a book containing the research by S. K. Nigam et al.\""}, {"color": "#070b92", "description": "\"These individuals are members of the JNLP team.\"", "entity_type": "\"PERSON\"", "id": "\"H. NGUYEN, H. T. VUONG, P. M. NGUYEN, T. B. DANG, Q. M. BU, V. T. SINH, C. M. NGUYEN, V. D. TRAN, K. SATO, AND M. L. NGUYEN\"", "label": "\"H. NGUYEN, H. T. VUONG, P. M. NGUYEN, T. B. DANG, Q. M. BU, V. T. SINH, C. M. NGUYEN, V. D. TRAN, K. SATO, AND M. L. NGUYEN\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"These individuals are members of the JNLP team.\""}, {"color": "#bed86e", "description": "\"These individuals are authors of Efficient Estimation of Word Representations in Vector Space paper.\"", "entity_type": "\"PERSON\"", "id": "\"T. MIKOLOV, K. CHEN, G. CORRADO, AND J. DEAN\"", "label": "\"T. MIKOLOV, K. CHEN, G. CORRADO, AND J. DEAN\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"These individuals are authors of Efficient Estimation of Word Representations in Vector Space paper.\""}, {"color": "#dcbc6a", "description": "\"Raffel et al. is a group that developed the Unified Text-to-Text Transformer model.\"", "entity_type": "\"ORGANIZATION\"", "id": "\"RAFFEL ET AL.\"", "label": "\"RAFFEL ET AL.\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"Raffel et al. is a group that developed the Unified Text-to-Text Transformer model.\""}, {"color": "#2e8bed", "description": "\"These individuals are members of the team that worked on ASIN 2 shared task.\"", "entity_type": "\"PERSON\"", "id": "\"C. REAL, E. FONSECA, AND H. G. OLIVEIRA\"", "label": "\"C. REAL, E. FONSECA, AND H. G. OLIVEIRA\"", "shape": "dot", "size": 10, "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"These individuals are members of the team that worked on ASIN 2 shared task.\""}, {"color": "#75af29", "description": "\"ROBERTSON, S. co-authors the Probabilistic Relevance Framework: BM25 and Beyond with Zaragoza, H.\"", "entity_type": "\"PERSON\"", "id": "\"ROBERTSON, S.\"", "label": "\"ROBERTSON, S.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"ROBERTSON, S. co-authors the Probabilistic Relevance Framework: BM25 and Beyond with Zaragoza, H.\""}, {"color": "#6250ac", "description": "\"ZARAGOZA, H. co-authors the Probabilistic Relevance Framework: BM25 and Beyond with Robertson, S.\"", "entity_type": "\"PERSON\"", "id": "\"ZARAGOZA, H.\"", "label": "\"ZARAGOZA, H.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"ZARAGOZA, H. co-authors the Probabilistic Relevance Framework: BM25 and Beyond with Robertson, S.\""}, {"color": "#56b155", "description": "\"RODRIGUES, J. is involved in advancing neural encoding of Portuguese with Transformer Albertina PT-*\"", "entity_type": "\"PERSON\"", "id": "\"RODRIGUES, J.\"", "label": "\"RODRIGUES, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"RODRIGUES, J. is involved in advancing neural encoding of Portuguese with Transformer Albertina PT-*\""}, {"color": "#23f05c", "description": "\"GOMES, L. collaborates with Rodrigues, J., Silva, J., Branco, A., Santos, R., Cardoso, H. L., and Os\u00f3rio, T.\"", "entity_type": "\"PERSON\"", "id": "\"GOMES, L.\"", "label": "\"GOMES, L.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"GOMES, L. collaborates with Rodrigues, J., Silva, J., Branco, A., Santos, R., Cardoso, H. L., and Os\u00f3rio, T.\""}, {"color": "#bf3d28", "description": "\"SILVA, J. collaborates with Rodrigues, J., Gomes, L., Branco, A., Santos, R., Cardoso, H. L., and Os\u00f3rio, R.\"\u003cSEP\u003e\"SILVA, J. collaborates with Rodrigues, J., Gomes, L., Branco, A., Santos, R., Cardoso, H. L., and Os\u00f3rio, T.\"", "entity_type": "\"PERSON\"", "id": "\"SILVA, J.\"", "label": "\"SILVA, J.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"SILVA, J. collaborates with Rodrigues, J., Gomes, L., Branco, A., Santos, R., Cardoso, H. L., and Os\u00f3rio, R.\"\u003cSEP\u003e\"SILVA, J. collaborates with Rodrigues, J., Gomes, L., Branco, A., Santos, R., Cardoso, H. L., and Os\u00f3rio, T.\""}, {"color": "#2174f2", "description": "\"BRANCO, A. collaborates with Rodrigues, J., Gomes, L., Silva, J., Santos, R., Cardoso, H. L., and Os\u00f3rio, T.\"", "entity_type": "\"PERSON\"", "id": "\"BRANCO, A.\"", "label": "\"BRANCO, A.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"BRANCO, A. collaborates with Rodrigues, J., Gomes, L., Silva, J., Santos, R., Cardoso, H. L., and Os\u00f3rio, T.\""}, {"color": "#6c68ae", "description": "\"SANTOS, R. collaborates with Rodrigues, J., Gomes, L., Branco, A., Silva, J., Cardoso, H. L., and Os\u00f3rio, R.\"", "entity_type": "\"PERSON\"", "id": "\"SANTOS, R.\"", "label": "\"SANTOS, R.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"SANTOS, R. collaborates with Rodrigues, J., Gomes, L., Branco, A., Silva, J., Cardoso, H. L., and Os\u00f3rio, R.\""}, {"color": "#3734b8", "description": "\"CARDOSO, H. L. collaborates with Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., and Os\u00f3rio, T.\"", "entity_type": "\"PERSON\"", "id": "\"CARDOSO, H. L.\"", "label": "\"CARDOSO, H. L.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"CARDOSO, H. L. collaborates with Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., and Os\u00f3rio, T.\""}, {"color": "#20a769", "description": "\"OS\u00b4ORIO, T. collaborates with Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., and Cardoso, H. L.\"", "entity_type": "\"PERSON\"", "id": "\"OS\u00b4ORIO, T.\"", "label": "\"OS\u00b4ORIO, T.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"OS\u00b4ORIO, T. collaborates with Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., and Cardoso, H. L.\""}, {"color": "#701ed0", "description": "\"RONG, X. is an author of a paper explaining Word2Vec parameter learning.\"", "entity_type": "\"PERSON\"", "id": "\"RONG, X.\"", "label": "\"RONG, X.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"RONG, X. is an author of a paper explaining Word2Vec parameter learning.\""}, {"color": "#5c69a5", "description": "\"V ASWANI, A. is an author of the Attention is All You Need paper.\"", "entity_type": "\"PERSON\"", "id": "\"V ASWANI, A.\"", "label": "\"V ASWANI, A.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"V ASWANI, A. is an author of the Attention is All You Need paper.\""}, {"color": "#8fb71c", "description": "\"J ONES, L. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., and U Szkoreit, J.\"", "entity_type": "\"PERSON\"", "id": "\"J ONES, L.\"", "label": "\"J ONES, L.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"J ONES, L. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., and U Szkoreit, J.\""}, {"color": "#38cdd2", "description": "\"G OMEZ, A. N. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., and U Szkoreit, J.\"", "entity_type": "\"PERSON\"", "id": "\"G OMEZ, A. N.\"", "label": "\"G OMEZ, A. N.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"G OMEZ, A. N. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., and U Szkoreit, J.\""}, {"color": "#ce83d5", "description": "\"K AISER, L.U. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., and U Szkoreit, J.\"", "entity_type": "\"PERSON\"", "id": "\"K AISER, L.U.\"", "label": "\"K AISER, L.U.\"", "shape": "dot", "size": 10, "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"K AISER, L.U. co-authors the Attention is All You Need paper with Vaswani, A., Shazeer, N., Parmar, N., Jones, L., and U Szkoreit, J.\""}, {"color": "#a8d74c", "description": "\"\u0027Attention is all you need\u0027 is a title of a paper by N., K. Kaiser, L. U. Polosukhin.\"", "entity_type": "\"EVENT\"", "id": "\"ATTENTION IS ALL YOU NEED\"", "label": "\"ATTENTION IS ALL YOU NEED\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"\u0027Attention is all you need\u0027 is a title of a paper by N., K. Kaiser, L. U. Polosukhin.\""}, {"color": "#781228", "description": "\"\u0027Advances in Neural Information Processing Systems (2017)\u0027 is the name of the journal where \u0027Attention is all you need\u0027 was published.\"", "entity_type": "\"LOCATION\"", "id": "\"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (2017)\"", "label": "\"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (2017)\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"\u0027Advances in Neural Information Processing Systems (2017)\u0027 is the name of the journal where \u0027Attention is all you need\u0027 was published.\""}, {"color": "#f8456d", "description": "\"Punta Cana, Dominican Republic is a location associated with an event where one of the papers was presented.\"", "entity_type": "\"LOCATION\"", "id": "\"PUNTA CANA, DOMINICAN REPUBLIC\"", "label": "\"PUNTA CANA, DOMINICAN REPUBLIC\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"Punta Cana, Dominican Republic is a location associated with an event where one of the papers was presented.\""}, {"color": "#2abeb6", "description": "\"New Orleans, Louisiana is a location associated with another event where a paper was presented.\"", "entity_type": "\"LOCATION\"", "id": "\"NEW ORLEANS, LOUISIANA\"", "label": "\"NEW ORLEANS, LOUISIANA\"", "shape": "dot", "size": 10, "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"New Orleans, Louisiana is a location associated with another event where a paper was presented.\""}]);
                  edges = new vis.DataSet([{"description": "\"The thesis aims to develop a Semantic Search System for the Supremo Tribunal de Justi\u00e7a, assisting it in its decision-making process.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTI\u00c7A\"", "keywords": "\"research collaboration, legal support\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"The thesis aims to develop a Semantic Search System for the Supremo Tribunal de Justi\u00e7a, assisting it in its decision-making process.\"", "to": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "width": 8.0}, {"description": "\"The thesis aims to develop a Semantic Search System for the Supremo Tribunal de Justi\u00e7a.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTI\u00c7A\"", "keywords": "\"legal support, research focus\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"The thesis aims to develop a Semantic Search System for the Supremo Tribunal de Justi\u00e7a.\"", "to": "\"THESIS TO OBTAIN THE MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING\"", "width": 8.0}, {"description": "\"Chapter 5 mentions legal documents related to the Supremo Tribunal de Justi\u00e7a, indicating a thematic relationship in the document corpus.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTI\u00c7A\"", "keywords": "\"legal system, jurisdiction\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Chapter 5 mentions legal documents related to the Supremo Tribunal de Justi\u00e7a, indicating a thematic relationship in the document corpus.\"", "to": "\"CHAPTER 5\"", "width": 7.0}, {"description": "\"Pedro Alexandre Sim\u00f5es dos Santos supervised Rui Filipe Coimbra Pereira de Melo throughout his thesis work.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"supervision, mentorship\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Pedro Alexandre Sim\u00f5es dos Santos supervised Rui Filipe Coimbra Pereira de Melo throughout his thesis work.\"", "to": "\"PROF. PEDRO ALEXANDRE SIM\u00d5ES DOS SANTOS\"", "width": 9.0}, {"description": "\"Jo\u00e3o Miguel De Sousa de Assis Dias was also involved in guiding Rui Filipe Coimbra Pereira de Melo\u0027s thesis work.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"collaboration, supervision\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Jo\u00e3o Miguel De Sousa de Assis Dias was also involved in guiding Rui Filipe Coimbra Pereira de Melo\u0027s thesis work.\"", "to": "\"PROF. JO\u00c3O MIGUEL DE SOUSA DE ASSIS DIAS\"", "width": 9.0}, {"description": "\"Maria Lu\u00edsa Torres Ribeiro Marques da Silva Coheur chaired the Examination Committee for his thesis.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"committee leadership, examination oversight\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Maria Lu\u00edsa Torres Ribeiro Marques da Silva Coheur chaired the Examination Committee for his thesis.\"", "to": "\"PROF. MARIA LU\u00cdSA TORRES RIBEIRO MARQUES DA SILVA COHEUR\"", "width": 8.0}, {"description": "\"Jos\u00e9 Lu\u00eds Brinquete Borbinha was a member of the Examination Committee who reviewed Rui Filipe Coimbra Pereira de Melo\u0027s thesis.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"committee participation, examination review\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Jos\u00e9 Lu\u00eds Brinquete Borbinha was a member of the Examination Committee who reviewed Rui Filipe Coimbra Pereira de Melo\u0027s thesis.\"", "to": "\"PROF. JOS\u00c9 LU\u00cdS BRINQUETE BORBINHA\"", "width": 8.0}, {"description": "\"The thesis is Rui Filipe Coimbra Pereira de Melo\u0027s work towards obtaining a Master of Science Degree in Computer Science and Engineering.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"academic pursuit, research work\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"The thesis is Rui Filipe Coimbra Pereira de Melo\u0027s work towards obtaining a Master of Science Degree in Computer Science and Engineering.\"", "to": "\"THESIS TO OBTAIN THE MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING\"", "width": 9.0}, {"description": "\"Rui Filipe Coimbra Pereira de Melo collaborated with Project IRIS members on a more significant project.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"team collaboration, shared experience\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Rui Filipe Coimbra Pereira de Melo collaborated with Project IRIS members on a more significant project.\"", "to": "\"PROJECT IRIS MEMBERS\"", "width": 8.0}, {"description": "\"Metadata Knowledge Distillation is part of Rui Filipe Coimbra Pereira de Melo\u0027s research work in his thesis.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"research focus, innovation\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Metadata Knowledge Distillation is part of Rui Filipe Coimbra Pereira de Melo\u0027s research work in his thesis.\"", "to": "\"METADATA KNOWLEDGE DISTILLATION\"", "width": 8.0}, {"description": "\"Rui Filipe Coimbra Pereira de Melo utilized Legal-BERTimbau variants in his Semantic Search System.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"technology use, research focus\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Rui Filipe Coimbra Pereira de Melo utilized Legal-BERTimbau variants in his Semantic Search System.\"", "to": "\"LEGAL-BERTIMBAU VARIANTS\"", "width": 9.0}, {"description": "\"Rui Filipe Coimbra Pereira de Melo implemented Hybrid Search Systems in his research work.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"technological innovation, research focus\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Rui Filipe Coimbra Pereira de Melo implemented Hybrid Search Systems in his research work.\"", "to": "\"HYBRID SEARCH SYSTEMS\"", "width": 9.0}, {"description": "\"Rui Filipe Coimbra Pereira de Melo compared his Semantic Search System with BM25.\"", "from": "\"RUI FILIPE COIMBRA PEREIRA DE MELO\"", "keywords": "\"methodology comparison, research focus\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Rui Filipe Coimbra Pereira de Melo compared his Semantic Search System with BM25.\"", "to": "\"BM25\"", "width": 8.0}, {"description": "\"Both professors are involved in supervising Rui Filipe Coimbra Pereira de Melo\u0027s thesis work.\"", "from": "\"PROF. PEDRO ALEXANDRE SIM\u00d5ES DOS SANTOS\"", "keywords": "\"collaborative supervision, mentorship\"", "source_id": "chunk-61af9c4cbc05bef525950862cfae5a86", "title": "\"Both professors are involved in supervising Rui Filipe Coimbra Pereira de Melo\u0027s thesis work.\"", "to": "\"PROF. JO\u00c3O MIGUEL DE SOUSA DE ASSIS DIAS\"", "width": 9.0}, {"description": "\"Both Multilingual and Metadata Knowledge Distillation involve knowledge distillation techniques but focus on different aspects of data.\"", "from": "\"METADATA KNOWLEDGE DISTILLATION\"", "keywords": "\u003crelationship_keywords\u003e", "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Both Multilingual and Metadata Knowledge Distillation involve knowledge distillation techniques but focus on different aspects of data.\"", "to": "\"MULTILINGUAL KNOWLEDGE DISTILLATION\"", "width": 21.0}, {"description": "\"The Legal-BERTimbau variants are part of the system being evaluated for their effectiveness in court settings.\"", "from": "\"LEGAL-BERTIMBAU VARIANTS\"", "keywords": "\"system component, evaluation focus\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"The Legal-BERTimbau variants are part of the system being evaluated for their effectiveness in court settings.\"", "to": "\"SYSTEM EVALUATION\"", "width": 8.0}, {"description": "\"The Systematic Search prototype outperforms BM25 by 335% in relevant passage retrieval.\"", "from": "\"BM25\"", "keywords": "\"performance comparison, information retrieval\"", "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"The Systematic Search prototype outperforms BM25 by 335% in relevant passage retrieval.\"", "to": "\"SYSTEMATIC SEARCH PROTOTYPE\"", "width": 27.0}, {"description": "\"The Lexical-First Search System employs BM25 as an initial filtering step before applying cosine similarity metrics to rank search results.\"", "from": "\"BM25\"", "keywords": "\"filtering, ranking\"", "source_id": "chunk-338cf6d446307fa476c0b025098bcd87", "title": "\"The Lexical-First Search System employs BM25 as an initial filtering step before applying cosine similarity metrics to rank search results.\"", "to": "\"LEXICAL-FIRST SEARCH SYSTEM\"", "width": 16.0}, {"description": "\"In the Lexical + Semantic Search System, BM25 scores are normalized and combined with cosine similarity values from Legal-BERTimbau for final result ordering.\"", "from": "\"BM25\"", "keywords": "\"score normalization, hybrid retrieval\"", "source_id": "chunk-338cf6d446307fa476c0b025098bcd87", "title": "\"In the Lexical + Semantic Search System, BM25 scores are normalized and combined with cosine similarity values from Legal-BERTimbau for final result ordering.\"", "to": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "width": 16.0}, {"description": "\"Search System Evaluation involves comparing the performance of the developed solution against BM25 to assess its effectiveness.\"", "from": "\"BM25\"", "keywords": "\"comparison, evaluation method\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"Search System Evaluation involves comparing the performance of the developed solution against BM25 to assess its effectiveness.\"", "to": "\"SEARCH SYSTEM EVALUATION\"", "width": 14.0}, {"description": "\"BM25 outperforms the Semantic Search System in several metrics, particularly in identifying query sources more effectively.\"", "from": "\"BM25\"", "keywords": "\u003c\"performance comparison, superior metric\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"BM25 outperforms the Semantic Search System in several metrics, particularly in identifying query sources more effectively.\"", "to": "\"SEMANTIC SEARCH SYSTEM\"", "width": 18.0}, {"description": "\"The Lexical-First approach performs closer to BM25 and can even surpass it in some evaluations, indicating a similar or better performance level.\"", "from": "\"BM25\"", "keywords": "\u003c\"approach comparison, performance similarity\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"The Lexical-First approach performs closer to BM25 and can even surpass it in some evaluations, indicating a similar or better performance level.\"", "to": "\"LEXICAL-FIRST APPROACH\"", "width": 16.0}, {"description": "\"The Lexical+Semantic approach can occasionally outperform BM25, suggesting that the combination of lexical and semantic techniques is competitive with BM25\u0027s capabilities.\"", "from": "\"BM25\"", "keywords": "\u003c\"approach comparison, performance competition\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"The Lexical+Semantic approach can occasionally outperform BM25, suggesting that the combination of lexical and semantic techniques is competitive with BM25\u0027s capabilities.\"", "to": "\"LEXICAL+SEMANTIC APPROACH\"", "width": 14.0}, {"description": "\"Models fine-tuned on the custom STS dataset perform marginally worse than those fine-tuned on pre-existing datasets, indicating a difference in model performance based on training data.\"", "from": "\"BM25\"", "keywords": "\u003c\"training impact, performance variation\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Models fine-tuned on the custom STS dataset perform marginally worse than those fine-tuned on pre-existing datasets, indicating a difference in model performance based on training data.\"", "to": "\"CUSTOM STS DATASET (V1 MODELS)\"", "width": 12.0}, {"description": "\"Models fine-tuned on pre-existing and manually annotated datasets show better performance than those fine-tuned on the custom STS dataset, indicating superior model performance with traditional training methods.\"", "from": "\"BM25\"", "keywords": "\u003c\"training impact, performance comparison\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Models fine-tuned on pre-existing and manually annotated datasets show better performance than those fine-tuned on the custom STS dataset, indicating superior model performance with traditional training methods.\"", "to": "\"PRE-EXISTING AND MANUALLY ANNOTATED DATASETS (V0 MODELS)\"", "width": 16.0}, {"description": "\"A Hybrid Search System can match and outperform BM25 in some capabilities, showing its advanced nature compared to BM25.\"", "from": "\"BM25\"", "keywords": "\u003c\"system comparison, advanced capabilities\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"A Hybrid Search System can match and outperform BM25 in some capabilities, showing its advanced nature compared to BM25.\"", "to": "\"HYBRID SEARCH SYSTEM\"", "width": 14.0}, {"description": "\"The BM25 model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The BM25 model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "from": "\"BM25\"", "keywords": "\"performance comparison, metrics evaluation\"\u003cSEP\u003e\u003c\"performance comparison, metrics evaluation\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The BM25 model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The BM25 model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "to": "\"TOP 1 TOP 2 TOP 3 TOP 5 TOP 10\"", "width": 16.0}, {"description": "\"The BM25 model is compared against the Lexical + Semantic Search System across different top results metrics, showing similar performance in some cases and less in others.\"::", "from": "\"BM25\"", "keywords": "\u003c\"performance comparison, architectural comparison\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The BM25 model is compared against the Lexical + Semantic Search System across different top results metrics, showing similar performance in some cases and less in others.\"::", "to": "\"LEXICAL + SEMANTIC\"", "width": 14.0}, {"description": "\"The Systematic Search prototype was developed to assist the Supremo Tribunal de Justi\u00e7a portugu\u00eas in its decision-making process.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTI\u00c7A PORTUGU\u00caS\"", "keywords": "\"assistance, decision support\"", "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"The Systematic Search prototype was developed to assist the Supremo Tribunal de Justi\u00e7a portugu\u00eas in its decision-making process.\"", "to": "\"SYSTEMATIC SEARCH PROTOTYPE\"", "width": 18.0}, {"description": "\"The Systematic Search prototype utilizes the Legal-BERTimbau model for improved semantic search capabilities.\"", "from": "\"SYSTEMATIC SEARCH PROTOTYPE\"", "keywords": "\"model integration, semantic search\"", "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"The Systematic Search prototype utilizes the Legal-BERTimbau model for improved semantic search capabilities.\"", "to": "\"LEGAL-BERTIMBAU\"", "width": 24.0}, {"description": "\"Alex is involved in developing and leading the Systematic Search prototype.\"", "from": "\"SYSTEMATIC SEARCH PROTOTYPE\"", "keywords": "\"leadership, development\"", "source_id": "chunk-206731ccea8c9feb0ba9feef0468e18a", "title": "\"Alex is involved in developing and leading the Systematic Search prototype.\"", "to": "\"ALEX\"", "width": 20.0}, {"description": "\"BERTimbau serves as the foundation for Legal-BERTimbau, providing the basis for its adaptations and fine-tuning.\"\u003cSEP\u003e\"Legal-BERTimbau is an adaptation of BERTimbau specifically designed for legal domain tasks.\"\u003cSEP\u003e\"Legal-BERTimbau is built on top of BERTimbau, which provides the initial training and adaptation for the Portuguese language.\"\u003c", "from": "\"LEGAL-BERTIMBAU\"", "keywords": "\"adaptation, fine-tuning\"\u003cSEP\u003e\"foundation, adaptation\"\u003cSEP\u003ebase model, adaptation", "source_id": "chunk-813c546217d2864adf1fc0789841ad36\u003cSEP\u003echunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"BERTimbau serves as the foundation for Legal-BERTimbau, providing the basis for its adaptations and fine-tuning.\"\u003cSEP\u003e\"Legal-BERTimbau is an adaptation of BERTimbau specifically designed for legal domain tasks.\"\u003cSEP\u003e\"Legal-BERTimbau is built on top of BERTimbau, which provides the initial training and adaptation for the Portuguese language.\"\u003c", "to": "\"BERTIMBAU\"", "width": 50.0}, {"description": "\"While Albertina PT -BR outperforms BERTimbau in certain tasks, both models are part of the broader semantic search system.\"::", "from": "\"LEGAL-BERTIMBAU\"", "keywords": "\"model comparison, system integration\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"While Albertina PT -BR outperforms BERTimbau in certain tasks, both models are part of the broader semantic search system.\"::", "to": "\"ALBERTINA PT -BR\"", "width": 12.0}, {"description": "\"HuggingFace provides access to the Legal-BERTimbau model used in the Lexical + Semantic Search System.\"", "from": "\"LEGAL-BERTIMBAU\"", "keywords": "\"model access, system integration\"", "source_id": "chunk-338cf6d446307fa476c0b025098bcd87", "title": "\"HuggingFace provides access to the Legal-BERTimbau model used in the Lexical + Semantic Search System.\"", "to": "\"HUGGINGFACE\"", "width": 9.0}, {"description": "\"The SBERT model Legal-BERTimbau is a specific application of Sentence-BERT within the broader Legal-BERTimbau framework.\"\u003c", "from": "\"LEGAL-BERTIMBAU\"", "keywords": "application, adaptation", "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"The SBERT model Legal-BERTimbau is a specific application of Sentence-BERT within the broader Legal-BERTimbau framework.\"\u003c", "to": "\"SBERT MODEL\"", "width": 16.0}, {"description": "\"BrWaC serves as the foundation for BERTimbau and indirectly influences the development of Legal-BERTimbau through its extensive Portuguese language data.\"\u003c", "from": "\"LEGAL-BERTIMBAU\"", "keywords": "data source, base model", "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"BrWaC serves as the foundation for BERTimbau and indirectly influences the development of Legal-BERTimbau through its extensive Portuguese language data.\"\u003c", "to": "\"BRWAC\"", "width": 14.0}, {"description": "\"The Legal-BERTimbau model is evaluated using the Search metric, which measures its ability to retrieve relevant documents based on query.\"", "from": "\"LEGAL-BERTIMBAU\"", "keywords": "\u003c\"model evaluation, search performance\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Legal-BERTimbau model is evaluated using the Search metric, which measures its ability to retrieve relevant documents based on query.\"", "to": "\"SEARCH METRIC\"", "width": 16.0}, {"description": "\"The Discovery metric evaluates how well Legal-BERTimbau can retrieve additional relevant documents beyond the original ones.\"", "from": "\"LEGAL-BERTIMBAU\"", "keywords": "\u003c\"document retrieval, relevance\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Discovery metric evaluates how well Legal-BERTimbau can retrieve additional relevant documents beyond the original ones.\"", "to": "\"DISCOVERY METRIC\"", "width": 14.0}, {"description": "\"The Legal-BERTimbau model shows worse performance than the Semantic Search System when used in a multilingual context, indicating its inferiority in this setup.\"\u003cSEP\u003e\"The Semantic Search System utilizes Legal-BERTimbau models, which were specifically fine-tuned for the Portuguese legal domain.\"", "from": "\"LEGAL-BERTIMBAU\"", "keywords": "\u003c\"model comparison, poor performance\"\u003cSEP\u003e\u003csource_entity\u003e", "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051\u003cSEP\u003echunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"The Legal-BERTimbau model shows worse performance than the Semantic Search System when used in a multilingual context, indicating its inferiority in this setup.\"\u003cSEP\u003e\"The Semantic Search System utilizes Legal-BERTimbau models, which were specifically fine-tuned for the Portuguese legal domain.\"", "to": "\"SEMANTIC SEARCH SYSTEM\"", "width": 13.0}, {"description": "\"Alex leads the team that might be making the First Contact with the unknown intelligence.\"", "from": "\"ALEX\"", "keywords": "\u003c\"leadership, exploration\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Alex leads the team that might be making the First Contact with the unknown intelligence.\"", "to": "\"FIRST CONTACT\"", "width": 10.0}, {"description": "\"Alex and his team are the key figures in Humanity\u0027s Response to the unknown intelligence.\"", "from": "\"ALEX\"", "keywords": "\u003c\"collective action, cosmic significance\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Alex and his team are the key figures in Humanity\u0027s Response to the unknown intelligence.\"", "to": "\"HUMANITY\u0027S RESPONSE\"", "width": 8.0}, {"description": "\"Both Alex and Sam Rivera are key members of the team involved in the development of the Semantic Search System.\"", "from": "\"ALEX\"", "keywords": "\u003csource_entity\u003e", "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Both Alex and Sam Rivera are key members of the team involved in the development of the Semantic Search System.\"", "to": "\"SAM RIVERA\"", "width": 1.0}, {"description": "\"Both GloVe and LexRank are techniques used in natural language processing, but they serve different purposes: word embeddings vs. text summarization.\" \"", "from": "\"GLOVE\"", "keywords": "\"techniques, application areas\"", "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Both GloVe and LexRank are techniques used in natural language processing, but they serve different purposes: word embeddings vs. text summarization.\" \"", "to": "\"LEXRANK\"", "width": 12.0}, {"description": "\"Stanford is also the institution where GloVe was developed.\"", "from": "\"GLOVE\"", "keywords": "\"origination, development site\"", "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"Stanford is also the institution where GloVe was developed.\"", "to": "\"STANFORD\"", "width": 8.0}, {"description": "\"The co-occurrence matrix is a key element used by GloVe to derive semantic relationships between words through probability calculations.\"\u003cSEP\u003e\"The co-occurrence matrix is fundamental to how GloVe learns word embeddings, as it calculates probabilities of words appearing together.\"", "from": "\"GLOVE\"", "keywords": "\"learning mechanism, core process\"\u003cSEP\u003e\"method, core component\"", "source_id": "chunk-073485a6071a38818c47ff7188ec860b\u003cSEP\u003echunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"The co-occurrence matrix is a key element used by GloVe to derive semantic relationships between words through probability calculations.\"\u003cSEP\u003e\"The co-occurrence matrix is fundamental to how GloVe learns word embeddings, as it calculates probabilities of words appearing together.\"", "to": "\"CO-OCCURRENCE MATRIX\"", "width": 16.0}, {"description": "\"The loss function in GloVe aims to optimize the prediction accuracy based on the co-occurrence matrix and vector operations.\"", "from": "\"GLOVE\"", "keywords": "\"optimization, prediction accuracy\"", "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"The loss function in GloVe aims to optimize the prediction accuracy based on the co-occurrence matrix and vector operations.\"", "to": "\"LOSS FUNCTION\"", "width": 9.0}, {"description": "\"Both RNN and LSTM are types of neural networks used in sequence modeling tasks, with LSTM being a more advanced variant that can better capture long-term dependencies.\" \"", "from": "\"RECURRENT NEURAL NETWORK (RNN)\"", "keywords": "\"sequence modeling, improvement\"", "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Both RNN and LSTM are types of neural networks used in sequence modeling tasks, with LSTM being a more advanced variant that can better capture long-term dependencies.\" \"", "to": "\"LONG SHORT-TERM MEMORY (LSTM)\"", "width": 14.0}, {"description": "\"Transformers and BERT both represent advancements in NLP, where Transformers provide the foundational architecture for models like BERT.\" \"", "from": "\"TRANSFORMERS\"", "keywords": "\"architecture evolution, innovation\"", "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Transformers and BERT both represent advancements in NLP, where Transformers provide the foundational architecture for models like BERT.\" \"", "to": "\"BERT\"", "width": 18.0}, {"description": "\"LexRank is a specific technique used in the broader field of text summarization, particularly an unsupervised extractive method.\"\u003cSEP\u003e\"LexRank is a specific technique used within Text Summarization, indicating its relevance and application in this domain.\" \"\u003cSEP\u003e\"LexRank is a specific technique used within text summarization, indicating its relevance and application in this domain.\" \"", "from": "\"LEXRANK\"", "keywords": "\"application, technique description\"\u003cSEP\u003e\"method, application\"", "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f\u003cSEP\u003echunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"LexRank is a specific technique used in the broader field of text summarization, particularly an unsupervised extractive method.\"\u003cSEP\u003e\"LexRank is a specific technique used within Text Summarization, indicating its relevance and application in this domain.\" \"\u003cSEP\u003e\"LexRank is a specific technique used within text summarization, indicating its relevance and application in this domain.\" \"", "to": "\"TEXT SUMMARIZATION\"", "width": 38.0}, {"description": "\"Both Semantic Search Type and LexRank are techniques used in information retrieval, but LexRank specifically uses a link analysis algorithm to summarize text.\" \"", "from": "\"LEXRANK\"", "keywords": "\"techniques, application areas\"", "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"Both Semantic Search Type and LexRank are techniques used in information retrieval, but LexRank specifically uses a link analysis algorithm to summarize text.\" \"", "to": "\"SEMANTIC SEARCH TYPE\"", "width": 6.0}, {"description": "\"LexRank is an application of semantic search techniques in text summarization using graph-based approaches for scoring sentences.\"", "from": "\"LEXRANK\"", "keywords": "\"application, process\"", "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"LexRank is an application of semantic search techniques in text summarization using graph-based approaches for scoring sentences.\"", "to": "\"SEMANTIC SEARCH\"", "width": 16.0}, {"description": "\"LexRank is a key component in implementing the Semantic Search System, contributing to its accuracy and relevance in suggesting documents.\"", "from": "\"LEXRANK\"", "keywords": "\u003csource_entity\u003e", "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"LexRank is a key component in implementing the Semantic Search System, contributing to its accuracy and relevance in suggesting documents.\"", "to": "\"SEMANTIC SEARCH SYSTEM\"", "width": 1.0}, {"description": "\"BERTimbau is an adaptation of BERT for the Portuguese language, specifically Brazilian Portuguese, indicating that BERT can be fine-tuned for specific languages.\" \"", "from": "\"BERT\"", "keywords": "\"language adaptation, specialization\"", "source_id": "chunk-cb545b2bab1e85e24c22c3bd238a556f", "title": "\"BERTimbau is an adaptation of BERT for the Portuguese language, specifically Brazilian Portuguese, indicating that BERT can be fine-tuned for specific languages.\" \"", "to": "\"BERTIMBAU\"", "width": 16.0}, {"description": "\"While not directly related, BERT can be used for tasks that might involve understanding sentence structure and importance similar to eigenvector centrality in graphs. Both focus on contextual understanding.\"", "from": "\"BERT\"", "keywords": "\"contextual understanding, task relevance\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"While not directly related, BERT can be used for tasks that might involve understanding sentence structure and importance similar to eigenvector centrality in graphs. Both focus on contextual understanding.\"", "to": "\"EIGENVECTOR CENTRALITY\"", "width": 6.0}, {"description": "\"BERT is an extension of unidirectional models but uses bidirectional training for better language understanding.\"", "from": "\"BERT\"", "keywords": "\"bidirectionality, improvement\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"BERT is an extension of unidirectional models but uses bidirectional training for better language understanding.\"", "to": "\"UNIDIRECTIONAL TRAINING\"", "width": 6.0}, {"description": "\"BERT uses Masked Language Modeling as a technique in its pre-training phase to reduce bias by masking words and predicting them based on context.\"", "from": "\"BERT\"", "keywords": "\"technique usage, reduction of bias\"", "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"BERT uses Masked Language Modeling as a technique in its pre-training phase to reduce bias by masking words and predicting them based on context.\"", "to": "\"MASKED LANGUAGE MODELING (MLM)\"", "width": 8.0}, {"description": "\"BERT employs Next Sentence Prediction as part of its pre-training phase to understand sentence relationships, which is crucial for tasks like QA and NLI.\"", "from": "\"BERT\"", "keywords": "\"understanding sentence relationships, task application\"", "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"BERT employs Next Sentence Prediction as part of its pre-training phase to understand sentence relationships, which is crucial for tasks like QA and NLI.\"", "to": "\"NEXT SENTENCE PREDICTION (NSP)\"", "width": 7.0}, {"description": "\"The Cross-Encoder in the Pseudo Labeling phase relies on BERT for calculating similarities.\"", "from": "\"BERT\"", "keywords": "\u003c\"model integration, similarity calculation\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"The Cross-Encoder in the Pseudo Labeling phase relies on BERT for calculating similarities.\"", "to": "\"CROSS-ENCODER\"", "width": 14.0}, {"description": "\"SBERT improves on BERT\u0027s speed and efficiency in comparing sentence pairs using cosine similarity and a siamese architecture.\"", "from": "\"BERT\"", "keywords": "\"speed improvement, model enhancement\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"SBERT improves on BERT\u0027s speed and efficiency in comparing sentence pairs using cosine similarity and a siamese architecture.\"", "to": "\"SBERT\"", "width": 9.0}, {"description": "\"Domain Adaptation helps improve the Model Generalization of models by addressing issues in new and unseen data.\"", "from": "\"DOMAIN ADAPTATION\"", "keywords": "\"improvement, generalization\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Domain Adaptation helps improve the Model Generalization of models by addressing issues in new and unseen data.\"", "to": "\"MODEL GENERALIZATION\"", "width": 9.0}, {"description": "\"BERTimbau and neuralmind/bert-base-portuguese-cased are both pre-trained models for the Portuguese language, with BERTimbau being a specific variant for that purpose.\"", "from": "\"BERTIMBAU\"", "keywords": "\"model relation, training focus\"", "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"BERTimbau and neuralmind/bert-base-portuguese-cased are both pre-trained models for the Portuguese language, with BERTimbau being a specific variant for that purpose.\"", "to": "\"NEURALMIND/BERT-BASE-PORTUGUESE-CASED\"", "width": 6.0}, {"description": "\"BERTimbau was pre-trained using data from the BrWaC corpus, which provides diverse text for training.\"", "from": "\"BERTIMBAU\"", "keywords": "\"data source, pre-training\"", "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"BERTimbau was pre-trained using data from the BrWaC corpus, which provides diverse text for training.\"", "to": "\"BRWAC CORPUS\"", "width": 16.0}, {"description": "\"The performance of BERTimbau was tested using the Robust04 dataset, which provides well-written text for context understanding.\"", "from": "\"BERTIMBAU\"", "keywords": "\"performance test, dataset\"", "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"The performance of BERTimbau was tested using the Robust04 dataset, which provides well-written text for context understanding.\"", "to": "\"ROBUST04\"", "width": 14.0}, {"description": "\"Zhuyun Dai and Jamie Callan studied the performance of BERTimbau using various datasets and techniques.\"", "from": "\"BERTIMBAU\"", "keywords": "\"research, model evaluation\"", "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"Zhuyun Dai and Jamie Callan studied the performance of BERTimbau using various datasets and techniques.\"", "to": "\"ZHUYUN DAI AND JAMIE CALLAN\"", "width": 16.0}, {"description": "\"Albertina PT-BR outperforms BERTimbau in certain STS tasks but falls short of its performance overall.\"::", "from": "\"BERTIMBAU\"", "keywords": "\"performance comparison\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"Albertina PT-BR outperforms BERTimbau in certain STS tasks but falls short of its performance overall.\"::", "to": "\"ALBERTINA PT -BR\"", "width": 14.0}, {"description": "\"BERTimbau was pre-trained on BrWaC, indicating their shared origin but distinct use cases.\"", "from": "\"BERTIMBAU\"", "keywords": "\"pre-training, dataset\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"BERTimbau was pre-trained on BrWaC, indicating their shared origin but distinct use cases.\"", "to": "\"BRWAC\"", "width": 16.0}, {"description": "\"Models like BERTimbau are hosted and managed through the HuggingFace Platform using its Transformers library.\"", "from": "\"BERTIMBAU\"", "keywords": "\"hosting, management\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"Models like BERTimbau are hosted and managed through the HuggingFace Platform using its Transformers library.\"", "to": "\"HUGGINGFACE PLATFORM\"", "width": 18.0}, {"description": "\"BERTimbau variants can be hosted on TensorFlow alongside other platforms like PyTorch and JAX, indicating compatibility.\"\u003cSEP\u003e\"BERTimbau variants can be hosted on TensorFlow alongside other platforms like PyTorch and JAX, showing platform versatility.\"", "from": "\"BERTIMBAU\"", "keywords": "\"compatibility, hosting\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"BERTimbau variants can be hosted on TensorFlow alongside other platforms like PyTorch and JAX, indicating compatibility.\"\u003cSEP\u003e\"BERTimbau variants can be hosted on TensorFlow alongside other platforms like PyTorch and JAX, showing platform versatility.\"", "to": "\"TENSORFLOW\"", "width": 16.0}, {"description": "\"BERTimbau variants can also be hosted on PyTorch, alongside HuggingFace Platform and TensorFlow, indicating a multi-platform approach for model deployment.\"\u003cSEP\u003e\"BERTimbau variants can also be hosted on PyTorch, alongside HuggingFace Platform and TensorFlow, showing platform versatility.\"", "from": "\"BERTIMBAU\"", "keywords": "\"compatibility, hosting\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"BERTimbau variants can also be hosted on PyTorch, alongside HuggingFace Platform and TensorFlow, indicating a multi-platform approach for model deployment.\"\u003cSEP\u003e\"BERTimbau variants can also be hosted on PyTorch, alongside HuggingFace Platform and TensorFlow, showing platform versatility.\"", "to": "\"PYTORCH\"", "width": 16.0}, {"description": "\"JAX is another platform used to host BERTimbau variants, indicating a multi-platform approach for model deployment.\"", "from": "\"BERTIMBAU\"", "keywords": "\"compatibility, hosting\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"JAX is another platform used to host BERTimbau variants, indicating a multi-platform approach for model deployment.\"", "to": "\"JAX\"", "width": 16.0}, {"description": "\"This variant of BERTimbau was created through MLM training on legal documents to adapt it specifically for Portuguese jurisprudence.\"", "from": "\"BERTIMBAU\"", "keywords": "\"adaptation, specific use case\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"This variant of BERTimbau was created through MLM training on legal documents to adapt it specifically for Portuguese jurisprudence.\"", "to": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM\"", "width": 9.0}, {"description": "\"This TSDAE variant of BERTimbau was created to adapt the model for a new task and domain, similar in approach but distinct in methodology.\"", "from": "\"BERTIMBAU\"", "keywords": "\"adaptation, specific use case\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"This TSDAE variant of BERTimbau was created to adapt the model for a new task and domain, similar in approach but distinct in methodology.\"", "to": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "width": 9.0}, {"description": "\"BERTimbau was fine-tuned with MLM to better understand technical language in legal documents.\"", "from": "\"BERTIMBAU\"", "keywords": "\"fine-tuning, specific use case\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"BERTimbau was fine-tuned with MLM to better understand technical language in legal documents.\"", "to": "\"MASKED LANGUAGE MODELING (MLM)\"", "width": 18.0}, {"description": "\"Both BERTimbau and TSDAE were adapted for new tasks, indicating a common goal of model adaptation but through different methods.\"", "from": "\"BERTIMBAU\"", "keywords": "\"model adaptation, task-specific\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"Both BERTimbau and TSDAE were adapted for new tasks, indicating a common goal of model adaptation but through different methods.\"", "to": "\"TSDAE\"", "width": 16.0}, {"description": "\"BERTimbau was adapted to perform STS evaluation, showing its versatility in handling different language modeling tasks.\"", "from": "\"BERTIMBAU\"", "keywords": "\"task adaptation, evaluation\"", "source_id": "chunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"BERTimbau was adapted to perform STS evaluation, showing its versatility in handling different language modeling tasks.\"", "to": "\"STS EVALUATION\"", "width": 18.0}, {"description": "\"The master\u2019s thesis focused on applying NLP techniques to Portuguese consumer law at Instituto Superior T\u00e9cnico, Universidade de Lisboa. This clearly relates the organization to this location.\"", "from": "\"NLP APPLIED TO PORTUGUESE CONSUMER LAW\"", "keywords": "\"host institution, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The master\u2019s thesis focused on applying NLP techniques to Portuguese consumer law at Instituto Superior T\u00e9cnico, Universidade de Lisboa. This clearly relates the organization to this location.\"", "to": "\"INSTITUTO SUPERIOR T\u00c9CNICO, UNIVERSIDADE DE LISBOA\"", "width": 18.0}, {"description": "\"Albertina PT could be a partner or client for the development of the Semantic Search System.\"", "from": "\"ALBERTINA PT\"", "keywords": "\"partnership, collaboration\"", "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"Albertina PT could be a partner or client for the development of the Semantic Search System.\"", "to": "\"SEMANTIC SEARCH SYSTEM\"", "width": 12.0}, {"description": "\"Albertina PT is based on DeBERTa, which provided its initial architecture.\"\u003cSEP\u003e\"Albertina PT is based on DeBERTa, which provided its initial architecture.\"::", "from": "\"ALBERTINA PT\"", "keywords": "\"architecture foundation\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"Albertina PT is based on DeBERTa, which provided its initial architecture.\"\u003cSEP\u003e\"Albertina PT is based on DeBERTa, which provided its initial architecture.\"::", "to": "\"DEBERTA [16]\"", "width": 16.0}, {"description": "\"Jo \u02dcao Rodrigues and his team shared Albertina PT after its development.\"::", "from": "\"ALBERTINA PT\"", "keywords": "\"research contribution\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"Jo \u02dcao Rodrigues and his team shared Albertina PT after its development.\"::", "to": "\"JO \u02dcAO RODRIGUES ET AL.\"", "width": 9.0}, {"description": "\"FCUL was one of the institutions that developed Albertina PT through collaboration with FEUP.\"::", "from": "\"ALBERTINA PT\"", "keywords": "\"collaboration, development\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"FCUL was one of the institutions that developed Albertina PT through collaboration with FEUP.\"::", "to": "\"FACULDADE DE CI\u00caNCIAS DA UNIVERSIDADE DE LISBOA (FCUL)\"", "width": 16.0}, {"description": "\"FEUP contributed to the development of Albertina PT through its groups like NLX and Laborat\u00f3rio de Intelig\u00eancia Artificial e Ci\u00eancia de Computadores.\"::", "from": "\"ALBERTINA PT\"", "keywords": "\"development contribution, collaboration\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"FEUP contributed to the development of Albertina PT through its groups like NLX and Laborat\u00f3rio de Intelig\u00eancia Artificial e Ci\u00eancia de Computadores.\"::", "to": "\"FACULDADEDE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)\"", "width": 16.0}, {"description": "\"Jo \u02dc\u00e3o Rodrigues and his team shared Albertina PT after its development.\"::", "from": "\"ALBERTINA PT\"", "keywords": "\"research contribution\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"Jo \u02dc\u00e3o Rodrigues and his team shared Albertina PT after its development.\"::", "to": "\"JO \u02dc\u00c3O RODRIGUES ET AL.\"", "width": 9.0}, {"description": "\"Both Semantic Search and Text Summarization are advanced text processing techniques, albeit focusing on different aspects of information retrieval and management.\"", "from": "\"TEXT SUMMARIZATION\"", "keywords": "\"techniques, relation\"", "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Both Semantic Search and Text Summarization are advanced text processing techniques, albeit focusing on different aspects of information retrieval and management.\"", "to": "\"SEMANTIC SEARCH\"", "width": 12.0}, {"description": "\"ElasticSearch is part of the Semantic Search System, providing core search functionalities.\"", "from": "\"SEMANTIC SEARCH SYSTEM\"", "keywords": "\"component, integration\"", "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"ElasticSearch is part of the Semantic Search System, providing core search functionalities.\"", "to": "\"ELASTICSEARCH\"", "width": 16.0}, {"description": "\"The corpus is an integral part of the Semantic Search System, providing the data needed for its operation.\"", "from": "\"SEMANTIC SEARCH SYSTEM\"", "keywords": "\"data source, core component\"", "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The corpus is an integral part of the Semantic Search System, providing the data needed for its operation.\"", "to": "\"THE CORPUS\"", "width": 9.0}, {"description": "\"The architecture of the Semantic Search System includes various components like ElasticSearch and Data Processing.\"", "from": "\"SEMANTIC SEARCH SYSTEM\"", "keywords": "\"design, structure\"", "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The architecture of the Semantic Search System includes various components like ElasticSearch and Data Processing.\"", "to": "\"ARCHITECTURE\"", "width": 8.0}, {"description": "\"The Purely Semantic Search System is a component within the broader Semantic Search System.\"", "from": "\"SEMANTIC SEARCH SYSTEM\"", "keywords": "\"component, subset\"", "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Purely Semantic Search System is a component within the broader Semantic Search System.\"", "to": "\"PURELY SEMANTIC SEARCH SYSTEM\"", "width": 7.0}, {"description": "\"The Lexical-First Search System is another component of the Semantic Search System, prioritizing lexical search before semantic methods.\"", "from": "\"SEMANTIC SEARCH SYSTEM\"", "keywords": "\"component, integration\"", "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Lexical-First Search System is another component of the Semantic Search System, prioritizing lexical search before semantic methods.\"", "to": "\"LEXICAL-FIRST SEARCH SYSTEM\"", "width": 8.0}, {"description": "\"The Lexical + Semantic Search System combines both lexical and semantic approaches for more comprehensive functionalities within the broader Semantic Search System.\"", "from": "\"SEMANTIC SEARCH SYSTEM\"", "keywords": "\"integration, combination\"", "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"The Lexical + Semantic Search System combines both lexical and semantic approaches for more comprehensive functionalities within the broader Semantic Search System.\"", "to": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "width": 9.0}, {"description": "\"Elasticsearch often uses BM25 for ranking search results, making it a key technology in the search engine domain.\"", "from": "\"ELASTICSEARCH\"", "keywords": "\"usage, engine application\"", "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Elasticsearch often uses BM25 for ranking search results, making it a key technology in the search engine domain.\"", "to": "\"OKAPI BM25 (BM25)\"", "width": 7.0}, {"description": "\"Elasticsearch was pre-defined as a technology to be used in Project IRIS, making it an integral part of the project\u0027s solution.\"", "from": "\"ELASTICSEARCH\"", "keywords": "\"technology constraint, implementation basis\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Elasticsearch was pre-defined as a technology to be used in Project IRIS, making it an integral part of the project\u0027s solution.\"", "to": "\"PROJECT IRIS\"", "width": 18.0}, {"description": "\"dgsi.pt serves as the source of legal documents to be indexed into Elasticsearch through ecli-indexer6.\"", "from": "\"ELASTICSEARCH\"", "keywords": "\"document source, data storage\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"dgsi.pt serves as the source of legal documents to be indexed into Elasticsearch through ecli-indexer6.\"", "to": "\"DGSI.PT\"", "width": 16.0}, {"description": "\"Elasticsearch uses BM25 as its default search algorithm, indicating a direct relationship in terms of technology implementation.\"", "from": "\"ELASTICSEARCH\"", "keywords": "\"default search engine, algorithm usage\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Elasticsearch uses BM25 as its default search algorithm, indicating a direct relationship in terms of technology implementation.\"", "to": "\"BM25 ALGORITHM\"", "width": 7.0}, {"description": "\"Elasticsearch supports Cosine Similarity through initial mapping of indices, showing the relationship between Elasticsearch and its alternative search function.\"", "from": "\"ELASTICSEARCH\"", "keywords": "\"search functionality, algorithm support\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Elasticsearch supports Cosine Similarity through initial mapping of indices, showing the relationship between Elasticsearch and its alternative search function.\"", "to": "\"COSINE SIMILARITY\"", "width": 8.0}, {"description": "\"HuggingFace provides access to Elasticsearch which is utilized by the Purely Semantic Search System.\"", "from": "\"ELASTICSEARCH\"", "keywords": "\"model access, system integration\"", "source_id": "chunk-338cf6d446307fa476c0b025098bcd87", "title": "\"HuggingFace provides access to Elasticsearch which is utilized by the Purely Semantic Search System.\"", "to": "\"HUGGINGFACE\"", "width": 16.0}, {"description": "\"The generated queries were used to test the search system\u0027s performance by querying stored embeddings in ElasticSearch.\"", "from": "\"ELASTICSEARCH\"", "keywords": "\u003c\"query generation, evaluation process\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"The generated queries were used to test the search system\u0027s performance by querying stored embeddings in ElasticSearch.\"", "to": "\"AUTOMATIC QUERY GENERATION\"", "width": 8.0}, {"description": "\"Data processing steps are applied to the corpus to make it suitable for use in the semantic search system.\"", "from": "\"THE CORPUS\"", "keywords": "\"processing, preparation\"", "source_id": "chunk-1721070d8e5848c74ff9604584ac59f8", "title": "\"Data processing steps are applied to the corpus to make it suitable for use in the semantic search system.\"", "to": "\"DATA PROCESSING\"", "width": 7.0}, {"description": "\"The Purely Semantic Search System uses the cosine similarity function for retrieving relevant embeddings from Elasticsearch.\"", "from": "\"PURELY SEMANTIC SEARCH SYSTEM\"", "keywords": "\"retrieval method, embedding similarity\"", "source_id": "chunk-338cf6d446307fa476c0b025098bcd87", "title": "\"The Purely Semantic Search System uses the cosine similarity function for retrieving relevant embeddings from Elasticsearch.\"", "to": "\"COSINE SIMILARITY FUNCTION\"", "width": 18.0}, {"description": "\"The Lexical + Semantic Search System uses the cosine similarity function alongside BM25 scores to combine both lexical and semantic information for search results.\"", "from": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "keywords": "\"information integration, ranking method\"", "source_id": "chunk-338cf6d446307fa476c0b025098bcd87", "title": "\"The Lexical + Semantic Search System uses the cosine similarity function alongside BM25 scores to combine both lexical and semantic information for search results.\"", "to": "\"COSINE SIMILARITY FUNCTION\"", "width": 18.0}, {"description": "\"Cosine similarity is a key step in the Lexical + Semantic Search System for determining relevance.\"\u003c", "from": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "keywords": "core process, system component", "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"Cosine similarity is a key step in the Lexical + Semantic Search System for determining relevance.\"\u003c", "to": "\"COSINE SIMILARITY CALCULATION\"", "width": 16.0}, {"description": "\"The Lexical + Semantic Search System utilizes transfer learning to adapt BERTimbau to legal domain tasks.\"\u003c", "from": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "keywords": "utilization, adaptation", "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"The Lexical + Semantic Search System utilizes transfer learning to adapt BERTimbau to legal domain tasks.\"\u003c", "to": "\"TRANSFER LEARNING TECHNIQUE\"", "width": 8.0}, {"description": "\"Project IRIS members worked on developing a prototype of the Lexical + Semantic Search System for STJ.\"::", "from": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "keywords": "\u003c\"project collaboration, system development\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"Project IRIS members worked on developing a prototype of the Lexical + Semantic Search System for STJ.\"::", "to": "\"PROJECT IRIS\"", "width": 18.0}, {"description": "\"The Lexical + Semantic Search System is designed to provide relevant and insightful information for judges and legal authorities, improving decision-making processes.\"::", "from": "\"LEXICAL + SEMANTIC SEARCH SYSTEM\"", "keywords": "\u003c\"user benefit, performance enhancement\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The Lexical + Semantic Search System is designed to provide relevant and insightful information for judges and legal authorities, improving decision-making processes.\"::", "to": "\"JUDGES AND LEGAL AUTHORITIES\"", "width": 20.0}, {"description": "\"Both Generative Pseudo Labeling and Multilingual Knowledge Distillation are events or sections that discuss different methodologies for training models in the document.\"", "from": "\"GENERATIVE PSEUDO LABELING\"", "keywords": "\u003crelationship_keywords\u003e", "source_id": "chunk-c905a9c26d22cc769d6639180aabeda9", "title": "\"Both Generative Pseudo Labeling and Multilingual Knowledge Distillation are events or sections that discuss different methodologies for training models in the document.\"", "to": "\"MULTILINGUAL KNOWLEDGE DISTILLATION\"", "width": 18.0}, {"description": "\"Chapter 6 provides details on the system evaluation process, including both language model and search system evaluations.\"", "from": "\"SYSTEM EVALUATION\"", "keywords": "\"content source, evaluation documentation\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"Chapter 6 provides details on the system evaluation process, including both language model and search system evaluations.\"", "to": "\"CHAPTER 6\"", "width": 9.0}, {"description": "\"Language Model Evaluation is a sub-component of Search System Evaluation that specifically assesses the performance of Legal-BERTimbau variants in STS tasks.\"", "from": "\"LANGUAGE MODEL EVALUATION\"", "keywords": "\"evaluation hierarchy, task-specific assessment\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"Language Model Evaluation is a sub-component of Search System Evaluation that specifically assesses the performance of Legal-BERTimbau variants in STS tasks.\"", "to": "\"SEARCH SYSTEM EVALUATION\"", "width": 7.0}, {"description": "\"Chapter 6 focuses on evaluating Legal-BERTimbau variants in various aspects such as domain adaptation and STS task performance.\"", "from": "\"LANGUAGE MODEL EVALUATION\"", "keywords": "\"evaluation focus, detailed insights\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"Chapter 6 focuses on evaluating Legal-BERTimbau variants in various aspects such as domain adaptation and STS task performance.\"", "to": "\"CHAPTER 6\"", "width": 8.0}, {"description": "\"tsdae-mkd-nli-sts-v1 is part of the Language Model Evaluation process.\"", "from": "\"LANGUAGE MODEL EVALUATION\"", "keywords": "\"model evaluation, variant\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"tsdae-mkd-nli-sts-v1 is part of the Language Model Evaluation process.\"", "to": "\"TSDAE-MKD-NLI-STS-V1\"", "width": 14.0}, {"description": "\"mlm-gpl-nli-sts-MetaKD-v0 is part of the Language Model Evaluation process.\"", "from": "\"LANGUAGE MODEL EVALUATION\"", "keywords": "\"model evaluation, variant\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"mlm-gpl-nli-sts-MetaKD-v0 is part of the Language Model Evaluation process.\"", "to": "\"MLM-GPL-NLI-STS-METAKD-V0\"", "width": 14.0}, {"description": "\"mlm-gpl-nli-sts-MetaKD-v1 is part of the Language Model Evaluation process.\"", "from": "\"LANGUAGE MODEL EVALUATION\"", "keywords": "\"model evaluation, variant\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"mlm-gpl-nli-sts-MetaKD-v1 is part of the Language Model Evaluation process.\"", "to": "\"MLM-GPL-NLI-STS-METAKD-V1\"", "width": 14.0}, {"description": "\"tsdae-gpl-nli-sts-MetaKD-v0 is part of the Language Model Evaluation process.\"", "from": "\"LANGUAGE MODEL EVALUATION\"", "keywords": "\"model evaluation, variant\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"tsdae-gpl-nli-sts-MetaKD-v0 is part of the Language Model Evaluation process.\"", "to": "\"TSDAE-GPL-NLI-STS-METAKD-V0\"", "width": 14.0}, {"description": "\"tsdae-gpl-nli-sts-MetaKD-v1 is part of the Language Model Evaluation process.\"", "from": "\"LANGUAGE MODEL EVALUATION\"", "keywords": "\"model evaluation, variant\"", "source_id": "chunk-3eda103fe58f7ddc99d8921614d8db3f", "title": "\"tsdae-gpl-nli-sts-MetaKD-v1 is part of the Language Model Evaluation process.\"", "to": "\"TSDAE-GPL-NLI-STS-METAKD-V1\"", "width": 14.0}, {"description": "\"These two models are both part of the broader Word2Vec family and share similar underlying concepts, differing in their approach to context.\"", "from": "\"WORD2VEC WITH COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE WORD CONTEXT\"", "keywords": "\u003crelationship_description\u003e", "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"These two models are both part of the broader Word2Vec family and share similar underlying concepts, differing in their approach to context.\"", "to": "\"WORD2VEC WITH CBOW MODEL BASED ON MULTIPLE WORDS CONTEXT\"", "width": 7.0}, {"description": "\"Both models are part of the Word2Vec family and serve similar purposes in predicting target words but differ in their approach to modeling context.\"", "from": "\"WORD2VEC WITH COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE WORD CONTEXT\"", "keywords": "\u003crelationship_description\u003e", "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Both models are part of the Word2Vec family and serve similar purposes in predicting target words but differ in their approach to modeling context.\"", "to": "\"WORD2VEC WITH SKIP-GRAM MODEL\"", "width": 6.0}, {"description": "\"These two models are both part of the broader Word2Vec family and share similar underlying concepts, differing in their approach to context.\"", "from": "\"WORD2VEC WITH CBOW MODEL BASED ON MULTIPLE WORDS CONTEXT\"", "keywords": "\u003crelationship_description\u003e", "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"These two models are both part of the broader Word2Vec family and share similar underlying concepts, differing in their approach to context.\"", "to": "\"WORD2VEC WITH COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE-WORD CONTEXT\"", "width": 14.0}, {"description": "\"Both models are part of the Word2Vec family and serve similar purposes in predicting target words but differ in their approach to modeling context.\"", "from": "\"WORD2VEC WITH SKIP-GRAM MODEL\"", "keywords": "\u003crelationship_description\u003e", "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Both models are part of the Word2Vec family and serve similar purposes in predicting target words but differ in their approach to modeling context.\"", "to": "\"WORD2VEC WITH COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE-WORD CONTEXT\"", "width": 6.0}, {"description": "\"Both involve recurrent neural networks, where Recurrent Network Fully Connected is a specific type of RNN while RNN Structure refers to the general architecture.\"", "from": "\"RECURRENT NETWORK FULLY CONNECTED\"", "keywords": "\u003crelationship_description\u003e", "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Both involve recurrent neural networks, where Recurrent Network Fully Connected is a specific type of RNN while RNN Structure refers to the general architecture.\"", "to": "\"RNN STRUCTURE\"", "width": 10.0}, {"description": "\"Both are structures within recurrent neural networks, with LSTM being a specific type that addresses long-term dependencies in sequence data.\"", "from": "\"RECURRENT NETWORK FULLY CONNECTED\"", "keywords": "\u003crelationship_description\u003e", "source_id": "chunk-4438e56d1dbc938d09784326b42337ca", "title": "\"Both are structures within recurrent neural networks, with LSTM being a specific type that addresses long-term dependencies in sequence data.\"", "to": "\"LSTM STRUCTURE\"", "width": 12.0}, {"description": "\"The Transformer Model uses Positional Encoding as part of its processing pipeline, where it provides context based on word positions.\"", "from": "\"TRANSFORMER MODEL\"", "keywords": "\"context provision, encoding mechanism\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"The Transformer Model uses Positional Encoding as part of its processing pipeline, where it provides context based on word positions.\"", "to": "\"POSITIONAL ENCODING\"", "width": 8.0}, {"description": "\"The Multi-Head Attention Layer is a component of the Transformer Model responsible for assigning weights to vectors of words based on their importance in the sentence.\"", "from": "\"TRANSFORMER MODEL\"", "keywords": "\"component integration, attention mechanism\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"The Multi-Head Attention Layer is a component of the Transformer Model responsible for assigning weights to vectors of words based on their importance in the sentence.\"", "to": "\"MULTI-HEAD ATTENTION LAYER\"", "width": 9.0}, {"description": "\"The Feed-Forward Neural Network is another component of the Transformer Model that processes the input after encoding by Positional Encoding and Multi-Head Attention Layer.\"", "from": "\"TRANSFORMER MODEL\"", "keywords": "\"processing stage, information flow\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"The Feed-Forward Neural Network is another component of the Transformer Model that processes the input after encoding by Positional Encoding and Multi-Head Attention Layer.\"", "to": "\"FEED-FORWARD NEURAL NETWORK\"", "width": 8.0}, {"description": "\"GenQ was developed by the Ubiquitous Knowledge Processing Lab team, showing an affiliation and development relationship.\"", "from": "\"GENQ\"", "keywords": "\"team affiliation, development\"", "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"GenQ was developed by the Ubiquitous Knowledge Processing Lab team, showing an affiliation and development relationship.\"", "to": "\"UBIQUITOUS KNOWLEDGE PROCESSING LAB TEAM\"", "width": 7.0}, {"description": "\"GenQ uses T5 for fine-tuning and query generation, showing a dependency relationship.\"", "from": "\"GENQ\"", "keywords": "\"dependency, development process\"", "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"GenQ uses T5 for fine-tuning and query generation, showing a dependency relationship.\"", "to": "\"TEXT-TO-TEXT TRANSFER TRANSFORMER (T5)\"", "width": 9.0}, {"description": "\"The TSDAE Training Loss is related to the variant created for STS evaluation.\"", "from": "\"TSDAE TRAINING LOSS\"", "keywords": "\"evaluation, loss metrics\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"The TSDAE Training Loss is related to the variant created for STS evaluation.\"", "to": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "width": 12.0}, {"description": "\"This figure represents the training loss of the TSDAE variant.\"", "from": "\"TSDAE TRAINING LOSS\"", "keywords": "\"figure representation, visual aid\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"This figure represents the training loss of the TSDAE variant.\"", "to": "\"FIGURE 5.3: TSDAE TRAINING LOSS\"", "width": 7.0}, {"description": "\"The ASSIN dataset was part of the evaluation architecture for assessing the system\u0027s performance.\"", "from": "\"EVALUATION ARCHITECTURE\"", "keywords": "\u003c\"dataset usage, evaluation process\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"The ASSIN dataset was part of the evaluation architecture for assessing the system\u0027s performance.\"", "to": "\"ASSIN DATASET\"", "width": 7.0}, {"description": "\"The STSbenchmark dataset is the source from which the stsb multi mt dataset is derived and used in the evaluation architecture.\"", "from": "\"EVALUATION ARCHITECTURE\"", "keywords": "\u003c\"data derivation, evaluation process\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"The STSbenchmark dataset is the source from which the stsb multi mt dataset is derived and used in the evaluation architecture.\"", "to": "\"STSBENCHMARK DATASET\"", "width": 8.0}, {"description": "\"Both events are part of the broader Search System Evaluation process, focusing on different metrics.\"", "from": "\"SEARCH SYSTEM EVALUATION \u2013 SEARCH METRIC \u2013 BEST MODEL\"", "keywords": "\"metrics comparison\"", "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"Both events are part of the broader Search System Evaluation process, focusing on different metrics.\"", "to": "\"SEARCH SYSTEM EVALUATION \u2013 DISCOVERY METRIC \u2013 BEST MODEL\"", "width": 6.0}, {"description": "\"Both metrics are part of model evaluation processes in natural language processing.\"", "from": "\"SBERT SPEARMAN CORRELATION\"", "keywords": "\"model evaluation, metrics comparison\"", "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"Both metrics are part of model evaluation processes in natural language processing.\"", "to": "\"SENTENCE-BERT (SBERT)\"", "width": 6.0}, {"description": "\"Both entities involve model evaluations in NLP tasks.\"", "from": "\"SBERT SPEARMAN CORRELATION\"", "keywords": "\"model evaluation, BERT variants\"", "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"Both entities involve model evaluations in NLP tasks.\"", "to": "\"BERTIMBAU VARIANTS\"", "width": 5.0}, {"description": "\"These are both types of models used for semantic text similarity and other NLP tasks.\"", "from": "\"SENTENCE-BERT (SBERT)\"", "keywords": "\"models comparison, NLP tasks\"", "source_id": "chunk-f66becb00b4d98284bacd25a49e26a3e", "title": "\"These are both types of models used for semantic text similarity and other NLP tasks.\"", "to": "\"BERTIMBAU VARIANTS\"", "width": 7.0}, {"description": "\"Sentence-BERT uses a Cross-Encoder in the Pseudo Labeling phase to calculate similarity scores between queries, positive, and negative passages for training purposes.\"", "from": "\"SENTENCE-BERT (SBERT)\"", "keywords": "\"embedding generation, similarity calculation\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"Sentence-BERT uses a Cross-Encoder in the Pseudo Labeling phase to calculate similarity scores between queries, positive, and negative passages for training purposes.\"", "to": "\"CROSS-ENCODER\"", "width": 8.0}, {"description": "\"BERTimbau variants are adaptations of SBERT and performed well in specific tasks but faced challenges with other models.\"", "from": "\"BERTIMBAU VARIANTS\"", "keywords": "\u003c\"adaptation, performance variation\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"BERTimbau variants are adaptations of SBERT and performed well in specific tasks but faced challenges with other models.\"", "to": "\"SBERT VARIANTS\"", "width": 6.0}, {"description": "\"FCUL is one of the institutions contributing to Project IRIS, which defines some pre-defined aspects for the implementation.\"::", "from": "\"FACULDADE DE CI\u00caNCIAS DA UNIVERSIDADE DE LISBOA (FCUL)\"", "keywords": "\"project contribution, predefined aspects\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"FCUL is one of the institutions contributing to Project IRIS, which defines some pre-defined aspects for the implementation.\"::", "to": "\"PROJECT IRIS\"", "width": 14.0}, {"description": "\"FEUP is another institution contributing to Project IRIS, ensuring alignment with pre-defined aspects.\"::", "from": "\"FACULDADEDE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)\"", "keywords": "\"project contribution, predefined aspects\"", "source_id": "chunk-a24ce5ad9bdfbbf32c946ee404c32fd6", "title": "\"FEUP is another institution contributing to Project IRIS, ensuring alignment with pre-defined aspects.\"::", "to": "\"PROJECT IRIS\"", "width": 14.0}, {"description": "\"INESC-ID Lisboa develops a project (IRIS) to assist the decision-making process of STJ through summarization techniques.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTIC \u00b8A (STJ)\"", "keywords": "\"judicial support, technology collaboration\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"INESC-ID Lisboa develops a project (IRIS) to assist the decision-making process of STJ through summarization techniques.\"", "to": "\"INESC-ID LISBOA\"", "width": 8.0}, {"description": "\"The IR system plays a critical role in assisting STJ\u0027s decision-making by ensuring efficient access to necessary legal information.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTIC \u00b8A (STJ)\"", "keywords": "\u003c\"judicial support, technology integration\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"The IR system plays a critical role in assisting STJ\u0027s decision-making by ensuring efficient access to necessary legal information.\"", "to": "\"INFORMATION RETRIEVAL\"", "width": 7.0}, {"description": "\"STJ can benefit from semantic techniques to improve decision-making by accessing a wider range of relevant legal resources.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTIC \u00b8A (STJ)\"", "keywords": "\u003c\"decision support, broader information retrieval\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"STJ can benefit from semantic techniques to improve decision-making by accessing a wider range of relevant legal resources.\"", "to": "\"SEMANTIC TECHNIQUES\"", "width": 7.0}, {"description": "\"The court can face limitations with traditional lexical methods, which might lead to incomplete or inaccurate information retrieval.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTIC \u00b8A (STJ)\"", "keywords": "\u003c\"information incompleteness, reliance on broader methods\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"The court can face limitations with traditional lexical methods, which might lead to incomplete or inaccurate information retrieval.\"", "to": "\"LEXICAL TECHNIQUES\"", "width": 6.0}, {"description": "\"The court relies on TF to understand the frequency of terms and phrases within legal texts for information retrieval tasks.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTIC \u00b8A (STJ)\"", "keywords": "\u003c\"information retrieval, term frequency\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"The court relies on TF to understand the frequency of terms and phrases within legal texts for information retrieval tasks.\"", "to": "\"TF TERM FREQUENCY\"", "width": 5.0}, {"description": "\"The IRIS project aims to contribute to this concept by promoting consistent legal application.\"", "from": "\"INESC-ID LISBOA\"", "keywords": "\u003c\"legal consistency, research contribution\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"The IRIS project aims to contribute to this concept by promoting consistent legal application.\"", "to": "\"CONSISTENCY IN APPLYING THE LAW\"", "width": 5.0}, {"description": "\"The IRIS project aims to prevent unjust precedents by improving legal information retrieval and decision-making.\"", "from": "\"INESC-ID LISBOA\"", "keywords": "\u003c\"legal integrity, prevention of imbalances\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"The IRIS project aims to prevent unjust precedents by improving legal information retrieval and decision-making.\"", "to": "\"UNJUST PRECEDENTS\"", "width": 6.0}, {"description": "\"INESC-ID Lisboa might utilize T5 for text generation tasks in their IRIS project.\"", "from": "\"INESC-ID LISBOA\"", "keywords": "\u003c\"technology collaboration, model usage\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"INESC-ID Lisboa might utilize T5 for text generation tasks in their IRIS project.\"", "to": "\"T5 TEXT-TO-TEXT TRANSFER TRANSFORMER\"", "width": 6.0}, {"description": "\"INECS-ID Lisboa might use SNLI for natural language inference tasks related to their IRIS project.\"", "from": "\"INESC-ID LISBOA\"", "keywords": "\u003c\"research collaboration, dataset usage\"", "source_id": "chunk-4ae3d638fab35f846eff55a5b4f9d288", "title": "\"INECS-ID Lisboa might use SNLI for natural language inference tasks related to their IRIS project.\"", "to": "\"SNLI STANFORD NATURAL LANGUAGE INFERENCE\"", "width": 6.0}, {"description": "\"INESC-ID Lisboa developed and leads the IRIS project.\"", "from": "\"INESC-ID LISBOA\"", "keywords": "\"leadership, development\"", "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"INESC-ID Lisboa developed and leads the IRIS project.\"", "to": "\"IRIS PROJECT\"", "width": 18.0}, {"description": "\"INESC-ID Lisboa is involved in developing the search system as part of the IRIS project.\"", "from": "\"INESC-ID LISBOA\"", "keywords": "\"development, contribution\"", "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"INESC-ID Lisboa is involved in developing the search system as part of the IRIS project.\"", "to": "\"SEARCH SYSTEM\"", "width": 8.0}, {"description": "\"The IRIS project aims to create solutions useful for STJ, indicating a potential end-user or beneficiary relationship.\"", "from": "\"IRIS PROJECT\"", "keywords": "\"benefit, support\"", "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"The IRIS project aims to create solutions useful for STJ, indicating a potential end-user or beneficiary relationship.\"", "to": "\"STJ\"", "width": 14.0}, {"description": "\"The HuggingFace platform hosts the datasets and language models developed by INESC-ID Lisboa for the IRIS project.\"", "from": "\"IRIS PROJECT\"", "keywords": "\"hosting, distribution\"", "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"The HuggingFace platform hosts the datasets and language models developed by INESC-ID Lisboa for the IRIS project.\"", "to": "\"HUGGINGFACE PLATFORM\"", "width": 7.0}, {"description": "\"Classical approaches form the foundation of Information Retrieval.\"", "from": "\"INFORMATION RETRIEVAL (IR)\"", "keywords": "\"foundation, base techniques\"", "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Classical approaches form the foundation of Information Retrieval.\"", "to": "\"CLASSICAL APPROACHES\"", "width": 8.0}, {"description": "\"Recent approaches are advancements in Information Retrieval that build upon and complement traditional methods.\"", "from": "\"INFORMATION RETRIEVAL (IR)\"", "keywords": "\"advancements, complementary techniques\"", "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Recent approaches are advancements in Information Retrieval that build upon and complement traditional methods.\"", "to": "\"RECENT APPROACHES\"", "width": 7.0}, {"description": "\"The Acord\u00e3o is a decision from the STJ.\"", "from": "\"STJ\"", "keywords": "\"judicial, ruling\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The Acord\u00e3o is a decision from the STJ.\"", "to": "\"ACORD\u00c3O\"", "width": 8.0}, {"description": "\"Project IRIS involves tasks related to STJ, including developing Semantic Search Systems for it.\"\u003cSEP\u003e\"Project IRIS involves tasks related to STJ, including the development of Semantic Search Systems for it.\"", "from": "\"STJ\"", "keywords": "\u003csource_entity\u003e", "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Project IRIS involves tasks related to STJ, including developing Semantic Search Systems for it.\"\u003cSEP\u003e\"Project IRIS involves tasks related to STJ, including the development of Semantic Search Systems for it.\"", "to": "\"PROJECT IRIS\"", "width": 2.0}, {"description": "\"Judges make use of court decisions in their work and may benefit from the search system developed by the IRIS project.\"", "from": "\"COURT DECISIONS\"", "keywords": "\"utilization, relevance\"", "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"Judges make use of court decisions in their work and may benefit from the search system developed by the IRIS project.\"", "to": "\"JUDGES\"", "width": 16.0}, {"description": "\"The search system aims to assist judges in their decision-making process based on court decisions.\"", "from": "\"JUDGES\"", "keywords": "\"support, improvement\"", "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"The search system aims to assist judges in their decision-making process based on court decisions.\"", "to": "\"SEARCH SYSTEM\"", "width": 8.0}, {"description": "\"Natural Language systems are crucial for achieving Semantic Textual Similarity in legal contexts.\"", "from": "\"NATURAL LANGUAGE (NL) SYSTEMS\"", "keywords": "\"technique, foundation\"", "source_id": "chunk-ae090822f3d4769354cc463665e2df89", "title": "\"Natural Language systems are crucial for achieving Semantic Textual Similarity in legal contexts.\"", "to": "\"SEMANTIC TEXTUAL SIMILARITY (STS)\"", "width": 18.0}, {"description": "\"The training dataset was shared on HuggingFace, which facilitates its use in various projects.\"", "from": "\"HUGGINGFACE PLATFORM\"", "keywords": "\"sharing, accessibility\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The training dataset was shared on HuggingFace, which facilitates its use in various projects.\"", "to": "\"TRAINING DATASET\"", "width": 7.0}, {"description": "\"The Portuguese legal sentences v0 dataset was published and made accessible through the HuggingFace platform.\"", "from": "\"HUGGINGFACE PLATFORM\"", "keywords": "\"publication, accessibility\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The Portuguese legal sentences v0 dataset was published and made accessible through the HuggingFace platform.\"", "to": "\"PORTUGUESE LEGAL SENTENCES V0\"", "width": 8.0}, {"description": "\"Entity recognition can help expand search query by including relevant entities, improving document retrieval.\"", "from": "\"SEARCH SYSTEM\"", "keywords": "\"query expansion, improved retrieval\"", "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"Entity recognition can help expand search query by including relevant entities, improving document retrieval.\"", "to": "\"ENTITY RECOGNITION\"", "width": 8.0}, {"description": "\"Embeddings are used in the Search System to process and improve the accuracy of text queries.\"", "from": "\"SEARCH SYSTEM\"", "keywords": "\"text processing, improved accuracy\"", "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"Embeddings are used in the Search System to process and improve the accuracy of text queries.\"", "to": "\"EMBEDDINGS\"", "width": 9.0}, {"description": "\"Machine Learning is a subset of Artificial Intelligence, focusing on algorithms for learning from data.\"", "from": "\"ARTIFICIAL INTELLIGENCE (AI)\"", "keywords": "\"subset, specialization\"", "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Machine Learning is a subset of Artificial Intelligence, focusing on algorithms for learning from data.\"", "to": "\"MACHINE LEARNING (ML)\"", "width": 8.0}, {"description": "\"BM25 relies heavily on TF-IDF to rank document relevance in information retrieval.\"", "from": "\"OKAPI BM25 (BM25)\"", "keywords": "\"reliance, algorithm basis\"", "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"BM25 relies heavily on TF-IDF to rank document relevance in information retrieval.\"", "to": "\"TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF)\"", "width": 9.0}, {"description": "\"Semantic search and word embedding both aim to understand contextual meaning rather than exact word matches, with word embeddings being a key component of semantic search techniques.\"", "from": "\"SEMANTIC SEARCH\"", "keywords": "\"contextual understanding, similarity measures\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Semantic search and word embedding both aim to understand contextual meaning rather than exact word matches, with word embeddings being a key component of semantic search techniques.\"", "to": "\"WORD EMBEDDING\"", "width": 8.0}, {"description": "\"Bi-Encoders and Cross-Encoders are types of semantic search approaches, with Bi-Encoders focusing on independent embedding generation while Cross-Encoders compare sentences directly.\"", "from": "\"SEMANTIC SEARCH\"", "keywords": "\"techniques, distinction\"", "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Bi-Encoders and Cross-Encoders are types of semantic search approaches, with Bi-Encoders focusing on independent embedding generation while Cross-Encoders compare sentences directly.\"", "to": "\"BI-ENCODERS\"", "width": 12.0}, {"description": "\"Both Semantic Search and Extractive Summarization are involved in processing text data but focus on different aspects: semantic understanding vs. sentence selection.\"", "from": "\"SEMANTIC SEARCH\"", "keywords": "\"processes, approach comparison\"", "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Both Semantic Search and Extractive Summarization are involved in processing text data but focus on different aspects: semantic understanding vs. sentence selection.\"", "to": "\"EXTRACTIVE SUMMARIZATION\"", "width": 10.0}, {"description": "\"Lexical search is a specific type of classical approach used in information retrieval.\"", "from": "\"CLASSICAL APPROACHES\"", "keywords": "\"type, subset\"", "source_id": "chunk-469ff73d10b043e9b3515dc8baa15494", "title": "\"Lexical search is a specific type of classical approach used in information retrieval.\"", "to": "\"LEXICAL SEARCH (TRADITIONAL)\"", "width": 6.0}, {"description": "\"The Jaccard Similarity measure is an example of lexical search, illustrating its limitations when only literal matches are considered.\"", "from": "\"JACCARD SIMILARITY MEASURE\"", "keywords": "\"literal match, shared information\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"The Jaccard Similarity measure is an example of lexical search, illustrating its limitations when only literal matches are considered.\"", "to": "\"LEXICAL SEARCHES\"", "width": 6.0}, {"description": "\"The Jaccard Similarity measure evaluates shared information between sets.\"\u003cSEP\u003e\"The Jaccard Similarity measure evaluates shared information between sets.\" \"", "from": "\"JACCARD SIMILARITY MEASURE\"", "keywords": "\"information evaluation, set similarity\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"The Jaccard Similarity measure evaluates shared information between sets.\"\u003cSEP\u003e\"The Jaccard Similarity measure evaluates shared information between sets.\" \"", "to": "\"SETS\"", "width": 14.0}, {"description": "\"Cosine Similarity is used in calculating distances between word vectors created by Word2Vec, demonstrating its application in the embedding space.\"", "from": "\"COSINE SIMILARITY\"", "keywords": "\"distance calculation, vector space\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Cosine Similarity is used in calculating distances between word vectors created by Word2Vec, demonstrating its application in the embedding space.\"", "to": "\"WORD2VEC\"", "width": 7.0}, {"description": "\"The connectivity matrix is based on cosine similarity to represent the relationships between sentences.\"", "from": "\"COSINE SIMILARITY\"", "keywords": "\"similarity measure, connection representation\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"The connectivity matrix is based on cosine similarity to represent the relationships between sentences.\"", "to": "\"CONNECTIVITY MATRIX\"", "width": 8.0}, {"description": "\"SBERT uses cosine similarity to measure semantic similarity between passages and queries during embedding generation.\"", "from": "\"COSINE SIMILARITY\"", "keywords": "\u003c\"semantic similarity, embedding comparison\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"SBERT uses cosine similarity to measure semantic similarity between passages and queries during embedding generation.\"", "to": "\"SBERT\"", "width": 16.0}, {"description": "\"Stanford is the institution where Word2Vec was developed.\"", "from": "\"WORD2VEC\"", "keywords": "\"origination, development site\"", "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"Stanford is the institution where Word2Vec was developed.\"", "to": "\"STANFORD\"", "width": 8.0}, {"description": "\"Tomas Mikolov et al. are the creators of Word2Vec.\u003c\"|\u003e\"creators, innovation\"", "from": "\"WORD2VEC\"", "keywords": "10", "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"Tomas Mikolov et al. are the creators of Word2Vec.\u003c\"|\u003e\"creators, innovation\"", "to": "\"TOMAS MIKOLOV ET AL.\"", "width": 10.0}, {"description": "\"The CBOW model is a part of Word2Vec designed for predicting words based on context.\"", "from": "\"WORD2VEC\"", "keywords": "\"component, prediction method\"", "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The CBOW model is a part of Word2Vec designed for predicting words based on context.\"", "to": "\"CBOW MODEL\"", "width": 9.0}, {"description": "\"The Skip-Gram model is another component of Word2Vec that focuses on word-context prediction from the reverse perspective.\"", "from": "\"WORD2VEC\"", "keywords": "\"component, prediction method\"", "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The Skip-Gram model is another component of Word2Vec that focuses on word-context prediction from the reverse perspective.\"", "to": "\"SKIP-GRAM MODEL\"", "width": 9.0}, {"description": "\"The output layer predicts words based on input from the context provided by the CBOW model.\"", "from": "\"WORD2VEC\"", "keywords": "\"prediction component, context dependency\"", "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The output layer predicts words based on input from the context provided by the CBOW model.\"", "to": "\"OUTPUT LAYER OF WORD2VEC\"", "width": 16.0}, {"description": "\"qifrequency is a component of the BM25 function.\" \"", "from": "\"QIFREQUENCY\"", "keywords": "\"component, function\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"qifrequency is a component of the BM25 function.\" \"", "to": "\"BM25 FUNCTION\"", "width": 6.0}, {"description": "\"k1 is a parameter in the BM25 function.\"\u003cSEP\u003e\"k1 is a parameter in the BM25 function.\" \"", "from": "\"K1\"", "keywords": "\"parameter, function\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"k1 is a parameter in the BM25 function.\"\u003cSEP\u003e\"k1 is a parameter in the BM25 function.\" \"", "to": "\"BM25 FUNCTION\"", "width": 12.0}, {"description": "\"bterms are used to optimize the BM25 function.\"\u003cSEP\u003e\"bterms are used to optimize the BM25 function.\" \"", "from": "\"BTERMS\"", "keywords": "\"optimization, parameter\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"bterms are used to optimize the BM25 function.\"\u003cSEP\u003e\"bterms are used to optimize the BM25 function.\" \"", "to": "\"BM25 FUNCTION\"", "width": 12.0}, {"description": "\"Queries can use exact words to find documents containing those terms.\" \"", "from": "\"QUERY\"", "keywords": "\"word match, retrieval\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"Queries can use exact words to find documents containing those terms.\" \"", "to": "\"EXACT WORD\"", "width": 12.0}, {"description": "\"The advanced approach focuses on retrieving information based on intrinsic meaning rather than literal word matches.\" \"", "from": "\"ADVANCE AND ROBUST APPROACH\"", "keywords": "\"meaning understanding, retrieval\"", "source_id": "chunk-8dc9bc0243bc2842874f8fe5a91e0b07", "title": "\"The advanced approach focuses on retrieving information based on intrinsic meaning rather than literal word matches.\" \"", "to": "\"INTRINSIC MEANING\"", "width": 14.0}, {"description": "\"The hidden layer processes the context provided by the CBOW model to predict words in the output layer.\"", "from": "\"CBOW MODEL ARCHITECTURE\"", "keywords": "\"context processing, prediction support\"", "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The hidden layer processes the context provided by the CBOW model to predict words in the output layer.\"", "to": "\"HIDDEN LAYER OF WORD2VEC\"", "width": 18.0}, {"description": "\"The weight matrix W is used in the hidden layer to process and predict words based on their context provided by surrounding words.\"", "from": "\"HIDDEN LAYER OF WORD2VEC\"", "keywords": "\"processing mechanism, prediction support\"", "source_id": "chunk-073485a6071a38818c47ff7188ec860b", "title": "\"The weight matrix W is used in the hidden layer to process and predict words based on their context provided by surrounding words.\"", "to": "\"WEIGHT MATRIX W OF WORD2VEC\"", "width": 16.0}, {"description": "\"Ice has higher co-occurrence with solid compared to gas, indicating a stronger relationship in context.\"", "from": "\"ICE\"", "keywords": "\"state association, frequent co-occurrence\"", "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Ice has higher co-occurrence with solid compared to gas, indicating a stronger relationship in context.\"", "to": "\"SOLID\"", "width": 8.0}, {"description": "\"Ice shows moderate co-occurrence with water but not as high as with solid, indicating a relevant but secondary relationship in terms of state associations.\"", "from": "\"ICE\"", "keywords": "\"association, moderate co-occurrence\"", "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Ice shows moderate co-occurrence with water but not as high as with solid, indicating a relevant but secondary relationship in terms of state associations.\"", "to": "\"WATER\"", "width": 6.0}, {"description": "\"Steam has higher co-occurrence with gas compared to ice, suggesting a strong relationship in the context of states of matter.\"", "from": "\"STEAM\"", "keywords": "\"state association, frequent co-occurrence\"", "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Steam has higher co-occurrence with gas compared to ice, suggesting a strong relationship in the context of states of matter.\"", "to": "\"GAS\"", "width": 9.0}, {"description": "\"Steam also shows moderate co-occurrence with water but less compared to its association with gas, suggesting a similar secondary relationship.\"", "from": "\"STEAM\"", "keywords": "\"association, moderate co-occurrence\"", "source_id": "chunk-c22c3b4107fedf31ff541c2373a8f1a6", "title": "\"Steam also shows moderate co-occurrence with water but less compared to its association with gas, suggesting a similar secondary relationship.\"", "to": "\"WATER\"", "width": 7.0}, {"description": "\"Both RNN and LSTM are types of neural networks, with LSTM being a specific variant designed to handle long-term dependencies.\"", "from": "\"RNN\"", "keywords": "\"neural network variants\"", "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Both RNN and LSTM are types of neural networks, with LSTM being a specific variant designed to handle long-term dependencies.\"", "to": "\"LSTM\"", "width": 7.0}, {"description": "\"The training process in RNNs often involves backpropagation through time for learning from sequential data.\"", "from": "\"RNN\"", "keywords": "\"training, gradient descent\"", "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"The training process in RNNs often involves backpropagation through time for learning from sequential data.\"", "to": "\"BACKPROPAGATION THROUGH TIME\"", "width": 8.0}, {"description": "\"Forget Gate and Input Gate are both components within the LSTM architecture that manage information flow.\"", "from": "\"FORGET GATE\"", "keywords": "\"LSTM components\"", "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Forget Gate and Input Gate are both components within the LSTM architecture that manage information flow.\"", "to": "\"INPUT GATE\"", "width": 6.0}, {"description": "\"Forget Gate, Input Gate, and Output Gate are all components in the LSTM architecture responsible for managing different aspects of the cell state.\"", "from": "\"FORGET GATE\"", "keywords": "\"LSTM components\"", "source_id": "chunk-3595353bc78e782128ef8148dfaf1357", "title": "\"Forget Gate, Input Gate, and Output Gate are all components in the LSTM architecture responsible for managing different aspects of the cell state.\"", "to": "\"OUTPUT GATE\"", "width": 6.0}, {"description": "\"The Forget Gate uses \u03c3 (sigmoid function) to determine which information is retained or discarded.\" \"", "from": "\"FORGET GATE\"", "keywords": "\"function usage, component interaction\"", "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"The Forget Gate uses \u03c3 (sigmoid function) to determine which information is retained or discarded.\" \"", "to": "\"\u03a3\"", "width": 14.0}, {"description": "\"The Input Gate works with the tanh layer to update and store new cell state values in the LSTM architecture.\" \"", "from": "\"INPUT GATE\"", "keywords": "\"layer interaction, component cooperation\"", "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"The Input Gate works with the tanh layer to update and store new cell state values in the LSTM architecture.\" \"", "to": "\"TANH\"", "width": 8.0}, {"description": "\"The Output Gate uses \u03c3 (sigmoid function) to decide what information will be outputted and passed on to the next layer.\" \"", "from": "\"OUTPUT GATE\"", "keywords": "\"function usage, decision making\"", "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"The Output Gate uses \u03c3 (sigmoid function) to decide what information will be outputted and passed on to the next layer.\" \"", "to": "\"\u03a3T\"", "width": 7.0}, {"description": "\"The Multi-Head Attention Layer is a key component of the Transformer architecture designed for efficient long-range dependency handling.\" \"", "from": "\"MULTI-HEAD ATTENTION LAYER\"", "keywords": "\"component role, architecture design\"", "source_id": "chunk-d7a96db6c2522c5e8ce8fe400d66902f", "title": "\"The Multi-Head Attention Layer is a key component of the Transformer architecture designed for efficient long-range dependency handling.\" \"", "to": "\"TRANSFORMER\"", "width": 10.0}, {"description": "\"Positional Encoding provides context that helps Multi-Head Attention Layer understand the importance of each word in a sentence.\"", "from": "\"MULTI-HEAD ATTENTION LAYER\"", "keywords": "\"context integration, attention accuracy\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Positional Encoding provides context that helps Multi-Head Attention Layer understand the importance of each word in a sentence.\"", "to": "\"POSITIONAL ENCODING\"", "width": 7.0}, {"description": "\"The attention mechanism is a key component of the Multi-Head Attention Layer responsible for assigning weights to vectors of words.\"", "from": "\"MULTI-HEAD ATTENTION LAYER\"", "keywords": "\"component integration, attention accuracy\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"The attention mechanism is a key component of the Multi-Head Attention Layer responsible for assigning weights to vectors of words.\"", "to": "\"ATTENTION MECHANISM\"", "width": 9.0}, {"description": "\"Sine and Cosine functions are used in Positional Encoding to provide context based on word positions.\"", "from": "\"POSITIONAL ENCODING\"", "keywords": "\"context provision, encoding mechanism\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Sine and Cosine functions are used in Positional Encoding to provide context based on word positions.\"", "to": "\"SINE AND COSINE FUNCTIONS\"", "width": 8.0}, {"description": "\"The Masked Multi-Head Attention layer is part of the Decoder Block in the Transformer Model, ensuring predictions are made based on past information.\"", "from": "\"MASKED MULTI-HEAD ATTENTION LAYER\"", "keywords": "\"attention mechanism, prediction control\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"The Masked Multi-Head Attention layer is part of the Decoder Block in the Transformer Model, ensuring predictions are made based on past information.\"", "to": "\"DECODER BLOCK\"", "width": 9.0}, {"description": "\"Query Embedding uses vectorial representations to understand and process user queries in Semantic Search.\"", "from": "\"VECTORIAL REPRESENTATIONS\"", "keywords": "\"representation understanding, query processing\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Query Embedding uses vectorial representations to understand and process user queries in Semantic Search.\"", "to": "\"QUERY EMBEDDING\"", "width": 8.0}, {"description": "\"Sentence Embeddings use vectorial representations of sentences for better understanding and processing in Semantic Search.\"", "from": "\"VECTORIAL REPRESENTATIONS\"", "keywords": "\"representation understanding, sentence processing\"", "source_id": "chunk-e27d93fef9843514dd8dbc524160d663", "title": "\"Sentence Embeddings use vectorial representations of sentences for better understanding and processing in Semantic Search.\"", "to": "\"SENTENCE EMBEDDINGS\"", "width": 8.0}, {"description": "\"Both Bi-Encoders and Cross-Encoders fall under the broader category of semantic search approaches with distinct methodologies in handling sentences for comparison.\"", "from": "\"BI-ENCODERS\"", "keywords": "\"comparison, methodology\"", "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"Both Bi-Encoders and Cross-Encoders fall under the broader category of semantic search approaches with distinct methodologies in handling sentences for comparison.\"", "to": "\"CROSS-ENCODERS\"", "width": 14.0}, {"description": "\"While both techniques are involved in text summarization, Extractive focuses on selecting important sentences while Abstractive aims to understand and generate a summary based on the document\u0027s semantics.\"", "from": "\"EXTRACTIVE SUMMARIZATION\"", "keywords": "\"techniques, contrast\"", "source_id": "chunk-5d200aadc0a3370985a8b9824fa2c738", "title": "\"While both techniques are involved in text summarization, Extractive focuses on selecting important sentences while Abstractive aims to understand and generate a summary based on the document\u0027s semantics.\"", "to": "\"ABSTRACTIVE SUMMARIZATION\"", "width": 12.0}, {"description": "\"Eigenvector centrality uses the connectivity matrix based on cosine similarity to rank sentences, indicating a dependency between these two concepts.\"", "from": "\"EIGENVECTOR CENTRALITY\"", "keywords": "\"ranking method, similarity measure\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Eigenvector centrality uses the connectivity matrix based on cosine similarity to rank sentences, indicating a dependency between these two concepts.\"", "to": "\"CONNECTIVITY MATRIX\"", "width": 8.0}, {"description": "\"Eigenvector centrality uses sentence similarity scores as a basis for ranking sentences in graph representations.\"", "from": "\"EIGENVECTOR CENTRALITY\"", "keywords": "\"ranking method, similarity measure\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Eigenvector centrality uses sentence similarity scores as a basis for ranking sentences in graph representations.\"", "to": "\"SENTENCE SIMILARITY SCORES\"", "width": 14.0}, {"description": "\"Both eigenvector centrality and state-of-the-art models like BERT focus on contextual understanding but operate in different domains.\"", "from": "\"EIGENVECTOR CENTRALITY\"", "keywords": "\"contextual understanding, domain-specific application\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Both eigenvector centrality and state-of-the-art models like BERT focus on contextual understanding but operate in different domains.\"", "to": "\"STATE-OF-THE-ART MODELS AND TECHNIQUES\"", "width": 5.0}, {"description": "\"The PageRank method is adapted for generating LexRank scores in the context of natural language processing, showing an application and extension of the original algorithm to a new domain.\"", "from": "\"PAGERANK METHOD\"", "keywords": "\"algorithm adaptation, relevance scoring\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"The PageRank method is adapted for generating LexRank scores in the context of natural language processing, showing an application and extension of the original algorithm to a new domain.\"", "to": "\"LEXRANK SCORE\"", "width": 9.0}, {"description": "\"The damping factor d ensures convergence in PageRank calculations, maintaining stable scores for nodes in the graph.\"", "from": "\"PAGERANK METHOD\"", "keywords": "\"convergence control, stability\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"The damping factor d ensures convergence in PageRank calculations, maintaining stable scores for nodes in the graph.\"", "to": "\"DAMPING FACTOR (D)\"", "width": 7.0}, {"description": "\"Eigenvalues derived from the similarity matrix are used to calculate LexRank scores, indicating sentence relevance.\"", "from": "\"LEXRANK SCORE\"", "keywords": "\"relevance scoring, eigenvalue decomposition\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Eigenvalues derived from the similarity matrix are used to calculate LexRank scores, indicating sentence relevance.\"", "to": "\"EIGENVALUES\"", "width": 8.0}, {"description": "\"Unidirectional training is a foundational concept for state-of-the-art models like BERT which build upon it to achieve better performance.\"", "from": "\"STATE-OF-THE-ART MODELS AND TECHNIQUES\"", "keywords": "\"foundation, improvement\"", "source_id": "chunk-abe3791c0e495b4a72310f8a36c50056", "title": "\"Unidirectional training is a foundational concept for state-of-the-art models like BERT which build upon it to achieve better performance.\"", "to": "\"UNIDIRECTIONAL TRAINING\"", "width": 6.0}, {"description": "\"Bidirectional training contrasts with unidirectional training by allowing models to understand words based on both their left and right context, whereas unidirectional training only looks at one direction.\"", "from": "\"UNIDIRECTIONAL TRAINING\"", "keywords": "\"contrast, improvement\"", "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Bidirectional training contrasts with unidirectional training by allowing models to understand words based on both their left and right context, whereas unidirectional training only looks at one direction.\"", "to": "\"BIDIRECTIONAL TRAINING\"", "width": 7.0}, {"description": "\"Both Masked Language Modeling and Next Sentence Prediction are techniques used in BERT\u0027s pre-training phase but serve different purposes: the former reduces bias by masking words, while the latter helps understand sentence relationships.\"", "from": "\"MASKED LANGUAGE MODELING (MLM)\"", "keywords": "\"technique complementarity\"", "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Both Masked Language Modeling and Next Sentence Prediction are techniques used in BERT\u0027s pre-training phase but serve different purposes: the former reduces bias by masking words, while the latter helps understand sentence relationships.\"", "to": "\"NEXT SENTENCE PREDICTION (NSP)\"", "width": 6.0}, {"description": "\"The Adam Optimizer is used in the fine-tuning phase of BERT to train the entire network after bidirectional training has enhanced model understanding.\"", "from": "\"BIDIRECTIONAL TRAINING\"", "keywords": "\"training method, optimization\"", "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"The Adam Optimizer is used in the fine-tuning phase of BERT to train the entire network after bidirectional training has enhanced model understanding.\"", "to": "\"ADAM OPTIMIZER\"", "width": 6.0}, {"description": "\"Transformer blocks include the Token Embedding Layer which assigns values to words based on vocabulary IDs during input embedding.\"", "from": "\"TRANSFORMER BLOCKS\"", "keywords": "\"architecture component, assignment of values\"", "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Transformer blocks include the Token Embedding Layer which assigns values to words based on vocabulary IDs during input embedding.\"", "to": "\"TOKEN EMBEDDING LAYER\"", "width": 5.0}, {"description": "\"Transformer blocks include the Segmentation Embedding Layer that distinguishes whether a word belongs to sentence A or B.\"", "from": "\"TRANSFORMER BLOCKS\"", "keywords": "\"architecture component, differentiation\"", "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Transformer blocks include the Segmentation Embedding Layer that distinguishes whether a word belongs to sentence A or B.\"", "to": "\"SEGMENTATION EMBEDDING LAYER\"", "width": 5.0}, {"description": "\"Transformer blocks also contain the Position Embedding Layer which indicates the position of each word in a sentence.\"", "from": "\"TRANSFORMER BLOCKS\"", "keywords": "\"architecture component, positioning\"", "source_id": "chunk-03104590ddb41a3cfeb2c96dccb5c2f1", "title": "\"Transformer blocks also contain the Position Embedding Layer which indicates the position of each word in a sentence.\"", "to": "\"POSITION EMBEDDING LAYER\"", "width": 5.0}, {"description": "\"Fine-tuning involves adding an additional layer and training the BERT BASE model for a few epochs.\"", "from": "\"BERT BASE\"", "keywords": "\u003csource_entity\u003efine-tuning", "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Fine-tuning involves adding an additional layer and training the BERT BASE model for a few epochs.\"", "to": "\"FINE-TUNING\"", "width": 1.0}, {"description": "\"Hidden size H is a parameter specified in the BERT architecture.\"", "from": "\"BERT ARCHITECTURE\"", "keywords": "\u003csource_entity\u003eBERT architecture", "source_id": "chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95", "title": "\"Hidden size H is a parameter specified in the BERT architecture.\"", "to": "\"HIDDEN SIZE H\"", "width": 1.0}, {"description": "\"Nils Reimers published TSDAE, indicating a creator-author relationship.\"", "from": "\"NILS REIMERS\"", "keywords": "\"creatorship, publication date\"", "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"Nils Reimers published TSDAE, indicating a creator-author relationship.\"", "to": "\"TSDAE\"", "width": 8.0}, {"description": "\"Both MLM and TSDAE are techniques used for model adaptation but serve different purposes, indicating a broader goal of model improvement.\"::\u003cSEP\u003e\"TSDAE outperforms MLM, indicating an improvement relationship.\"", "from": "\"MLM\"", "keywords": "\"model improvement, task specialization\"\u003cSEP\u003e\"performance comparison\"", "source_id": "chunk-dece8100a2db817f460c5edaaa852208\u003cSEP\u003echunk-baa53bb86fb07d800e53012d99a5e681", "title": "\"Both MLM and TSDAE are techniques used for model adaptation but serve different purposes, indicating a broader goal of model improvement.\"::\u003cSEP\u003e\"TSDAE outperforms MLM, indicating an improvement relationship.\"", "to": "\"TSDAE\"", "width": 32.0}, {"description": "\"TSDAE aims to improve the domain knowledge for STS tasks on specific domains.\"", "from": "\"STS TASK\"", "keywords": "\"task improvement\"", "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"TSDAE aims to improve the domain knowledge for STS tasks on specific domains.\"", "to": "\"TSDAE\"", "width": 7.0}, {"description": "\"The NLI concept involves determining sentence relationships, which can be extended to tasks like STS where similar principles are applied for semantic textual similarity evaluation.\"", "from": "\"STS TASK\"", "keywords": "\"task relationship, inference evaluation\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The NLI concept involves determining sentence relationships, which can be extended to tasks like STS where similar principles are applied for semantic textual similarity evaluation.\"", "to": "\"NLI\"", "width": 6.0}, {"description": "\"The dataset is used to train models for the STS task. This indicates the purpose of the dataset in relation to a specific NLP task.\"", "from": "\"STS TASK\"", "keywords": "\"task-relevance, training data\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"The dataset is used to train models for the STS task. This indicates the purpose of the dataset in relation to a specific NLP task.\"", "to": "\"STS DATASETS: ASSIN, ASSIN2 AND STSB MULTI MT PORTUGUESE SUB-DATASET\"", "width": 16.0}, {"description": "\"The STS task involves determining the semantic similarity between sentences, which is a similar concept to Semantic Textual Similarity.\"", "from": "\"STS TASK\"", "keywords": "\"semantics, comparison\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"The STS task involves determining the semantic similarity between sentences, which is a similar concept to Semantic Textual Similarity.\"", "to": "\"SEMANTIC TEXTUAL SIMILARITY\"", "width": 8.0}, {"description": "\"MS MARCO was used to fine-tune the T5 model, indicating data usage in training.\"", "from": "\"MS MARCO [26]\"", "keywords": "\"training data, dataset use\"", "source_id": "chunk-dece8100a2db817f460c5edaaa852208", "title": "\"MS MARCO was used to fine-tune the T5 model, indicating data usage in training.\"", "to": "\"T5\"", "width": 8.0}, {"description": "\"The T5 model uses the Query Generation approach to generate multiple questions from a passage, which can be far from ideal due to the model\u0027s general nature.\"", "from": "\"T5 MODEL\"", "keywords": "\"query generation, model limitations\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"The T5 model uses the Query Generation approach to generate multiple questions from a passage, which can be far from ideal due to the model\u0027s general nature.\"", "to": "\"QUERY GENERATION (GENQ)\"", "width": 8.0}, {"description": "\"The T5 model uses multiple passages during its training and generation of queries.\"", "from": "\"T5 MODEL\"", "keywords": "\u003c\"training data, query generation\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"The T5 model uses multiple passages during its training and generation of queries.\"", "to": "\"PASSAGES\"", "width": 14.0}, {"description": "\"The Margin Mean Squared Error Loss is used to train the T5 model in certain phases like Pseudo Labeling.\"", "from": "\"T5 MODEL\"", "keywords": "\u003c\"training loss, relevance identification\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"The Margin Mean Squared Error Loss is used to train the T5 model in certain phases like Pseudo Labeling.\"", "to": "\"MARGIN MEAN SQUARED ERROR LOSS\"", "width": 14.0}, {"description": "\"Both models were used in an attempt to generate queries, with T5 providing a more balanced approach compared to GPT3.\"", "from": "\"T5 MODEL\"", "keywords": "\u003c\"query generation methods comparison\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"Both models were used in an attempt to generate queries, with T5 providing a more balanced approach compared to GPT3.\"", "to": "\"GPT3 MODEL PROVIDED BY OPEN AI\"", "width": 6.0}, {"description": "\"SBERT is fine-tuned using the Multiple Negatives Ranking (MNR) loss with passages and synthetically generated query pairs to improve its performance on downstream tasks.\"", "from": "\"SBERT\"", "keywords": "\"fine-tuning, loss function\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"SBERT is fine-tuned using the Multiple Negatives Ranking (MNR) loss with passages and synthetically generated query pairs to improve its performance on downstream tasks.\"", "to": "\"MULTIPLE NEGATIVES RANKING (MNR) LOSS\"", "width": 9.0}, {"description": "\"SBERT utilizes the Cross-Encoder in the Pseudo Labeling phase for calculating similarity scores between positive and negative passages.\"", "from": "\"SBERT\"", "keywords": "\u003c\"embedding comparison, similarity calculation\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"SBERT utilizes the Cross-Encoder in the Pseudo Labeling phase for calculating similarity scores between positive and negative passages.\"", "to": "\"CROSS-ENCODER\"", "width": 16.0}, {"description": "\"Reimers and Gurevych showed the effectiveness of SBERT compared to BERT in tasks like semantic similarity comparison.\"", "from": "\"SBERT\"", "keywords": "\"research contribution, performance benchmark\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"Reimers and Gurevych showed the effectiveness of SBERT compared to BERT in tasks like semantic similarity comparison.\"", "to": "\"REIMERS AND GUREVYCH\"", "width": 8.0}, {"description": "\"The SNLI dataset is used for fine-tuning SBERT through a softmax classifier approach in natural language inference tasks.\"", "from": "\"SBERT\"", "keywords": "\"dataset usage, fine-tuning method\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The SNLI dataset is used for fine-tuning SBERT through a softmax classifier approach in natural language inference tasks.\"", "to": "\"SNLI\"", "width": 7.0}, {"description": "\"The Multi-Genre NLI dataset complements the SNLI dataset by providing additional contexts for training SBERT\u0027s natural language inference capabilities.\"", "from": "\"SBERT\"", "keywords": "\"context diversity, data augmentation\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The Multi-Genre NLI dataset complements the SNLI dataset by providing additional contexts for training SBERT\u0027s natural language inference capabilities.\"", "to": "\"MULTI-GENRE NLI\"", "width": 6.0}, {"description": "\"SBERT uses a Siamese architecture that involves two BERT models with entangled weights for sentence pair comparison.\"", "from": "\"SBERT\"", "keywords": "\"architecture usage, model structure\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"SBERT uses a Siamese architecture that involves two BERT models with entangled weights for sentence pair comparison.\"", "to": "\"SIAMESE ARCHITECTURE\"", "width": 8.0}, {"description": "\"The SBERT model utilizes a pooling operation to transform token embeddings into fixed-size vectors for comparison.\"", "from": "\"SBERT\"", "keywords": "\"embedding processing, vectorization\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The SBERT model utilizes a pooling operation to transform token embeddings into fixed-size vectors for comparison.\"", "to": "\"POOLING OPERATION\"", "width": 7.0}, {"description": "\"SBERT is fine-tuned using the Softmax loss approach by applying a softmax classifier on top of its siamese architecture model.\"", "from": "\"SBERT\"", "keywords": "\"fine-tuning method, classification approach\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"SBERT is fine-tuned using the Softmax loss approach by applying a softmax classifier on top of its siamese architecture model.\"", "to": "\"SOFTMAX LOSS APPROACH\"", "width": 9.0}, {"description": "\"SBERT evaluation uses Multilingual Knowledge Distillation to introduce techniques for multilingual sentence embeddings.\"", "from": "\"SBERT\"", "keywords": "\"technique introduction, evaluation method\"", "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"SBERT evaluation uses Multilingual Knowledge Distillation to introduce techniques for multilingual sentence embeddings.\"", "to": "\"MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)\"", "width": 8.0}, {"description": "\"The JNLP team focused on using a self-labeled approach and fine-tuning pre-trained models, which may involve SBERT as part of their semantic techniques.\"", "from": "\"SBERT\"", "keywords": "\"technique combination, NLP approach\"", "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The JNLP team focused on using a self-labeled approach and fine-tuning pre-trained models, which may involve SBERT as part of their semantic techniques.\"", "to": "\"JNLP TEAM\"", "width": 14.0}, {"description": "\"The OvGU team used Sentence-BERT embeddings in Task 3 for the third task of COLIEE 2021.\"", "from": "\"SBERT\"", "keywords": "\"embedding technique, effective use\"", "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The OvGU team used Sentence-BERT embeddings in Task 3 for the third task of COLIEE 2021.\"", "to": "\"OVGU TEAM\"", "width": 18.0}, {"description": "\"The nigam team combined SBERT with Sent2Vec and BM25 scores to achieve their results in Task 1 of the competition.\"", "from": "\"SBERT\"", "keywords": "\"technique integration, scoring method\"", "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The nigam team combined SBERT with Sent2Vec and BM25 scores to achieve their results in Task 1 of the competition.\"", "to": "\"NIGAM TEAM\"", "width": 16.0}, {"description": "\"Generative Pseudo Labeling involves three phases: Query Generation, Negative Mining, and Pseudo Labeling, with the latter step using a Cross-Encoder to calculate score margins between positive and negative passages.\"", "from": "\"GENERATIVE PSEUDO LABELING (GPL)\"", "keywords": "\"domain adaptation, pseudo labeling\"", "source_id": "chunk-6eeb6febf5ce46ec96655d84dc54cd2f", "title": "\"Generative Pseudo Labeling involves three phases: Query Generation, Negative Mining, and Pseudo Labeling, with the latter step using a Cross-Encoder to calculate score margins between positive and negative passages.\"", "to": "\"PSEUDO LABELING\"", "width": 9.0}, {"description": "\"The STS12-16 datasets and the STS benchmark (STSb) are both used to evaluate SBERT\u0027s performance in semantic textual similarity tasks, though they differ slightly in structure and scope.\"", "from": "\"STS12-16 DATASETS\"", "keywords": "\"dataset usage, evaluation framework\"", "source_id": "chunk-b6270162d82d1fef624d494a11c5caca", "title": "\"The STS12-16 datasets and the STS benchmark (STSb) are both used to evaluate SBERT\u0027s performance in semantic textual similarity tasks, though they differ slightly in structure and scope.\"", "to": "\"STS BENCHMARK (STSB)\"", "width": 7.0}, {"description": "\"The SBERT -STSb-base model was specifically trained on the STSb benchmark dataset, which measures its effectiveness in sentence embedding tasks.\"", "from": "\"STSB DATASET\"", "keywords": "\"training, evaluation\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"The SBERT -STSb-base model was specifically trained on the STSb benchmark dataset, which measures its effectiveness in sentence embedding tasks.\"", "to": "\"SBERT -STSB-BASE\"", "width": 8.0}, {"description": "\"Similarly, SRoBERTa-STSb-base is also a model trained on the STSb benchmark dataset to enhance performance in sentence embedding.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"training, evaluation\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"Similarly, SRoBERTa-STSb-base is also a model trained on the STSb benchmark dataset to enhance performance in sentence embedding.\"", "to": "\"SROBERTA-STSB-BASE\"", "width": 7.0}, {"description": "\"InferSent - GloVe was evaluated using the STSb dataset to measure its performance in sentence embedding tasks.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"evaluation, model\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"InferSent - GloVe was evaluated using the STSb dataset to measure its performance in sentence embedding tasks.\"", "to": "\"INFERSENT - GLOVE\"", "width": 7.0}, {"description": "\"The Universal Sentence Encoder also participated in evaluations on the STSb benchmark dataset.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"evaluation, model\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"The Universal Sentence Encoder also participated in evaluations on the STSb benchmark dataset.\"", "to": "\"UNIVERSAL SENTENCE ENCODER\"", "width": 8.0}, {"description": "\"BERT -STSb-large was specifically trained and evaluated on the STSb benchmark dataset to ensure high performance in sentence embedding tasks.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"training, evaluation\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"BERT -STSb-large was specifically trained and evaluated on the STSb benchmark dataset to ensure high performance in sentence embedding tasks.\"", "to": "\"BERT -STSB-LARGE\"", "width": 9.0}, {"description": "\"SBERT -STSb-large is another model that underwent training and evaluation on the STSb benchmark dataset for its superior sentence embedding capabilities.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"training, evaluation\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SBERT -STSb-large is another model that underwent training and evaluation on the STSb benchmark dataset for its superior sentence embedding capabilities.\"", "to": "\"SBERT -STSB-LARGE\"", "width": 9.0}, {"description": "\"SRoBERTa-STSb-large was trained and evaluated on the STSb benchmark dataset to enhance performance in sentence embedding tasks.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"training, evaluation\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SRoBERTa-STSb-large was trained and evaluated on the STSb benchmark dataset to enhance performance in sentence embedding tasks.\"", "to": "\"SROBERTA-STSB-LARGE\"", "width": 8.0}, {"description": "\"BERT -NLI-STSb-base participated in evaluations on the STSb benchmark dataset along with other models for comprehensive sentence embedding performance.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"evaluation, model\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"BERT -NLI-STSb-base participated in evaluations on the STSb benchmark dataset along with other models for comprehensive sentence embedding performance.\"", "to": "\"BERT -NLI-STSB-BASE\"", "width": 9.0}, {"description": "\"SBERT -NLI-STSb-base was also evaluated using the STSb benchmark dataset to measure its effectiveness in sentence embedding tasks.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"evaluation, model\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SBERT -NLI-STSb-base was also evaluated using the STSb benchmark dataset to measure its effectiveness in sentence embedding tasks.\"", "to": "\"SBERT -NLI-STSB-BASE\"", "width": 8.0}, {"description": "\"SRoBERTa-NLI-STSb-base was trained and evaluated on the STSb benchmark dataset to ensure robust performance in sentence embedding tasks.\"", "from": "\"STSB DATASET\"", "keywords": "\u003c\"training, evaluation\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"SRoBERTa-NLI-STSb-base was trained and evaluated on the STSb benchmark dataset to ensure robust performance in sentence embedding tasks.\"", "to": "\"SROBERTA-NLI-STSB-BASE\"", "width": 8.0}, {"description": "\"The introduction of Multilingual Knowledge Distillation (MKD) aims to improve the SBERT -STSb-base model\u0027s performance across multiple languages.\"", "from": "\"SBERT -STSB-BASE\"", "keywords": "\u003c\"improvement, technique\"", "source_id": "chunk-53db1ddb7040c30dab02f6b0bd355f81", "title": "\"The introduction of Multilingual Knowledge Distillation (MKD) aims to improve the SBERT -STSb-base model\u0027s performance across multiple languages.\"", "to": "\"MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)\"", "width": 6.0}, {"description": "\"Neil Reimers is credited for developing the MKD technique, which is crucial in this work.\"", "from": "\"MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)\"", "keywords": "\"invention, development\"", "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Neil Reimers is credited for developing the MKD technique, which is crucial in this work.\"", "to": "\"NEIL REIMERS\"", "width": 8.0}, {"description": "\"Both techniques are part of the broader NLP knowledge transfer methods but serve different purposes. \"", "from": "\"MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)\"", "keywords": "\"techniques, application\"", "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Both techniques are part of the broader NLP knowledge transfer methods but serve different purposes. \"", "to": "\"METADATA KNOWLEDGE DISTILLATION (METAKD)\"", "width": 5.0}, {"description": "\"The Teacher Model M provides the reference vectors that the Student Model \u02c6M aims to mimic through Multilingual Knowledge Distillation.\"", "from": "\"TEACHER MODEL M\"", "keywords": "\"reference model, student model training\"", "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"The Teacher Model M provides the reference vectors that the Student Model \u02c6M aims to mimic through Multilingual Knowledge Distillation.\"", "to": "\"STUDENT MODEL \u02c6M\"", "width": 9.0}, {"description": "\"Multilingual SBERT versions like paraphrase-multilingual-mpnet-base are more advanced and can provide better embeddings compared to the BERT Base variant in Portuguese.\"", "from": "\"MULTILINGUAL SBERT VERSIONS\"", "keywords": "\"model comparison, embedding accuracy\"", "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"Multilingual SBERT versions like paraphrase-multilingual-mpnet-base are more advanced and can provide better embeddings compared to the BERT Base variant in Portuguese.\"", "to": "\"NEURALMIND/BERT-BASE-PORTUGUESE-CASED\"", "width": 7.0}, {"description": "\"Both Multilingual SBERT versions like paraphrase-multilingual-MiniLM-L12 and neuralmind/bert-large-portuguese-cased are used for generating embeddings in multiple languages, with MiniLM being a smaller model compared to BERT Large.\"", "from": "\"MULTILINGUAL SBERT VERSIONS\"", "keywords": "\"model comparison, embedding size\"", "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"Both Multilingual SBERT versions like paraphrase-multilingual-MiniLM-L12 and neuralmind/bert-large-portuguese-cased are used for generating embeddings in multiple languages, with MiniLM being a smaller model compared to BERT Large.\"", "to": "\"NEURALMIND/BERT-LARGE-PORTUGUESE-CASED\"", "width": 7.0}, {"description": "\"Both neuralmind/bert-large-portuguese-cased and neuralmind/bert-base-portuguese-cased are pre-trained models for the Portuguese language, with BERT Large having more parameters than BERT Base.\"", "from": "\"NEURALMIND/BERT-BASE-PORTUGUESE-CASED\"", "keywords": "\"model relation, parameter comparison\"", "source_id": "chunk-493aab369fd44529a838be55e938c506", "title": "\"Both neuralmind/bert-large-portuguese-cased and neuralmind/bert-base-portuguese-cased are pre-trained models for the Portuguese language, with BERT Large having more parameters than BERT Base.\"", "to": "\"NEURALMIND/BERT-LARGE-PORTUGUESE-CASED\"", "width": 5.0}, {"description": "\"Zhuyun Dai and Jamie Callan utilized the BrWaC corpus in their research on contextual neural language models.\"", "from": "\"BRWAC CORPUS\"", "keywords": "\"research, data usage\"", "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"Zhuyun Dai and Jamie Callan utilized the BrWaC corpus in their research on contextual neural language models.\"", "to": "\"ZHUYUN DAI AND JAMIE CALLAN\"", "width": 12.0}, {"description": "\"The performance of BERTimbau was also tested using the ClueWeb09-B dataset, which is used alongside Robust04 for comparison.\"", "from": "\"ZHUYUN DAI AND JAMIE CALLAN\"", "keywords": "\"performance test, dataset\"", "source_id": "chunk-813c546217d2864adf1fc0789841ad36", "title": "\"The performance of BERTimbau was also tested using the ClueWeb09-B dataset, which is used alongside Robust04 for comparison.\"", "to": "\"CLUEWEB09-B\"", "width": 14.0}, {"description": "\"The JNLP team concentrated on dealing with large articles by conducting text chunking and using a self-labeled approach for the first task.\"", "from": "\"JNLP TEAM\"", "keywords": "\"text processing, NLP strategy\"", "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The JNLP team concentrated on dealing with large articles by conducting text chunking and using a self-labeled approach for the first task.\"", "to": "\"TASK 1 OF COLIEE 2021\"", "width": 16.0}, {"description": "\"The JNLP team\u0027s work is included in a book edited by T. Nigam et al.\" \"", "from": "\"JNLP TEAM\"", "keywords": "\"publication, inclusion\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"The JNLP team\u0027s work is included in a book edited by T. Nigam et al.\" \"", "to": "\"T. NGUYEN ET AL.\"", "width": 16.0}, {"description": "\"The JNLP team participated in COLIEE 2020 and presented their work on deep learning for legal processing.\" \"", "from": "\"JNLP TEAM\"", "keywords": "\"participation, presentation\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"The JNLP team participated in COLIEE 2020 and presented their work on deep learning for legal processing.\" \"", "to": "\"COLIEE 2020\"", "width": 18.0}, {"description": "\"The OvGU team focused on statute law retrieval using a two-stage TF-IDF vectorization combined with Sentence-BERT embeddings for the third task.\"", "from": "\"OVGU TEAM\"", "keywords": "\"task-specific approach, effective technique\"", "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The OvGU team focused on statute law retrieval using a two-stage TF-IDF vectorization combined with Sentence-BERT embeddings for the third task.\"", "to": "\"TASK 3 OF COLIEE 2021 - STATUTE LAW RETRIEVAL\"", "width": 18.0}, {"description": "\"The nigam team proposed an approach using SBERT and Sent2Vec in combination with BM25 for the first task of the competition.\"", "from": "\"NIGAM TEAM\"", "keywords": "\"technique integration, scoring method implementation\"", "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"The nigam team proposed an approach using SBERT and Sent2Vec in combination with BM25 for the first task of the competition.\"", "to": "\"TASK 1 OF COLIEE 2021\"", "width": 16.0}, {"description": "\"Mi-Young Kim et al. discussed their work in the context of the COLIEE 2021 competition, specifically focusing on legal information retrieval and question-answering tasks.\"", "from": "\"COLIEE 2021\"", "keywords": "\"research contribution, task focus\"", "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"Mi-Young Kim et al. discussed their work in the context of the COLIEE 2021 competition, specifically focusing on legal information retrieval and question-answering tasks.\"", "to": "\"MI-YOUNG KIM ET AL.\"", "width": 16.0}, {"description": "\"Nuno Cordeiro created the LeSSE system as part of his master\u2019s thesis work, which merges common document retrieval techniques with semantic search abilities.\"", "from": "\"NUNO CORDEIRO\"", "keywords": "\"system creation, research application\"", "source_id": "chunk-4178cfa608054c267be41d058b830af4", "title": "\"Nuno Cordeiro created the LeSSE system as part of his master\u2019s thesis work, which merges common document retrieval techniques with semantic search abilities.\"", "to": "\"LESSE\"", "width": 18.0}, {"description": "\"Table 3.4 references the results of Task 1, indicating a direct relationship between the table and task results.\"", "from": "\"TABLE 3.4\"", "keywords": "\"documentation, task results\"", "source_id": "chunk-afd8ce7e1a7d61d34be9bdded6cff755", "title": "\"Table 3.4 references the results of Task 1, indicating a direct relationship between the table and task results.\"", "to": "\"TASK 1 RESULTS\"", "width": 6.0}, {"description": "\"The Legal-BERTimbau model is discussed in Chapter 5 and contributes to the overall effectiveness of the semantic search system developed under Project IRIS.\"", "from": "\"PROJECT IRIS\"", "keywords": "\"model development, system enhancement\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"The Legal-BERTimbau model is discussed in Chapter 5 and contributes to the overall effectiveness of the semantic search system developed under Project IRIS.\"", "to": "\"LEGAL-BERTIMBAU MODEL\"", "width": 16.0}, {"description": "\"ecli-indexer6 was used by Project IRIS members for data collection, indicating a direct relationship in the project workflow.\"", "from": "\"PROJECT IRIS\"", "keywords": "\"data acquisition, tool usage\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"ecli-indexer6 was used by Project IRIS members for data collection, indicating a direct relationship in the project workflow.\"", "to": "\"ECLI-INDEXER6\"", "width": 14.0}, {"description": "\"The work presented originates from Project IRIS, which is a more significant project involving multiple members and tasks.\"::", "from": "\"PROJECT IRIS\"", "keywords": "\u003c\"project origin, collaboration\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The work presented originates from Project IRIS, which is a more significant project involving multiple members and tasks.\"::", "to": "\"WORK PRESENTED IN THIS DOCUMENT\"", "width": 16.0}, {"description": "\"Project IRIS is the context within which developing a prototype occurred, providing the framework and resources.\"", "from": "\"PROJECT IRIS\"", "keywords": "\u003csource_entity\u003e", "source_id": "chunk-fc9187ad3fd00c96d11f9934e91f7051", "title": "\"Project IRIS is the context within which developing a prototype occurred, providing the framework and resources.\"", "to": "\"DEVELOPING A PROTOTYPE\"", "width": 1.0}, {"description": "\"Chapter 5 provides detailed information on the Legal-BERTimbau model, which is essential for understanding its contribution to the semantic search system.\"", "from": "\"CHAPTER 5\"", "keywords": "\"document content, model description\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Chapter 5 provides detailed information on the Legal-BERTimbau model, which is essential for understanding its contribution to the semantic search system.\"", "to": "\"LEGAL-BERTIMBAU MODEL\"", "width": 16.0}, {"description": "\"Relator 1 is mentioned in an indexed document discussed in Chapter 5, showing a direct relationship through legal documents and their content.\"", "from": "\"CHAPTER 5\"", "keywords": "\"document content, person reference\"", "source_id": "chunk-9c13271d2c9dfd85cce22bb863dd2aa7", "title": "\"Relator 1 is mentioned in an indexed document discussed in Chapter 5, showing a direct relationship through legal documents and their content.\"", "to": "\"RELATOR 1\"", "width": 6.0}, {"description": "\"The training dataset relies on the data pre-processing step for preparing the documents used in model training.\"", "from": "\"TRAINING DATASET\"", "keywords": "\"preparation, dependency\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The training dataset relies on the data pre-processing step for preparing the documents used in model training.\"", "to": "\"DATA PRE-PROCESSING STEP\"", "width": 8.0}, {"description": "\"The testing dataset benefits from the data pre-processing step as it uses cleaned and split sentences for evaluating model performance.\"", "from": "\"TESTING DATASET\"", "keywords": "\"evaluation, dependency\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The testing dataset benefits from the data pre-processing step as it uses cleaned and split sentences for evaluating model performance.\"", "to": "\"DATA PRE-PROCESSING STEP\"", "width": 7.0}, {"description": "\"Similar to the testing dataset, the validation dataset also depends on the data pre-processing step for its preparation and use in validating models.\"", "from": "\"VALIDATION DATASET\"", "keywords": "\"validation, dependency\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"Similar to the testing dataset, the validation dataset also depends on the data pre-processing step for its preparation and use in validating models.\"", "to": "\"DATA PRE-PROCESSING STEP\"", "width": 8.0}, {"description": "\"The data pre-processing step prepares the text which is later fed into the Bi-Encoder for creating document embeddings.\"", "from": "\"DATA PRE-PROCESSING STEP\"", "keywords": "\"preparation, embedding creation\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The data pre-processing step prepares the text which is later fed into the Bi-Encoder for creating document embeddings.\"", "to": "\"BI-ENCODER\"", "width": 9.0}, {"description": "\"The Acord\u00e3o involves the defendant (Re) in a legal dispute.\"", "from": "\"ACORD\u00c3O\"", "keywords": "\"legal proceeding, defendant\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The Acord\u00e3o involves the defendant (Re) in a legal dispute.\"", "to": "\"RE\"", "width": 7.0}, {"description": "\"The Acord\u00e3o involves the plaintiff (Autor) in a legal dispute.\"", "from": "\"ACORD\u00c3O\"", "keywords": "\"legal proceeding, plaintiff\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The Acord\u00e3o involves the plaintiff (Autor) in a legal dispute.\"", "to": "\"AUTOR\"", "width": 7.0}, {"description": "\"The Acord\u00e3o originates from the DGSI as indicated by its URL.\"", "from": "\"ACORD\u00c3O\"", "keywords": "\"origin, source\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The Acord\u00e3o originates from the DGSI as indicated by its URL.\"", "to": "\"DGSI\"", "width": 8.0}, {"description": "\"The Acord\u00e3o contributes to the body of jurisprudence.\"", "from": "\"ACORD\u00c3O\"", "keywords": "\"legal precedent, ruling\"", "source_id": "chunk-a8661700ca5b769e561d0c13fe452ae7", "title": "\"The Acord\u00e3o contributes to the body of jurisprudence.\"", "to": "\"JURISPRUD\u00caNCIA\"", "width": 9.0}, {"description": "\"The HuggingFace platform is where the stjiris/IRIS sts dataset is publicly available.\"", "from": "\"HUGGINGFACE\"", "keywords": "\"dataset availability, resource location\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"The HuggingFace platform is where the stjiris/IRIS sts dataset is publicly available.\"", "to": "\"STJIRIS/IRIS STS DATASET\"", "width": 8.0}, {"description": "\"Neural Networks enable transfer learning by facilitating the adaptation of large pre-trained models like BERTimbau.\"\u003c", "from": "\"NEURAL NETWORKS\"", "keywords": "methodology, enabling technique", "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"Neural Networks enable transfer learning by facilitating the adaptation of large pre-trained models like BERTimbau.\"\u003c", "to": "\"TRANSFER LEARNING TECHNIQUE\"", "width": 7.0}, {"description": "\"Large pre-trained language models are foundational in the training process for Legal-BERTimbau.\"\u003c", "from": "\"LARGE PRE-TRAINED LANGUAGE MODELS\"", "keywords": "foundation, basis", "source_id": "chunk-ca0f485e268dd70b104be9c53d4b68fd", "title": "\"Large pre-trained language models are foundational in the training process for Legal-BERTimbau.\"\u003c", "to": "\"LEGAL-BERTIMBAU TRAINING\"", "width": 9.0}, {"description": "\"assins is part of the resources used for the STS evaluation task.\"", "from": "\"STS EVALUATION\"", "keywords": "\"dataset usage, training resource\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"assins is part of the resources used for the STS evaluation task.\"", "to": "\"ASSINS\"", "width": 7.0}, {"description": "\"assin2 is part of the resources used for the STS evaluation task.\"", "from": "\"STS EVALUATION\"", "keywords": "\"dataset usage, training resource\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"assin2 is part of the resources used for the STS evaluation task.\"", "to": "\"ASSIN2\"", "width": 7.0}, {"description": "\"stsb multi mt Portuguese sub-dataset is part of the resources used for the STS evaluation task.\"", "from": "\"STS EVALUATION\"", "keywords": "\"dataset usage, training resource\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"stsb multi mt Portuguese sub-dataset is part of the resources used for the STS evaluation task.\"", "to": "\"STSB MULTI MT PORTUGUESE SUB-DATASET\"", "width": 7.0}, {"description": "\"stjiris/bert-large-portuguese-cased-legal-tsdae is based on rufimelo/Legal-BERTimbau-large for STS evaluation.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "keywords": "\"model derivation, STS task\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"stjiris/bert-large-portuguese-cased-legal-tsdae is based on rufimelo/Legal-BERTimbau-large for STS evaluation.\"", "to": "\"RUFIMELO/LEGAL-BERTIMBAU-LARGE\"", "width": 16.0}, {"description": "\"The learning rate is a hyperparameter used to train the STS model variant.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "keywords": "\"hyperparameter tuning, training process\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"The learning rate is a hyperparameter used to train the STS model variant.\"", "to": "\"LEARNING RATE\"", "width": 8.0}, {"description": "\"The Adam optimization algorithm was used during the training of the BERT variant.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "keywords": "\"optimization technique, training methodology\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"The Adam optimization algorithm was used during the training of the BERT variant.\"", "to": "\"ADAM OPTIMIZATION ALGORITHM\"", "width": 9.0}, {"description": "\"Both models were tested on the same dataset, with stjiris/bert-large-portuguese-cased-legal-tsdae showing better performance.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "keywords": "\"comparison, evaluation\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Both models were tested on the same dataset, with stjiris/bert-large-portuguese-cased-legal-tsdae showing better performance.\"", "to": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM\"", "width": 8.0}, {"description": "\"The TSDAE technique was applied to stjiris/bert-large-portuguese-cased-legal-tsdae, resulting in better performance on the MLM task.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "keywords": "\"application, improvement\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"The TSDAE technique was applied to stjiris/bert-large-portuguese-cased-legal-tsdae, resulting in better performance on the MLM task.\"", "to": "\"TSDAE TECHNIQUE\"", "width": 9.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae model\u0027s AVG loss is shown in Table 6.1.1, indicating its performance on the MLM task.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "keywords": "\"model evaluation, performance metric\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae model\u0027s AVG loss is shown in Table 6.1.1, indicating its performance on the MLM task.\"", "to": "\"AVG LOSS\"", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae model was compared against the All-mpnet-base-v2 in STS task evaluations.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE\"", "keywords": "\"model comparison, evaluation base\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae model was compared against the All-mpnet-base-v2 in STS task evaluations.\"", "to": "\"ALL-MPNET-BASE-V2\"", "width": 9.0}, {"description": "\"rufimelo/Legal-BERTimbau-large is used in the SentenceTransformer library to create SBERT models.\"", "from": "\"RUFIMELO/LEGAL-BERTIMBAU-LARGE\"", "keywords": "\"library usage, model creation\"", "source_id": "chunk-2abe609e17efe1ef0aa4e13a6b7ccddc", "title": "\"rufimelo/Legal-BERTimbau-large is used in the SentenceTransformer library to create SBERT models.\"", "to": "\"SENTENCETRANSFORMER LIBRARY\"", "width": 14.0}, {"description": "\"Legal-BERTimbau-large uses sentence-transformers/stsb-roberta-large as the teacher model for knowledge distillation. \"", "from": "\"LEGAL-BERTIMBAU-LARGE\"", "keywords": "\"knowledge transfer, learning\"", "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Legal-BERTimbau-large uses sentence-transformers/stsb-roberta-large as the teacher model for knowledge distillation. \"", "to": "\"SENTENCE-TRANSFORMERS/STSB-ROBERTA-LARGE\"", "width": 7.0}, {"description": "\"Legal-BERTimbau-large is the base model for stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1, indicating a source relationship.\"", "from": "\"LEGAL-BERTIMBAU-LARGE\"", "keywords": "\"base model, derived from\"", "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Legal-BERTimbau-large is the base model for stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1, indicating a source relationship.\"", "to": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1\"", "width": 9.0}, {"description": "\"The learning rate of 10\u207b\u2076 influences how quickly or slowly the model learns during training by controlling the size of updates to weights.\"", "from": "\"LEARNING RATE\"", "keywords": "\"learning speed, optimization\"", "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"The learning rate of 10\u207b\u2076 influences how quickly or slowly the model learns during training by controlling the size of updates to weights.\"", "to": "\"TRAINING MODEL\"", "width": 14.0}, {"description": "\"STJIRIS sets specific learning rates during the training process of their models.\"", "from": "\"LEARNING RATE\"", "keywords": "\"optimization techniques, model training\"", "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"STJIRIS sets specific learning rates during the training process of their models.\"", "to": "\"STJIRIS\"", "width": 12.0}, {"description": "\"The large models use the Adam optimization algorithm during their training.\"", "from": "\"ADAM OPTIMIZATION ALGORITHM\"", "keywords": "\"training, algorithm usage\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The large models use the Adam optimization algorithm during their training.\"", "to": "\"LARGE MODELS\"", "width": 6.0}, {"description": "\"The Adam optimization algorithm uses a learning rate of 10\u22125 during the training process.\"", "from": "\"ADAM OPTIMIZATION ALGORITHM\"", "keywords": "\"learning rate, optimization algorithm\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The Adam optimization algorithm uses a learning rate of 10\u22125 during the training process.\"", "to": "\"LEARNING RATE 10\u22125\"", "width": 6.0}, {"description": "\"The STS dataset is accessible through HuggingFace platform. This relationship indicates the source and availability of the data.\"", "from": "\"STS DATASETS: ASSIN, ASSIN2 AND STSB MULTI MT PORTUGUESE SUB-DATASET\"", "keywords": "\"data accessibility, platform utilization\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"The STS dataset is accessible through HuggingFace platform. This relationship indicates the source and availability of the data.\"", "to": "\"HUGGINGFACE: STJIRIS/IRIS STS\"", "width": 18.0}, {"description": "\"The dataset used in this research is based on the methodology described in BERTimbau\u2019s paper. This relationship indicates the theoretical basis for model training.\"", "from": "\"STS DATASETS: ASSIN, ASSIN2 AND STSB MULTI MT PORTUGUESE SUB-DATASET\"", "keywords": "\"theoretical foundation, dataset origin\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"The dataset used in this research is based on the methodology described in BERTimbau\u2019s paper. This relationship indicates the theoretical basis for model training.\"", "to": "\"BERTIMBAU\u2019S PAPER\"", "width": 24.0}, {"description": "\"Both tasks use sentence pairs to train models. This relationship indicates shared training data across different NLP sub-tasks.\"", "from": "\"STS DATASETS: ASSIN, ASSIN2 AND STSB MULTI MT PORTUGUESE SUB-DATASET\"", "keywords": "\"task similarity, shared training set\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"Both tasks use sentence pairs to train models. This relationship indicates shared training data across different NLP sub-tasks.\"", "to": "\"NATURAL LANGUAGE INFERENCE (NLI)\"", "width": 15.0}, {"description": "\"The large models were trained using the STS dataset, indicating their training data source.\"", "from": "\"STS DATASETS: ASSIN, ASSIN2 AND STSB MULTI MT PORTUGUESE SUB-DATASET\"", "keywords": "\"model training, dataset utilization\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"The large models were trained using the STS dataset, indicating their training data source.\"", "to": "\"LARGE MODELS\"", "width": 21.0}, {"description": "\"The custom SBERT models were trained using the STS dataset, indicating their training data source.\"", "from": "\"STS DATASETS: ASSIN, ASSIN2 AND STSB MULTI MT PORTUGUESE SUB-DATASET\"", "keywords": "\"model training, dataset utilization\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"The custom SBERT models were trained using the STS dataset, indicating their training data source.\"", "to": "\"SBERT VARIANTS: STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-STS-V1, STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-STS-V1\"", "width": 21.0}, {"description": "\"The SBERT models are trained using datasets available on HuggingFace. This relationship indicates the dataset\u0027s use in model development.\"", "from": "\"HUGGINGFACE: STJIRIS/IRIS STS\"", "keywords": "\"model training, dataset utilization\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"The SBERT models are trained using datasets available on HuggingFace. This relationship indicates the dataset\u0027s use in model development.\"", "to": "\"SBERT VARIANTS\"", "width": 14.0}, {"description": "\"SBERT variants performed better on specific Portuguese datasets but not all, indicating a comparative advantage in certain tasks.\"", "from": "\"SBERT VARIANTS\"", "keywords": "\u003c\"technology superiority, performance comparison\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"SBERT variants performed better on specific Portuguese datasets but not all, indicating a comparative advantage in certain tasks.\"", "to": "\"STATE-OF-THE-ART MULTILINGUAL MODELS\"", "width": 8.0}, {"description": "\"The NLI task uses the SNLI dataset for training models, indicating its importance in the field of NLP.\"", "from": "\"NATURAL LANGUAGE INFERENCE (NLI)\"", "keywords": "\"task-relevance, data utilization\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"The NLI task uses the SNLI dataset for training models, indicating its importance in the field of NLP.\"", "to": "\"SNLI DATASET\"", "width": 24.0}, {"description": "\"The large models were trained using a specific learning rate and optimization algorithm, indicating their fine-tuning configuration.\"", "from": "\"LARGE MODELS\"", "keywords": "\"training parameters, model optimization\"", "source_id": "chunk-de764363085fa944c487f5f0db894d4d", "title": "\"The large models were trained using a specific learning rate and optimization algorithm, indicating their fine-tuning configuration.\"", "to": "\"LEARNING RATE OF 10\u22125, ADAM OPTIMIZATION ALGORITHM\"", "width": 24.0}, {"description": "\"The large models were trained with a batch size of 8.\"", "from": "\"LARGE MODELS\"", "keywords": "\"batch size, model training\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The large models were trained with a batch size of 8.\"", "to": "\"8 BATCH SIZE\"", "width": 5.0}, {"description": "\"The large models were trained for 5 epochs.\"", "from": "\"LARGE MODELS\"", "keywords": "\"epochs, model training\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The large models were trained for 5 epochs.\"", "to": "\"5 EPOCHS\"", "width": 4.0}, {"description": "\"The large models were trained using assim and assin2 NLI information as data sets.\"", "from": "\"LARGE MODELS\"", "keywords": "\"data set usage, model training\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The large models were trained using assim and assin2 NLI information as data sets.\"", "to": "\"ASSIM AND ASSIN2 NLI INFORMATION\"", "width": 7.0}, {"description": "\"Stjiris developed or trained the large models based on specific data sets.\"", "from": "\"LARGE MODELS\"", "keywords": "\"model development, organization involvement\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"Stjiris developed or trained the large models based on specific data sets.\"", "to": "\"STJIRIS/", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v0 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V0\"", "keywords": "\"model dependency, data generation\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v0 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "to": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v0 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0\"", "keywords": "\"model dependency, data generation\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v0 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "to": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v1 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V1\"", "keywords": "\"model dependency, data generation\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v1 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "to": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v1 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V1\"", "keywords": "\"model dependency, data generation\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v1 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "to": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v0 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V0\"", "keywords": "\"model dependency, data generation\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v0 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "to": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v0 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V0\"", "keywords": "\"model dependency, data generation\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v0 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "to": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v1 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V1\"", "keywords": "\"model dependency, data generation\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v1 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "to": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "width": 8.0}, {"description": "\"Both are models developed or used within the same research context, suggesting potential integration or comparison.\"\u003cSEP\u003e\"The stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1\"", "keywords": "\"model comparison, integration\"\u003cSEP\u003e\"model dependency, data generation\"", "source_id": "chunk-ce4847f54b29367988561206721bdbb7\u003cSEP\u003echunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"Both are models developed or used within the same research context, suggesting potential integration or comparison.\"\u003cSEP\u003e\"The stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1 model was trained using data generated by the Pierreguillou/t5-base-qa-squad-v1.1-portuguese model.\"", "to": "\"PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE\"", "width": 14.0}, {"description": "\"The TED 2020 dataset is used as part of the training for stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1 model.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1\"", "keywords": "\"dataset usage, training source\"", "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"The TED 2020 dataset is used as part of the training for stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1 model.\"", "to": "\"TED 2020 \u2013 PARALLEL SENTENCES CORPUS\"", "width": 6.0}, {"description": "\"Both are part of a broader set of model variants, indicating similar development efforts.\"\u003cSEP\u003e\"Both are variants of the same base model, suggesting a lineage or development relationship. \"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1\"", "keywords": "\"model evolution, development\"\u003cSEP\u003e\"model family, related models\"", "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Both are part of a broader set of model variants, indicating similar development efforts.\"\u003cSEP\u003e\"Both are variants of the same base model, suggesting a lineage or development relationship. \"", "to": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V0\"", "width": 14.0}, {"description": "\"Similar to the above, these are different versions of a model, indicating ongoing development. \"\u003cSEP\u003e\"These are different versions of the same model, suggesting a development lineage.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1\"", "keywords": "\"model evolution, development\"\u003cSEP\u003e\"model evolution, versioning\"", "source_id": "chunk-ce4847f54b29367988561206721bdbb7", "title": "\"Similar to the above, these are different versions of a model, indicating ongoing development. \"\u003cSEP\u003e\"These are different versions of the same model, suggesting a development lineage.\"", "to": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V0\"", "width": 15.0}, {"description": "\"The Query Generation step used a T5 model to generate queries from document summaries.\"", "from": "\"T5\"", "keywords": "\"query generation, model usage\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"The Query Generation step used a T5 model to generate queries from document summaries.\"", "to": "\"QUERY GENERATION\"", "width": 7.0}, {"description": "\"Neil Reimers developed MKD as a technique for training multilingual models.\"", "from": "\"NEIL REIMERS\"", "keywords": "\"technique development, authorship\"", "source_id": "chunk-20dc031e50b0aef84979653c6aeb3a8d", "title": "\"Neil Reimers developed MKD as a technique for training multilingual models.\"", "to": "\"MKD\"", "width": 7.0}, {"description": "\"Documents generate embeddings, which are then manipulated and used for various tasks such as clustering and training models.\"", "from": "\"DOCUMENTS\"", "keywords": "\"data processing, semantic representation\"", "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"Documents generate embeddings, which are then manipulated and used for various tasks such as clustering and training models.\"", "to": "\"EMBEDDINGS\"", "width": 16.0}, {"description": "\"STJIRIS is responsible for generating and processing the documents related to COVID-19.\"", "from": "\"DOCUMENTS\"", "keywords": "\"document creation, data management\"", "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"STJIRIS is responsible for generating and processing the documents related to COVID-19.\"", "to": "\"STJIRIS\"", "width": 16.0}, {"description": "\"The centroid calculation is an event that adjusts the embeddings to make them closer, based on their tags and centroids.\"", "from": "\"EMBEDDINGS\"", "keywords": "\"alignment, centralization\"", "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"The centroid calculation is an event that adjusts the embeddings to make them closer, based on their tags and centroids.\"", "to": "\"CENTROID CALCULATION\"", "width": 18.0}, {"description": "\"STJIRIS uses embeddings in their models to process and understand the content of the documents.\"", "from": "\"EMBEDDINGS\"", "keywords": "\"model development, semantic analysis\"", "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"STJIRIS uses embeddings in their models to process and understand the content of the documents.\"", "to": "\"STJIRIS\"", "width": 14.0}, {"description": "\"The batch size affects the frequency and pace at which the model\u0027s parameters are updated, impacting the learning process.\"", "from": "\"BATCH SIZE\"", "keywords": "\"update frequency, learning efficiency\"", "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"The batch size affects the frequency and pace at which the model\u0027s parameters are updated, impacting the learning process.\"", "to": "\"TRAINING MODEL\"", "width": 12.0}, {"description": "\"STJIRIS employs a batch size of 3 sentences per epoch in their training process.\"", "from": "\"BATCH SIZE\"", "keywords": "\"batch processing, training efficiency\"", "source_id": "chunk-427843b4c7ba44f1dcc8f571081e36ae", "title": "\"STJIRIS employs a batch size of 3 sentences per epoch in their training process.\"", "to": "\"STJIRIS\"", "width": 10.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"TSDAE-MKD-NLI-STS-V1\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "width": 24.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"MLM-GPL-NLI-STS-METAKD-V0\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "width": 15.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"MLM-GPL-NLI-STS-METAKD-V1\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "width": 25.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"TSDAE-GPL-NLI-STS-METAKD-V0\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "width": 16.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"TSDAE-GPL-NLI-STS-METAKD-V1\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "width": 26.0}, {"description": "\"Both BERTimbau large and stjiris/bert-large-portuguese-cased-legal-mlm were fine-tuned for STS tasks on Portuguese legal documents, with the latter performing better.\"", "from": "\"BERTIMBAU LARGE\"", "keywords": "\"fine-tuning, performance\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Both BERTimbau large and stjiris/bert-large-portuguese-cased-legal-mlm were fine-tuned for STS tasks on Portuguese legal documents, with the latter performing better.\"", "to": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM\"", "width": 7.0}, {"description": "\"The BERTimbau large model\u0027s AVG loss is shown in Table 6.1.1, indicating its performance on the MLM task.\"", "from": "\"BERTIMBAU LARGE\"", "keywords": "\"model evaluation, performance metric\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"The BERTimbau large model\u0027s AVG loss is shown in Table 6.1.1, indicating its performance on the MLM task.\"", "to": "\"AVG LOSS\"", "width": 8.0}, {"description": "\"BERTimbau large is used to create sentence embeddings which are stored in an ElasticSearch index for search and retrieval purposes.\"", "from": "\"BERTIMBAU LARGE\"", "keywords": "\u003c\"embedding creation, storage\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"BERTimbau large is used to create sentence embeddings which are stored in an ElasticSearch index for search and retrieval purposes.\"", "to": "\"ELASTICSEARCH INDEX\"", "width": 18.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-mlm model\u0027s AVG loss is shown in Table 6.1.1, indicating its performance on the MLM task.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM\"", "keywords": "\"model evaluation, performance metric\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"The stjiris/bert-large-portuguese-cased-legal-mlm model\u0027s AVG loss is shown in Table 6.1.1, indicating its performance on the MLM task.\"", "to": "\"AVG LOSS\"", "width": 8.0}, {"description": "\"The stjiris/bert-large-portuguese-cased-legal-mlm model was compared against the Paraphrase-multilingual-mpnet-base-v2 in STS task evaluations.\"", "from": "\"STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM\"", "keywords": "\"model comparison, evaluation base\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"The stjiris/bert-large-portuguese-cased-legal-mlm model was compared against the Paraphrase-multilingual-mpnet-base-v2 in STS task evaluations.\"", "to": "\"PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2\"", "width": 9.0}, {"description": "\"Subsection 4.2 generated the splits or datasets that were used to evaluate models in Table 6.1.1.\"", "from": "\"SUBSECTION 4.2\"", "keywords": "\"evaluation dataset generation, testing splits\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Subsection 4.2 generated the splits or datasets that were used to evaluate models in Table 6.1.1.\"", "to": "\"TABLE 6.1.1\"", "width": 8.0}, {"description": "\"Table 6.1.1 presents the AVG loss values for different models evaluated using the MLM task.\"", "from": "\"TABLE 6.1.1\"", "keywords": "\"loss function, evaluation results\"", "source_id": "chunk-41c849f637d2cf0401100a6cc855d2d2", "title": "\"Table 6.1.1 presents the AVG loss values for different models evaluated using the MLM task.\"", "to": "\"AVG LOSS\"", "width": 7.0}, {"description": "\"Both models are variants used in STS evaluation, potentially competing or being compared for performance.\"", "from": "\"PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are variants used in STS evaluation, potentially competing or being compared for performance.\"", "to": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "width": 10.0}, {"description": "\"Both model names indicate variants used in STS tasks and may be part of a broader evaluation framework.\"", "from": "\"PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2\"", "keywords": "\"model family, evaluation\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both model names indicate variants used in STS tasks and may be part of a broader evaluation framework.\"", "to": "\"MLM-STS-V0\"", "width": 7.0}, {"description": "\"The Paraphrase-multilingual-mpnet-base-v2 model is also used as a comparison baseline for the Search metric evaluation.\"", "from": "\"PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2\"", "keywords": "\u003c\"benchmarking, model evaluation\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Paraphrase-multilingual-mpnet-base-v2 model is also used as a comparison baseline for the Search metric evaluation.\"", "to": "\"SEARCH METRIC\"", "width": 10.0}, {"description": "\"Both models are part of the evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"ALL-MPNET-BASE-V2\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "width": 6.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"MLM-STS-V1\"", "width": 23.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"MLM-STS-V0\"", "width": 7.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"MLM-NLI-STS-V0\"", "width": 8.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"MLM-GPL-NLI-STS-V0\"", "width": 9.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"MLM-MKD-NLI-STS-V0\"", "width": 10.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"TSDAE-STS-V0\"", "width": 11.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"TSDAE-NLI-STS-V0\"", "width": 12.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"TSDAE-GPL-NLI-STS-V0\"", "width": 13.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"TSDAE-MKD-NLI-STS-V0\"", "width": 14.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"MLM-NLI-STS-V1\"", "width": 18.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"MLM-GPL-NLI-STS-V1\"", "width": 19.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"MLM-MKD-NLI-STS-V1\"", "width": 20.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"TSDAE-STS-V1\"", "width": 21.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"TSDAE-NLI-STS-V1\"", "width": 22.0}, {"description": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "from": "\"IMBAUMODEL_LARGE_FINE-TUNED_FOR_STS_0.81289_0.84133_0.77958_0.81126\"", "keywords": "\"model comparison\"", "source_id": "chunk-67e58f1a8dccff23d808e5fa663753db", "title": "\"Both models are part of the same evaluation framework, indicating they may be compared in terms of performance\"", "to": "\"TSDAE-GPL-NLI-STS-V1\"", "width": 23.0}, {"description": "\"These models engaged with multiple translations during training, making them suitable for handling multilingual data.\"", "from": "\"STSB MULTI MT DATASET\"", "keywords": "\u003c\"training process, multilingual capability\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"These models engaged with multiple translations during training, making them suitable for handling multilingual data.\"", "to": "\"MULTILINGUAL MODELS\"", "width": 7.0}, {"description": "\"Both techniques were part of an attempt to generate queries for evaluating a search system, but only the latter was used for the final query set.\"", "from": "\"LEXRANK SUMMARIZATION TECHNIQUE\"", "keywords": "\u003c\"query generation methods, evaluation process\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"Both techniques were part of an attempt to generate queries for evaluating a search system, but only the latter was used for the final query set.\"", "to": "\"GPT3 MODEL PROVIDED BY OPEN AI\"", "width": 5.0}, {"description": "\"Embeddings of legal documents were stored in an ElasticSearch index for efficient retrieval during evaluations.\"", "from": "\"LEGAL DOCUMENTS COLLECTION\"", "keywords": "\u003c\"document storage, query generation\"", "source_id": "chunk-dc1fa210fa8cf3125e9c46996f5dbd40", "title": "\"Embeddings of legal documents were stored in an ElasticSearch index for efficient retrieval during evaluations.\"", "to": "\"ELASTICSEARCH INDEX\"", "width": 9.0}, {"description": "\"ElasticSearch index stores sentence embeddings used in evaluating performance across different top result sizes.\"", "from": "\"ELASTICSEARCH INDEX\"", "keywords": "\u003c\"storage, performance evaluation\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"ElasticSearch index stores sentence embeddings used in evaluating performance across different top result sizes.\"", "to": "\"TOP RESULTS SIZES\"", "width": 12.0}, {"description": "\"The Sentence-transformers/all-mpnet-base-v2 model is used as a comparison baseline with BM25 for evaluating the Semantic Search System\u0027s performance.\"", "from": "\"BM25 TECHNIQUE\"", "keywords": "\u003c\"comparison, benchmarking\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Sentence-transformers/all-mpnet-base-v2 model is used as a comparison baseline with BM25 for evaluating the Semantic Search System\u0027s performance.\"", "to": "\"SENTENCE-TRANSFORMERS/ALL-MPNET-BASE-V2\"", "width": 12.0}, {"description": "\"The Search metric utilizes cosine similarity to evaluate the system\u0027s ability to find relevant documents based on query.\"", "from": "\"SEARCH METRIC\"", "keywords": "\u003c\"performance evaluation, similarity measurement\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Search metric utilizes cosine similarity to evaluate the system\u0027s ability to find relevant documents based on query.\"", "to": "\"COSINE SIMILARITY METRIC\"", "width": 8.0}, {"description": "\"The Discovery metric evaluates the system\u0027s ability to retrieve additional relevant documents within specified top result sizes.\"", "from": "\"DISCOVERY METRIC\"", "keywords": "\u003c\"retrieval effectiveness, result evaluation\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Discovery metric evaluates the system\u0027s ability to retrieve additional relevant documents within specified top result sizes.\"", "to": "\"TOP 1, TOP 2, TOP 3, TOP 5, TOP 10, AND TOP 20\"", "width": 7.0}, {"description": "\"The Cosine similarity metric is used alongside the STS (SemEval Task 4) dataset to evaluate model performance.\"", "from": "\"COSINE SIMILARITY METRIC\"", "keywords": "\u003c\"evaluation method, dataset usage\"", "source_id": "chunk-437fca5e9e86e40c8ca3cf7fc41a8c65", "title": "\"The Cosine similarity metric is used alongside the STS (SemEval Task 4) dataset to evaluate model performance.\"", "to": "\"STS DATASET\"", "width": 7.0}, {"description": "\"Models fine-tuned on the custom STS dataset show a slight performance drop in both Search and Discovery metrics compared to Lexical-First approaches, indicating that the custom datasets have less impact than lexical-first techniques.\"", "from": "\"LEXICAL-FIRST APPROACH\"", "keywords": "\u003c\"training method comparison, performance impact\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Models fine-tuned on the custom STS dataset show a slight performance drop in both Search and Discovery metrics compared to Lexical-First approaches, indicating that the custom datasets have less impact than lexical-first techniques.\"", "to": "\"CUSTOM STS DATASET (V1 MODELS)\"", "width": 12.0}, {"description": "\"Models fine-tuned on pre-existing datasets show better performance in both Search and Discovery metrics compared to Lexical+Semantic approaches, indicating that traditional methods are more effective.\"", "from": "\"LEXICAL+SEMANTIC APPROACH\"", "keywords": "\u003c\"training method comparison, performance impact\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Models fine-tuned on pre-existing datasets show better performance in both Search and Discovery metrics compared to Lexical+Semantic approaches, indicating that traditional methods are more effective.\"", "to": "\"PRE-EXISTING AND MANUALLY ANNOTATED DATASETS (V0 MODELS)\"", "width": 16.0}, {"description": "\"The Hybrid Search System uses query expansion to improve its retrieval capabilities.\"", "from": "\"HYBRID SEARCH SYSTEM\"", "keywords": "\"improvement, context understanding\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The Hybrid Search System uses query expansion to improve its retrieval capabilities.\"", "to": "\"QUERY EXPANSION\"", "width": 8.0}, {"description": "\"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence.\"", "from": "\"SAM RIVERA\"", "keywords": "\u003c\"communication, learning process\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence.\"", "to": "\"INTELLIGENCE\"", "width": 9.0}, {"description": "\"The concept of Control is challenged by the Intelligence that writes its own rules.\"", "from": "\"CONTROL\"", "keywords": "\u003c\"power dynamics, autonomy\"", "source_id": "chunk-23944cdf583b2c2b5fcf537fea3f8421", "title": "\"The concept of Control is challenged by the Intelligence that writes its own rules.\"", "to": "\"INTELLIGENCE\"", "width": 7.0}, {"description": "\"The Purely Semantic model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The Purely Semantic model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "from": "\"PURELY SEMANTIC\"", "keywords": "\"performance comparison, metrics evaluation\"\u003cSEP\u003e\u003c\"performance comparison, metrics evaluation\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The Purely Semantic model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The Purely Semantic model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "to": "\"TOP 1 TOP 2 TOP 3 TOP 5 TOP 10\"", "width": 16.0}, {"description": "\"Both models involve the use of semantic capabilities but differ significantly in their approach to combining lexical elements, with Purely Semantic being more isolated.\"::", "from": "\"PURELY SEMANTIC\"", "keywords": "\u003c\"approach comparison, integration strategy\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"Both models involve the use of semantic capabilities but differ significantly in their approach to combining lexical elements, with Purely Semantic being more isolated.\"::", "to": "\"LEXICAL + SEMANTIC\"", "width": 12.0}, {"description": "\"The Purely Semantic model also shows improvement percentages compared to other models in Table 6.4.\"::", "from": "\"PURELY SEMANTIC\"", "keywords": "\u003c\"performance comparison, metric evaluation\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The Purely Semantic model also shows improvement percentages compared to other models in Table 6.4.\"::", "to": "\"IMPROVEMENT\"", "width": 14.0}, {"description": "\"The Lexical-First model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The Lexical-First model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "from": "\"LEXICAL-FIRST\"", "keywords": "\"performance comparison, metrics evaluation\"\u003cSEP\u003e\u003c\"performance comparison, metrics evaluation\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The Lexical-First model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The Lexical-First model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "to": "\"TOP 1 TOP 2 TOP 3 TOP 5 TOP 10\"", "width": 16.0}, {"description": "\"Both the Lexical-First and Lexical + Semantic models share a common approach to integrating lexical capabilities with semantic ones, differing in how they prioritize these aspects.\"::", "from": "\"LEXICAL-FIRST\"", "keywords": "\u003c\"approach comparison, integration strategy\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"Both the Lexical-First and Lexical + Semantic models share a common approach to integrating lexical capabilities with semantic ones, differing in how they prioritize these aspects.\"::", "to": "\"LEXICAL + SEMANTIC\"", "width": 12.0}, {"description": "\"The Lexical-First model shows improvement percentages compared to other models in Table 6.4.\"::", "from": "\"LEXICAL-FIRST\"", "keywords": "\u003c\"performance comparison, metric evaluation\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The Lexical-First model shows improvement percentages compared to other models in Table 6.4.\"::", "to": "\"IMPROVEMENT\"", "width": 14.0}, {"description": "\"The Lexical + Semantic model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The Lexical + Semantic model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "from": "\"LEXICAL + SEMANTIC\"", "keywords": "\"performance comparison, metrics evaluation\"\u003cSEP\u003e\u003c\"performance comparison, metrics evaluation\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The Lexical + Semantic model provides specific top results for different metrics in Table 6.3 and 6.4.\"\u003cSEP\u003e\"The Lexical + Semantic model provides specific top results for different metrics in Table 6.3 and 6.4.\"::", "to": "\"TOP 1 TOP 2 TOP 3 TOP 5 TOP 10\"", "width": 16.0}, {"description": "\"The Lexical + Semantic model demonstrates specific improvement percentages compared to other models across different metrics in Table 6.4.\"::", "from": "\"LEXICAL + SEMANTIC\"", "keywords": "\u003c\"performance comparison, metric evaluation\"", "source_id": "chunk-87a104ccfd8b71d9a4e7ad0343692cac", "title": "\"The Lexical + Semantic model demonstrates specific improvement percentages compared to other models across different metrics in Table 6.4.\"::", "to": "\"IMPROVEMENT\"", "width": 16.0}, {"description": "\"The performance metrics indicate the effectiveness of Legal-BERTimbau models in semantic search tasks.\"", "from": "\"LEGAL-BERTIMBAU MODELS\"", "keywords": "\"performance, evaluation, effectiveness\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The performance metrics indicate the effectiveness of Legal-BERTimbau models in semantic search tasks.\"", "to": "\"ROUGE-1 SCORE OF 47.92 AND A ROUGE-2 SCORE OF 22.50\"", "width": 8.0}, {"description": "\"Both are involved in the research as Supremo Tribunal de Justica is part of EPIA\u0027s conference papers and presentations may be based on its data.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTICA\"", "keywords": "\"research, application, validation\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"Both are involved in the research as Supremo Tribunal de Justica is part of EPIA\u0027s conference papers and presentations may be based on its data.\"", "to": "\"EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE\"", "width": 7.0}, {"description": "\"Knowledge graph embeddings could potentially improve the system\u0027s understanding of queries from this legal entity, enhancing retrieval accuracy.\"", "from": "\"SUPREMO TRIBUNAL DE JUSTICA\"", "keywords": "\"improvement, query expansion, context understanding\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"Knowledge graph embeddings could potentially improve the system\u0027s understanding of queries from this legal entity, enhancing retrieval accuracy.\"", "to": "\"KNOWLEDGE GRAPH EMBEDDINGS\"", "width": 7.0}, {"description": "\"The conference may discuss or showcase research related to legal data filtering as presented in this paper.\"", "from": "\"EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE\"", "keywords": "\"research, collaboration, discussion\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The conference may discuss or showcase research related to legal data filtering as presented in this paper.\"", "to": "\"PILE OF LAW: LEARNING RESPONSIBLE DATA FILTERING FROM THE LAW AND A 256 GB OPEN-SOURCE LEGAL DATASET\"", "width": 7.0}, {"description": "\"The publication targets a specific legal domain, Portuguese jurisprudence, indicating the relevance of the model to this field.\"", "from": "\"ALBERTINA PT-PT\"", "keywords": "\"domain adaptation, specific language variant\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The publication targets a specific legal domain, Portuguese jurisprudence, indicating the relevance of the model to this field.\"", "to": "\"PORTUGUESE JURISPRUDENCE\"", "width": 9.0}, {"description": "\"The dataset developed by Pile of Law can be used for training models related to Portuguese jurisprudence, highlighting its relevance.\"", "from": "\"PORTUGUESE JURISPRUDENCE\"", "keywords": "\"dataset use, legal domain\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The dataset developed by Pile of Law can be used for training models related to Portuguese jurisprudence, highlighting its relevance.\"", "to": "\"PILE OF LAW: LEARNING RESPONSIBLE DATA FILTERING FROM THE LAW AND A 256 GB OPEN-SOURCE LEGAL DATASET\"", "width": 8.0}, {"description": "\"The ethical challenges faced in the dataset development are addressed by papers like \"Pile of Law,\" which discuss handling such issues.\"", "from": "\"PILE OF LAW: LEARNING RESPONSIBLE DATA FILTERING FROM THE LAW AND A 256 GB OPEN-SOURCE LEGAL DATASET\"", "keywords": "\"data filtering, ethical considerations\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"The ethical challenges faced in the dataset development are addressed by papers like \"Pile of Law,\" which discuss handling such issues.\"", "to": "\"BIAS, OBSCENE, COPYRIGHTED, AND PRIVATE INFORMATION\"", "width": 8.0}, {"description": "\"Manual work involves the use of tools like assassin and assassin2 for dataset annotation.\"", "from": "\"MANUAL WORK\"", "keywords": "\"data preparation, annotation\"", "source_id": "chunk-6c7ea4dc0b9ee1c35294eedffacbeb38", "title": "\"Manual work involves the use of tools like assassin and assassin2 for dataset annotation.\"", "to": "\"ASSASSIN AND ASSASSIN2\"", "width": 7.0}, {"description": "\"Active learning can be applied to improve GLMs, specifically GPT-3, through user feedback.\"", "from": "\"ACTIVE LEARNING\"", "keywords": "\"user feedback, continuous improvement\"", "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"Active learning can be applied to improve GLMs, specifically GPT-3, through user feedback.\"", "to": "\"GLM (GENERATIVE LANGUAGE MODEL)\"", "width": 7.0}, {"description": "\"The tasks like SemEval-2015 can provide the basis for testing active learning techniques.\"", "from": "\"ACTIVE LEARNING\"", "keywords": "\"evaluation framework, feedback mechanism\"", "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"The tasks like SemEval-2015 can provide the basis for testing active learning techniques.\"", "to": "\"SEMEVAL-2015 TASK 2: SEMANTIC TEXTUAL SIMILARITY, ENGLISH, SPANISH AND PILOT ON INTERPRETABILITY\"", "width": 6.0}, {"description": "\"GPT-3 is a specific type of GLM that can benefit from improvements in prompts and interactions.\"", "from": "\"GLM (GENERATIVE LANGUAGE MODEL)\"", "keywords": "\"specific model, improvement method\"", "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"GPT-3 is a specific type of GLM that can benefit from improvements in prompts and interactions.\"", "to": "\"GPT-3\"", "width": 8.0}, {"description": "\"Improvements in GLMs can be evaluated using tasks like SemEval-2015 for semantic textual similarity.\"", "from": "\"GLM (GENERATIVE LANGUAGE MODEL)\"", "keywords": "\"evaluation framework, model improvement\"", "source_id": "chunk-d23192c04e35b99777b833e26dafed9f", "title": "\"Improvements in GLMs can be evaluated using tasks like SemEval-2015 for semantic textual similarity.\"", "to": "\"SEMEVAL-2015 TASK 2: SEMANTIC TEXTUAL SIMILARITY, ENGLISH, SPANISH AND PILOT ON INTERPRETABILITY\"", "width": 7.0}, {"description": "\"The event SemEval-2016 was held in San Diego, California, making it clearly related to this location.\"", "from": "\"SEMEVAL-2016\"", "keywords": "\"host city, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The event SemEval-2016 was held in San Diego, California, making it clearly related to this location.\"", "to": "\"SAN DIEGO, CALIFORNIA\"", "width": 18.0}, {"description": "\"The 2013 shared task of Semantic Textual Similarity took place in Atlanta, Georgia, USA. This clearly relates the event to this location.\"", "from": "\"SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY\"", "keywords": "\"host city, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2013 shared task of Semantic Textual Similarity took place in Atlanta, Georgia, USA. This clearly relates the event to this location.\"", "to": "\"ATLANTA, GEORGIA, USA\"", "width": 18.0}, {"description": "\"The 2012 task was a part of the first joint conference held in USA. This clearly relates the event to this location.\"", "from": "\"SEMEVAL-2012 TASK 6: A PILOT ON SEMANTIC TEXTUAL SIMILARITY\"", "keywords": "\"host city, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2012 task was a part of the first joint conference held in USA. This clearly relates the event to this location.\"", "to": "\"FIRST JOINT CONFERENCE ON LEXICAL AND COMPUTATIONAL SEMANTICS\"", "width": 18.0}, {"description": "\"The Computational Processing of the Portuguese Language conference took place in Cham, Switzerland, making it clearly related to this location.\"", "from": "\"COMPUTATIONAL PROCESSING OF THE PORTUGUESE LANGUAGE\"", "keywords": "\"host city, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The Computational Processing of the Portuguese Language conference took place in Cham, Switzerland, making it clearly related to this location.\"", "to": "\"CHAM, SWITZERLAND\"", "width": 18.0}, {"description": "\"The 2015 Conference on Empirical Methods in Natural Language Processing held in Lisbon, Portugal featured a large annotated corpus. This clearly relates the event to this location.\"", "from": "\"A LARGE ANNOTATED CORPUS FOR LEARNING NATURAL LANGUAGE INFERENCE\"", "keywords": "\"host city, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2015 Conference on Empirical Methods in Natural Language Processing held in Lisbon, Portugal featured a large annotated corpus. This clearly relates the event to this location.\"", "to": "\"LISBON, PORTUGAL\"", "width": 18.0}, {"description": "\"The 2017 SemEval task took place in Vancouver, Canada, making it clearly related to this location.\"", "from": "\"SEMEVAL-2017 TASK 1: SEMANTIC TEXTUAL SIMILARITY MULTILINGUAL AND CROSSLINGUAL FOCUSED EVALUATION\"", "keywords": "\"host city, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2017 SemEval task took place in Vancouver, Canada, making it clearly related to this location.\"", "to": "\"VANCOUVER, CANADA\"", "width": 18.0}, {"description": "\"The 2017 Conference on Empirical Methods in Natural Language Processing held in Copenhagen, Denmark featured a study. This clearly relates the event to this location.\"", "from": "\"SUPERVISED LEARNING OF UNIVERSAL SENTENCE REPRESENTATIONS FROM NATURAL LANGUAGE INFERENCE DATA\"", "keywords": "\"host city, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2017 Conference on Empirical Methods in Natural Language Processing held in Copenhagen, Denmark featured a study. This clearly relates the event to this location.\"", "to": "\"COPENHAGEN, DENMARK\"", "width": 18.0}, {"description": "\"The 2019 ACM SIGIR Conference held in Dublin, Ireland featured a study on deeper text understanding. This clearly relates the event to this location.\"", "from": "\"DEEPER TEXT UNDERSTANDING FOR IR WITH CONTEXTUAL NEURAL LANGUAGE MODELING\"", "keywords": "\"host city, venue\"", "source_id": "chunk-914c2300c13f4d0b3d2cd59e1e8f3bce", "title": "\"The 2019 ACM SIGIR Conference held in Dublin, Ireland featured a study on deeper text understanding. This clearly relates the event to this location.\"", "to": "\"DUBLIN, IRELAND\"", "width": 18.0}, {"description": "\"Both authors have contributed to advancements in natural language processing techniques.\"", "from": "\"CALLAN, J.\"", "keywords": "\"collaboration, NLP\"", "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Both authors have contributed to advancements in natural language processing techniques.\"", "to": "\"DEVLIN, J.\"", "width": 8.0}, {"description": "\"Both authors have contributed to advancements in natural language processing techniques.\"\u003cSEP\u003e\"While their fields of study differ, both have made contributions to text understanding and information retrieval.\"", "from": "\"CALLAN, J.\"", "keywords": "\"collaboration, NLP\"\u003cSEP\u003e\"different fields, collaboration\"", "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Both authors have contributed to advancements in natural language processing techniques.\"\u003cSEP\u003e\"While their fields of study differ, both have made contributions to text understanding and information retrieval.\"", "to": "\"ERKAN, G.\"", "width": 23.0}, {"description": "\"While their fields of study differ, both have made contributions to text understanding and information retrieval.\"", "from": "\"CALLAN, J.\"", "keywords": "\"different fields, collaboration\"", "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"While their fields of study differ, both have made contributions to text understanding and information retrieval.\"", "to": "\"FONSECA, E.\"", "width": 14.0}, {"description": "\"Both authors have contributed significantly to the field of natural language processing through different approaches.\"", "from": "\"DEVLIN, J.\"", "keywords": "\"NLP advancements, different methods\"", "source_id": "chunk-2016e530048d82125b70492619ae1cd8", "title": "\"Both authors have contributed significantly to the field of natural language processing through different approaches.\"", "to": "\"KIM, M.\"", "width": 27.0}, {"description": "\"FONSECA, E. and OLIVEIRA, H. G. are both involved in the ASSIN 2 shared task with REAL, L., indicating a collaborative relationship.\"", "from": "\"FONSECA, E.\"", "keywords": "\"collaboration\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"FONSECA, E. and OLIVEIRA, H. G. are both involved in the ASSIN 2 shared task with REAL, L., indicating a collaborative relationship.\"", "to": "\"OLIVEIRA, H. G.\"", "width": 7.0}, {"description": "\"ICLR 2015 took place in San Diego, CA, USA.\"", "from": "\"ICLR 2015\"", "keywords": "\"location, venue\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"ICLR 2015 took place in San Diego, CA, USA.\"", "to": "\"SAN DIEGO, CA, USA\"", "width": 18.0}, {"description": "\"P. May developed the Machine translated multilingual STS benchmark dataset.\"", "from": "\"MACHINE TRANSLATED MULTILINGUAL STS BENCHMARK DATASET\"", "keywords": "\"creation, development\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"P. May developed the Machine translated multilingual STS benchmark dataset.\"", "to": "\"P. MAY\"", "width": 16.0}, {"description": "\"ICLR 2013 took place in Scottsdale, Arizona, USA.\"", "from": "\"SCOTTSDALE, ARIZONA, USA\"", "keywords": "\"location, venue\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"ICLR 2013 took place in Scottsdale, Arizona, USA.\"", "to": "\"ICLR 2013\"", "width": 18.0}, {"description": "\"T. Nguyen et al. developed the MS MARCO dataset.\"", "from": "\"MS MARCO: A HUMAN GENERATED MACHINE READING COMPREHENSION DATASET\"", "keywords": "\"creation, development\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"T. Nguyen et al. developed the MS MARCO dataset.\"", "to": "\"T. NGUYEN ET AL.\"", "width": 16.0}, {"description": "\"The research by S. K. Nigam et al. is included in the book published by New Frontiers in Artificial Intelligence.\"", "from": "\"NIGAM@COLIEE-22: LEGAL CASE RETRIEVAL AND ENTAILMENT USING CASCADING OF LEXICAL AND SEMANTIC-BASED MODELS\"", "keywords": "\"publication, inclusion\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"The research by S. K. Nigam et al. is included in the book published by New Frontiers in Artificial Intelligence.\"", "to": "\"S. K. NIGAM, K. YADA, K. SATOH, AND S. ARAI\"", "width": 16.0}, {"description": "\"J. Pennington, R. Socher, and C. Manning developed the GloVe technology.\"", "from": "\"GLOVE: GLOBAL VECTORS FOR WORD REPRESENTATION\"", "keywords": "\"creation, development\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"J. Pennington, R. Socher, and C. Manning developed the GloVe technology.\"", "to": "\"J. PENNINGTON, R. SOCHER, AND C. MANNING\"", "width": 16.0}, {"description": "\"The research by L. Real et al. is about the ASIN 2 shared task.\"", "from": "\"THE ASSIN 2 SHARED TASK: A QUICK OVERVIEW\"", "keywords": "\"research, development\"", "source_id": "chunk-9b1e1352f338d948b4876b635d24d01c", "title": "\"The research by L. Real et al. is about the ASIN 2 shared task.\"", "to": "\"L. REAL, E. FONSECA, AND H. G. OLIVEIRA\"", "width": 16.0}, {"description": "\"ANDLIU, P. J. and REIMERS, N. are both authors in the field of transfer learning research but there is no direct relationship established between them based on this text.\"\u003cSEP\u003e\"Both ANDLIU, P. J. and Reimers, N. are researchers in the field of transfer learning but there is no direct relationship established between them based on this text.\"", "from": "\"ANDLIU, P. J.\"", "keywords": "\"research collaboration\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"ANDLIU, P. J. and REIMERS, N. are both authors in the field of transfer learning research but there is no direct relationship established between them based on this text.\"\u003cSEP\u003e\"Both ANDLIU, P. J. and Reimers, N. are researchers in the field of transfer learning but there is no direct relationship established between them based on this text.\"", "to": "\"REIMERS, N.\"", "width": 18.0}, {"description": "\"REAL, L. and REIMERS, N. are involved in different tasks (ASSIN 2 shared task vs Sentence-BERT), showing no direct relationship based on this text.\"", "from": "\"REAL, L.\"", "keywords": "\"task involvement\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"REAL, L. and REIMERS, N. are involved in different tasks (ASSIN 2 shared task vs Sentence-BERT), showing no direct relationship based on this text.\"", "to": "\"REIMERS, N.\"", "width": 5.0}, {"description": "\"GUREVYCH, I. and REIMERS, N. collaborate on the BEIR benchmark but there is no direct relationship established between them based on this text.\"\u003cSEP\u003e\"Gurevych, I. and Reimers, N. collaborate on the BEIR benchmark but there is no direct relationship established between them based on this text.\"\u003cSEP\u003e\"REIMERS, N. and GUREVYCH, I. collaborate on multiple works, indicating a strong research partnership.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"GUREVYCH, I. and REIMERS, N. collaborate on the BEIR benchmark but there is no direct relationship established between them based on this text.\"\u003cSEP\u003e\"Gurevych, I. and Reimers, N. collaborate on the BEIR benchmark but there is no direct relationship established between them based on this text.\"\u003cSEP\u003e\"REIMERS, N. and GUREVYCH, I. collaborate on multiple works, indicating a strong research partnership.\"", "to": "\"GUREVYCH, I.\"", "width": 27.0}, {"description": "\"Both Souza, F. and Reimers, N. have research interests in BERT-related technologies but they do not collaborate directly as per the given context.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research interest\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Both Souza, F. and Reimers, N. have research interests in BERT-related technologies but they do not collaborate directly as per the given context.\"", "to": "\"SOUZA, F.\"", "width": 10.0}, {"description": "\"Nogueira, R. collaborates with Reimers, N. on Sentence-BERT and monolingual embeddings but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"collaboration\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Nogueira, R. collaborates with Reimers, N. on Sentence-BERT and monolingual embeddings but there is no direct relationship established between them based on this text.\"", "to": "\"NOGUEIRA, R.\"", "width": 14.0}, {"description": "\"De Alencar Lotufo, R. collaborates with Reimers, N. on monolingual embeddings but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"collaboration\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"De Alencar Lotufo, R. collaborates with Reimers, N. on monolingual embeddings but there is no direct relationship established between them based on this text.\"", "to": "\"DE ALENCAR LOTUFO, R.\"", "width": 14.0}, {"description": "\"THAKUR, N. and REIMERS, N. co-author the BEIR benchmark but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"THAKUR, N. and REIMERS, N. co-author the BEIR benchmark but there is no direct relationship established between them based on this text.\"", "to": "\"THAKUR, N.\"", "width": 18.0}, {"description": "\"R\u00fcckle, A. and Reimers, N. collaborate on the BEIR benchmark but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"R\u00fcckle, A. and Reimers, N. collaborate on the BEIR benchmark but there is no direct relationship established between them based on this text.\"", "to": "\"R\u00dcCKLE, A.\"", "width": 18.0}, {"description": "\"Srivastava, A. and Reimers, N. collaborate on the BEIR benchmark but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Srivastava, A. and Reimers, N. collaborate on the BEIR benchmark but there is no direct relationship established between them based on this text.\"", "to": "\"SRIVASTAVA, A.\"", "width": 18.0}, {"description": "\"Both Vaswani, A. and Reimers, N. are involved in BERT-related research but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research interest\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Both Vaswani, A. and Reimers, N. are involved in BERT-related research but there is no direct relationship established between them based on this text.\"", "to": "\"VASWANI, A.\"", "width": 10.0}, {"description": "\"Both Shazeer, N. and Reimers, N. are involved in BERT-related research but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research interest\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Both Shazeer, N. and Reimers, N. are involved in BERT-related research but there is no direct relationship established between them based on this text.\"", "to": "\"SHAZEER, N.\"", "width": 10.0}, {"description": "\"Parmar, N. collaborates with Reimers, N. on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Parmar, N. collaborates with Reimers, N. on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "to": "\"PARMAR, N.\"", "width": 18.0}, {"description": "\"Both U Szkoreit, J. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Both U Szkoreit, J. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "to": "\"U SZKOREIT, J.\"", "width": 18.0}, {"description": "\"Both Jones, L. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Both Jones, L. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "to": "\"JONES, L.\"", "width": 18.0}, {"description": "\"Both Gomez, A. N. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Both Gomez, A. N. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "to": "\"GOMEZ, A. N.\"", "width": 18.0}, {"description": "\"Both Kaiser, L.U. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Both Kaiser, L.U. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "to": "\"KAISER, L.U.\"", "width": 18.0}, {"description": "\"Both Polosukhin, I. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "from": "\"REIMERS, N.\"", "keywords": "\"research collaboration, co-authorship\"", "source_id": "chunk-dc84a2863207e7e3c024c8d2254064a2", "title": "\"Both Polosukhin, I. and Reimers, N. collaborate on the Attention is All You Need paper but there is no direct relationship established between them based on this text.\"", "to": "\"POLOSUKHIN, I.\"", "width": 18.0}, {"description": "\"N., Kaiser contributed to the development and research of the SciPy library.\"", "from": "\"N., K AISER \"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"N., Kaiser contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"L. U. and Polosukhin contributed to the development and research of the SciPy library.\"", "from": "\"L. U. AND POLOSUKHIN , I.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"L. U. and Polosukhin contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"P. Virtanen contributed to the development and research of the SciPy library.\"", "from": "\"V IRTANEN , P.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"P. Virtanen contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"R. Gommers contributed to the development and research of the SciPy library.\"", "from": "\"G OMMERS , R.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"R. Gommers contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"T. E. Oliphant contributed to the development and research of the SciPy library.\"", "from": "\"O LIPHANT , T. E.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"T. E. Oliphant contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"M. Haberland contributed to the development and research of the SciPy library.\"", "from": "\"H ABERLAND , M.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"M. Haberland contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"T. Reddy contributed to the development and research of the SciPy library.\"", "from": "\"R EDDY , T.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"T. Reddy contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"D. Cornuelle contributed to the development and research of the SciPy library.\"", "from": "\"C OURNAPEAU , D.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"D. Cornuelle contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"E. Burovski contributed to the development and research of the SciPy library.\"", "from": "\"B UROVSKI , E.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. Burovski contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"P. Peterson contributed to the development and research of the SciPy library.\"", "from": "\"P ETERSON , P.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"P. Peterson contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"W. Eckesser contributed to the development and research of the SciPy library.\"", "from": "\"W ECKESSER , W.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"W. Eckesser contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"J. Bright contributed to the development and research of the SciPy library.\"", "from": "\"B RIGHT , J.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"J. Bright contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"S. J. Van der Walt contributed to the development and research of the SciPy library.\"", "from": "\"V AN DER WALT, S. J.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"S. J. Van der Walt contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"M. Brett contributed to the development and research of the SciPy library.\"", "from": "\"B RETT , M.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"M. Brett contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"J. Wilson contributed to the development and research of the SciPy library.\"", "from": "\"WILSON , J.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"J. Wilson contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"K. J. Millman contributed to the development and research of the SciPy library.\"", "from": "\"M ILLMAN , K. J.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"K. J. Millman contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"N. Mayorov contributed to the development and research of the SciPy library.\"", "from": "\"M AYOROV , N.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"N. Mayorov contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"A. R. J. Nelson contributed to the development and research of the SciPy library.\"", "from": "\"N ELSON , A. R. J.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"A. R. J. Nelson contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"E. Jones contributed to the development and research of the SciPy library.\"", "from": "\"J ONES , E.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. Jones contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"R. Kern contributed to the development and research of the SciPy library.\"", "from": "\"K ERN, R.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"R. Kern contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"E. Larson contributed to the development and research of the SciPy library.\"", "from": "\"L ARSON , E.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. Larson contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"C. J. Carey contributed to the development and research of the SciPy library.\"", "from": "\"C AREY , C. J.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"C. J. Carey contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"I. Polat contributed to the development and research of the SciPy library.\"", "from": "\"P OLAT,\u02d9I.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"I. Polat contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"Y. Feng contributed to the development and research of the SciPy library.\"", "from": "\"F ENG, Y.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"Y. Feng contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"E. W. Moore contributed to the development and research of the SciPy library.\"", "from": "\"M OORE , E. W.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. W. Moore contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"J. Vander Plas contributed to the development and research of the SciPy library.\"", "from": "\"V ANDER PLAS, J.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"J. Vander Plas contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"D. Laxalde contributed to the development and research of the SciPy library.\"", "from": "\"L AXALDE , D.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"D. Laxalde contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"J. Perktold contributed to the development and research of the SciPy library.\"", "from": "\"P ERKTOLD, J.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"J. Perktold contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"R. Cimrman contributed to the development and research of the SciPy library.\"", "from": "\"C IMRMAN , R.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"R. Cimrman contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"I. Henriksen contributed to the development and research of the SciPy library.\"", "from": "\"H ENRIKSEN , I.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"I. Henriksen contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"E. A. Quintero contributed to the development and research of the SciPy library.\"", "from": "\"Q UINTERO , E. A.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"E. A. Quintero contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"C. R. Harris contributed to the development and research of the SciPy library.\"", "from": "\"H ARRIS, C. R.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"C. R. Harris contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"A. M. Archibald contributed to the development and research of the SciPy library.\"", "from": "\"A RCHIBALD , A. M.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"A. M. Archibald contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"A. H. Ribeiro contributed to the development and research of the SciPy library.\"", "from": "\"RIBEIRO , A. H.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"A. H. Ribeiro contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"F. Pedregosa contributed to the development and research of the SciPy library.\"", "from": "\"P EDREGOSA , F.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"F. Pedregosa contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"P. Van Mulbregt contributed to the development and research of the SciPy library.\"", "from": "\"V ANMULBREGT, P.\"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"P. Van Mulbregt contributed to the development and research of the SciPy library.\"", "to": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "width": 27.0}, {"description": "\"K. Wang contributed to the development and research of the SciPy library.\"", "from": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"K. Wang contributed to the development and research of the SciPy library.\"", "to": "\"W ANG, K.\"", "width": 16.0}, {"description": "\"N. Reimers contributed to the development and research of the SciPy library.\"", "from": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"N. Reimers contributed to the development and research of the SciPy library.\"", "to": "\"R EIMERS , N.\"", "width": 16.0}, {"description": "\"I. Gurevych contributed to the development and research of the SciPy library.\"", "from": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"I. Gurevych contributed to the development and research of the SciPy library.\"", "to": "\"GUREVYCH , I.\"", "width": 16.0}, {"description": "\"A. Williams contributed to the development and research of the SciPy library.\"", "from": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"A. Williams contributed to the development and research of the SciPy library.\"", "to": "\"W ILLIAMS , A.\"", "width": 16.0}, {"description": "\"N. Angia contributed to the development and research of the SciPy library.\"", "from": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"N. Angia contributed to the development and research of the SciPy library.\"", "to": "\"N ANGIA , N.\"", "width": 16.0}, {"description": "\"S. Bowman contributed to the development and research of the SciPy library.\"", "from": "\"AND SCIPY1.0 C ONTRIBUTORS \"", "keywords": "\"contribution, collaboration\"", "source_id": "chunk-90c5c9d551d02ddad6bd6b872760fc23", "title": "\"S. Bowman contributed to the development and research of the SciPy library.\"", "to": "\"BOWMAN , S.\"", "width": 16.0}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>