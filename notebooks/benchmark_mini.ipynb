{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init device: cuda\n",
      "Loading embedding tokenizer/model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_entities_name.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_chunks.json'} 0 data\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Loads embedding model, builds embedding_func, and instantiates a MiniRAG object.\n",
    "# Does NOT ingest documents. Use the next cell to index.\n",
    "\n",
    "import os, torch, sys\n",
    "import minirag\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.llm import ollama\n",
    "from minirag.llm.openai import openai_complete, openai_queue_completion\n",
    "from minirag import MiniRAG\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set in environment. Set it before running this cell.\")\n",
    "\n",
    "sys.path.append(r'c:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\Chatbot\\lightrag')\n",
    "\n",
    "# Core configuration (shared by later cells)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\"\n",
    "LLM_MODEL_NAME = \"qwen2m:latest\"  # set to None if no local Ollama model\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Init device:\", device)\n",
    "\n",
    "print(\"Loading embedding tokenizer/model...\")\n",
    "_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "_embed_model = AutoModel.from_pretrained(EMBEDDING_MODEL).to(device)\n",
    "_embed_model.eval()\n",
    "\n",
    "async def _embed_batch(texts: list[str]):\n",
    "    return await hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model)\n",
    "\n",
    "async def _embed_dispatch(input_text):\n",
    "    if isinstance(input_text, str):\n",
    "        return (await _embed_batch([input_text]))[0]\n",
    "\n",
    "\n",
    "        \n",
    "    if isinstance(input_text, (list, tuple)) and all(isinstance(t, str) for t in input_text):\n",
    "        return await _embed_batch(list(input_text))\n",
    "    raise TypeError(f\"Unsupported input type for embedding_func: {type(input_text)}\")\n",
    "\n",
    "_embedding_func = EmbeddingFunc(\n",
    "    embedding_dim=_embed_model.config.hidden_size,\n",
    "    max_token_size=_tokenizer.model_max_length,\n",
    "    func = lambda texts: hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model),\n",
    ")\n",
    "# rag = minirag.MiniRAG(\n",
    "#     working_dir=WORKING_DIR,\n",
    "#     llm_model_func=ollama.ollama_model_complete if LLM_MODEL_NAME else None,\n",
    "#     llm_model_name=LLM_MODEL_NAME,\n",
    "#     embedding_func=_embedding_func,\n",
    "#     log_level=LOG_LEVEL,\n",
    "#     suppress_httpx_logging=True\n",
    "# )\n",
    "rag = minirag.MiniRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=openai_queue_completion,\n",
    "    llm_model_max_token_size=200,\n",
    "    llm_model_kwargs={\"api_key\": api_key},\n",
    "    # llm_model_name=LLM_MODEL_NAME,\n",
    "    llm_model_name=\"gpt-5-nano\",\n",
    "    embedding_func=_embedding_func,\n",
    "    log_level=LOG_LEVEL,\n",
    "    suppress_httpx_logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 442 docs in 12.25s\n",
      "Start RSS: 884.8 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4054e3ee67440885a4759c961455d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ingest:   0%|          | 0/442 [00:00<?, ?doc/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch Submit] id=batch_68ad8833b80c819089247b550e5877be size=16 total_batches=1 total_reqs=16\n",
      "[Batch Submit] id=batch_68ad8917847881908143bf12a03e06f5 size=16 total_batches=2 total_reqs=32\n",
      "⠼ Processed 4 chunks, 36 entities(duplicated), 27 relations(duplicated)d)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Status] queue=16 total_batches=2 submitted_reqs=32\n",
      "[Batch Submit] id=batch_68ad89cc73dc81909f021552f607a79a size=16 total_batches=3 total_reqs=48\n",
      "[Batch Submit] id=batch_68ad8ab2ed988190818935c5c3e81e5e size=16 total_batches=4 total_reqs=64\n",
      "[Batch Submit] id=batch_68ad8b841b0c8190be79facf40c82c57 size=16 total_batches=5 total_reqs=80\n",
      "[Batch Submit] id=batch_68ad8c3c4e008190aca8ff2f550388b2 size=16 total_batches=6 total_reqs=96\n",
      "[Status] queue=16 total_batches=6 submitted_reqs=96elations(duplicated)ed)\n",
      "[Batch Submit] id=batch_68ad8d0b7f2c8190bd69f558fa811689 size=16 total_batches=7 total_reqs=112\n",
      "[Batch Submit] id=batch_68ad8f289e108190971460c47ffc3572 size=16 total_batches=8 total_reqs=128\n",
      "[Batch Submit] id=batch_68ad9542ce888190b30b69fb10e7fca3 size=16 total_batches=9 total_reqs=144\n",
      "[Batch Submit] id=batch_68ad9fafcc84819097cbc3e3fb3e4123 size=16 total_batches=10 total_reqs=160\n",
      "[Batch Submit] id=batch_68ada063d06081909e2ed7300b1039aa size=16 total_batches=11 total_reqs=176\n",
      "[Batch Submit] id=batch_68ada182daa88190b5aec4138807074c size=16 total_batches=12 total_reqs=192\n",
      "[Batch Submit] id=batch_68ada662ed14819085535cae75435245 size=16 total_batches=13 total_reqs=208\n",
      "[Status] queue=0 total_batches=13 submitted_reqs=208\n",
      "[Batch Submit] id=batch_68ada71cacd08190a993b45b72715506 size=16 total_batches=14 total_reqs=224\n",
      "[Batch Submit] id=batch_68adac4df47c8190b9454735d12a5b7b size=16 total_batches=15 total_reqs=240\n",
      "[Batch Submit] id=batch_68adae4e57dc8190babebcbaac0519bd size=16 total_batches=16 total_reqs=256\n",
      "[Status] queue=0 total_batches=16 submitted_reqs=256 relations(duplicated)\n",
      "[Batch Submit] id=batch_68adaf1da5008190b2a836f9d00a2553 size=16 total_batches=17 total_reqs=272\n",
      "[Batch Submit] id=batch_68adb014bec881909623a7afbe6e3784 size=16 total_batches=18 total_reqs=288\n",
      "[Batch Submit] id=batch_68adb0e3bb108190b9e37d34ac945187 size=16 total_batches=19 total_reqs=304\n",
      "[Batch Submit] id=batch_68adb1995c548190b432d3a2af95a21a size=16 total_batches=20 total_reqs=320\n",
      "[Status] queue=16 total_batches=20 submitted_reqs=320elations(duplicated)\n",
      "[Batch Submit] id=batch_68adb24ed7bc81908a1a621d7b2cbdcb size=16 total_batches=21 total_reqs=336\n",
      "[Batch Submit] id=batch_68adb30129c0819080c7bd6b6e70a161 size=16 total_batches=22 total_reqs=352\n",
      "[Batch Submit] id=batch_68adb3b68c148190acb4b4527174afe7 size=16 total_batches=23 total_reqs=368\n",
      "[Batch Submit] id=batch_68adb46bf6e0819098a57d7d9de08c00 size=16 total_batches=24 total_reqs=384\n",
      "[Batch Submit] id=batch_68adb5277bd481909f50ed6b1193133a size=16 total_batches=25 total_reqs=400\n",
      "[Batch Submit] id=batch_68adb5dc2e5c8190b0c1821aa9cc19d6 size=16 total_batches=26 total_reqs=416\n",
      "[Batch Submit] id=batch_68adb690f5a081908769476db231224d size=16 total_batches=27 total_reqs=432\n",
      "[Batch Submit] id=batch_68adb75fdd748190843365bf5b5d7505 size=16 total_batches=28 total_reqs=448\n",
      "[Batch Submit] id=batch_68adb81b7424819084843e45b8af1b81 size=16 total_batches=29 total_reqs=464\n",
      "[Status] queue=0 total_batches=29 submitted_reqs=464\n",
      "[Batch Submit] id=batch_68adb8d120a8819097c2d0789e10b295 size=16 total_batches=30 total_reqs=480\n",
      "[Batch Submit] id=batch_68adb9a375e08190a2826dbbfcb2c93c size=16 total_batches=31 total_reqs=496\n",
      "[Batch Submit] id=batch_68adbac83e948190be45dd0a73f5a44c size=16 total_batches=32 total_reqs=512\n",
      "[Status] queue=16 total_batches=32 submitted_reqs=512relations(duplicated)\n",
      "[Batch Submit] id=batch_68adbb7cd26081908fad1f2737e3e2d5 size=16 total_batches=33 total_reqs=528\n",
      "[Batch Submit] id=batch_68adbc653ba88190a856e5340b645cd0 size=16 total_batches=34 total_reqs=544\n",
      "[Batch Submit] id=batch_68adbe2d08a481909021d1fe36ee69ba size=16 total_batches=35 total_reqs=560\n",
      "[Batch Submit] id=batch_68adbee1fb84819097e2b2d6162d2fa5 size=16 total_batches=36 total_reqs=576\n",
      "[Batch Submit] id=batch_68adbfc7d2ec8190a65c6360ac0cabfc size=16 total_batches=37 total_reqs=592\n",
      "[Batch Submit] id=batch_68adc07cac5c8190aaed2e1e1d0967cb size=16 total_batches=38 total_reqs=608\n",
      "[Batch Submit] id=batch_68adc139a9e48190b566e9f7066dbec9 size=16 total_batches=39 total_reqs=624\n",
      "[Batch Submit] id=batch_68adc1ef2e0c8190baba269380e134d4 size=16 total_batches=40 total_reqs=640\n",
      "[Batch Submit] id=batch_68adc339d9288190b90dd230cceb95ff size=16 total_batches=41 total_reqs=656\n",
      "[Batch Submit] id=batch_68adc3eea1d081908a9df126325dbea0 size=16 total_batches=42 total_reqs=672\n",
      "[Batch Submit] id=batch_68adc4e77864819085338078d77579d6 size=16 total_batches=43 total_reqs=688\n",
      "[Batch Submit] id=batch_68adc768f990819089191a21d18407a6 size=16 total_batches=44 total_reqs=704\n",
      "[Status] queue=0 total_batches=44 submitted_reqs=704 relations(duplicated)\n",
      "[Batch Submit] id=batch_68adc96ce570819095bcf7032653a441 size=16 total_batches=45 total_reqs=720\n",
      "[Batch Submit] id=batch_68adcb4808488190b5ffd018a9e84bf4 size=16 total_batches=46 total_reqs=736\n",
      "[Status] queue=16 total_batches=46 submitted_reqs=736relations(duplicated)\n",
      "[Batch Submit] id=batch_68adcbfc330c8190b01955e9a3da9b62 size=16 total_batches=47 total_reqs=752\n",
      "[Batch Submit] id=batch_68add22d6cdc8190931c81ff0e319ec8 size=16 total_batches=48 total_reqs=768\n",
      "[Batch Submit] id=batch_68add2e2da288190a34bd5b395ccf7b4 size=16 total_batches=49 total_reqs=784\n",
      "[Batch Submit] id=batch_68add39fa49c8190aeb3a905c84b63c2 size=16 total_batches=50 total_reqs=800\n",
      "[Batch Submit] id=batch_68add45c5efc8190ad9d35bd915a1ee0 size=16 total_batches=51 total_reqs=816\n",
      "[Batch Submit] id=batch_68add510b8348190a034f06e997dd960 size=16 total_batches=52 total_reqs=832\n",
      "[Batch Submit] id=batch_68add5c548d0819086d81edb14414ad7 size=16 total_batches=53 total_reqs=848\n",
      "[Batch Submit] id=batch_68add67b2fd0819090d3fa52fc5ff9f0 size=16 total_batches=54 total_reqs=864\n",
      "[Batch Submit] id=batch_68add86e0c9c81908788146c2b7bfb59 size=16 total_batches=55 total_reqs=880\n",
      "[Batch Submit] id=batch_68add9251b6c8190a7b286d195a4857c size=16 total_batches=56 total_reqs=896\n",
      "[Batch Submit] id=batch_68add9d733188190ab7883c3c363e310 size=16 total_batches=57 total_reqs=912\n",
      "[Batch Submit] id=batch_68adda8bdc08819099c1c69603cbe2a8 size=16 total_batches=58 total_reqs=928\n",
      "[Batch Submit] id=batch_68adde46e3d48190828edf366e83d063 size=16 total_batches=59 total_reqs=944\n",
      "[Batch Submit] id=batch_68addf404fc88190b0605208bfc69a0d size=16 total_batches=60 total_reqs=960\n",
      "[Status] queue=0 total_batches=60 submitted_reqs=960 relations(duplicated)\n",
      "[Batch Submit] id=batch_68ade024fc9c8190a11e48475a6bd92e size=16 total_batches=61 total_reqs=976\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m     end_mem \u001b[38;5;241m=\u001b[39m memory_mb()\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_mem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd RSS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_mem\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB (Δ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_mem\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_mem\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m index_documents()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing complete. Proceed to Cell 2 for querying & evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 114\u001b[0m, in \u001b[0;36mindex_documents\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# Backpressure on number of in-flight tasks\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(in_flight) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m IN_FLIGHT_LIMIT:\n\u001b[1;32m--> 114\u001b[0m         done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(in_flight, return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_COMPLETED)\n\u001b[0;32m    115\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(done))\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m spawn(doc)\n",
      "File \u001b[1;32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\asyncio\\tasks.py:464\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(fs, timeout, return_when)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing coroutines is forbidden, use tasks explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    463\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[1;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[1;32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\asyncio\\tasks.py:550\u001b[0m, in \u001b[0;36m_wait\u001b[1;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[0;32m    547\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, time, json, random, gc, asyncio\n",
    "from pathlib import Path\n",
    "import psutil, torch\n",
    "import minirag\n",
    "from minirag.llm import ollama\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\"\"\"\n",
    "Cell 1: Document Ingestion / Indexing Only\n",
    "-----------------------------------------\n",
    "Uses openai_queue_completion so LLM calls are queued into Batch API.\n",
    "We schedule a controlled number of concurrent ainsert tasks (IN_FLIGHT_LIMIT)\n",
    "so memory stays bounded. Each flush prints: batch id, size, cumulative totals.\n",
    "\"\"\"\n",
    "# ---------------- User Config ----------------\n",
    "RANDOM_SEED = 42\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None\n",
    "DATASET_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\dataset\\\\LiHua-World\\\\data\\\\\"\n",
    "IN_FLIGHT_LIMIT = 10          # max concurrent rag.ainsert tasks\n",
    "TARGET_BATCH_QUEUE = 350      # flush once queued pending >= this (manager max_batch_size may also trigger)\n",
    "FINAL_FLUSH = True\n",
    "STATUS_EVERY = 60.0           # seconds\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "def memory_mb():\n",
    "    return PROCESS.memory_info().rss / 1048576 if PROCESS else None\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suf = path.suffix.lower()\n",
    "    try:\n",
    "        if suf in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suf == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            if isinstance(data, dict):\n",
    "                for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                    if k in data and isinstance(data[k], str):\n",
    "                        return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suf in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str): lines.append(obj[k]); break\n",
    "                            else: lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        txt = read_text_from_file(p).strip()\n",
    "        if not txt: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": txt, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "async def index_documents():\n",
    "    from tqdm.auto import tqdm\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs: return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.1f} MB\")\n",
    "\n",
    "    # Access batch manager to monitor queue len\n",
    "    from minirag.llm.openai import init_openai_batch_manager\n",
    "    mgr = init_openai_batch_manager()\n",
    "\n",
    "    in_flight: set[asyncio.Task] = set()\n",
    "    total_started = 0\n",
    "    last_status = time.time()\n",
    "\n",
    "    async def spawn(doc):\n",
    "        nonlocal total_started\n",
    "        t = asyncio.create_task(rag.ainsert(doc['text'], metadata={\"id\": doc['id'], \"source\": doc['source_path']}, file_path=doc['source_path']))\n",
    "        in_flight.add(t)\n",
    "        t.add_done_callback(lambda f: in_flight.discard(t))\n",
    "        total_started += 1\n",
    "\n",
    "    pbar = tqdm(total=len(docs), desc=\"Ingest\", unit=\"doc\")\n",
    "    for doc in docs:\n",
    "        # Backpressure on number of in-flight tasks\n",
    "        while len(in_flight) >= IN_FLIGHT_LIMIT:\n",
    "            done, _ = await asyncio.wait(in_flight, return_when=asyncio.FIRST_COMPLETED)\n",
    "            pbar.update(len(done))\n",
    "        await spawn(doc)\n",
    "        # Flush condition: queued pending requests beyond threshold\n",
    "        qlen = getattr(mgr, 'queue_len', 0)\n",
    "        if qlen >= TARGET_BATCH_QUEUE:\n",
    "            print(f\"[Trigger Flush] pending_queue={qlen} >= {TARGET_BATCH_QUEUE}\")\n",
    "            await mgr.flush()  # prints batch submit inside\n",
    "        # Periodic status\n",
    "        if time.time() - last_status >= STATUS_EVERY:\n",
    "            snap = mgr.status_snapshot()\n",
    "            print(f\"[Status] queue={snap['queue_len']} total_batches={snap.get('total_batches')} submitted_reqs={snap.get('total_submitted_reqs')}\")\n",
    "            last_status = time.time()\n",
    "    # Drain remaining tasks\n",
    "    while in_flight:\n",
    "        done, _ = await asyncio.wait(in_flight, return_when=asyncio.FIRST_COMPLETED)\n",
    "        pbar.update(len(done))\n",
    "    pbar.close()\n",
    "\n",
    "    if FINAL_FLUSH and getattr(mgr, 'queue_len', 0):\n",
    "        print(f\"[Final Flush] remaining_queue={mgr.queue_len}\")\n",
    "        await mgr.flush()\n",
    "\n",
    "    # Wait for any outstanding batch polling completions\n",
    "    # Poll until all futures settled (approx): check active batches statuses\n",
    "    pending_batches = True\n",
    "    while pending_batches:\n",
    "        snap = mgr.status_snapshot()\n",
    "        active = [s for s in snap['batches'].values() if s not in ('completed','failed','cancelled','expired')]\n",
    "        if not active:\n",
    "            break\n",
    "        print(f\"[Await Batches] active={len(active)} snapshot={snap['batches']}\")\n",
    "        await asyncio.sleep(15)\n",
    "\n",
    "    dur = time.perf_counter()-t0\n",
    "    print(f\"Inserted {len(docs)} docs in {dur:.2f}s\")\n",
    "    end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.1f} MB (Δ {end_mem - start_mem:.1f} MB)\")\n",
    "\n",
    "await index_documents()\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 637 QA pairs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9c4839fb744c0d833b123fe15d9c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval-mini:   0%|          | 0/637 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Did Adam Smith send a message to Li Hua about the upcoming building maintenance ...\n",
      "Answer: Sorry, I'm not able to provide an answer to that question.\n",
      "Gold: Yes\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.000', 'jaccard': '0.000', 'levenshtein': '0.000', 'rouge1_f': '0.000', 'rouge2_f': '0.000', 'overlap': '0.000', 'bleu': '0.000', 'bert_cos': '0.251'} Latency: 2668.8 ms\n",
      "-\n",
      "Q2: Did Wolfgang ask Li Hua about watching \"Star Wars: A New Hope\" after he asked Li...\n",
      "Answer: Sorry, I'm not able to provide an answer to that question.\n",
      "Gold: Yes\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.000', 'jaccard': '0.000', 'levenshtein': '0.000', 'rouge1_f': '0.000', 'rouge2_f': '0.000', 'overlap': '0.000', 'bleu': '0.000', 'bert_cos': '0.251'} Latency: 634.9 ms\n",
      "-\n",
      "\n",
      "Aggregate: exact=0.00% substring=33.33% token_recall=33.33%\n",
      "  jaccard: 0.000\n",
      "  levenshtein: 0.000\n",
      "  rouge1_f: 0.000\n",
      "  rouge2_f: 0.000\n",
      "  overlap: 0.000\n",
      "  bleu: 0.003\n",
      "  bert_cos: 0.208\n",
      "Latency: avg=1898.3 ms p95=2391.1 ms\n",
      "Saved results to C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\\results_mini5.csv\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Query & QA Evaluation\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Supports:\n",
    "#  - Loading LiHua-World QA pairs from query_set.csv\n",
    "#  - Evaluating answer quality with simple + lexical + semantic metrics\n",
    "#  - Optional CSV logging\n",
    "\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from pathlib import Path\n",
    "# from minirag import QueryParam\n",
    "from minirag import QueryParam\n",
    "from minirag.utils import calculate_similarity  # legacy helper (returns indices) – not used now\n",
    "\n",
    "# Extra metric libs (lazy loads handled in compute_similarity)\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------- Configuration --------\n",
    "QA_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\dataset\\LiHua-World\\qa\\query_set.csv\"\n",
    "OUTPUT_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\"  # set to None to skip saving\n",
    "QUERY_MODE = \"naive\"      # mini | light | naive | doc | meta | bm25\n",
    "TOP_K = 5\n",
    "MAX_Q = None             # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True       # toggle semantic similarity (slower)\n",
    "EVAL_CONCURRENCY = 12      # max concurrent rag.aquery calls\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# -------- Metrics Helpers --------\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "\n",
    "# lazy globals\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH_FN = SmoothingFunction().method1\n",
    "\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return TOKEN_SPLIT_RE.sub(\" \", s.lower()).strip()\n",
    "\n",
    "\n",
    "def token_set(s: str) -> set[str]:\n",
    "    return {t for t in normalize_text(s).split() if t}\n",
    "\n",
    "_BERT_MODEL = None\n",
    "_ROUGE = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def calculate_best_similarity(sentences: list[str], target: str, method=\"levenshtein\", n=1):\n",
    "    \"\"\"\n",
    "    Returns the highest similarity score (float) between any sentence in `sentences` and `target`.\n",
    "    Methods: jaccard | levenshtein | rouge | bert | overlap | bleu\n",
    "    For rouge, n=1 or 2 selects rouge-1 or rouge-2 F.\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    tgt_tokens = target.lower().split()\n",
    "    scores = []\n",
    "\n",
    "    if method == \"jaccard\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_tokens = set(s.lower().split())\n",
    "            inter = set(s_tokens).intersection(set(tgt_set))\n",
    "            union = set(s_tokens).union(set(tgt_set))\n",
    "            scores.append(len(inter) / len(union) if union else 0.0)\n",
    "\n",
    "    elif method == \"levenshtein\":\n",
    "        tgt_len = max(len(tgt_tokens), 1)\n",
    "        for s in sentences:\n",
    "            dist = edit_distance(tgt_tokens, s.lower().split())\n",
    "            norm = max(tgt_len, len(s.split()))\n",
    "            scores.append(1 - dist / norm if norm else 0.0)\n",
    "\n",
    "    elif method == \"rouge\":\n",
    "        global _ROUGE\n",
    "        if _ROUGE is None:\n",
    "            _ROUGE = Rouge()\n",
    "        key = f\"rouge-{n}\"\n",
    "        for s in sentences:\n",
    "            r = _ROUGE.get_scores(s, target)\n",
    "            scores.append(r[0].get(key, {}).get(\"f\", 0.0))\n",
    "\n",
    "    elif method == \"bert\":\n",
    "        global _BERT_MODEL\n",
    "        if _BERT_MODEL is None:\n",
    "            _BERT_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = _BERT_MODEL.encode(sentences + [target], show_progress_bar=False)\n",
    "        tgt_vec = embeddings[-1]\n",
    "        tgt_norm = np.linalg.norm(tgt_vec)\n",
    "        for i in range(len(sentences)):\n",
    "            v = embeddings[i]\n",
    "            denom = (np.linalg.norm(v) * tgt_norm)\n",
    "            scores.append(float(np.dot(v, tgt_vec) / denom) if denom else 0.0)\n",
    "\n",
    "    elif method == \"overlap\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_set = set(s.lower().split())\n",
    "            inter = s_set & tgt_set\n",
    "            denom = min(len(s_set), len(tgt_set))\n",
    "            scores.append(len(inter) / denom if denom else 0.0)\n",
    "\n",
    "    elif method == \"bleu\":\n",
    "        tgt_bleu = word_tokenize(target.lower())\n",
    "        for s in sentences:\n",
    "            s_bleu = word_tokenize(s.lower())\n",
    "            scores.append(sentence_bleu([tgt_bleu], s_bleu, smoothing_function=_SMOOTH))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method.\")\n",
    "\n",
    "    return max(scores) if scores else 0.0\n",
    "\n",
    "def compute_similarity(answer: str, gold: str, use_bert: bool = True) -> dict:\n",
    "    \"\"\"Compute a bundle of similarity scores between answer and gold.\n",
    "\n",
    "    Returns keys:\n",
    "      jaccard, levenshtein, rouge1_f, rouge2_f, overlap, bleu, bert_cos (optional)\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(answer)\n",
    "    jaccard = calculate_best_similarity(sentences, gold, method=\"jaccard\")\n",
    "    levenshtein = calculate_best_similarity(sentences, gold, method=\"levenshtein\")\n",
    "    rouge1_f = calculate_best_similarity(sentences, gold, method=\"rouge\", n=1)\n",
    "    rouge2_f = calculate_best_similarity(sentences, gold, method=\"rouge\", n=2)\n",
    "    overlap = calculate_best_similarity(sentences, gold, method=\"overlap\")\n",
    "    bleu = calculate_best_similarity(sentences, gold, method=\"bleu\")\n",
    "    bert_cos = calculate_best_similarity(sentences, gold, method=\"bert\") if use_bert else None\n",
    "\n",
    "    result = {\n",
    "        'jaccard': jaccard,\n",
    "        'levenshtein': levenshtein,\n",
    "        'rouge1_f': rouge1_f,\n",
    "        'rouge2_f': rouge2_f,\n",
    "        'overlap': overlap,\n",
    "        'bleu': bleu,\n",
    "    }\n",
    "    if bert_cos is not None:\n",
    "        result['bert_cos'] = bert_cos\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics(answer: str, gold: str) -> dict:\n",
    "    # Basic lexical metrics\n",
    "    a_norm, g_norm = normalize_text(answer), normalize_text(gold)\n",
    "    exact = bool(g_norm) and a_norm == g_norm\n",
    "    substring = bool(g_norm) and g_norm in a_norm\n",
    "    ts_a, ts_g = token_set(answer), token_set(gold)\n",
    "    token_recall = (len(ts_a & ts_g) / len(ts_g)) if ts_g else 0.0\n",
    "\n",
    "    sim_bundle = compute_similarity(answer, gold, use_bert=USE_BERT_SIM)\n",
    "\n",
    "    return {\n",
    "        'exact': exact,\n",
    "        'substring': substring,\n",
    "        'token_recall': token_recall,\n",
    "        **sim_bundle,\n",
    "    }\n",
    "\n",
    "# -------- Load QA Pairs --------\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_CSV_PATH):\n",
    "    with open(QA_CSV_PATH, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if \"Question\" in row and \"Gold Answer\" in row:\n",
    "                qa_pairs.append((row[\"Question\"].strip(), row[\"Gold Answer\"].strip()))\n",
    "else:\n",
    "    print(\"QA CSV not found. Provide QA_CSV_PATH or create synthetic pairs manually.\")\n",
    "\n",
    "if MAX_Q:\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} QA pairs.\")\n",
    "if not qa_pairs:\n",
    "    raise SystemExit(\"No QA data available.\")\n",
    "\n",
    "assert 'rag' in globals(), \"rag not found. Run Cell 1 first.\"\n",
    "\n",
    "# -------- Evaluation (Concurrency-Limited) --------\n",
    "from asyncio import Semaphore, create_task, as_completed\n",
    "\n",
    "async def run_eval(mode, n, concurrency=EVAL_CONCURRENCY):\n",
    "    qp = QueryParam(mode=mode, top_k=TOP_K)\n",
    "    sem = Semaphore(concurrency)\n",
    "    rows = []\n",
    "    latencies = []\n",
    "\n",
    "    async def one(i, question, gold):\n",
    "        async with sem:\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                answer = await rag.aquery(question, param=qp)\n",
    "            except TypeError:\n",
    "                answer = await rag.aquery(question)\n",
    "            latency = time.perf_counter() - t0\n",
    "            m = compute_metrics(answer, gold)\n",
    "            return {\n",
    "                \"i\": i,\n",
    "                \"question\": question,\n",
    "                \"gold\": gold,\n",
    "                \"answer\": answer,\n",
    "                \"latency_s\": latency,\n",
    "                **m\n",
    "            }\n",
    "\n",
    "    tasks = [create_task(one(i, q, g)) for i, (q, g) in enumerate(qa_pairs, start=1)]\n",
    "    pbar = tqdm(total=len(tasks), desc=f\"Eval-{mode}\", unit=\"q\")\n",
    "\n",
    "    for fut in as_completed(tasks):\n",
    "        try:\n",
    "            row = await fut\n",
    "        except Exception as e:\n",
    "            # Capture exceptions as a row with error info\n",
    "            row = {\"i\": -1, \"question\": None, \"gold\": None, \"answer\": f\"ERROR: {e}\", \"latency_s\": 0.0}\n",
    "        rows.append(row)\n",
    "        if 'latency_s' in row:\n",
    "            latencies.append(row.get(\"latency_s\", 0.0))\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # Remove errored placeholder rows from metrics\n",
    "    rows_ok = [r for r in rows if r.get('i', -1) != -1]\n",
    "    if not rows_ok:\n",
    "        print(\"All queries failed.\")\n",
    "        return rows\n",
    "\n",
    "    rows_ok.sort(key=lambda r: r[\"i\"])  # original order\n",
    "\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows_ok if isinstance(r.get(key), (int, float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "\n",
    "    exact_rate = _avg('exact')\n",
    "    substr_rate = _avg('substring')\n",
    "    avg_token_recall = _avg('token_recall')\n",
    "    avg_lat = sum(latencies)/len(latencies) if latencies else 0.0\n",
    "    p95_lat = (sorted(latencies)[int(len(latencies)*0.95)-1] if len(latencies) > 1 else (latencies[0] if latencies else 0.0))\n",
    "\n",
    "    print(f\"\\nAggregate: exact={exact_rate:.2%} substring={substr_rate:.2%} token_recall={avg_token_recall:.2%}\")\n",
    "    for mkey in ['jaccard','levenshtein','rouge1_f','rouge2_f','overlap','bleu','bert_cos']:\n",
    "        if rows_ok and mkey in rows_ok[0]:\n",
    "            print(f\"  {mkey}: {_avg(mkey):.3f}\")\n",
    "    print(f\"Latency: avg={avg_lat*1000:.1f} ms p95={p95_lat*1000:.1f} ms (concurrency={concurrency})\")\n",
    "\n",
    "    if OUTPUT_CSV_PATH and rows_ok:\n",
    "        os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "        OUTPUT_CSV = os.path.join(OUTPUT_CSV_PATH, f\"results_{mode}{n}.csv\")\n",
    "        write_header = not os.path.exists(OUTPUT_CSV)\n",
    "        with open(OUTPUT_CSV, 'a', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(rows_ok[0].keys()))\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            writer.writerows(rows_ok)\n",
    "        print(f\"Saved results to {OUTPUT_CSV}\")\n",
    "    return rows_ok\n",
    "\n",
    "# Run evaluation (adjust EVAL_CONCURRENCY above as needed)\n",
    "eval_results1 = await run_eval(\"light\", 5)\n",
    "eval_results2 = await run_eval(\"mini\", 5)\n",
    "eval_results3 = await run_eval(\"naive\", 5)\n",
    "eval_results4 = await run_eval(\"bypass\", 5)\n",
    "print(\"Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
