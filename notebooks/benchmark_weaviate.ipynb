{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutConfig not available in this weaviate version; using default client timeouts.\n",
      "Server modules detected: ['generative-anthropic', 'generative-anyscale', 'generative-aws', 'generative-cohere', 'generative-databricks', 'generative-friendliai', 'generative-google', 'generative-mistral', 'generative-nvidia', 'generative-octoai', 'generative-ollama', 'generative-openai', 'generative-xai', 'multi2multivec-jinaai', 'multi2vec-cohere', 'multi2vec-google', 'multi2vec-jinaai', 'multi2vec-nvidia', 'multi2vec-voyageai', 'reranker-cohere', 'reranker-jinaai', 'reranker-nvidia', 'reranker-voyageai', 'text2multivec-jinaai', 'text2vec-aws', 'text2vec-cohere', 'text2vec-databricks', 'text2vec-google', 'text2vec-huggingface', 'text2vec-jinaai', 'text2vec-mistral', 'text2vec-nvidia', 'text2vec-octoai', 'text2vec-openai', 'text2vec-transformers', 'text2vec-voyageai', 'text2vec-weaviate']\n"
     ]
    },
    {
     "ename": "UnexpectedStatusCodeError",
     "evalue": "Collection may not have been created properly.! Unexpected status code: 422, with response body: {'error': [{'message': \"'LiHua World' is not a valid class name\"}]}.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnexpectedStatusCodeError\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     47\u001b[39m api_endpoint = \u001b[33m\"\u001b[39m\u001b[33mhttp://host.docker.internal:11434\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Ollama on host\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollections\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNAME_DATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mA collection of time-indexed stories for LiHua. The time is indexed in the file_path property. And the text property contains the story content.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mProperty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDataType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTEXT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTokenization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLOWERCASE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mProperty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDataType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTEXT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvectorizer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mConfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNamedVectors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext2vec_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_vector\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpooling_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmasked_mean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerative_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mConfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerative\u001b[49m\u001b[43m.\u001b[49m\u001b[43mollama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLLM_MODEL_NAME\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m client.collections.exists(NAME_DATASET)\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCollection \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNAME_DATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m ready (text2vec-transformers).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\collections\\collections\\executor.py:240\u001b[39m, in \u001b[36m_CollectionsExecutor.create\u001b[39m\u001b[34m(self, name, description, generative_config, inverted_index_config, multi_tenancy_config, properties, references, replication_config, reranker_config, sharding_config, vector_index_config, vectorizer_config, vector_config, data_model_properties, data_model_references, skip_argument_validation)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateInvalidInputError(\n\u001b[32m    237\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid collection config create parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    238\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_model_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_model_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_model_references\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_model_references\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_argument_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_argument_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\collections\\collections\\executor.py:104\u001b[39m, in \u001b[36m_CollectionsExecutor.__create\u001b[39m\u001b[34m(self, config, data_model_properties, data_model_references, skip_argument_validation)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__create\u001b[39m(\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     95\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     Awaitable[CollectionAsync[Properties, References]],\n\u001b[32m    103\u001b[39m ]:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/schema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweaviate_object\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_msg\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCollection may not have been created properly.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatus_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_ExpectedStatusCodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mok_in\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCreate collection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Awaitable):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\connect\\v4.py:824\u001b[39m, in \u001b[36m_ConnectionBase.post\u001b[39m\u001b[34m(self, path, weaviate_object, params, error_msg, status_codes, is_gql_query)\u001b[39m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m    816\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    817\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    822\u001b[39m     is_gql_query: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    823\u001b[39m ) -> executor.Result[Response]:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_version_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweaviate_object\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweaviate_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_msg\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_msg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatus_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatus_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_gql_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_gql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\connect\\v4.py:718\u001b[39m, in \u001b[36m_ConnectionBase._send\u001b[39m\u001b[34m(self, method, url, error_msg, status_codes, is_gql_query, weaviate_object, params, check_is_connected)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexc\u001b[39m(e: \u001b[38;5;167;01mException\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    716\u001b[39m     \u001b[38;5;28mself\u001b[39m.__handle_exceptions(e, error_msg)\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexception_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\connect\\executor.py:99\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(method, response_callback, exception_callback, *args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp_call\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(T, \u001b[43mexception_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\connect\\v4.py:716\u001b[39m, in \u001b[36m_ConnectionBase._send.<locals>.exc\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexc\u001b[39m(e: \u001b[38;5;167;01mException\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__handle_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_msg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\connect\\v4.py:672\u001b[39m, in \u001b[36m_ConnectionBase.__handle_exceptions\u001b[39m\u001b[34m(self, e, error_msg)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeout):\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateTimeoutError(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\connect\\executor.py:95\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(method, response_callback, exception_callback, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m cast(T, exception_callback(e))\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _execute()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m resp_call = \u001b[43mresponse_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp_call, Awaitable)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp_call\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\connect\\v4.py:713\u001b[39m, in \u001b[36m_ConnectionBase._send.<locals>.resp\u001b[39m\u001b[34m(res)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresp\u001b[39m(res: Response) -> Response:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus_codes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\connect\\v4.py:683\u001b[39m, in \u001b[36m_ConnectionBase.__handle_response\u001b[39m\u001b[34m(self, response, error_msg, status_codes)\u001b[39m\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InsufficientPermissionsError(response)\n\u001b[32m    682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status_codes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m status_codes.ok:\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedStatusCodeError(error_msg, response)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mUnexpectedStatusCodeError\u001b[39m: Collection may not have been created properly.! Unexpected status code: 422, with response body: {'error': [{'message': \"'LiHua World' is not a valid class name\"}]}."
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Sets up Weaviate collection using text2vec-transformers (external inference container).\n",
    "# Ensure docker-compose is up with services: weaviate + t2v-transformers.\n",
    "#   ENABLE_MODULES=text2vec-transformers,generative-ollama\n",
    "#   DEFAULT_VECTORIZER_MODULE=text2vec-transformers\n",
    "#   TRANSFORMERS_INFERENCE_API=http://t2v-transformers:8080\n",
    "# Optional: generative-ollama (Ollama running on host for qwen2m:latest)\n",
    "# Timeout tuning: Some weaviate client versions expose TimeoutConfig; if not, we fall back.\n",
    "\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "\n",
    "# Attempt optional TimeoutConfig (newer weaviate client). If missing, continue with defaults.\n",
    "try:\n",
    "    from weaviate.connect import TimeoutConfig  # may not exist in older versions\n",
    "    TIMEOUTS = TimeoutConfig(init=60, query=180, insert=120)\n",
    "    client = weaviate.connect_to_local(timeout_config=TIMEOUTS)\n",
    "    print(\"Custom timeouts applied:\", TIMEOUTS)\n",
    "except ImportError:\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"TimeoutConfig not available in this weaviate version; using default client timeouts.\")\n",
    "except TypeError:\n",
    "    # Signature mismatch (older version). Reconnect without custom timeouts.\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"TimeoutConfig signature unsupported; using default timeouts.\")\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"  # must match the model available to Ollama on host\n",
    "NAME_DATASET = \"LiHuaWorld\"\n",
    "\n",
    "# Diagnostics\n",
    "try:\n",
    "    meta = client.get_meta()\n",
    "    print(\"Server modules detected:\", list(meta.get(\"modules\", {}).keys()))\n",
    "except Exception as e:\n",
    "    print(\"Meta fetch failed:\", e)\n",
    "\n",
    "# Recreate collection for a clean slate\n",
    "try:\n",
    "    client.collections.delete(NAME_DATASET)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "api_endpoint = \"http://host.docker.internal:11434\"  # Ollama on host\n",
    "\n",
    "client.collections.create(\n",
    "    NAME_DATASET,\n",
    "    description=\"A collection of time-indexed stories for LiHua. The time is indexed in the file_path property. And the text property contains the story content.\",\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT, tokenization=Tokenization.LOWERCASE),\n",
    "        Property(name=\"file_path\", data_type=DataType.TEXT)\n",
    "    ],\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_transformers(\n",
    "            name=\"text_vector\",\n",
    "            source_properties=[\"text\"],\n",
    "            pooling_strategy=\"masked_mean\",\n",
    "        )\n",
    "    ],\n",
    "    generative_config=Configure.Generative.ollama(\n",
    "        api_endpoint=api_endpoint,\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "assert client.collections.exists(NAME_DATASET)\n",
    "print(f\"Collection '{NAME_DATASET}' ready (text2vec-transformers).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n",
      "Loaded 442 docs in 0.08s\n",
      "Start RSS: 519.76 MB\n",
      "Indexing (batched)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3ad67565684fcea612313e35b28a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 442 / 442 docs in 7.27s (60.82 docs/s)\n",
      "End RSS: 525.43 MB (Δ 5.66 MB)\n",
      "Indexing complete. Proceed to Cell 2 for querying & evaluation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json, random, time, gc\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "# ---------------- User Config ----------------\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\dataset\\\\LiHua-World\\\\data\\\\\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\"\n",
    "LLM_MODEL_NAME = \"qwen2m:latest\"\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents(rag):\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing (batched)...\")\n",
    "    t1 = time.perf_counter()\n",
    "    failed = 0\n",
    "    with rag.batch.dynamic() as batch:\n",
    "        for text, metadata in tqdm(zip(texts, metas), desc=\"Indexing\", total=len(texts)):\n",
    "            try:\n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"text\": text,\n",
    "                        \"file_path\": metadata.get(\"source\")\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                if failed < 5:\n",
    "                    print(f\"Failed {metadata.get('id')}: {e}\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)-failed} / {len(texts)} docs in {dur:.2f}s ({((len(texts)-failed)/dur) if dur>0 else 0:.2f} docs/s)\")\n",
    "    if failed:\n",
    "        print(f\"Total failed: {failed}\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Δ {end_mem - start_mem:.2f} MB)\")\n",
    "\n",
    "rag = client.collections.get(NAME_DATASET)\n",
    "await index_documents(rag)\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 637 QA pairs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e12c34db314a518ca74029ffe9c1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval-light:   0%|          | 0/637 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Did Adam Smith send a message to Li Hua about the upcoming building maintenance ...\n",
      "Answer: [timeout after 75.0s attempts=2 last_error=Query call with protocol GRPC search failed with message Deadline Exceeded.]\n",
      "Gold: Yes\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.000', 'jaccard': '0.000', 'levenshtein': '0.000', 'rouge1_f': '0.000', 'rouge2_f': '0.000', 'overlap': '0.000', 'bleu': '0.000', 'bert_cos': '-0.028'} Latency: 75015.9 ms attempts=2\n",
      "-\n",
      "Q2: Did Wolfgang ask Li Hua about watching \"Star Wars: A New Hope\" after he asked Li...\n",
      "Answer: No, Wolfgang did not ask Li Hua about watching \"Star Wars: A New Hope\" after asking about going to see \"Overwatch 3.\" The conversation in the provided text only\n",
      "Gold: Yes\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.000', 'jaccard': '0.000', 'levenshtein': '0.000', 'rouge1_f': '0.000', 'rouge2_f': '0.000', 'overlap': '0.000', 'bleu': '0.000', 'bert_cos': '0.097'} Latency: 17600.8 ms attempts=1\n",
      "-\n",
      "Q2: Did Wolfgang ask Li Hua about watching \"Star Wars: A New Hope\" after he asked Li...\n",
      "Answer: No, Wolfgang did not ask Li Hua about watching \"Star Wars: A New Hope\" after asking about going to see \"Overwatch 3.\" The conversation in the provided text only\n",
      "Gold: Yes\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.000', 'jaccard': '0.000', 'levenshtein': '0.000', 'rouge1_f': '0.000', 'rouge2_f': '0.000', 'overlap': '0.000', 'bleu': '0.000', 'bert_cos': '0.097'} Latency: 17600.8 ms attempts=1\n",
      "-\n",
      "\n",
      "Aggregate: exact=0.16% substring=15.07% token_recall=39.63%\n",
      "  jaccard: 0.073\n",
      "  levenshtein: 0.073\n",
      "  rouge1_f: 0.125\n",
      "  rouge2_f: 0.034\n",
      "  overlap: 0.276\n",
      "  bleu: 0.025\n",
      "  bert_cos: 0.333\n",
      "Latency: avg=4795.4 ms p95=8749.1 ms\n",
      "Saved results to C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\\results_light7.csv\n",
      "Evaluation complete.\n",
      "\n",
      "Aggregate: exact=0.16% substring=15.07% token_recall=39.63%\n",
      "  jaccard: 0.073\n",
      "  levenshtein: 0.073\n",
      "  rouge1_f: 0.125\n",
      "  rouge2_f: 0.034\n",
      "  overlap: 0.276\n",
      "  bleu: 0.025\n",
      "  bert_cos: 0.333\n",
      "Latency: avg=4795.4 ms p95=8749.1 ms\n",
      "Saved results to C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\\results_light7.csv\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Query & QA Evaluation\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Timeout mitigation strategies applied: reduced limit, concise prompts, retries with backoff.\n",
    "\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from pathlib import Path\n",
    "from minirag import QueryParam\n",
    "from minirag.utils import calculate_similarity  # legacy helper (returns indices) – not used now\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------- Configuration --------\n",
    "QA_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\dataset\\LiHua-World\\qa\\query_set.csv\"\n",
    "OUTPUT_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\"  # set to None to skip saving\n",
    "TOP_K = 4            # lower to reduce vector fetch time\n",
    "MAX_Q = None         # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True  # semantic metrics cost\n",
    "PER_QUERY_DEADLINE = 55.0  # seconds, must be < client.query timeout\n",
    "MAX_RETRIES = 2\n",
    "RETRY_BACKOFF = 5.0  # seconds added each retry\n",
    "PROMPT_PREFIX = \"Answer briefly: \"  # keep prompt short -> faster generation\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return TOKEN_SPLIT_RE.sub(\" \", s.lower()).strip()\n",
    "\n",
    "def token_set(s: str) -> set[str]:\n",
    "    return {t for t in normalize_text(s).split() if t}\n",
    "\n",
    "def calculate_best_similarity(sentences: list[str], target: str, method=\"levenshtein\", n=1):\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    tgt_tokens = target.lower().split()\n",
    "    scores = []\n",
    "    if method == \"jaccard\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_tokens = set(s.lower().split())\n",
    "            inter = set(s_tokens) & tgt_set\n",
    "            union = set(s_tokens) | tgt_set\n",
    "            scores.append(len(inter) / len(union) if union else 0.0)\n",
    "    elif method == \"levenshtein\":\n",
    "        tgt_len = max(len(tgt_tokens), 1)\n",
    "        for s in sentences:\n",
    "            dist = edit_distance(tgt_tokens, s.lower().split())\n",
    "            norm = max(tgt_len, len(s.split()))\n",
    "            scores.append(1 - dist / norm if norm else 0.0)\n",
    "    elif method == \"rouge\":\n",
    "        key = f\"rouge-{n}\"\n",
    "        r_inst = _lazy_rouge()\n",
    "        for s in sentences:\n",
    "            r = r_inst.get_scores(s, target)\n",
    "            scores.append(r[0].get(key, {}).get(\"f\", 0.0))\n",
    "    elif method == \"bert\":\n",
    "        model = _lazy_bert()\n",
    "        embeddings = model.encode(sentences + [target], show_progress_bar=False)\n",
    "        tgt_vec = embeddings[-1]\n",
    "        tgt_norm = np.linalg.norm(tgt_vec)\n",
    "        for i in range(len(sentences)):\n",
    "            v = embeddings[i]\n",
    "            denom = (np.linalg.norm(v) * tgt_norm)\n",
    "            scores.append(float(np.dot(v, tgt_vec) / denom) if denom else 0.0)\n",
    "    elif method == \"overlap\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_set = set(s.lower().split())\n",
    "            inter = s_set & tgt_set\n",
    "            denom = min(len(s_set), len(tgt_set))\n",
    "            scores.append(len(inter) / denom if denom else 0.0)\n",
    "    elif method == \"bleu\":\n",
    "        tgt_bleu = word_tokenize(target.lower())\n",
    "        for s in sentences:\n",
    "            s_bleu = word_tokenize(s.lower())\n",
    "            scores.append(sentence_bleu([tgt_bleu], s_bleu, smoothing_function=_SMOOTH))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method.\")\n",
    "    return max(scores) if scores else 0.0\n",
    "\n",
    "def compute_similarity(answer: str, gold: str, use_bert: bool = True) -> dict:\n",
    "    sentences = sent_tokenize(answer)\n",
    "    return {\n",
    "        'jaccard': calculate_best_similarity(sentences, gold, method=\"jaccard\"),\n",
    "        'levenshtein': calculate_best_similarity(sentences, gold, method=\"levenshtein\"),\n",
    "        'rouge1_f': calculate_best_similarity(sentences, gold, method=\"rouge\", n=1),\n",
    "        'rouge2_f': calculate_best_similarity(sentences, gold, method=\"rouge\", n=2),\n",
    "        'overlap': calculate_best_similarity(sentences, gold, method=\"overlap\"),\n",
    "        'bleu': calculate_best_similarity(sentences, gold, method=\"bleu\"),\n",
    "        'bert_cos': calculate_best_similarity(sentences, gold, method=\"bert\") if use_bert else None,\n",
    "    }\n",
    "\n",
    "def compute_metrics(answer: str, gold: str) -> dict:\n",
    "    a_norm, g_norm = normalize_text(answer), normalize_text(gold)\n",
    "    exact = bool(g_norm) and a_norm == g_norm\n",
    "    substring = bool(g_norm) and g_norm in a_norm\n",
    "    ts_a, ts_g = token_set(answer), token_set(gold)\n",
    "    token_recall = (len(ts_a & ts_g) / len(ts_g)) if ts_g else 0.0\n",
    "    sim = compute_similarity(answer, gold, use_bert=USE_BERT_SIM)\n",
    "    if sim.get('bert_cos') is None:\n",
    "        sim.pop('bert_cos', None)\n",
    "    return {'exact': exact, 'substring': substring, 'token_recall': token_recall, **sim}\n",
    "\n",
    "# -------- Load QA Pairs --------\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_CSV_PATH):\n",
    "    with open(QA_CSV_PATH, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if \"Question\" in row and \"Gold Answer\" in row:\n",
    "                qa_pairs.append((row[\"Question\"].strip(), row[\"Gold Answer\"].strip()))\n",
    "else:\n",
    "    print(\"QA CSV not found. Provide QA_CSV_PATH or create synthetic pairs manually.\")\n",
    "if MAX_Q:\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "print(f\"Loaded {len(qa_pairs)} QA pairs.\")\n",
    "if not qa_pairs:\n",
    "    raise SystemExit(\"No QA data available.\")\n",
    "\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from weaviate.exceptions import WeaviateQueryError\n",
    "\n",
    "async def run_eval(mode, n):\n",
    "    rows, latencies = [], []\n",
    "    for i, (question, gold) in enumerate(tqdm(qa_pairs, total=len(qa_pairs), desc=f\"Eval-{mode}\", unit=\"q\"), start=1):\n",
    "        attempt = 0\n",
    "        answer = None\n",
    "        start_overall = time.perf_counter()\n",
    "        last_error = None\n",
    "        while attempt <= MAX_RETRIES and (time.perf_counter() - start_overall) < PER_QUERY_DEADLINE and not answer:\n",
    "            attempt += 1\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                # output = rag.generate.near_text(\n",
    "                #     query=question,\n",
    "                #     limit=TOP_K,\n",
    "                #     target_vector=\"text_vector\",\n",
    "                #     grouped_task=PROMPT_PREFIX + question[:512],  # concise prompt\n",
    "                #     return_metadata=MetadataQuery(distance=True)\n",
    "                # )\n",
    "                \n",
    "                output = rag.generate.bm25(\n",
    "                    query=question,\n",
    "                    limit=TOP_K,\n",
    "                    grouped_task=PROMPT_PREFIX + question,  # concise prompt\n",
    "                    grouped_properties=[\"text\"],\n",
    "                    return_metadata=MetadataQuery(distance=True)\n",
    "                )\n",
    "                gen = getattr(output, 'generative', None)\n",
    "                if isinstance(gen, dict):\n",
    "                    answer = gen.get('groupedResult') or gen.get('singleResult')\n",
    "                else:\n",
    "                    answer = getattr(gen, 'text', None)\n",
    "                if not answer:\n",
    "                    # fallback: concatenate retrieved texts\n",
    "                    retrieved = []\n",
    "                    for obj in getattr(output, 'objects', [])[:2]:\n",
    "                        try:\n",
    "                            retrieved.append(obj.properties.get('text', '')[:600])\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    answer = (\" \\n---\\n\".join(retrieved) or \"[no answer]\")\n",
    "            except WeaviateQueryError as e:\n",
    "                last_error = e\n",
    "                if attempt <= MAX_RETRIES:\n",
    "                    time.sleep(RETRY_BACKOFF * attempt)\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                if attempt <= MAX_RETRIES:\n",
    "                    time.sleep(RETRY_BACKOFF * attempt)\n",
    "            finally:\n",
    "                latency = time.perf_counter() - t0\n",
    "        total_elapsed = time.perf_counter() - start_overall\n",
    "        if not answer:\n",
    "            answer = f\"[timeout after {total_elapsed:.1f}s attempts={attempt} last_error={last_error}]\"\n",
    "        latencies.append(total_elapsed)\n",
    "        m = compute_metrics(answer, gold)\n",
    "        rows.append({\"question\": question, \"gold\": gold, \"answer\": answer, \"latency_s\": total_elapsed, **m})\n",
    "        if i <= 2:\n",
    "            tqdm.write(f\"Q{i}: {question[:80]}...\")\n",
    "            tqdm.write(\"Answer: \" + answer[:160].replace('\\n',' '))\n",
    "            tqdm.write(\"Gold: \" + gold[:160])\n",
    "            fmt = {k: (f\"{v:.3f}\" if isinstance(v,(int,float)) and not (isinstance(v,float) and math.isnan(v)) else v) for k,v in m.items()}\n",
    "            tqdm.write(f\"Metrics: {fmt} Latency: {total_elapsed*1000:.1f} ms attempts={attempt}\")\n",
    "            tqdm.write('-')\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "    print(f\"\\nAggregate: exact={_avg('exact'):.2%} substring={_avg('substring'):.2%} token_recall={_avg('token_recall'):.2%}\")\n",
    "    for mkey in ['jaccard','levenshtein','rouge1_f','rouge2_f','overlap','bleu','bert_cos']:\n",
    "        if mkey in rows[0]:\n",
    "            print(f\"  {mkey}: {_avg(mkey):.3f}\")\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    p95_lat = sorted(latencies)[int(len(latencies)*0.95)-1] if len(latencies)>1 else latencies[0]\n",
    "    print(f\"Latency: avg={avg_lat*1000:.1f} ms p95={p95_lat*1000:.1f} ms\")\n",
    "    if OUTPUT_CSV_PATH and rows:\n",
    "        os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "        out_file = os.path.join(OUTPUT_CSV_PATH, f\"results_{mode}{n}.csv\")\n",
    "        write_header = not os.path.exists(out_file)\n",
    "        with open(out_file,'a',encoding='utf-8',newline='') as f:\n",
    "            w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "            if write_header: w.writeheader()\n",
    "            w.writerows(rows)\n",
    "        print(\"Saved results to\", out_file)\n",
    "    return rows\n",
    "\n",
    "# Run evaluation\n",
    "eval_results1 = await run_eval(\"light\", 7)\n",
    "print(\"Evaluation complete.\")\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
