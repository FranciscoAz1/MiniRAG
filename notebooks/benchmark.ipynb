{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init device: cuda\n",
      "Loading embedding tokenizer/model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m initialize_pipeline_status()\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rag\n\u001b[1;32m---> 76\u001b[0m rag \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitialize_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG ready. Proceed to Cell 1 to ingest documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\asyncio\\runners.py:191\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug, loop_factory)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Loads embedding model, builds embedding_func, and instantiates a MiniRAG object.\n",
    "# Does NOT ingest documents. Use the next cell to index.\n",
    "\n",
    "import os, torch, sys\n",
    "import minirag\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.llm import ollama\n",
    "from minirag import MiniRAG\n",
    "from tqdm.auto import tqdm\n",
    "import asyncio\n",
    "\n",
    "\n",
    "sys.path.append(r'c:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\Chatbot\\lightrag')\n",
    "from lightrag import LightRAG\n",
    "from lightrag.llm.ollama import ollama_model_complete\n",
    "\n",
    "# Core configuration (shared by later cells)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\"\n",
    "LLM_MODEL_NAME = \"qwen2m:latest\"  # set to None if no local Ollama model\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Init device:\", device)\n",
    "\n",
    "print(\"Loading embedding tokenizer/model...\")\n",
    "_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "_embed_model = AutoModel.from_pretrained(EMBEDDING_MODEL).to(device)\n",
    "_embed_model.eval()\n",
    "\n",
    "async def _embed_batch(texts: list[str]):\n",
    "    return await hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model)\n",
    "\n",
    "async def _embed_dispatch(input_text):\n",
    "    if isinstance(input_text, str):\n",
    "        return (await _embed_batch([input_text]))[0]\n",
    "\n",
    "\n",
    "        \n",
    "    if isinstance(input_text, (list, tuple)) and all(isinstance(t, str) for t in input_text):\n",
    "        return await _embed_batch(list(input_text))\n",
    "    raise TypeError(f\"Unsupported input type for embedding_func: {type(input_text)}\")\n",
    "\n",
    "_embedding_func = EmbeddingFunc(\n",
    "    embedding_dim=_embed_model.config.hidden_size,\n",
    "    max_token_size=_tokenizer.model_max_length,\n",
    "    func=_embed_dispatch,\n",
    ")\n",
    "\n",
    "# rag = minirag.MiniRAG(\n",
    "#     working_dir=WORKING_DIR,\n",
    "#     llm_model_func=ollama.ollama_model_complete if LLM_MODEL_NAME else None,\n",
    "#     llm_model_name=LLM_MODEL_NAME,\n",
    "#     embedding_func=_embedding_func,\n",
    "#     log_level=LOG_LEVEL,\n",
    "#     suppress_httpx_logging=True\n",
    "# )\n",
    "async def initialize_rag():\n",
    "    rag = LightRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        llm_model_func=ollama.ollama_model_complete if LLM_MODEL_NAME else None,\n",
    "        llm_model_name=LLM_MODEL_NAME,\n",
    "        embedding_func=_embedding_func,\n",
    "        log_level=LOG_LEVEL,\n",
    "    )\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "    return rag\n",
    "\n",
    "rag = asyncio.run(initialize_rag())\n",
    "\n",
    "print(\"RAG ready. Proceed to Cell 1 to ingest documents.\")\n",
    "import os, time, json, random, gc, asyncio\n",
    "from pathlib import Path\n",
    "import psutil, torch\n",
    "import minirag\n",
    "from minirag.llm import ollama\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "\n",
    "\"\"\"\n",
    "Cell 1: Document Ingestion / Indexing Only\n",
    "-----------------------------------------\n",
    "Run this FIRST. It builds the MiniRAG index (vectors + KG) from source documents.\n",
    "No query / evaluation logic here.\n",
    "\"\"\"\n",
    "# ---------------- User Config ----------------\n",
    "RANDOM_SEED = 42\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\dataset\\\\LiHua-World\\\\data\\\\\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\"\n",
    "LLM_MODEL_NAME = \"qwen2m:latest\"  # set to None if no LLM available\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "assert 'rag' in globals(), \"rag not found. Run Cell 1 first.\"\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents():\n",
    "    global rag  # expose for cell 2\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing with ainsert() ...\")\n",
    "    t1 = time.perf_counter()\n",
    "    for d in tqdm(docs, desc=\"Indexing docs\", unit=\"doc\"):\n",
    "        if 'text' not in d or not d['text'].strip():\n",
    "            print(f\"Skipping empty doc {d.get('id')}\")\n",
    "        try:\n",
    "            rag.ainsert(input=d['text'], ids=d['id'], file_paths=d['source_path'])\n",
    "        except Exception as batch_e:\n",
    "            print(f\"Batch insert failed: {batch_e}; fallback per-doc\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)} docs in {dur:.2f}s ({len(texts)/dur:.2f} docs/s)\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Δ {end_mem - start_mem:.2f} MB)\")\n",
    "\n",
    "await index_documents()\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Query & QA Evaluation\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Supports:\n",
    "#  - Loading LiHua-World QA pairs from query_set.csv\n",
    "#  - Evaluating answer quality with simple + lexical + semantic metrics\n",
    "#  - Optional CSV logging\n",
    "\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from pathlib import Path\n",
    "# from minirag import QueryParam\n",
    "from lightrag import QueryParam\n",
    "from minirag.utils import calculate_similarity  # legacy helper (returns indices) – not used now\n",
    "\n",
    "# Extra metric libs (lazy loads handled in compute_similarity)\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# -------- Configuration --------\n",
    "QA_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\dataset\\LiHua-World\\qa\\query_set.csv\"\n",
    "OUTPUT_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\"  # set to None to skip saving\n",
    "QUERY_MODE = \"naive\"      # mini | light | naive | doc | meta | bm25\n",
    "TOP_K = 5\n",
    "MAX_Q = None             # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True       # toggle semantic similarity (slower)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# -------- Metrics Helpers --------\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "\n",
    "# lazy globals\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH_FN = SmoothingFunction().method1\n",
    "\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return TOKEN_SPLIT_RE.sub(\" \", s.lower()).strip()\n",
    "\n",
    "\n",
    "def token_set(s: str) -> set[str]:\n",
    "    return {t for t in normalize_text(s).split() if t}\n",
    "\n",
    "_BERT_MODEL = None\n",
    "_ROUGE = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def calculate_best_similarity(sentences: list[str], target: str, method=\"levenshtein\", n=1):\n",
    "    \"\"\"\n",
    "    Returns the highest similarity score (float) between any sentence in `sentences` and `target`.\n",
    "    Methods: jaccard | levenshtein | rouge | bert | overlap | bleu\n",
    "    For rouge, n=1 or 2 selects rouge-1 or rouge-2 F.\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    tgt_tokens = target.lower().split()\n",
    "    scores = []\n",
    "\n",
    "    if method == \"jaccard\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_tokens = set(s.lower().split())\n",
    "            inter = set(s_tokens).intersection(set(tgt_set))\n",
    "            union = set(s_tokens).union(set(tgt_set))\n",
    "            scores.append(len(inter) / len(union) if union else 0.0)\n",
    "\n",
    "    elif method == \"levenshtein\":\n",
    "        tgt_len = max(len(tgt_tokens), 1)\n",
    "        for s in sentences:\n",
    "            dist = edit_distance(tgt_tokens, s.lower().split())\n",
    "            norm = max(tgt_len, len(s.split()))\n",
    "            scores.append(1 - dist / norm if norm else 0.0)\n",
    "\n",
    "    elif method == \"rouge\":\n",
    "        global _ROUGE\n",
    "        if _ROUGE is None:\n",
    "            _ROUGE = Rouge()\n",
    "        key = f\"rouge-{n}\"\n",
    "        for s in sentences:\n",
    "            r = _ROUGE.get_scores(s, target)\n",
    "            scores.append(r[0].get(key, {}).get(\"f\", 0.0))\n",
    "\n",
    "    elif method == \"bert\":\n",
    "        global _BERT_MODEL\n",
    "        if _BERT_MODEL is None:\n",
    "            _BERT_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = _BERT_MODEL.encode(sentences + [target], show_progress_bar=False)\n",
    "        tgt_vec = embeddings[-1]\n",
    "        tgt_norm = np.linalg.norm(tgt_vec)\n",
    "        for i in range(len(sentences)):\n",
    "            v = embeddings[i]\n",
    "            denom = (np.linalg.norm(v) * tgt_norm)\n",
    "            scores.append(float(np.dot(v, tgt_vec) / denom) if denom else 0.0)\n",
    "\n",
    "    elif method == \"overlap\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_set = set(s.lower().split())\n",
    "            inter = s_set & tgt_set\n",
    "            denom = min(len(s_set), len(tgt_set))\n",
    "            scores.append(len(inter) / denom if denom else 0.0)\n",
    "\n",
    "    elif method == \"bleu\":\n",
    "        tgt_bleu = word_tokenize(target.lower())\n",
    "        for s in sentences:\n",
    "            s_bleu = word_tokenize(s.lower())\n",
    "            scores.append(sentence_bleu([tgt_bleu], s_bleu, smoothing_function=_SMOOTH))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method.\")\n",
    "\n",
    "    return max(scores) if scores else 0.0\n",
    "\n",
    "def compute_similarity(answer: str, gold: str, use_bert: bool = True) -> dict:\n",
    "    \"\"\"Compute a bundle of similarity scores between answer and gold.\n",
    "\n",
    "    Returns keys:\n",
    "      jaccard, levenshtein, rouge1_f, rouge2_f, overlap, bleu, bert_cos (optional)\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(answer)\n",
    "    jaccard = calculate_best_similarity(sentences, gold, method=\"jaccard\")\n",
    "    levenshtein = calculate_best_similarity(sentences, gold, method=\"levenshtein\")\n",
    "    rouge1_f = calculate_best_similarity(sentences, gold, method=\"rouge\", n=1)\n",
    "    rouge2_f = calculate_best_similarity(sentences, gold, method=\"rouge\", n=2)\n",
    "    overlap = calculate_best_similarity(sentences, gold, method=\"overlap\")\n",
    "    bleu = calculate_best_similarity(sentences, gold, method=\"bleu\")\n",
    "    bert_cos = calculate_best_similarity(sentences, gold, method=\"bert\") if use_bert else None\n",
    "\n",
    "    result = {\n",
    "        'jaccard': jaccard,\n",
    "        'levenshtein': levenshtein,\n",
    "        'rouge1_f': rouge1_f,\n",
    "        'rouge2_f': rouge2_f,\n",
    "        'overlap': overlap,\n",
    "        'bleu': bleu,\n",
    "    }\n",
    "    if bert_cos is not None:\n",
    "        result['bert_cos'] = bert_cos\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics(answer: str, gold: str) -> dict:\n",
    "    # Basic lexical metrics\n",
    "    a_norm, g_norm = normalize_text(answer), normalize_text(gold)\n",
    "    exact = bool(g_norm) and a_norm == g_norm\n",
    "    substring = bool(g_norm) and g_norm in a_norm\n",
    "    ts_a, ts_g = token_set(answer), token_set(gold)\n",
    "    token_recall = (len(ts_a & ts_g) / len(ts_g)) if ts_g else 0.0\n",
    "\n",
    "    sim_bundle = compute_similarity(answer, gold, use_bert=USE_BERT_SIM)\n",
    "\n",
    "    return {\n",
    "        'exact': exact,\n",
    "        'substring': substring,\n",
    "        'token_recall': token_recall,\n",
    "        **sim_bundle,\n",
    "    }\n",
    "\n",
    "# -------- Load QA Pairs --------\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_CSV_PATH):\n",
    "    with open(QA_CSV_PATH, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if \"Question\" in row and \"Gold Answer\" in row:\n",
    "                qa_pairs.append((row[\"Question\"].strip(), row[\"Gold Answer\"].strip()))\n",
    "else:\n",
    "    print(\"QA CSV not found. Provide QA_CSV_PATH or create synthetic pairs manually.\")\n",
    "\n",
    "if MAX_Q:\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} QA pairs.\")\n",
    "if not qa_pairs:\n",
    "    raise SystemExit(\"No QA data available.\")\n",
    "\n",
    "assert 'rag' in globals(), \"rag not found. Run Cell 1 first.\"\n",
    "\n",
    "# -------- Evaluation --------\n",
    "async def run_eval(mode, n):\n",
    "    qp = QueryParam(mode=mode, top_k=TOP_K)\n",
    "    rows = []\n",
    "    latencies = []\n",
    "\n",
    "    for i, (question, gold) in enumerate(tqdm(qa_pairs, total=len(qa_pairs), desc=f\"Eval-{mode}\", unit=\"q\"), start=1):\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            answer = await rag.aquery(question, param=qp)\n",
    "        except TypeError:\n",
    "            answer = await rag.aquery(question)\n",
    "        latency = time.perf_counter() - t0\n",
    "        latencies.append(latency)\n",
    "\n",
    "        m = compute_metrics(answer, gold)\n",
    "        row = {\"question\": question, \"gold\": gold, \"answer\": answer, \"latency_s\": latency, **m}\n",
    "        rows.append(row)\n",
    "\n",
    "        if i <= 0:\n",
    "            # use tqdm.write to avoid breaking the progress bar formatting\n",
    "            tqdm.write(f\"Q{i}: {question[:80]}...\")\n",
    "            tqdm.write(\"Answer: \" + answer[:180].replace(\"\\n\", \" \"))\n",
    "            tqdm.write(\"Gold: \" + gold[:180])\n",
    "            # Format numeric (non-NaN) metrics to 3 decimals\n",
    "            fmt_metrics = {\n",
    "                k: (f\"{v:.3f}\" if isinstance(v, (int, float)) and not (isinstance(v, float) and math.isnan(v)) else v)\n",
    "                for k, v in m.items()\n",
    "            }\n",
    "            tqdm.write(f\"Metrics: {fmt_metrics} Latency: {latency*1000:.1f} ms\")\n",
    "            tqdm.write('-')\n",
    "    # Aggregates\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "\n",
    "    exact_rate = _avg('exact')\n",
    "    substr_rate = _avg('substring')\n",
    "    avg_token_recall = _avg('token_recall')\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    p95_lat = sorted(latencies)[int(len(latencies)*0.95)-1] if len(latencies) > 1 else latencies[0]\n",
    "\n",
    "    print(f\"\\nAggregate: exact={exact_rate:.2%} substring={substr_rate:.2%} token_recall={avg_token_recall:.2%}\")\n",
    "    for mkey in ['jaccard','levenshtein','rouge1_f','rouge2_f','overlap','bleu','bert_cos']:\n",
    "        if mkey in rows[0]:\n",
    "            print(f\"  {mkey}: {_avg(mkey):.3f}\")\n",
    "    print(f\"Latency: avg={avg_lat*1000:.1f} ms p95={p95_lat*1000:.1f} ms\")\n",
    "\n",
    "    os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "    OUTPUT_CSV = os.path.join(OUTPUT_CSV_PATH, f\"results_{mode}{n}.csv\")\n",
    "    # Optional CSV\n",
    "    if OUTPUT_CSV and rows:\n",
    "        write_header = not os.path.exists(OUTPUT_CSV)\n",
    "        with open(OUTPUT_CSV, 'a', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "            if write_header: writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        print(f\"Saved results to {OUTPUT_CSV}\")\n",
    "    return rows\n",
    "\n",
    "# Run evaluation\n",
    "eval_results1 = await run_eval(\"light\", 5)\n",
    "eval_results2 = await run_eval(\"mini\", 5)\n",
    "eval_results3 = await run_eval(\"naive\", 5)\n",
    "eval_results4 = await run_eval(\"bypass\", 5)\n",
    "print(\"Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
