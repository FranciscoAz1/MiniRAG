{
  "doc-3f0a036d7aec87b9bf4e52a3a2633b54": {
    "content": "A Semantic Search System for the Supremo Tribunal de\nJustic ¸a\nRui Filipe Coimbra Pereira de Melo\nThesis to obtain the Master of Science Degree in\nComputer Science and Engineering\nSupervisors: Prof. Pedro Alexandre Sim ˜oes dos Santos\nProf. Jo ˜ao Miguel De Sousa de Assis Dias\nExamination Committee\nChairperson: Prof. Maria Lu ´ısa Torres Ribeiro Marques da Silva Coheur\nSupervisor: Prof. Pedro Alexandre Sim ˜oes dos Santos\nMember of the Committee: Prof. Jos ´e Lu´ıs Brinquete Borbinha\nJune 2023\n\nAcknowledgments\nI want to express my sincerest gratitude and appreciation to my supervisors, Professor Pedro Alexan-\ndre Santos and Professor Jo ˜ao Dias, who guided me throughout the entirety of this work and relentlessly\noffered much-needed advice, sound counsel and honest feedback. They were infallible in being reliable\nand continuously kept in touch with the progress of this thesis. They always provided input on the many\nupdates, advancements, and setbacks that occurred, and I am genuinely grateful.\nOn the same note, I would like to thank all Project IRIS members I co-operated with in the past\nmonths. The resonating experience of partaking in a more significant project with brilliant minds will\nshape my future ventures.\nLastly, there is no way I could wholly express in words the unwavering support and unconditional love\nI received from my family. To my mother and father, from the bottom of my heart. . . thank you.\ni\n\nAbstract\nMany information retrieval systems use lexical approaches to retrieve information. Such approaches\nhave multiple limitations, and these constraints are exacerbated when tied to specific domains, such as\nthe legal one. Large language models, such as BERT, deeply understand a language and may overcome\nthe limitations of older methodologies, such as BM25.\nThis work investigated and developed a prototype of a Semantic Search System to assist the Supremo\nTribunal de Justic ¸a (Portuguese Supreme Court of Justice) in its decision-making process.\nWe built a Semantic Search System that uses specially trained BERT models (Legal-BERTimbau\nvariants) and Hybrid Search Systems that incorporate both lexical and semantic techniques by com-\nbining the capabilities of BM25 and the potential of Legal-BERTimbau. In this context, we obtained a\n335% increase on the discovery metric when compared to BM25 for the first query result. This work also\nprovides information on the most relevant techniques for training a Large Language Model adapted to\nPortuguese jurisprudence and introduces a new technique of Metadata Knowledge Distillation.\nKeywords\nArtificial Intelligence; BERT; Information Retrieval; Natural Language Processing; Jurisprudence; SBERT\niii\n\nResumo\nOs sistemas de recuperac ¸ ˜ao de informac ¸ ˜ao utilizam frequentemente abordagens lexicais para recuperar\ninformac ¸ ˜ao. Tais abordagens t ˆem m ´ultiplas limitac ¸ ˜oes, e estas limitac ¸ ˜oes s ˜ao agravadas quando ligadas\na dom ´ınios espec ´ıficos, tais como o legal. Large Language Models, como o BERT, compreendem\nprofundamente uma linguagem e podem ultrapassar as limitac ¸ ˜oes de metodologias mais antigas, como\no BM25.\nEste trabalho investigou e desenvolveu um prot ´otipo de um Sistema de Busca Sem ˆantica para assi-\nstir o Supremo Tribunal de Justic ¸a portugu ˆes no seu processo de tomada de decis ˜ao.\nConstru ´ımos um Sistema de Pesquisa Sem ˆantica que utiliza modelos BERT especialmente treina-\ndos (variantes Legal-BERTimbau) e Sistemas de Pesquisa h ´ıbrida que incorporam tanto t ´ecnicas lex-\nicais como sem ˆanticas, combinando as capacidades da BM25 e o potencial da Legal-BERTimbau.\nReportamos um aumento de desempenho de 335% na recuperac ¸ ˜ao de passagens relevantes quando\ncomparado com BM25 para o resultado da primeira consulta.\nEste trabalho tamb ´em fornece informac ¸ ˜oes sobre as t ´ecnicas mais relevantes para a formac ¸ ˜ao de\num Modelo de Grandes L ´ınguas adaptado `a jurisprud ˆencia portuguesa e introduz uma nova t ´ecnica,\nMetadata Knowledge Distillation.\nPalavras Chave\nBERT; Intelig ˆencia Artificial; Jurisprud ˆencia; Processamento de Linguagem Natural; Recuperac ¸ ˜ao de\nInformac ¸ ˜ao; SBERT\nv\n\nContents\n1 Introduction 1\n1.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2 Background Review 5\n2.1 Lexical approaches for Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.1 Term Frequency Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.2 Best Matching Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.3 Distance metrics for lexical approaches . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Neural Networks for Semantic Information Retrieval . . . . . . . . . . . . . . . . . . . . . 9\n2.2.1 Word and Sentence Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2.2 Word2Vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2.3 GloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.4 Recurrent Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2.5 Long Short-Term Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.6 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3 Semantic Search Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4 Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.4.1 LexRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3 State of the Art 25\n3.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.1.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.1.2 Fine-tuning on Downstream Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.2 BERTimbau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3 Deeper Text Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.4 Legal Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.5 NLP Applied To Portuguese Consumer Law . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nvii\n3.6 Albertina PT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4 Semantic Search System 43\n4.1 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.1.1 ElasticSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.2 The Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.2.1 Data Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.3.1 Purely Semantic Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3.2 Lexical-First Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3.3 Lexical + Semantic Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n5 Legal Language Model 53\n5.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n5.1.1 Masked Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n5.1.2 Transformer-based Sequential Denoising Auto-Encoder . . . . . . . . . . . . . . . 57\n5.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.2.1 Semantic Textual Similarity Custom Dataset . . . . . . . . . . . . . . . . . . . . . . 59\n5.3 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.4 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n5.5 Multilingual Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n5.6 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.7 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6 System Evaluation 65\n6.1 Language Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.1.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.1.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.2 Search System Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2.1 Automatic Query Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n7 Conclusion 79\n7.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.1.1 Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2.1 Albertina . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2.2 Dataset Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\nviii\n7.2.3 Architecture Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\nBibliography 85\nix\nx\nList of Figures\n2.1 Example of Word Embeddings in a 3D vector space . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Word2Vec with Common Bag Of Words (CBOW) model based on a one word context. . . 11\n2.3 Word2Vec with CBOW model based on multiple words context. . . . . . . . . . . . . . . . 11\n2.4 Word2Vec with Skip-Gram model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.5 Recurrent Network Fully Connected . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.6 RNN Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.7 LSTM Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.8 Transformer Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.9 Scaled Dot-Product Attention and Multi-Head Attention . . . . . . . . . . . . . . . . . . . . 19\n2.10 Vector space with a query embedding and multiple sentence embeddings . . . . . . . . . 20\n2.11 Bi-Encoder and Cross-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.12 LexRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.1 BERT input representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.2 Data Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.3 Masked Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.4 Transformer-based Sequential Denoising Auto-Encoder (TSDAE) Architecture . . . . . . . 31\n3.5 T5 diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.6 GenQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.7 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.8 Fine-Tuning SBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.9 Multilingual Knowledge Distillation Process . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n4.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.2 Lexical-First Search System Retrieval Method . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3 Lexical + Semantic Search System Retrieval Method . . . . . . . . . . . . . . . . . . . . . 51\n5.1 Training Tasks Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\nxi\n5.2 Masked Language Modeling (MLM) Training Loss . . . . . . . . . . . . . . . . . . . . . . . 57\n5.3 TSDAE Training Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n5.4 Metadata Knowledge Distillation Ideology . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.5 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.1 Evaluation Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2 Search System Evaluation – Search metric - Models V0 . . . . . . . . . . . . . . . . . . . 73\n6.3 Search System Evaluation – Search metric - Models V1 . . . . . . . . . . . . . . . . . . . 74\n6.4 Search System Evaluation - Discovery metric - Models V0 . . . . . . . . . . . . . . . . . . 75\n6.5 Search System Evaluation - Discovery metric - Models V1 . . . . . . . . . . . . . . . . . . 76\nxii\nList of Tables\n3.1 SBERT Spearman correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.2 Sentence-BERT (SBERT) evaluation on the Semantic Textual Similarity (STS) benchmark 36\n3.3 BERTimbau variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.4 COLIEE 2021 - Task 1 results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.5 COLIEE 2021 - Task 3 results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.1 Legal-BERTimbau variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.1 MLM average loss for legal documents in the test set . . . . . . . . . . . . . . . . . . . . . 68\n6.2 STS evaluation on Portuguese datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.3 Search System Evaluation – Search metric – Best Model . . . . . . . . . . . . . . . . . . 77\n6.4 Search System Evaluation – Discovery metric – Best Model . . . . . . . . . . . . . . . . . 77\nxiii\nxiv\nAcronyms\nAI Artificial Intelligence\nBERT Bidirectional Encoder Representations from Transformers\nBM25 Okapi BM25\nBrWaC Brazilian Web as Corpus\nCBOW Common Bag Of Words\nCOLIEE Competition on Legal Information Extraction and Entailment\nDA Domain Adaptation\nDH Distributional Hypothesis\nFCUL Faculdade de Ci ˆencias da Universidade de Lisboa\nFEUP Faculdadede Engenharia da Universidade do Porto\nGPL Generative Pseudo Labeling\nGLM Generative Language Model\nGloVe Global Vectors for Word Representation\nIDF Inverse Document Frequency\nIR Information Retrieval\nLSTM Long Short-Term Memory\nLeSSE Legal Semantic Search Engine\nMetaKD Metadata Knowledge Distillation\nMLM Masked Language Modeling\nMKD Multilingual Knowledge Distillation\nML Machine Learning\nMNR Multiple Negatives Ranking\nxv\nNLI Natural Language Inference\nNLP Natural Language Processing\nNL Natural Language\nNN Neural Network\nNSP Next Sentence Prediction\nQA Question and Answer\nRNN Recurrent Neural Network\nSBERT Sentence-BERT\nSNLI Stanford Natural Language Inference\nSTJ Supremo Tribunal de Justic ¸a\nSTS Semantic Textual Similarity\nT5 Text-to-Text Transfer Transformer\nTF-IDF Term Frequency-Inverse Document Frequency\nTF Term Frequency\nTSDAE Transformer-based Sequential Denoising Auto-Encoder\nUKP Ubiquitous Knowledge Processing\nxvi\n1\nIntroduction\nContents\n1.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1\n2\nSupremo Tribunal de Justic ¸a (STJ) serves as Portugal’s highest judiciary court, also known as the\nSupreme Court of Justice. It plays a crucial role in making well-informed, lawful, and ethical decisions\nthat have a profound impact on the specific case at hand and future cases. To arrive at a decision, a\ncomprehensive examination of the relevant jurisprudence is indispensable. This exhaustive consultation\nserves as the bedrock for a thoughtful and principled judgment.\nWhen a judge is tasked with formulating a decision, whether it involves researching specific legis-\nlation or referring to precedents from similar cases, the process of locating the necessary information\nis far from trivial, speedy, or efficient. Effectively managing and accessing such vast volumes of infor-\nmation necessitates the utilization of a robust Information Retrieval (IR) system. Such a system is an\nindispensable tool in ensuring efficient access to the required legal resources.\nThe court decision process could be improved based on a search system that would help retrieve\nthe desired information and identify that information source. Even though traditional lexical techniques\ncould be implemented to solve this need, they might raise other issues. Lexical techniques work by\nsearching for specific word matches from a given query. Using lexical techniques in a search system\ncan only retrieve so much information that might include only some cases or sections of legislation that\nare relevant to that specific instance. A search system that utilizes semantic techniques, that work by\ntrying to understand the semantic meaning of a query, and finding document passages closer in meaning\nto the query, is essential to reach a broader range of relevant documents.\nA country’s judiciary system should resist bad decisions based on incomplete or inaccurate informa-\ntion. The occurrence of such events opens the door to unjust precedents in future cases, thus leading\nto imbalances in the overall decision-making, which ultimately leads to the weakening and destabiliza-\ntion of the justice system. Establishing a reliable legal search system is necessary since it promotes\nconsistency in applying the law.\n1.1 Objectives\nThe research presented in this thesis is a contribution to the IRIS project developed by INESC-ID Lisboa\nfor STJ. Project IRIS aims to develop summarization approaches for court decisions and create a\nrepresentation able to be browsed in a way that is helpful in the court decision process.\nAs mentioned beforehand, retrieving desired information might be more complex than one wishes.\nThe first challenge someone faces when procuring information, is in the choosing of (adequate) words\nto input into the search query. Even though judges might use specific terminology terms, the system\nshould be resilient enough to access information based on Natural Language (NL). One will always find\nadversity when constructing a sophisticated and appropriate query that expresses the correct need for\nspecific information.\n3\nHaving these motivations well-defined, it is the aim of this thesis to develop a reliable search system\nthat uses semantic strategies for STJ that will help in the court decision process.\n1.2 Contributions\nThrough the making of this work, three separate types of assets were produced, and promptly deliv-\nered available to appropriate recipients, under the form of contributions. Firstly, we developed multiple\ndatasets from the various Portuguese legal documents available and published them on the Internet.\nThese datasets are publicly available in the HuggingFace platform, allowing them to be easily used\nin other scenarios. Secondly, numerous generic and fine-tuned for Semantic Textual Similarity (STS)\nlanguage models were developed and made available on the same platform. All developed large lan-\nguage models were successfully adapted to the Portuguese legal domain. Furthermore, we developed\na search system with the intent of helping judges in the court decision process. Finally, from the re-\nsults of this work, two papers were written. The first one, Exploring Embeddings Models for Portuguese\nSupreme Court Judgments Summarization, is currently on the verge of being submitted. The second\npaper, Semantic Search System for Supremo Tribunal de Justic ¸a, was submitted to the EPIA Conference\non Artificial Intelligence and it is currently under review.\n1.3 Thesis Outline\nChapter 2 provides essential elements for understanding subsequent content, while Chapter 3 show-\ncases state-of-the-art models and relevant scientific papers used in the final solution.\nIn Chapter 4 we enumerate the implementation requirements and explain the search system architec-\nture. Next, Chapter 5 explains the development of the language model used in our system. Furthermore,\nin Chapter 6, we analyse the performance of our search system and developed language models.\nFinally, Chapter 7 provides an overall retrospective pondering of the developed work and the achieved\nmilestones and gives a quick insight into future work.\n4\n2\nBackground Review\nContents\n2.1 Lexical approaches for Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Neural Networks for Semantic Information Retrieval . . . . . . . . . . . . . . . . . . . 9\n2.3 Semantic Search Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4 Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5\n6\nIn this chapter, we will present and explain some core aspects necessary to fully understand the ex-\nposed content in the following chapters, such as state-of-the-art techniques and the developed solution.\nIn the last few years, Artificial Intelligence (AI) has played an increasingly important role in almost\nevery aspect of modern society. Subsequently, Machine Learning (ML) has found itself as an alternative\nto traditional solutions in multiple fields. [18]\nThere are two main types of approaches regarding IR from documents. The classical approaches\nare based on lexical search, whereas the more recent approaches are based on semantic search.\n2.1 Lexical approaches for Information Retrieval\nTraditionally, when a system searches for document content related to a query, it uses techniques that\nsearch for documents or entries that present those exact query words. This process is called lexi-\ncal search. The basic algorithm for lexical search is Okapi BM25 (BM25) which incorporates Term\nFrequency-Inverse Document Frequency (TF-IDF). BM25 is a widely used algorithm in information re-\ntrieval and has been shown to perform well in various benchmarks and evaluations.\n2.1.1 Term Frequency Algorithm\nTF-IDF [20] is a ranking function for document search and information retrieval. It evaluates how relevant\na term tis relative to a document dbelonging to a set of documents Dwhile being based on a group of\nNdocuments. TF-IDF is defined as follows:\nTF-IDF (t, d, D ) =TF(t, d)×IDF(t, D) (2.1)\nTF-IDF is thus the product of two factors, Term Frequency (TF), and Inverse Document Frequency\n(IDF). TF represents how often a term appears in a specific document. It is calculated as follows:\nTF(t, d) := log(1 + freq(t, d)) (2.2)\nwhere\nfreq(t, d) :=number of times tappeares in d\nnumber of terms in d(2.3)\n7\nIDF represents the rarity of a term in the entire group of documents, where values near 0 show that\nterms are very common and values near 1 show that terms are rarer. IDF is defined as follows:\nIDF(t, D) := log\u0012N\ncount (d∈D:t∈d)\u0013\n(2.4)\n2.1.2 Best Matching Algorithm\nBM25 [34], in which BM stands for Best Matching, is a ranking function usually used in search engines\nlike Elasticsearch (explored later in the document) to estimate the relevance of documents given a query.\nBM25 relies on a bag-of-words logic, by which it ranks a collection of documents based on the query\nterms that appear in which document, independently of the position in that same file. The equation is as\nfollows:\nscore (d, Q) =nX\ni=1IDF (qi)·f(qi, d)·(ki+ 1)\nf(qi, d) +k1·(1−b+b·|d|\navgdl)(2.5)\nwhere Qrepresents the Query given (with q1, ..., q nbeing the keywords), IDF is defined above in (2.4)\nanddrepresents the Document. f(qi, d)represents the qifrequency in the document dthat has a |d|\nlength. Both k1andbterms are free terms that are used to optimize BM25 function.\n2.1.3 Distance metrics for lexical approaches\nA traditional way to search for similar documents is through the Jaccard Similarity measure between\ntwo sets. It evaluates the amount of shared information or content between both sets. In practice, it\nrepresents the extent of the intersection divided by the size of their union. It is defined as follows:\nSim(C1, C2) :=|C1∩C2|\n|C1∪C2|(2.6)\nwhere C1andC2represent two sets.\nThe drawbacks of lexical searches are evident when some important passages or documents are not\nretrieved because some words need to be present. For example: “I walked through the public garden. I\nenjoyed it.” We can easily understand that “it” was an underlying meaning of “public garden”. A similar\nproblem occur when the query uses a synonym to an expression existing in a document and not the\nexact word. That document will not be returned. To tackle these downsides of lexical approaches, we\n8\nexplore a more advance and robust approach that aims to retrieve information based on the intrinsic\nmeaning and not the literal word matches themselves.\n2.2 Neural Networks for Semantic Information Retrieval\nSemantic search is a technique that, instead of searching for specific words, aims to search for the\ncontextual meaning of those words. Unlike lexical search, which only searches for literal matches of\nwords, semantic search tries to understand the user’s intention and the overall sentence’s meaning.\n2.2.1 Word and Sentence Embeddings\nWord Embedding is a mathematical technique and implementation of the Distributional Hypothesis (DH).\nDH defends that if words appear in similar contexts, they must have similar meanings.\nWord embeddings provide a verification of which words are similar to each other based on an initial\ncorpus of sentences. Each word embedding represents a word in a multidimensional space. With\nmultiple multidimensional word representations, verifying which words are related to each is possible\nbased on the distance between words.\nFigure 2.1: Example of Word Embeddings in a 3D vector space\nThe distance can be calculated through the Cosine Similarity, which measures the cosine of the\nangle, θ, between two vector embeddings( AandB):\n9\nCosSim (A, B) :=cos(θ)\n=A·B\n||A||||B||\n=Pn\ni=1Ai·BipPn\ni=1A2\ni·pPn\ni=1B2\ni(2.7)\nSince embedding spaces are linear systems, it is possible to perform arithmetic operations in the em-\nbedding space. If we get a word “England” and subtract the word “London” and add the word “Portugal”,\nwe should be able to get the word “Lisbon”.\nWord (“England ”)−Word (“London ”) +Word (“Portugal ”) = Word (“Lisbon ”) (2.8)\nThere are different implementations of word embeddings, such as Word2Vec proposed by a Google\nTeam led by Tomas Mikolov in 2013 [24] or Global Vectors for Word Representation (GloVe) [28] devel-\noped at Stanford in 2014. Both make use of neural networks to create their models through unsupervised\ntraining. Both implementations have their unique advantages, but what distinguishes them mainly are\ntheir fundamentals of solution formulation.\n2.2.2 Word2Vec\nWord2Vec is a two-layer neural network proposed in 2013 by Tomas Mikolov et al., and it revolves around\nthe idea that words that appear close to one another have similar meanings.\nWord2Vec algorithm uses one of two methods that utilise neural networks [36]:\n• Common Bag Of Words (CBOW);\n• Skip-Gram.\nCBOW model, through the representation of context using the surrounding words as an input, aims\nto predict the corresponding word. Considering the example: “I walked through the public garden.”, we\ncan input the phrase without the word “public” in the Neural Network. By using this single input, it aims\nto predict the word “public” just by interpreting its surroundings.\nThe architecture of the CBOW model is defined in Figure 2.2.\nIn this example, we provide one word X={x1, ..., x V}in the form of a one-hot encoding with size\nVas the context and the network aims to predict another word. Vrepresents the vocabulary size. The\n10\nFigure 2.2: Word2Vec with CBOW model based on a one word context. Figure based on [36]\nhidden layer is composed by Nneurons, and the output layer is a vector of size Vthat represents the\npredicted word. The weight matrix Whas size V×N. The choice of hidden layer size in a Word2Vec\nmodel depends on various factors such as the size of the vocabulary, the complexity of the task, and the\navailable computational resources. Similarly to the input, the yielded word Yfrom the output layer is a\none-hot encoded vector of size V.\nIf the objective is to use multiple words for the context to predict a word, the neural network would\nneed to increase the input layer, as suggested in Figure 2.3.\nFigure 2.3: Word2Vec with CBOW model based on multiple words context. Figure based on [36]\n11\nThe Skip-Gram model’s function is to use a word as an input and generate the context of that same\nword. The architecture is shown in Figure 2.4.\nFigure 2.4: Word2Vec with Skip-Gram model. Figure based on [36]\nWord2Vec relies on local information, meaning words are only affected by words in the surroundings.\nThe technique can not associate a word as a stop-word or a word that has meaning in a phrase. Stop\nwords are everyday words that have no complex meaning. For instance, in the sentence: “The cat sat\non the mat” , Word2Vec cannot identify if the word “The” is a particular context of the words “cat” and\n“mat” or if it is just a stop-word. Nevertheless, this technique performs very well in analogy tasks.\n2.2.3 GloVe\nGloVe is an unsupervised learning algorithm developed by Stanford University researchers aiming to\nrepresent words as vectors. It focuses on the idea that it can derive semantic relationships between\nwords based on a co-occurrence matrix. Each value in the co-occurrence matrix, M, represents a pair\nof words occurring together. For example, a co-occurrence matrix entry Mijrepresents the probability\nof a word jappearing next to the word i.\n12\nThe probability of a word jco-occurring with a word iis the ratio of the number of times word j\nappears in the context of word ito the number of times any word appears in the context of word i. It is\ndefined as follows:\nPij:=P(j|i)\n=MijP\nk∈contextMik(2.9)\nGloVe’s loss function is formally defined as follows:\nJ=VX\ni,j=1f(Xij)(WT\ni˙˜Wj+bi+˜bj−log(Xij))2(2.10)\n, where f(Xij)represents the weighting function, WT\ni˜Wjis the dot product of the input vectors, bi+\n˜bjrepresents the bias, which aims to mitigate the impact of common words and stop-words, and V\nrepresents the size of the vocabulary.\nThe original paper demonstrated the co-occurrence matrix produced with probabilities for targets\nword iceandsteam , as shown in Table 2.2.3. The word iceco-occurs more frequently with the word\nsolid than it does with the word gas, whereas steam behaviours in the opposite way. Also, both are\nrelated to water and do not show a strong co-occurrence with the word fashion . The last row shows\nwhether a word relates more with ice(values much bigger than 1), steam (values much lesser than 1)\nor presents a neutral co-occurrence (close to 1).\nProbably and Ratio k=solid k =gas k =water k =fashion\nP(k—ice) 1.9×10−46.6×10−53.0×10−41.7×10−5\nP(k—steam) 2.2×10−57.8×10−42.2×10−31.8×10−5\nP(k—ice)/P(k—steam) 8.9 8 .5×10−21.36 0 .96\nIn a sense, GloVe receives a corpus of text as an input and transforms each word in that corpus into\na position in a high-dimensional space based purely on statistics through a co-occurrence matrix. With\nthe produced vectors, it is possible to retrieve related words based on the distance between vectors.\n2.2.4 Recurrent Neural Network\nRecurrent Neural Network (RNN) is a type of neural network used for processing sequential or time\nseries data, where the input has some defined order. One way to visualize RNN is by viewing the\narchitecture as multiple feed-forward neural networks that feed information from one network to another.\nIn practice, it is one network where the cells iterate over themselves for every input they acquire.\n13\nFigure 2.5: Recurrent Network Fully Connected.1\nRNN makes use of a hidden vector that has information from the last iteration. In doing so, its actual\nvector, a<t>, depends on the previous vector, a<t−1>, and the current input, x<t>. It is defined by the\nfollowing:\na<t>:=g1(a<t−1>, x<t>)\n=f(Waa·a<t−1>+Wax·x<t>+ba)(2.11)\nwhere WahandWaxrepresent the weight matrix for the hidden vectors and inputs, respectively, brep-\nresents the associated bias, and g1(and subsequently g2) representing an activation function. Usually,\nthe activation functions used for this RNN are either the logistic function (Sigmoid), Hyperbolic Tangent\n(Tanh), or Rectified Linear Unit (ReLU).\nThe output vector (prediction), y<t>, depends on the hidden state vector, a<t>.\ny<t>:=g2(Wya·a<t>+by) (2.12)\nwhere g is another activation function.\nThis architecture allows the networks to analyse any input with unspecified length while maintaining\nthe model size, considering historical information, with weights being shared across time.\nThe loss function used in this architecture has to be defined at each timestep:\nL(ˆy, y) :=TX\nt=1L(ˆyt, yt) (2.13)\nIn the training process, the RNN makes use of a gradient-based technique named Backpropagation\n1Figure based on https://stanford.edu/^shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n14\nThrough Time, calculated at each point in time. At a timestep, T, the derivative of the loss function, L,\nwith respect to the weight matrix Wis as follows:\n∂L(T)\n∂W:=TX\nt=1∂L(t)\n∂W(2.14)\nIn the Backpropagation Through Time mechanism, since it is hard to capture long-term dependen-\ncies because of the multiplicative gradient, the propagated errors might either tend to zero or increase\nexponentially (vanishing or exploding gradient phenomena). One method to help deal with the exploding\ngradient phenomena is by capping the maximum value for the gradient: Gradient clipping. On the other\nhand, several types of gates with well-defined purposes are used to deal with the vanishing gradient\nphenomena.\nOne problem raised by RNN is related to long-term dependencies. For example: “I like flowers a lot.\nToday, I walked through the public garden .”. The information that the user likes flowers should indicate\nthat he might walk through the “garden”. It is a possibility that the distance between the information and\nthe location where it is required is too great. In these circumstances, RNNs cannot learn to connect the\ninformation. Long Short-Term Memory (LSTM) solve this problem.\n2.2.5 Long Short-Term Memory\nLSTM network is a variant of RNN that is able to deal with long-term dependencies.\nStandard RNNs have a very simple structure. They can have, for instance, a single tanh layer\nrepresented by g1. (See Figure 2.6)\nFigure 2.6: RNN.2\n15\nLSTMs contain a slightly more complex structure (See Figure 2.7). They are specifically designed to\ntackle the problem with long-dependencies that ordinary RNNs would struggle with.\nFigure 2.7: LSTM.2\nThe cell state, c, flows across the entire chain, interacting linearly occasionally. If the information pro-\nvided by the cell state is barely changed, it is easy to preserve it over time. The gates can remove or\nadd information to the cell state. In Fig. 2.7, these gates are represented by σsince they use sigmoid\nfunctions. There are three main types of gates: Forget Gate, Input Gate, and Output gate.\nThe Forget Gate layer is responsible for selecting which information is staying or not in the cell state.\nIt makes use of a sigmoid function and, by looking at ht−1andxt, it outputs a number between 0 and 1\nthat represents how much information is kept:\nft:=σ(Wf· |ht−1, xt|+bf) (2.15)\nThe Input Gate layer aims to determine which values will be updated. In conjunction with a tanh layer, it\nhelps determine what information to store in the cell state.\nThe Input Gate output is as follows:\nit:=σ(Wi· |ht−1, xt|+bi) (2.16)\nwhile the output of the tanh layer, the new cell state candidate vector, ˜Ct, is given by:\n˜Ct:=tanh(WC· |ht−1, xt|+bC) (2.17)\n2Figure based on https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n16\nThis way, the cell state, Ctis defined as:\nCt:=ft∗Ct−1+it∗˜Ct (2.18)\nThe Output Gate layer is responsible for deciding what will be outputted and, in conjunction with a tanh\nlayer, deciding what will pass to the next cell. The Output Gate is defined by:\nσt:= (Wo· |ht−1, xt|+bo) (2.19)\nThis output vector originating from the Output Gate is used to determine the hidden vector, ht, that is\ngoing to be transmitted to the next cell.\nht:=σt·tanh(Ct) (2.20)\n2.2.6 Transformers\nSince it is possible to assume that RNNs are unrolled arbitrarily deep networks, they might take too\nlong to train (LSTMs even more due to their complexity). LSTMs, even though they can tackle long-\nterm dependencies, need long training times. Also, the inherent recurrence prevents the use of parallel\ncomputation (widely used in modern computer processors). With that in mind, a Google Team has\nsuggested a new Neural Network (NN) structure, Transformers [40], that introduces the self-attention\nmechanism.\nTransformers using the new mechanism of self-attention facilitate long-range dependencies, elimi-\nnate the Gradient Vanishing and Explosion phenomena, and, by using techniques that do not involve\nrecurrence, facilitate parallel computation, reducing the training time.\nThe encoder is responsible for receiving an input sequence, x= (x1, ..., x n), and map it into a\ncontextualized encoding sequence, z= (z1, ..., z n). The encoder is composed of N= 6identical layers.\nEach layer contains two sub-layers, a Multi-Head Attention Layer and a Feed-Forward Neural Network,\nthat produces outputs with 512 dimensions.\nThe Transformer Model is composed of different components. The Input Embedding is responsible\nfor transforming the inputs into a scalar vector and mapping them into space where words with similar\nmeanings are close to one another. These words might have different meanings depending on their\ncontext and position in a sentence. Positional Encoding tackles this problem.\nPositional Encoding is a vector that provides context depending on the position of a particular word\nin a sentence. In the original paper, it is used sine and cosine functions with different frequencies.\n17\nFigure 2.8: Transformer Model. Figure based on [40]\nPE(pos,2i)=sin(pos/100002i/d model) (2.21)\nPE(pos,2i+1)=cos(pos/100002i/d model) (2.22)\nwhere posis the position and iis the positional encoding dimension.\nAfter the input is passed through the Input Embedding and the Positional Embedding, it is passed\ntowards the Encoding Block. The Transformer Model contains the Multi-Head Attention Layer and Feed-\nForward Neural Network, as mentioned before.\nThe Multi-Head Attention Layer uses an attention mechanism to assign different weights that, for\nevery word, generate a vector that captures the importance between words of a specific sentence.\nThe attention function receives a query, a key, and a value and aims to pair it to an output. All the\ninputs are vectors and the output is calculated based on a weighted sum of the values.\n18\nFigure 2.9: Scaled Dot-Product Attention (left). Multi-Head Attention (right). Figure based on [40]\nAttention (Q, K, V ) :=softmax (Q·KT\n√dK)·V (2.23)\nwhere Qis a set of queries, Kis a matrix with the keys of the words from the query and Vis a matrix that\nholds the values of the words from the query. The input consists of queries and keys of dimension dk\nand values of dimension dv. Instead of running the attention function once for each set of keys, values,\nand queries, the original paper states a solution based on projecting linearly the same sets htimes with\ndifferent dk,dk, and dvdimensions, respectively. This way, the projected images of the keys, values,\nand queries are run in parallel throughout each Attention Head (Fig. 2.9).\nMultiHead (Q, K, V ) :=Concat (head 1, ..., head h)W0(2.24)\nwhere\nhead i:=Attention (QWQ\ni, KWK\ni, V WV\ni) (2.25)\nand where the projection matrices WQ\ni∈Rdmodel dk,WK\ni∈Rdmodel dk,WV\ni∈Rdmodel dVandWO\ni∈\nRhdVdmodel andh= 8parallel attention layers.\nFinally, there is the Decoder Block. The Decoder comprises a Masked Multi-Head Attention layer, a\nMulti-Head Attention layer, and a Feed Forward layer. The Multi-Head and the Feed Forward compo-\nnents are similar to the Encoder Block. When it comes to the Masked Multi-Head Attention Component,\nthere are some differences. The overall architecture was designed for translation purposes. Thus, the\n19\nMasked Attention component applies a mask to the attention scores so that the model only attends to\npositions before the current position in the input sequence, to ensure the predictions are made only on\nthe basis of past information. This mask application is applied by converting the impact words after the\ninput sequence current position to 0.\n2.3 Semantic Search Type\nSemantic search aims to improve the overall search quality by understanding the underlining query\nand sentence meaning. It achieves that by creating embeddings, which are vectorial representations of\nwords, paragraphs, or even documents, into a vector space. Both queries and sentences are embedded\ninto the same vector space, and the closest embeddings are found.\nFigure 2.10: Vector space with a query embedding and multiple sentence embeddings\nThere are two types of semantic search: Symmetric semantic search and Asymmetric semantic\nsearch. Symmetric semantic search aims to match a query input with text. For instance, if a user inputs\nin the search system “Capital crimes correspond to the worst types of crimes one can commit”, it should\nbe expected to receive retrievals that contain a similar meaning, such as “Capital crimes represent the\nworst type of crimes”. In practice, both query and result should have similar lengths.\nAsymmetric semantic search, on the other hand, provides answers to questions. Usually, the query\nis short and expects a more significant paragraph to be returned. A user might search “What are the\n20\nconsequences of robbing?” and it is expected to retrieve a sentence similar to “The consequences of\nrobbing are various from case to case. First, we need to identify the object or quantity being stolen. . . .”\nWhen applying a Symmetric semantic search, there are two approaches: Bi-Encoders and Cross-\nEncoders. Bi-Encoders generate a sentence embedding for a given sentence. Using Sentence-BERT\n(SBERT), we embed sentences AandBindependently, producing the sentence embeddings uandv,\nrespectively. These sentence embeddings can be later compared using cosine similarity. On the other\nhand, Cross-Encoders receives both sentences AandBsimultaneously and outputs a value between 0\nand1, representing the similarity of those sentences. Cross-Encoders do not return a sentence embed-\nding. It only compares the similarity between two sentences, which must be passed simultaneously.\nFigure 2.11: Bi-Encoder (left). Cross-Encoder (right).3\n2.4 Text Summarization\nText Summarization is the problem of reducing the number of sentences and words from a document,\nmaintaining its original meaning. It is the task of creating a shortened version of a given text document\nwhile retaining its most important and relevant information.\nThere are multiple techniques to extract information. These techniques can be categorized as Extrac-\ntive or Abstractive. Extractive techniques aim to retrieve the most important sentences from a document\n3Figure based on https://www.sbert.net/examples/applications/cross-encoder\n21\nwithout considering their meaning. On the other hand, Abstractive uses more complex and harder-to-\ntrain models to understand the semantics and meaning of the document text to create a proper summary.\nThis section will cover a extractive summarization technique that was used in the development of this\nwork.\nText summarization is an important field of research in Natural Language Processing (NLP), such\nas assisting search engines in generating relevant results and enabling machines to write shorter sum-\nmaries of news items or other text-based information.\n2.4.1 LexRank\nLexRank [13] is an unsupervised Extractive summarization technique. It uses a graph-based approach\nfor automatic text summarization. The score of each sentence is based on the concept of eigenvector\ncentrality in a sentence’s graph representation.\nThis algorithm has a connectivity matrix based on intra-sentence cosine similarity, which is used as\nthe adjacency matrix of the graph representation of sentences. In other words, sentences are placed\nas the graph vertices, and the edge weights are calculated using cosine similarity or Jaccard similarity.\n(See Figure 2.12)\nThe sentence similarity scores are then used to build a sentence similarity graph, in which each\nphrase is represented as a node and an edge between two nodes denotes the similarity score of the\nrespective sentences.\nThe PageRank method, which is well-known for calculating the relevance of nodes in a graph, is then\nused to generate the LexRank score of each phrase. The LexRank score is calculated by the PageRank\nalgorithm as follows:\nLexRank (i) :=d\nN+ (1−d)NX\nj=1LexRank (j)\ndeg(j)(2.26)\n, where irepresents a sentence, Nis the total number of sentences in the document, dis a damping\nfactor used to ensure that the scores converge, deg(j)is the number of sentences that are similar to\nsentence j, and the summation is over all sentences jthat are similar to sentence i.\nLexRank scores can be generated repeatedly until convergence, or by decomposing the similarity\nmatrix into eigenvalues. After that, the sentences with the highest LexRank scores are chosen for\nsummarizing.\n22\nFigure 2.12: Weighted cosine similarity graph from LexRak. Figure based on [13]\n23\n24\n3\nState of the Art\nContents\n3.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.2 BERTimbau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3 Deeper Text Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.4 Legal Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.5 NLP Applied To Portuguese Consumer Law . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.6 Albertina PT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n25\n26\nIn this chapter, we will cover some state-of-the-art models and techniques, as well as some relevant\nscientific papers that lead to the implementation of the final solution.\n3.1 BERT\nBidirectional Encoder Representations from Transformers (BERT) is a model proposed by researchers\nat Google AI Language in 2019 [12]. Just like humans tend to understand a word based on its context\nand surroundings, BERT aims to do that. Traditionally, models would look at text sequences from left\nto right or vice-versa. BERT is trained bidirectionally to obtain a higher understanding of the language\nusing the Transformers architecture. It makes use of two phases: pre-training and fine-tuning.\nThe problem when applying bidirectional unsupervised training is the trivial prediction of words. In\nunidirectional training, words can only see their left or right context. The existence of direction in training\nallows for an unbiased prediction of each subsequent word. When that does not exist, as in bidirectional\ntraining, the prediction of each word — based on both its left and right words — becomes biased, and\nwords can then predict themselves.\nOne of the tasks of using language models is to predict the next word in a sentence. BERT aims not\nonly to be able to predict that, but also to understand the language effectively. However, this trivial pro-\ncess would produce a biased model since it uses a particular direction. To tackle this phenomenon and\nreduce the bias, the original paper used two techniques in the pre-training phase while using BooksCor-\npus (800M words) and English Wikipedia (2500M words) as the datasets:\n• Masked Language Modeling (MLM);\n• Next Sentence Prediction (NSP).\nMLM is a technique that revolves around applying a [MASK] token to words, so their prediction can\nbe less biased. In the original paper, in a normal pre-training cycle, 15% of the words in each sequence\nare selected for a masking treatment. The model then aims to predict some masked words based on\nthe surrounding unmasked words. The words that were selected for the masking treatment were not\nall treated the same way. 80% of the selected words are replaced with a [MASK] token. 10% of the\nselected words are replaced with a random token, and the remaining 10% just keep the original token.\nNSP is a task in which the model aims to understand sentence relationships. Many applications like\nQuestion and Answer (QA) and Natural Language Inference (NLI) (See Section 3.1.2.A) revolve around\nthis sense of understanding. In the BERT training process, BERT receives multiple pairs of sentences\nas input. It proceeds to predict if the second sentence is a subsequent sentence of the first one. In\nthis training process, 50% of the inputs are indeed subsequent statements, and are deemed positive\n27\n(labelled as IsNext ), and the other 50% are deemed negative (labelled as NotNext ), meaning that the\nsecond sentence is completely disconnected from the first one.\nIn order to represent the input words as scalar vectors that can be read, BERT creates word embed-\ndings based on three components. Thus, the input embeddings are the sum of the token embeddings,\nthe segmentation embeddings and the position embeddings. The Token Embedding Layer is responsible\nfor assigning a value to each word based on vocabulary IDs. The second component is the Segmen-\ntation Embedding Layer, which allow distinguishing whether a word belongs to sentence A or B. The\nthird component is the Position Embedding Layer, which is responsible for indicating the position of each\nword in a sentence (See Figure 3.1).\nFigure 3.1: BERT input representation. Figure adapted from [12]\nIn the second phase, fine-tuning, one additional layer is added after the final BERT layer, and the\nentire network is trained for a few epochs with the Adam Optimizer.\nIn the original paper, the authors denoted the number of layers (i.e. Transformer blocks) as L, the\nhidden size as H, and the number of self-attention heads as A. With this architecture, two model sizes\nwere reported: BERT BASE andBERT LARGE . They have sizes L= 12 ,H= 768 ,A= 12 with Total\nParameters = 110 MandL= 24,H= 1024 ,A= 16 with Total Parameters = 340 M.\n3.1.1 Domain Adaptation\nNNs require significant amounts of data for proper training, especially labelled data. Usually, such large\nquantities of data are unavailable and training deep learning models can be very time-consuming, often\nrequiring specialized and expensive hardware. Deep learning models can perform well on a test dataset\nfrom the same domain as the training dataset. However, they tend to be less efficient with dataset from\ndifferent domains.\nTask data consists of any observable task distribution information. Data that can be used for a\nspecific task is usually non-randomly sampled from a wider distribution from a larger target domain.\n28\nNevertheless, this task data might not be present within the original data used to train a large language\nmodel. [15]\nTraining a model with data from a different domain of the one it will be used for can lead to some\nperformance degradation. This is because the model is likely to have learned patterns specific to the\noriginal domain, and these patterns may not generalize well to the target domain.\nFigure 3.2: Data Distributions. Figure based on [15]\nDomain Adaptation (DA) is a technique that aims to tackle this issue. With this technique, a model\nshould perform on a new dataset that comes from a different domain similarly to as it would on the\ntesting dataset. Usually, in order to achieve this type of result, deep learning models are re-trained\non unsupervised learning tasks. DA saves large amounts of computational resources and, by utilizing\nunsupervised learning methods, reduces the necessity of manual annotation of labelled datasets.\nSubsection 3.1.1 covers multiple DA techniques that can be used for BERT models.\n3.1.1.A Masked Language Modeling\nMLM, as mentioned in Section 3.1, is a task originally introduced by BERT. Words selected at random\nwith a 15% chance are masked from the input sentence with a predefined probability (80%), and the\nmodel aims to predict those masked words. (see Figure 3.3)\n4Figure based on https://www.sbert.net/examples/unsupervised_learning/MLM\n29\nFigure 3.3: Masked Language Modeling.4\nTo apply MLM with the intent of adapting the domain a model performs in, the model is re-trained\nover one epoch on a new dataset. The learning rate used is the same or slightly lower than the MLM\ntask performed on the model pre-training. The reasoning behind such choice is to slightly change on the\nweights used on the NN, without destabilizing the network completely.\nThe goal of the MLM task is to maximize the likelihood of predicting the correct tokens for the masked\npositions, which is equivalent to minimizing the negative log-likelihood loss function LMLM. The loss\nfunction is defined as:\nLMLM(x, y) :=−NX\nn=1wynxn, yn (3.1)\n, where xis the input, yis the target, wis the weight, and Nis the batch size.\n3.1.1.B Transformer-based Sequential Denoising Auto-Encoder\nTransformer-based Sequential Denoising Auto-Encoder (TSDAE) is an unsupervised state-of-the-art\nsentence embedding method which outperforms previous approaches, such as MLM. Firstly published\non April 14th of 2021 by Nils Reimers [42], this technique aims to improve the domain knowledge of a\nmodel. They state that TSDAE fills the gap between models that usually only perform the STS task on\na certain domain. TSDAE’s model architecture is a modified encoder-decoder Transformer, with the key\nand value of the cross-attention mechanism limited to the sentence embedding.\nIn a sense, it is similar to MLM, but instead of swapping words for [MASK] tokens, TSDAE introduces\nnoise to the sentences by deleting or swapping words. The encoder transforms the sentence into a\n30\nFigure 3.4: TSDAE Architecture. Figure based on [42]\nvector, and a decoder is supposed to reconstruct the original sentence. Formally, the training objective\nis shown in the equation 3.2.\nJTSDAE (Θ) = Ex∼D[logPΘ(x|˜x)]\n=Ex∼D[lX\nL=1logPΘ(x|˜x)]\n=Ex∼D[lX\nL=1logexp(hT\ntet)PN\ni=1exp(hT\ntei)](3.2)\nwhere Dis our training corpus, x=x1, x2...xlis the input sentence with ltokens, ˜xis the corresponding\ndamaged sentence. etis the word embedding of xt, and htrepresents the hidden state at decoding step\nt.\n3.1.1.C GenQ\nGenQ [39], published in October 2021 by the Ubiquitous Knowledge Processing Lab team, is an unsu-\npervised domain adaptation method for dense retrieval models, allowing the query generation from given\npassages. This approach aims for a semantic search system to be asymmetric (explored in Subsection\n31\n2.3), supporting question-answering scenarios simply by training with synthetically generated data.\nFirstly, GenQ requires a Text-to-Text Transfer Transformer (T5) model fine-tuned for question-answering.\nA T5 is a Transformer based architecture that uses a text-to-text approach [29]. This new model archi-\ntecture achieves state-of-the-art results on many NLP benchmarks while maintaining the ability of being\nfine-tuned for numerous downstream tasks. T5 authors found that training on in-domain unlabelled data\ncan improve performance, but using a large and diverse data set is better for generic language under-\nstanding tasks.\nFigure 3.5: T5 diagram. Figure based on [29]\nIn the original paper, they fine-tuned a T5 model on MS MARCO [26] for two epochs. After that, it\nutilised the T5 model to generate queries from original passages. The idea behind T5 models is that\nall NLP tasks can be defined as a text-to-text problem, so they are trained on numerous tasks with\nimmense amounts of data. One of these tasks is query generation. By feeding a passage, T5 models\ncan generate multiple questions the passage may answer. These generated queries might be far from\nideal. The T5 model used is one for general purposes, which can lead to noisy data with plenty of\nrandomnesses. t\nThus, a dense model, such as SBERT, can be fine-tuned with the passages and synthetically gen-\nerated query pairs using the Multiple Negatives Ranking (MNR) loss. Figure 3.6 shows the overall\narchitecture.\n3.1.1.D Generative Pseudo Labeling\nGenerative Pseudo Labeling (GPL) [43] is an improved state-of-the-art technique to perform domain\nadaptation of dense models. It comprises three phases, as shown in Figure 3.7. First, a Query Gen-\n5Figure based on https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html\n32\nFigure 3.6: GenQ5\neration step uses the GenQ approach to generate queries for multiple passages. We have access to\nqueries and passages, called positive passages, since they can answer the generated queries.\nThen, there is a Negative Mining step. Negative mining is a technique to retrieve passages that would\nnot answer the previously generated queries. Nevertheless, these negative passages are similar to the\npositive passages in the vector space. In practice, a dense index would be filled with multiple passages,\nand the closest ones to the positive passages would be retrieved and noted as negative passages.\nThe last and fundamental step is called Pseudo Labeling. In this step, a Cross-Encoder receives the\ntriplets composed of a query, a positive, and a negative passage. The Cross-Encoder calculates the\nscore margin between the negative and positive passages. The training model will use the Margin Mean\nSquared Error Loss [ ?] to identify whether these passages are also relevant to the given query. The\nloss function is calculated based on the sim(Query, Pos )−sim(Query, Neg )|and|gold sim(Q, Pos )−\ngold sim(Query, Neg )|. As stated in the original paper, the dot product was used as the default for\ncalculating the similarity between passages and queries.\n3.1.2 Fine-tuning on Downstream Tasks\n3.1.2.A Sentence-BERT\nThere was a need for representing an entire sentence in an embedding instead of only one word or\ntoken. One implementation was using the BERT model on all the tokens present in a sentence and\n33\nFigure 3.7: Generative Pseudo Labeling. Figure based on [43]\nproducing a mean pooling. Even though this technique was fast enough for the desired problem, it was\nnot very accurate (GloVe embeddings, designed in 2014, produced better results than this technique).\nSBERT [31], implemented in 2019 by a group of researchers from the Ubiquitous Knowledge Pro-\ncessing (UKP) lab, is a modification of the BERT network using siamese and triplet networks which can\ncreate sentence embeddings that are semantically meaningful. This modification of BERT allows for\nnew tasks, such as semantic similarity comparison or information retrieval via semantic search.\nReimers and Gurevych showed that SBERT is dramatically faster than BERT in comparing sentence\npairs. From 10 000 Sentences, it took BERT 65 hours to find the most similar sentence. In contrast,\nSBERT produced the embeddings in approximately 5 seconds and, using the cosine similarity, took\napproximately 0.01 seconds.\nThe siamese architecture comprises two BERT models with weights entangled between them. In\nthe training process, the sentences would be fed to both BERT models, which then would go through a\npooling operation that would transform the token embeddings of size 512×768into a vector of a fixed\nsize of 768.\nOne way of fine-tuning SBERT is through the Softmax loss approach (See Figure 3.8). Applying\na softmax classifier on top of a siamese network can improve sentence representation [9]. NLI is the\nchallenge of determining if the premise implies the hypothesis, whether they are contradictory or neutral.\nIt used a combination of the Stanford Natural Language Inference (SNLI) [7] and the Multi-Genre NLI [44]\ndatasets. The datasets contained pairs of sentences (premises and hypothesis) that could be related\nthrough a label feature. This label feature determines whether the sentences are related or not.\n• 0 – “entailment”, the premise suggests the hypothesis\n• 1 – “neutral”, the premise and hypothesis could not be related\n• 2 – “contradiction”, the premise and hypothesis contradict each other\nThis type of fine-tuning aims to train the model to identify the relationship between sentences.\n34\nFigure 3.8: Fine-Tuning SBERT. Based on [31]\nAnother task that SBERT can be fine-tuned is the STS task. The siamese architecture calculates the\ncosine similarity between uandvsentence embeddings. The researchers also tried negative Manhattan\nand Euclidean distances as similarity measures, but the results were similar. The model performance\nevaluation, in terms of Semantic Textual Similarity, was evaluated on Supervised and Unsupervised\nLearning.\nRegarding the Unsupervised Learning, the researchers used the STS tasks 2012 – 2016 by Agirre\net al. [1–5], the STS benchmark by Cer et al. [8] and the SICK-Relatedness dataset [22]. Each of these\nthree datasets contained gold labels between 0 and 5 that reflects how similar each sentence pair is.\nTable 3.1 shows the Spearman correlation ρbetween the cosine similarity of sentence representations\nand the gold labels for various Textual Similarity (STS) tasks. The performance is reported by convention\nasρx 100.\nTo evaluate Supervised learning, the researchers chose to fine-tune SBERT only with the STS bench-\nmark (STSb), which is “a popular dataset to evaluate supervised STS systems” (Reimers et al., 2019)\nand first train on NLI and then on STSb. (See Figure 3.2 .) BERT systems were trained with ten random\nseeds and four epochs. SBERT was fine-tuned on the STSb dataset, SBERT -NLI was pre-trained on\nthe NLI datasets, and then fine-tuned on the STSb dataset.\n35\nModel STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.\nAvg. GloVe embedd. 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\nAvg. BERT embedd. 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81\nBERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19\nInferSent - GloVe 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01\nUni. Sentence Enc. 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22\nSBERT -NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89\nSBERT -NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55\nSRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21\nSRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68\nTable 3.1: SBERT Spearman correlation ρresults. From [31]\nModel Spearman\nNot trained for STS\navg. GloVe embeddings 58.02\navg. BERT embeddings 46.35\nInferSent - GloVe 68.03\nUniversal Sentence Encoder 74.92\nSBERT -NLI-base 77.03\nSBERT -NLI-large 79.23\nTrained on STS benchmark dataset\nBERT -STSb-base 84.30 ±0.76\nSBERT -STSb-base 84.67 ±0.19\nSRoBERTa-STSb-base 84.92 ±0.34\nBERT -STSb-large 85.64 ±0.81\nSBERT -STSb-large 84.45 ±0.43\nSRoBERTa-STSb-large 85.02 ±0.76\nTrained on NLI data + STS benchmark dataset\nBERT -NLI-STSb-base 88.33 ±0.19\nSBERT -NLI-STSb-base 85.35 ±0.17\nSRoBERTa-NLI-STSb-base 84.79 ±0.38\nBERT -NLI-STSb-large 88.77 ±0.46\nSBERT -NLI-STSb-large 86.10 ±0.13\nSRoBERTa-NLI-STSb-large 86.15 ±0.35\nTable 3.2: SBERT evaluation on the STS benchmark test set. Retrieved from [31].\n36\n3.1.2.B Multilingual Sentence Embeddings\nOne issue raised by using pre-trained models is that the embedding models are usually monolingual\nsince they are usually trained in English. In 2020, the lead researcher from the team that published\nSBERT introduced a technique called Multilingual Knowledge Distillation (MKD) [32]. It relies on the\npremises that for a set of parallel sentences ((s1, t1), ...,(sn, tn))withtibeing the translation of siand a\nteacher Model M, a Model ˆMwould produce vectors for both siandticlose to the teacher Model M\nsentence vectors (See Figure 5.5). For a given batch of sentences β, they minimize the mean-squared\nloss as follows:\n1\n|β|X\nj∈β[(M(sj)−ˆM(sj))2+ (M(sj)−ˆM(tj))2] (3.3)\nFigure 3.9: Multilingual Knowledge Distillation Process. Figure based on [32]\nThere are two different models: a student and a teacher model. Assume that we intend for our\nstudent model to learn Portuguese and that our teacher model already knows English. Both models\nwill receive pairs of sentences with the same meaning, one sentence written in Portuguese while the\nother is in English. The teacher model will encode the English sentence for each pair, while the student\nmodel will encode both sentences. Since both sentences have the same meaning, both embeddings\nshould be similar, if not equal. Consequently, the student model embeddings will be compared to the\nteacher model embedding, which will back-propagate the error using a mean squared error approach.\nOver time, the student model embeddings will become closer to the teacher model embedding.\nMultilingual SBERT versions, such as paraphrase-multilingual-mpnet-base (768 Dimensions) or paraphrase-\nmultilingual-MiniLM-L12 (384 Dimensions), would provide relatively accurate embeddings to this context.\nmpnet model, as a larger model than MiniLM, should be able to comprehend the meaning of a text better\nthat it has yet to explicitly see, such as queries with only one or two words.\n37\n3.2 BERTimbau\nDespite the existence of multilingual BERT models trained on multiple languages, there was an effort to\ntrain monolingual BERT models on single languages. BERTimbau is a BERT model pre-trained for the\nPortuguese language.\nBoth BERT -Base ( 12layers, 768hidden dimension, 12attention heads, and 110M parameters) and\nBERT -Large ( 24layers, 1024 hidden dimension, 16attention heads and 330M parameters) variants were\ntrained with Brazilian Web as Corpus (BrWaC) [6], a large Portuguese corpus, for 1,000,000steps, using\na learning rate of 1e−4. The maximum sentence length is S= 512 tokens\nModel Arch. #Layers #Params\nneuralmind/bert-base-portuguese-cased BERT -Base 12 110M\nneuralmind/bert-large-portuguese-cased BERT -Large 24 335M\nTable 3.3: BERTimbau variants\nBrWaC corpus contained over 2.68billions tokens retrieved from across 3.53million documents,\nproviding a well diverse dataset. They utilised the HTML body, ignoring the titles and possible footnotes.\nThe researchers removed the HTML tags and fixed possible “mojibakes”, a type of text corruption that\noccurs when strings are decoded using the incorrect character encoding, producing a processed corpus\nwith17.5GB of raw text.\nThe pretraining stage was identical to BERT. BERTimbau was trained using MLM and NSP methods\n(Explained previously in Section 3.1) with the exact technique probabilities. Each pretraining example is\ngenerated by concatenating two sequences of tokens x= (x1, . . . , xn )andy= (y1, . . . , ym )separated\nby special [CLS] and [SEP] tokens as follows:\n[CLS]x1. . . x n[SEP ]y1. . . y m[SEP ] (3.4)\nFor each corpus sentence x, 50% of the time an adjacent sequence y is chosen to form a contiguous\npiece of text, and on the remaining 50% of the time, a random sentence from a completely different\ndocument from the corpus is selected as the token y.\n15% of the tokens of every example pair xandyare replaced by 1 of 3 options. Each token can\nbe replaced with a special [MASK] token with 80% probability. With 10% probability, it is replaced with\na random token from the vocabulary or, with the remaining 10% probability, the original token remains\nunchanged.\nBERTimbau serves as the foundation for our language model, Legal-BERTimbau. Even though\nBERTimbau is already a language model adapted to the Portuguese language, it was necessary to\n38\ndevelop a model for our legal domain. This fine-tuning stage is essential since we needed to ensure that\nthe model producing the embeddings could adequately understand the records.\n3.3 Deeper Text Understanding\nZhuyun Dai and Jamie Callan in [11] explore the use of a contextual neural language model, BERT, for\nad-hoc document retrieval in IR. The authors of the paper found that using BERT for text representations\nwas more effective than traditional word embeddings like Word2Vec. The contextual language model\nwas able to better leverage language structures and achieve improved performance on queries written\nin natural language. The authors also show that fine-tuning pre-trained BERT models with a limited\namount of search data could outperform strong baselines. Furthermore, they found that stopwords and\npunctuation, which are often ignored by traditional IR approaches, played a key role in understanding\nnatural language queries.\nIn [11], Zhuyun Dai and Jamie Callan studied the performance of information retrieval associated\nwith two different datasets ( Robust04 andClueWeb09-B ) and different techniques. It included testing\nthe performance with scores related to the score of the first passage (BERT -FirstP), the best passage\n(BERT -MaxP), or the sum of all passage scores (BERT -SumP). The paper showed that simply searching\nthe passage with the best score would provide better results when the dataset has well written text ( Ro-\nbust04 ) since it could understand the context and proper meaning. The results also showed that BERT\nperformed better on description queries than title queries, and that longer natural language queries are\nmore expressive than keywords.\nThey also suggest that BERT, more specifically the SBERT modification, should be applied to small\nportions since it would be “less effective on the long text”. Such implies that embedding small passages,\neither paragraphs, portions of paragraphs or only phrases, is more effective for documents. Another\nexciting approach studied in this research was adding the title to the beginning of every passage to\nprovide context, but it did not produce satisfying results.\n3.4 Legal Information Retrieval\nIn the Competition on Legal Information Extraction and Entailment (COLIEE) edition of 2021, there were\nexplored multiple techniques focused on four specific challenges in the legal domain: case law retrieval,\ncase law entailment, statute law retrieval, and statute law entailment.\nIn [19], Mi-Y oung Kim et al. discuss the use of deep learning techniques for legal information retrieval\nand question-answering tasks in the context of that same edition, focusing, especially, on the University\nof Alberta’s participation.\n39\nTeam F1-score Precision Recall\nJNLP 0.2813 0.3211 0.2502\nnigam 0.2809 0.2587 0.3072\nTable 3.4: COLIEE 2021 - Task 1 results\nTeam Return Retrieved F2 Precision Recall MAP\nOvGU 161 96 0.779 0.778 0.805 0.836\nTable 3.5: COLIEE 2021 - Task 3 results\nTwo teams achieved top COLIEE scores on the first task by combining lexical and semantic tech-\nniques. Task 1 relied on, in total, on a dataset comprised of a total of 5978 case law files. The teams\nwere provided with a labelled training set of 4415 case law files of which 900 query cases, meaning\nthere were approximately 4.9noticed cases per query case. For the third task, Statute Law Retrieval,\nthe goal was to extract a subset of Japanese Civil Code Articles from the Civil Code articles considered\nappropriate for answering legal bar exam questions.\nThe first team, OvGU, presents a two-stage TF-IDF vectorization combined with Sentence-BERT\nembeddings for the third task. Regarding the Task 1, the second team, JNLP , concentrated on dealing\nwith large articles by conducting text chunking on the supplied training data and used a self-labeled\napproach while fine-tuning pre-trained models. They began by preparing the training data using the\napproach given in 25. They employed a TF-IDF vectorizer to encode all the articles and queries into\nvectors, and then used Cosine Similarity to rank the articles. A question and an accompanying article\nare called positive training examples, whereas the converse is termed negative training examples.\nIn that same competition, a third team, nigam [27], proposed an approach where it combined transformer-\nbased and traditional IR techniques for the first task. The team made use of SBERT and Sent2Vec for\nthe semantic component and combined the scores with BM25. They first selected a pre-defined amount\nof results based on BM25, and then they proceeded to embed those documents’ sentences. The final\nresult would be based on the cosine similarity metric.\nThe Task 1 results are shown in Table 3.4 and Task 3 results are shown in Table 3.5.\n3.5 NLP Applied To Portuguese Consumer Law\nIn 2022, Nuno Cordeiro, as part of his master’s thesis [10], created a system, Legal Semantic Search\nEngine (LeSSE), that merges common document retrieval techniques with semantic search abilities on\nPortuguese consumer law. The system was developed in partnership with INESC-ID and Imprensa\n40\nNacional-Casa da Moeda, with the goal of making the Consumer Law more accessible and understand-\nable to the Portuguese citizens. The overall goal and context of his thesis are similar to the context of\nthis research. Even though Nuno’s work focuses on Portuguese Consumer Law, several state-of-the-art\ntechniques, such his usage of BERT and BM25, for Information Retrieval are relevant for our scenario.\nThe system starts by pre-processing all the law documents and the query, followed by text segmenta-\ntion and semantic and syntactic pre-processing. Embeddings are then generated from the segments and\nquery, and a search index is created. In search time, the query is processed and scores are assigned\nto the segments based on their semantic and syntactic similarity. The final stage involves reordering the\nresults using a trained semantic similarity model and presenting the results to the user. The semantic\npipeline uses BERTimbau Base (BERT -Base), a BERT model trained on the Brazilian Portuguese Web.\nThe implemented search system combines the 20 retrievals with the highest scores using BM25 with 50\nretrievals with the highest scores using the cosine similarity measure. Consequently, it orders the results\nthrough a reordering model to produce the final results.\nThe pre-processing of legal documents differs from the one needed in our context. The implemented\nsystem required tokenization to help construct the bag-of-words necessary for the BM25 algorithm,\nremoval of punctuation, and stop-words, which is unnecessary for SBERT.\nIn his work, the language model had to be fine-tuned on a corpus that included legislative jargon to\nproduce the desired results. This corpus is a Portuguese corpus with the help of annotated questions\nfrom the Official Portuguese Gazette (Di ´ario da Rep ´ublica) search database. The fine-tuning was done\nusing a machine with 2 NVIDIA GeForce RTX 3090 GPUs, each with 24 GB of memory and 10496\ncores. The hyperparameter optimization was performed using Population Based Training, which is a\ncombination of Grid search and Hand Tuning. The hyperparameter optimization was performed using\nthe Ray Tune library, which was integrated into the Trainer class function hyperparameter search. The\ntraining dataset was divided into three subsets: training set, validation set, and test set, with 80%, 10%,\nand 10% of the original dataset, respectively. The datasets were shuffled before division to ensure that\nevery training batch was representative of the dataset as a whole. This fine-tuning stage is an important\nstep, since it would help ensure that the model could create proper relationships with words not seen in\nthe pre-training stage. We implemented this approach in this thesis work, since there will be legal terms\nand jargon that the model has not seen in the pre-training phase.\n3.6 Albertina PT\nIn the closing stages of this research work, in May 2023, Jo ˜ao Rodrigues et al., shared their brand\nnew state-of-the-art model, Albertina [35]. This BERT model represents the new state-of-the-art for\nEuropean Portuguese (PT -PT) and Brazilian Portuguese (PT -BR) encoder models. It was developed\n41\nin a partnership between Faculdade de Ci ˆencias da Universidade de Lisboa (FCUL) and Faculdadede\nEngenharia da Universidade do Porto (FEUP), more concretely, NLX–Natural Language and Speech\nGroup, and Laborat ´orio de Intelig ˆencia Artificial e Ci ˆencia de Computadores, respectively.\nThe starting point was DeBERTa [16] architecture, and the pre-training was done over data sets\nof Portuguese for the PT -PT version and the BrWaC corpus for PT -BR, allowing for comparison with\nBERTimbau It has 24layers with a hidden size of 1536 and a total of 900million parameters.\nAlbertina PT -BR outperforms BERTimbau in the STS task over the assin2 dataset, as well as on the\nSTS-B dataset. Interestedly enough, Albertina PT -BR fails to match that performance, falling short of\nBERTimbau.\nNevertheless, Albertina PT -BR appears to be an improved version of BERTimbau, mainly due to its\nlarger architecture. Albertina PT -PT comes up as the first substantial BERT model completely trained\nfor European Portuguese, which is a breakthrough on its own.\n42\n4\nSemantic Search System\nContents\n4.1 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.2 The Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n43\n44\nThis thesis aimed to implement a semantic search system to help the court decision process. The\nproposed system aims to address this challenge by implementing a semantic search system that can\nefficiently retrieve and present relevant legal information.\nIn Chapter 4 of this thesis, we provide an overview of the implemented search system architecture\nand the requirements that the system is expected to meet. The Chapter delves into the different stages\nof the system and introduces the key components that make up the architecture. The developed large\nlanguage model, Legal-BERTimbau, was designed to achieve the project’s goals and is a crucial element\nof the system. Chapter 4 presents an overview of Legal-BERTimbau’s role in the overall search system.\nChapter 5 takes a more detailed look at the Legal-BERTimbau model and explores its inner workings.\nThe Chapter examines the model’s architecture, training data, and the different techniques used to\nfine-tune the model for our legal applications. Chapter 5 will provide a better understanding of how\nthe Legal-BERTimbau model works and how it contributes to the overall effectiveness of the semantic\nsearch system.\n4.1 Constraints\nAs mentioned in Chapter 1, this work is a segment of Project IRIS. Consequently, there were some\npre-defined aspects, such as which technologies the search system should be implemented in.\n4.1.1 ElasticSearch\nElasticsearch, released in 2010, is a distributed, open-source search and analytics engine built over\nApache Lucene. It works as a NO-SQL JSON document-based datastore. A user can interact with\nElasticsearch similarly to interactions with REST APIs, meaning that every request, either POSTs, GETs,\nor PUTs, is sent in a JSON format to different indices. An index is used to store documents in dedicated\ndata structures and allows the user to partition the data in a certain way within a specific namespace.\nElasticsearch uses a complex architecture to ensure the scalability and resilience of the system.\nIt comprises node clusters, where nodes are single instances of Elasticsearch. It also makes use of\nshards, which are subsets of index documents. Shards allow splitting the data from indices to maintain\na good performance and replicate information to handle failures.\nElasticSearch was a pre-defined constraint since the Project IRIS solution was based on the Elas-\nticSearch engine. To utilise ElasticSearch for our use case, it was required to understand how to utilise\nthe provided engine with embeddings.\nBy default, Elasticsearch uses BM25 to search through documents. However, it can support other\nsearch functions, such as Cosine Similarity. To utilise Cosine Similarity, ElasticSearch requires an initial\nmapping of indices, pre-defining its document structure and allowing some fields to be dense vectors.\n45\nThese dense vectors can store embeddings. This powerful edge provides an ideal solution for a se-\nmantic search system implementation, allowing to search and analyse of huge volumes of data in near\nreal-time.\n4.2 The Corpus\nLegal documents contain specific language not easily found in conventional websites or books. To\ncreate a semantic search system adapted to the Portuguese legal domain, it was required to collect\nmany records. Project IRIS members performed the collecting data process through ecli-indexer6. This\nrepository contains multiple tools to extract documents from dgsi.pt, a public database, and index them\ninto ElasticSearch. The retrieval process recovered the HTML content from multiple web domain pages\ncontaining legal documents, summing up to 31690 documents.\nThe structure of each indexed document is as follows:\n1{'_index ':'jurisprudencia .1.0 ',\n2'_id':'- B5mRoABpM44h1Fg -6QX ',\n3'_score ': 1.0,\n4'_source ':{'ECLI ':'ECLI :PT:STJ : 2022 : 251.18.1 T8CSC .L2.S1 ',\n5 'Tribunal ':'Supremo Tribunal de Justica ',\n6 'Processo ':'251/18.1 T8CSC .L2.S1 ',\n7 'Relator ': Relator 1 ',\n8 'Data ':'17/03/2022 ',\n9 'Descritores ': ['CONTRATO DE TRABALHO ',\n10 'CONTRATO DE PRESTACAO DE SERVICO '],\n11 'Sumario ':'\\n<p>I- Subjacente ao contrato de trabalho existe uma\nrelacao de dependencia necessaria ... \\n </p><p> ',\n12 'Texto ':'... <p><i>d) Deve a Re ser condenada a pagar ao Autor a\ndiferenca entre os vencimentos pagos desde julho de 2011 e o\nvencimento que venha a ser determinado nos termos dos pedidos\nformulados em b) ou c) ... ',\n13 'Tipo ':'Acordao ',\n14 'Original URL ':'http :// www . dgsi .pt/ jstj .nsf /12345 ',\n15 'Votacao ':'UNANIMIDADE ',\n16 'Meio Processual ':'REVISTA ',\n6https://github.com/diogoalmiro/ecli-indexer\n46\n17 'Seccao ':'4a SECCAO ',\n18 'Especie ': None ,\n19 'Decisao ':'<b> NEGADA A REVISTA .</b> ',\n20 'Aditamento ': None ,\n21 'Jurisprudencia ':'unknown ',\n22 'Origem ':'dgsi - indexer -STJ ',\n23 'Data do Acordao ':'17/03/2022 '}}\nThe partition of utilised data was mainly the “Texto” (Text) and “Sum ´ario” (Summary) fields. It con-\ntained the HTML content of a legal document corpus. This data needed further processing to create a\nreliable semantic search system. The Summary section reflects not a summary of the full document, but\ninstead, it is a summary of the judgement ruling decision and the newly established jurisprudence.\nThree dataset splits were generated to train, test, and validate the produced models. The percent-\nages for each split were as follows: 80% for the training dataset, 10% for the testing dataset, and 10%\nfor the validation dataset. This dataset was published to the HuggingFace platform7to facilitate model\nreproducibility and future project use. The divisions were as follows:\n• Training dataset – 26952 documents\n• Testing dataset – 3169 documents\n• Validation dataset – 3169 documents\n4.2.1 Data Processing\nWith all the documents properly indexed, it was necessary to clean the available text and split the content\ninto multiple sentences. Our search system acts on singular sentences, as explored further in Chapter\n4 and 5.\nFirstly, HTML tags needed to be removed as well as some unexpected characters, such as “&”. It\nwas required to identify and remove Roman numeration from the text and, more importantly, not take\ninto consideration sections or Subsection titles.\nTexts also contained references to other sections, such as “como referido em a) e b)”. This example\nshows a possible problem a semantic search system can face due to the difficulty of demonstrating\nto the system that there is relevant information outside that section. When the system identifies that\nthere is referred information from other sections, it is required to handle that dependency. A solution\nto this issue could start by incorporating a summarization technique to join the information in the same\n7https://huggingface.co/datasets/stjiris/portuguese-legal-sentences-v0\n47\nplace. We decided to utilise a more straightforward approach. In the data pre-processing step, these\noccurrences are removed from the text. Even though it does not contain all the related information as\napplying some summarization technique would provide, the focus was to further simplify the text. This\naimed to improve the semantic search system’s performance, since the sentences themselves would\npresent a clearer meaning without depending on other sections.\nIn our scenario, with over 30000 documents, the solution involves implementing a Bi-Encoder. We\nneed to create the embeddings independently of each other, allowing us to search later using the cosine\nsimilarity. This way, the overall search system performance is doable, whereas the search would not be\nfeasible if we utilise a Cross-Encoder.\n4.3 Architecture\nSection 4.3 provides insights on the implemented Search Systems’ architectures. Throughout this work,\nwe developed three different search systems: one semantic search and two hybrid search systems that\ncombine both semantic and lexical approaches.\nInitially, there was a pre-processing of the documents in the original dataset to split entire documents\ninto smaller units. This pre-processing was essential to separate the text into smaller passages since\nLegal-BERTimbau would be less effective on large sentences. Legal-BERTimbau’s siamese and triplet\nnetwork structures depend on the available training data, implying that the effectiveness of itself may\nvary depending on the size and nature of the input data. In our scenario, Legal-BERTimbau’s training\ndata were not very extensive, comprehending only a few tokens each time. Nevertheless, it was required\nto analyse the documents in more detail to verify if the phrases are too long, too short or if they raised\nother concerns, such as referenced in Subsection 4.2.1. On the other hand, stop-words removal was\nnot necessary, since Legal-BERTimbau, being a version of SBERT, is designed to receive meaningful\nsentences rather than isolated keywords.\nThe proposed solution architecture is illustrated in Figure 4.1.\nFor implementing the search system, we utilised Elasticsearch, which allows for scalability and fast\nretrieval of results while using the cosine similarity function to search through embeddings. Elasticsearch\nwas a requirement for this project, as stated in Subsection 4.1.1. Elasticsearch is used as a dense\nvector database where embeddings will be stored in indices. Such indices require initial mapping, which\nmandates the size of the embeddings and other extra information, such as the original document from\nwhere the sentence was retrieved.\nAfter the pre-processing described in the previous section, the next step was generating the embed-\ndings. The sentence embeddings were created by making use of the Legal-BERTimbau model hosted\n48\nFigure 4.1: System Architecture\non the Hugging Face Hub8, through the SentenceTransformers Python9library. With the embeddings\ngenerated, it is possible to populate the indexes on Elasticsearch through its Python Client.\nFor retrieving specific query results, that exact query would be transformed into an embedding by\nLegal-BERTimbau. Then the system can proceed to search similar sentences by executing a ranking\nfunction on Elasticsearch using the query embedding. Depending on which search system architecture,\nthe Ranking function varies slightly.\nFinally, the relevant passages are included in a prompt for a Generative Language Model (GLM),\nGPT3.5, providing a user-friendly response, yet based on our retrieved results. For example, with a\nquery as such: “Furto de Armas”, the system’s output is the following:\n— A passagem relevante para a quest ˜ao´e a seguinte: ”Quem, de noite e acompanhado, entra\nnuma casa museu, depois de arrombar a porta e de l ´a retira v ´arias armas pec ¸as de museu e delas se\napropria, contra vontade do dono, pratica os crimes de introduc ¸ ˜ao em lugar vedado ao p ´ublico e furto\nqualificado.” (Documento ID: 9EWRY oMBF lErWh5 w2g).\n8https://huggingface.co/\n9https://www.sbert.net/\n49\n4.3.1 Purely Semantic Search System\nThe Purely Semantic Search System makes use of only the semantic capabilities of the embedding\nmodel. The system utilises Elasticsearch and cosine similarity function to search through the embed-\ndings and retrieve relevant search results.\nWe chose to implement a symmetric semantic search system. The reasoning behind such a decision\nwas predominantly due to the need for more queries and results from pairs examples to implement a\nproper asymmetric semantic search. On the same note, when a judge interacts with a search system,\nthe judge is more likely to insert an extensive query with proper terminology than inputting a question\nthat wants to be answered.\nTo be noted, there are other distance metrics, such as the Dot Product, but Cosine Similarity is\nregarded as the most prominent one for use cases as this.\n4.3.2 Lexical-First Search System\nThis work also presents a different version of a search system, a Hybrid Search System, that combines\nthe potential of lexical search techniques and the reach of large language models. We called it Lexical-\nFirst Search System.\nThe architecture is similar to the developed Purely Semantic Search System. The pre-processing\nand usage of ElasticSearch are equal, and only the retrieval method changes slightly. Instead of yielding\nthe best matches using the cosine similarity metric, it combines the use of BM25 before evaluating the\nsimilarity of the embeddings using the cosine similarity metric.\nThe method retrieves a pre-defined number of top results using BM25 (i.e Top 20 results). Afterwards,\nit ranks the outcomes using the cosine similarity metric, as shown in Figure 4.2.\nFigure 4.2: Lexical-First Search System Retrieval Method\n50\n4.3.3 Lexical + Semantic Search System\nLexical-First Search System presents promising results, which are explored in more detail in Section 6.2.\nHowever, such hybrid system architecture relies immensely on BM25 results. In a more straightforward\noverview, the presented Hybrid Search System filters the possible results using BM25 and then verifies\nwhich results should be retrieved based on a Legal-BERTimbau model.\nThis Subsection introduces a more flexible architecture for a Hybrid Search System, designated as\nLexical + Semantic Search System. Instead of filtering the first batch of results using BM25, we propose\nto combine the scores of both lexical and semantic information retrieval methods.\nLexical + Semantic Search System utilises the scores provided by BM25 and the cosine similarity\nvalue from the Legal-BERTimbau embedding space. BM25 scores are normalised using the maximum\nscore obtained using a specific query. This implies that the highest score using BM25 will be 1.\nIn parallel, the cosine similarity between the dense vector and the query embedding is calculated.\nThe cosine similarity value does not need to be normalised.\nFinally, we sum the scores using both methods and proceed to reorder the results and present them.\nThe retrieved method is illustrated in Figure 4.3.\nFigure 4.3: Lexical + Semantic Search System Retrieval Method\n51\n52\n5\nLegal Language Model\nContents\n5.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n5.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.3 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.4 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n5.5 Multilingual Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n5.6 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.7 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n53\n54\nThe embedding’s creation required a proper language model adapted to the Portuguese language\nand, more specifically, the Portuguese legal domain. This Chapter will explain the explored approaches\nand the final implementations of Legal-BERTimbau.10Advances in language representation using Neu-\nral Networks allowed for the usage of this concept in facilitating the transfer of “learned internal states of\nlarge pre-trained language models” [37,38]. Transfer learning is a technique that allows a model trained\nfor a general task to be later fine-tuned for other specific tasks.\nBERT contains numerous parameters, reaching over 300 M on the large version. Training a BERT\nmodel from scratch, with a relatively small dataset, would cause overfitting. In our case, having nearly 30\n000 documents, the correct approach is to use a model to fulfil our needs that is already trained with a\nlarger corpus than the one available in our domain. With BERTimbau (See Section 3.2) as a base model,\nwe were able to create our own model for finding similar passages, a SBERT model: Legal-BERTimbau.\nChapter 5 provides an overview of the training involved in the generation of Legal-BERTimbau. It\nshares an overview of the different techniques used for each different version of the model. Figure 5\npresents an overview of the different training tasks through this research to attain the different model\nversions. The presented additional tasks were combined with each other to generate multiple model\nversions.\n5.1 Domain Adaptation\nDomain adaptation is a type of fine-tuning regarding language models. As the name suggests, the\ngoal is that the model can further understand a new domain. This scenario arises when a model is\npre-trained on a specific dataset but needs to be used with a different (but related) dataset. In our\ncontext, BERTimbau was trained with a large Portuguese corpus, BrWaC, but we wanted to use a\nmodel on Portuguese jurisprudence. Legal documents are very distinct from BrWaC, only sharing the\nsame language. Documents provided by official courts should contain text better structured than BrWaC\nand contain some jargon and technical language of the domain. Over the years, multiple techniques\nhave been aiming to adapt a language model to a new domain. This stage was done on the BERTimbau\nlarge variant, which can produce embeddings of 1024 dimensions. The Domain Adaptation stages were\nperformed on a NVIDIA GeForce RTX 3090 24 GB GPU. The developed variants can be easily used\nwith SentenceTransformers Python Library, TensorFlow, PyTorch, or JAX, since each model is hosted\non the HuggingFace Platform, using the HuggingFace’s Transformers library.\n10Models available on https://huggingface.co/stjiris\n55\nFigure 5.1: Training Tasks Overview\n5.1.1 Masked Language Modeling\nMLM, as mentioned in Section 3.1, is a task originally introduced by BERT. The training consisted\nin applying the traditional BERT MLM training over our training dataset. With this approach, the model\nbecame more familiarized with technical language or jargon presented in those documents. For the MLM\ntask, we defined the learning rate as 10−5. We want the learning rate in this stage to be significantly lower\nthan in the initial training stage itself. Since we are training the model with such numerous parameters\non a rather small dataset, it would easily overfit. In the same vein, the performed fine-tuning was carried\nout by employing the procedure for a single epoch. This fine-tuning stage, performed with a batch size\nof 2, generated a BERTimbau variant. The loss associated with the training process can be shown in\nthe following image:\nThe selected MLM model variant is the one obtained at the 770 training steps mark. The model\nselection was based on the best model performance within our evaluation split. The variant that was\n56\nFigure 5.2: MLM Training Loss\ncreated is:\n• stjiris/bert-large-portuguese-cased-legal-mlm\n5.1.2 Transformer-based Sequential Denoising Auto-Encoder\nAs described in Subsection 3.1.1.B, TSDAE is an unsupervised sentence embedding approach. TSDAE\nencodes damaged sentences into fixed-sized vectors during training and needs the decoder to recover\nthe original sentences from this sentence embedding.\nWhen using the TSDAE technique, we used a learning rate of 5∗10−6over our training dataset and\na batch size of 2. The loss associated with the training process can be shown in the Figure 5.1.2.\nThe selected TSDAE model variant was the one with the lowest loss value: 1300 training steps. The\nloss value was calculated using the evaluation split. The variant that was created is:\n• stjiris/bert-large-portuguese-cased-legal-tsdae\n5.2 Semantic Textual Similarity\nThe task our language model needs to perform is STS evaluation. STS is a regression task that deter-\nmines how similar two text segments are on a numeric scale, ranging from 1 to 5.\nTo adapt the generated variants to this task, we created SBERT versions of themselves and trained\nthem with four distinct datasets. We attached an independent linear layer to each Legal-BERTimbau\n57\nFigure 5.3: TSDAE Training Loss\nvariant and fine-tuned the model using a mean squared error loss. The SBERT version of Legal-\nBERTimbau-large, utilising the SentenceTransformer library, is defined as follows:\n1bertmodelname = 'rufimelo/Legal-BERTimbau-large'\n2wordembedding model = models.Transformer(bert modelname, max seqlength=256)\n3pooling model = models.Pooling(\n4 wordembedding model.get wordembedding dimension())\n5densemodel = models.Dense(\n6 infeatures=pooling model.get sentence embedding dimension(),\n7 outfeatures=256,\n8 activation function=nn.Tanh())\n9model = SentenceTransformer(modules=[word embedding model, pooling model, dense model])\nThis code snippet shows how we can create a SBERT model from scratch, using a BERT model as\nthe foundation. Since we did not add a Pooling layer in our architecture, which would lower the accuracy\nof the embeddings for the STS task, this SBERT model variant generates 1024 dimension embeddings.\nTo train the models for the STS task, the datasets assin [14] and assin2 [30] were used, as well as\nthe stsb multi mt [23] Portuguese sub-dataset. Each dataset contained pairs of sentences and a label\nvalue representing both sentences’ similarities.\nThe assin dataset contains 10 000 pairs of sentences, 5 000 of which were used for training. Similarly,\nthe assin2 dataset contains 9 448 pairs of sentences, from which 6 500 were also used for training.\nFinally, stsb multi mt Portuguese sub-dataset contains 8 628 pairs of sentences, from which we used\n5749 to fine-tune the model. In a nutshell, for the STS task, our models were trained with 20 197\n58\nPortuguese sentence pairs, allowing the model to be more familiarized with the Portuguese language.\nBoth assin and assin2 are Brazilian Portuguese datasets.\nFollowing the MLM and TSDAE domain adaptation performed on BERTimbau, we trained the large\nversion with a learning rate of 10−5, making use of the Adam optimization algorithm [21]. We trained\nwith a batch size of 8 during five epochs.\nThis type of fine-tuning, generated the following SBERT variants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-sts-v0\n5.2.1 Semantic Textual Similarity Custom Dataset\nThe solution we present implies training using the STS task. The resources available for this effect\nare slim. For this effect, there are only 3 Portuguese STS datasets: assin ,assin2 andstsb multi mt\nPortuguese sub-dataset, which is a translation from the English sub-dataset version. On top of this fact,\nthe Portuguese legal domain is unique on its own. To improve the STS task, we developed a unique\ndataset for further training our models. The dataset is publicly available at HuggingFace:\n• stjiris/IRIS sts\nThe dataset creation process was automated. The dataset, similarly to assin and assin2, contained\nrelatedness scores from 0 to 5. When training, these scores are normalized for values ranging between\n0 and 1. Sentence pairs selected randomly across our document collection were given relatedness\nvalues from 0 to 1. Values 1 to 4 were attributed to sentence pairs selected from the same summary.\nThe summaries are short, and thus, they might imply some entailment. Finally, we selected sentences\nfrom our collection and proceeded to generate their pairs using OpenAI’s GPT3 text-davinci-003 model\nAPI, publicly available since November 29th11. Such pairs received a relatedness score from 4 to 5. The\nGPT3 model received the following request:\n• ”Escreve por outras palavras: Entrada: sentence Sa´ıda:”\n, which translates to:\n• ”Write, in other words: Input: sentence Output:”\nThis custom dataset also presents NLI annotations. Sentences pairs with relatedness values above\nfour were given a “2” as the entailment label, meaning they are entailed. Pairs with relatedness scores\n11https://beta.openai.com/playground\n59\nbetween one and four were given a “1” as the entailment label, meaning those sentences have no rela-\ntionship. Finally, sentence pairs with a relatedness score below one were associated with an entailment\nlabel of “0”, meaning they are contradictory.\nSimilarly to the STS fine-tuning stage described in BERTimbau’s paper, the models were trained with\na learning rate of 10−5, also making use of the Adam optimization algorithm, but only performed on a\nbatch size of 8during five epochs.\nThis fine-tuning, with a custom dataset, generated various SBERT variants. To differentiate the\nmodels that were trained on this custom STS dataset, we denoted sts-v0 when it was trained on the\nthree original datasets and sts-v1 when a model was trained on all four datasets, including /IRIS sts.\nWe utilised this new dataset and trained the following variants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-sts-v1\n5.3 Natural Language Inference\nAccording to [31], a slight STS performance improvement when the models were subjected to NLI data\nis reported (See Section 3.1.2.A). The previously used assin and assin2 datasets also contain informa-\ntion about the relatedness between sentence pairs. Similarly to SNLI, assin contains a label feature\nindicating if a sentence entails the other (0), if they have no apparent relationship between them (1)\nor if they contradict each other (2). In the case of assin2 it contains labels 1 or 0, representing if the\nsentences are entailed or not, respectively.\nWe trained the large models on assin and assin2 NLI information with an 8 batch size for five epochs\nwith a learning rate of 10−5, using the Adam optimization algorithm. The variants produced with this\napproach outperform others that do not. (Discussed in more detail in Chapter 6)\nThis fine-tuning, combined with STS, generated 4 different SBERT variants. To denote the models\nsubjected to NLI data, we add nliin the model name.\nAdding this type of differentiation, we trained the following models.\n• stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v1\n60\n5.4 Generative Pseudo Labeling\nGPL, as mentioned in Subsection 3.1.1.D, is a state-of-the-art unsupervised technique to fine-tune ex-\nisting models in different domain. This technique allows a model to understand which sentences can\nanswer a question.\nAs explained, GPL has three different stages: Query Generation (from GenQ), Negative Mining and\nPseudo Labeling.\nIn the Query Generation step, we created 10000 Queries for 10000 legal documents. We used\na pre-trained T5 model, fine-tuned for the Portuguese Language, pierreguillou/t5-base-qa-squad-v1.1-\nportuguese12, to generate queries from each document summary. After this step, we have a collection\nof queries that each summary (positive passage) should be able to answer individually.\nIn the Negative Mining stage, we retrieved passages very similar to our initial passage, but that should\nnot be able to answer the generated queries. For this purpose, we created an index on ElasticSearch\nwhere we stored the embeddings of the other summaries used for the previous step. To reduce the bias\nin the system, we used an original BERTimbau large fine-tuned for STS. This model was fine-tuned\nfollowing the guidelines in the original paper. We used assin and assin2 datasets for five epochs, using\n3∗10−5for the learning rate.\nIn the final step, we used the same model to calculate the margin between positive and negative\npassages using the dot product. We trained our models using the created triplets (positive passage,\nnegative passage and margin score), applying the Margin Mean Squared Error Loss with a learning rate\nof2∗10−5on one epoch. The variants produced with this approach outperform the variants that were\nnot subject to this technique. (Discussed in more detail in Chapter 6)\nThis technique was applied to models that were trained using both NLI and STS data.\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1\n12https://huggingface.co/pierreguillou/t5-base-qa-squad-v1.1-portuguese\n61\n5.5 Multilingual Knowledge Distillation\nMKD is a technique developed by Neil Reimers, as stated in Subsection 3.1.2.B. This sort of technique\nallows for a model to extend its knowledge over a language, i.e. English, to different languages such as\nPortuguese. This technique is attractive, especially when we intend to create a model that should learn\na language. However, only a few datasets are available in that same language.\nIn this work, we developed a language model that utilises this technique. However, the goal was\nnot to create a multilingual model, but rather to improve the knowledge a model already has of the\nPortuguese language.\nThe dataset used was: TED 2020 – Parallel Sentences Corpus [33]. TED 2020 contains around\n4000 TED13and TED-X transcripts from July 2020. These transcripts were translated by volunteers into\nmore than 100 languages, adding up to a total of 10 544 174 sentences. All the sentences were aligned\nto generate a parallel corpus for training tasks such as this.\nWith the explained end goal, this technique was applied to Legal-BERTimbau-large . It was desig-\nnated as the student model, supporting the English Language already, and we intended for it to learn\nPortuguese. The chosen teacher model was sentence-transformers/stsb-roberta-large14. It was defined\nthat the number of warm-up steps should be 10000 . The training was performed with a 10−5learning\nrate using the Adam optimization algorithm during five epochs.\nFurthermore, after this extra training step, we fine-tuned the models for the STS regression task as\ndescribed in Section 5.2. With the integration of this different application of the technique, it was possible\nto further train a model for the Portuguese language by mimicking a teacher model that knew how to\nencode English Sentences properly.\nThe application of the MKD technique and the classical STS fine-tuning produced a different BERT\nvariants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1\n13https://www.ted.com/\n14https://huggingface.co/sentence-transformers/stsb-roberta-large\n62\n5.6 Metadata Knowledge Distillation\nThis work presents a new technique developed to improve information retrieval through dense vectors:\nMetadata Knowledge Distillation (MetaKD)\nIn our dataset, multiple documents have associated with them “Descritores”, brief tags manually\nannotated by experts. These tags intend to identify the main document subjects. These tags could\nindicate if a crime was committed with knives or even if it is related to COVID-19.\nWith such annotation, we assumed that the documents are, in a way, related to one another. Thus,\nthe sentences from each document have some trim level of entailment between each other.\nWe started by identifying the documents related to a subject, COVID-19, i.e. and we proceeded\nto encode those documents’ sentences. The generated embeddings form a cluster. We processed to\ncalculate the centroid of those embeddings and adjusted the embeddings slightly to the centroid. (1-5%)\nThis minor adjustment is based on the assumption that those sentences are related and, thus, they\nshould be closer to one another. This process is done through the tags we have available. This ideology\ncan be shown in Figure 5.6. Finally, the updated embeddings will serve as gold labels for what the\nembeddings of the same model should look like. We then applied the mean-squared error loss, similar\nto Multilingual Knowledge Distillation, to train the model. The process is illustrated in Figure 5.6.\nFigure 5.4: Metadata Knowledge Distillation Ideology\nWe used this technique with a learning rate of 10−6and a batch size of 3sentences for one epoch.\nWe adjusted the embeddings of a 1000 document sample selected at random from the training document\nsubset. The embeddings were adjusted based on each tag’s centroids, centralizing the embeddings by\n1%. These hyperparameter choices were defined based on a grid search optimization algorithm where\nwe tried to maximize the STS task evaluation. With this technique, the following models were generated:\n63\nFigure 5.5: Metadata Knowledge Distillation\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-MetaKD-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-MetaKD-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\n5.7 Overview\nTable 5.1 provides an intuitive summary of the different versions created throughout this work, where it\npoints out the different techniques used for each variant.\nModel MLM TSDAE STS NLI GPL MKD MetaKD\nmlm x\ntsdae x\nmlm-sts-v0 x x\ntsdae-sts-v0 x x\nmlm-sts-v1 x x\ntsdae-sts-v1 x x\nmlm-nli-sts-v0 x x x\nmlm-nli-sts-v1 x x x\ntsdae-nli-sts-v0 x × x\ntsdae-nli-sts-v1 x x x\nmlm-gpl-nli-sts-v0 x x x x\nmlm-gpl-nli-sts-v1 x x x x\ntsdae-gpl-nli-sts-v0 x × x x\ntsdae-gpl-nli-sts-v1 x x x x\nmlm-mkd-nli-sts-v0 x x x x\nmlm-mkd-nli-sts-v1 x x x x\ntsdae-mkd-nli-sts-v0 x × x x\ntsdae-mkd-nli-sts-v1 x x x x\nmlm-gpl-nli-sts-MetaKD-v0 x x x x x\nmlm-gpl-nli-sts-MetaKD-v1 x x x x x\ntsdae-gpl-nli-sts-MetaKD-v0 x x x x x\ntsdae-gpl-nli-sts-MetaKD-v1 x x x x x\nTable 5.1: Legal-BERTimbau variants\n64\n6\nSystem Evaluation\nContents\n6.1 Language Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.2 Search System Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n65\n66\nThis work aimed to improve the information retrieval process court professionals have to go through,\nwith the ultimate goal of helping in the court decision process. Judges would have to go through immense\nand often dispersed information to formulate a decision. This process is well known for being time-\nconsuming and complex, and there is no guarantee that all the relevant documents are retrieved for user\nconsultation. A poor-made decision based on incomplete information can open a precedent for future\nscenarios and should be avoided.\nEvaluating the performance of a search system is a critical task in information retrieval research. A\ngood evaluation methodology can provide insights into the strengths and weaknesses of the system and\nguide the development of future improvements. To realise the search system’s performance, we require\nexamples of queries and expected results. Unfortunately, the required data was not available, and\ngenerating the aforementioned subsets of data is a complex process since it requires manual annotation.\nAnd even trying to replicate those assets, can introduce bias to our evaluation system.\nChapter 6 provides insights into the project’s evaluation. Throughout this project, we explored differ-\nent search system architectures with different trained language models. These language models were\ntailored to our scenario, understanding the Portuguese legal context better than state-of-the-art generic\nmodels. Since the large language models play a critical role in our final solution, Chapter 6 also provides\ninsights into such models. As such, the evaluation is composed of two parts.\nFirstly, we evaluate the developed language models. We evaluate whether the developed models are\ntrimmed for the Portuguese legal context and their performance on their task: STS.\nThe second part will be a quantitative evaluation of the search system itself. We compare the perfor-\nmance of our solution, comparing it to traditional techniques such as BM25 on two different “dimensions”.\nSuch evaluation provides insights into the system’s ability to retrieve relevant information helpful to the\njudges.\n67\n6.1 Language Model Evaluation\nThis thesis led to the development of multiple Legal-BERTimbau versions. From fine-tuning large vari-\nants of BERTimbau to applying the Multilingual Knowledge Distillation technique to models, there were\nnumerous variants implemented. This section will provide the details on the evaluation performed on the\ndifferent Legal-BERTimbau variants. It starts by verifying if the models were successfully adapted to the\nPortuguese legal domain. Then it will verify if the variants have a good performance in the STS task.\n6.1.1 Domain Adaptation\nThe domain adaptation is a crucial technique as it helps to address the problem of model generalization,\nespecially when there is a lack of labelled data in the target domain. The importance of domain adapta-\ntion lies in its ability to improve the effectiveness of models when applied to new and unseen data (See\nSection 5.1).\nIn order to evaluate whether the domain adaptation stage was successful or not, we verified how\nwell the do Legal-BERTimbau variants handle Portuguese legal text. Such was done by comparing\nhow successful the Legal-BERTimbau could replace [MASK] tokens for the correct words in the MLM\ntask. We made use of the same loss function used in the MLM task, negative log-likelihood loss. In this\nscenario, a lower average loss value would indicate that the model performs well.\nWe utilised the testing dataset from the splits generated in Subsection 4.2 and compared the average\nloss produced (Shown in Table 6.1.1). We also submitted the models subjected to the TSDAE technique\nto this evaluation.\nAVG loss\nBERTimbau large 18.60706\nstjiris/bert-large-portuguese-cased-legal-mlm 0.02679\nstjiris/bert-large-portuguese-cased-legal-tsdae 10.51178\nTable 6.1: MLM average loss for legal documents in the test set\nRegarding our testing split, both stjiris/bert-large-portuguese-cased-legal-mlm andstjiris/bert-large-\nportuguese-cased-legal-tsdae perform better on the MLM task than BERTimbau . Both models sub-\njected to MLM and TSDAE domain adaptation techniques can predict more accurately which words\nshould replace in the [MASK] tokens. The model subjected to TSDAE presents a more significant aver-\nage loss than the model subjected to the MLM technique. This is to be expected since the model was\nnot trained on that same task. Nevertheless, despite not being trained for the MLM task, it still outper-\nformed BERTimbau for legal documents in our test set. Thus, validating our conjecture, our models are\nsuccessfully adapted to the Portuguese legal domain.\n68\n6.1.2 Semantic Textual Similarity\nThe main task of a large language model within our search system architecture is to determine how\nsemantically similar two sentences are, STS. To evaluate a model’s STS task performance, we evaluate\nif a model is able to provide accurate similarity scores for a given sentence pair. For such effect, it is\ncalculated the Pearson correlation [41] between the expected and projected similarity score between\ndifferent sentence pairs.\nThe Portuguese datasets used in the fine-tuning stage were also utilised to evaluate these mod-\nels. We proceeded to evaluate using those datasets’ test splits for the STS task. We can analyse the\nperformance of the different versions of Legal-BERTimbau against state-of-the-art multilingual models\nwhen performing the same task. These state-of-the-art multilingual models serve as a baseline for our\nevaluation. For this effect, we selected the following multilingual models: paraphrase-multilingual-mpnet-\nbase-v2 andall-mpnet-base-v2\nassin assin2 stsb multi mt Avg.\npt\nBERTimbau large Fine-tuned for STS 0.81289 0.84133 0.77958 0.81126\nparaphrase-multilingual-mpnet-base-v2 0.74373 0.83999 0.71468 0.76613\nall-mpnet-base-v2 0.56306 0.62126 0.51287 0.56573\nmlm-sts-v0 0.78509 0.81158 0.83625 0.81097\nmlm-nli-sts-v0 0.78095 0.81001 0.83684 0.80927\nmlm-gpl-nli-sts-v0 0.78119 0.81187 0.83543 0.80950\nmlm-mkd-nli-sts-v0 0.77634 0.80976 0.84779 0.81130\ntsdae-sts-v0 0.78597 0.81542 0.84424 0.81521\ntsdae-nli-sts-v0 0.78430 0.80311 0.83842 0.80861\ntsdae-gpl-nli-sts-v0 0.77862 0.80675 0.83925 0.80821\ntsdae-mkd-nli-sts-v0 0.78008 0.84145 0.85060 0.80503\nmlm-gpl-nli-sts-MetaKD-v0 0.81115 0.83634 0.78210 0.80987\ntsdae-gpl-nli-sts-MetaKD-v0 0.80743 0.84041 0.78294 0.81026\nmlm-sts-v1 0.78025 0.81479 0.83460 0.80988\nmlm-nli-sts-v1 0.77740 0.80975 0.83588 0.80768\nmlm-gpl-nli-sts-v1 0.78143 0.80964 0.83610 0.80905\nmlm-mkd-nli-sts-v1 0.77274 0.812149 0.84997 0.81162\ntsdae-sts-v1 0.78433 0.81610 0.84320 0.81454\ntsdae-nli-sts-v1 0.78251 0.80494 0.84077 0.80941\ntsdae-gpl-nli-sts-v1 0.77634 0.80673 0.83889 0.80731\ntsdae-mkd-nli-sts-v1 0.77368 0.81603 0.85495 0.81489\nmlm-gpl-nli-sts-MetaKD-v1 0.80767 0.83701 0.77955 0.80807\ntsdae-gpl-nli-sts-MetaKD-v1 0.80543 0.83467 0.77749 0.80586\nTable 6.2: STS evaluation on Portuguese datasets\nIt is possible to verify that our SBERT variants performed better than state-of-the-art multilingual mod-\nels on the STS task for both assin andassin2 datasets. Regarding the performance on the stsb multi mt\ndataset, the values obtained do not outperform multilingual models. stsb multi mtis a dataset composed\nof different multilingual translations from the original STSbenchmark dataset. Consequently, multilingual\nmodels did engage with multiple translations from the same sentence during the training process. Nev-\nertheless, the score is similar, and, more importantly, BERTimbau variants are adapted to our domain,\nas exposed previously. This quantitive evaluation aims to verify if our models still understand the Por-\n69\ntuguese Language in general and can comprehend the sentence similarities in our domain through our\ncustom STS dataset.\n6.2 Search System Evaluation\n6.2.1 Automatic Query Generation\nIn order to evaluate our search system, we needed a group of queries and expected retrieved results to\nevaluate its performance. In our scenario, there is no such group of data for this effect. For this reason,\nwe had to explore an automatic generation of examples to provide a preliminary evaluation – or at least\nto help us understand the potential performance of the system. With such queries and information about\nthe document from which they were generated, we are able to assess our system’s performance.\nOur solution to evaluate the system performance passed through creating embeddings from a col-\nlection of 1000 legal documents and store them in ElasticSearch. Then we generate queries from each\ndocument and utilise those same queries to test the system. Our assumption is that a search system\nshould be able to return as a result the original document (or passage) used to automatically build a\nparticular query. The evaluation architecture was implemented as described in Figure 6.2.1.\nFigure 6.1: Evaluation Architecture\n70\nThe first step is related to creating queries from each document summary that we use later on. For\nthis effect, we tried two different approaches.\nInitially, we utilised the LexRank summarization technique, using the original BERTimbau large\nmodel, to retrieve the sentence with the highest centrality. This approach would provide the most impor-\ntant sentence in the summary, but it would yield a sentence that contains no new words, meaning we are\nnot exploring the full capabilities of a Semantic Search System. If a user inputs a query with words that\ndo not appear within the document, the lexical techniques would not provide adequate results. Thus,\naiming to explore the full extent of this scenario, we processed all the queries through a GPT3 model\nprovided by Open AI to rewrite the sentences, whilst keeping the same meaning. Unfortunately, this ap-\nproach did not provide the query examples we were hoping for. Often it would use the exact keywords,\nonly in a different order.\nAfter further iterations, we utilised a T5 model to generate queries reducing bias. Nevertheless, this\napproach lead to queries that made use of keywords present within the summary. Subsequently, we\ntreated the queries, so that they maintained a similar meaning, but did not contain every exact key-\nword. We identified the top 20 keywords from the summary with TF-IDF and exchanged them with syn-\nonyms or similar expressions. The synonyms gathering was performed by producing multiple requests\ntohttps://www.sinonimos.com.br/ , a website that provides synonyms for Brazilian Portuguese words.\nThe selected synonyms and expressions that replaced the keywords were chosen by evaluating which\nexchange would better preserve the meaning of the queries. To do so, we used BERTimbau large and\nevaluated the different sentences using cosine similarity.\nThe second step was creating embeddings for each sentence using different versions of our devel-\noped model, Legal-BERTimbau. Each sentence embedding is stored in an ElasticSearch index. Each\nindexed document would be composed of a text data field, a dense vector data field that contained the\nsentence embedding, and an indication of the document from where the sentence was retrieved.\n6.2.2 Results\nWe retrieve the results using the cosine similarity metric for the Semantic Search System and combined\nthe cosine similarity metric with BM25 technique to replicate the proposed Hybrid Search Systems. Our\ncomparison baseline consisted on utilising BM25 searches with the same queries and other multilingual\nmodels such as “sentence-transformers/all-mpnet-base-v2” and “paraphrase-multilingual-mpnet-base-\nv2” instead of Legal-BERTimbau versions. The evaluation dataset, since it was generated automatically,\ndid not allow an overview of False or True Positives, and neither it did for False and True Negatives.\nAs such, we could not use traditional metrics such as Precision and Recall to evaluate the system’s\nperformance. We defined two alternative metrics to evaluate the system’s performance: Search and\nDiscovery.\n71\nThe Search metric allows gathering insights into the system’s ability to find which document a certain\nquery refers to. Initially, each query was generated from individual summaries. If the search system re-\nceives a query based on document x, the retrieved result should be from the document x. We evaluated\nwhether this happens within the first result or the first group of results for a specific query. If the retrieved\ndocument is the same as the one used for creating the query, it increases the evaluation score by 1.\nThe Discovery metric provides interpretability on a search system’s ability to retrieve additional doc-\numents that might be relevant to the user. The search system should retrieve documents that are\nimportant for a given query, even if they are not the original document from which the query was gen-\nerated. Each legal document has “descritores” (tags) that were manually annotated. Similarly to the\nSearch metric, if the retrieved document within a group of results contains a tag equal to the original\ndocument’s tags from which the query was generated, we increase the score. For each equal tag, the\nDiscovery metric score is increased by one, calculating the intersection between each retrieved passage\nand the original document tags. In practice, if the search system receives a query that was created from\na document ycontaining the tag “Knives”, we want to evaluate whether the same tags appear within the\ntop results.\nSearch metric results for models fine-tuned without and with the custom STS dataset are shown in\nFigures 6.2 and 6.3, respectively.\nWe evaluated the Semantic Search System performance based on the different top results sizes. We\nverified whether the queries’ original document is suggested within the first result (Top 1), the first two\nresults (Top 2), and so forth. Likewise, we evaluated the Top 1, Top 2, Top 3, Top 5, Top 10, and Top 20.\nBM25 outperforms our original Semantic Search System in the Search metric. It also shows that\na Semantic Search System using a Multilingual model would perform considerably worse than when\nusing Legal-BERTimbau. For this metric, the Lexical-First approach that we proposed performs closer to\nBM25 and the Lexical+Semantic can even occasionally surpass BM25, maintaining a tight performance\nbetween the two. This metric verifies that BM25 can identify a query source better than a Semantic\nSearch System. However, a Hybrid Search System such as the one we developed can match and even\noutperform BM25 capabilities. Search Systems using models that were fine-tuned on the custom STS\ndataset (V1 models) present a slightly lower performance than the ones fine-tuned only on pre-existing\nand manually annotated datasets (V0 models).\nSimilarly, for the Discovery metric, the system performance using models fine-tuned without and with\nthe custom STS dataset are shown in Figures 6.4 and 6.5, respectively.\n72\nFigure 6.2: Search System Evaluation – Search metric - Models V0\nWe evaluated the Semantic Search System performance based on the different top results sizes.\nWe verified whether the queries’ original document tags appeared within the first result (Top 1), the first\ntwo results (Top 2), and so forth. Likewise, we also evaluated the Top 1, Top 2, Top 3, Top 5, Top 10,\n73\nFigure 6.3: Search System Evaluation – Search metric - Models V1\nand Top 20. Search Systems using models fine-tuned on the custom STS dataset (V1 models) perform\nmarginally worse than those fine-tuned on pre-existing and manually annotated datasets (V0 models).\nThis metric shows that BM25 does not have the same capability to recommend similar documents.\nWhen searching for a specific matter, it is often beneficial for a Search System to retrieve relevant docu-\nments we were unaware of. As such, we can verify that our Semantic Search System can suggest similar\n74\nFigure 6.4: Search System Evaluation - Discovery metric - Models V0\ndocuments more regularly than BM25 or any other search system using a multilingual model. Surpris-\ningly, both our Lexical-First and Lexical+Semantic Search Systems outperform our semantic search\nsystem significantly.\n75\nFigure 6.5: Search System Evaluation - Discovery metric - Models V1\nWe averaged the Top 1, Top 2, Top 3, Top 5, Top 10, and Top 20 metric results for each metric, raising\nsome understanding of the techniques used in the research. The variations that received NLI training\nhad a 4.3%improvement in the Search metric and a 5.4%improvement in the Discovery measure.\nModels subjected to the GPL training approach show a 3.2%improvement in the Search metric and a\n1.7%improvement in the Discovery metric. Finally, models that used our novel method exhibited a 2.0%\n76\ndrop in the Search metric but a 1.7%decrease in the Discovery metric.\nWith these results, we selected our best-performing model, tsdae-gpl-nli-sts-MetaKD-v0, to analyse\nin more detail. Tables 6.3 and 6.4 provide more quantitive insights on the development.\nModel Top 1 Top 2 Top 3 Top 5 Top 10\nBM25 629 696 722 760 799\nPurely Semantic 411 471 518 559 618\nLexical-First 581 635 668 716 754\nLexical + Semantic 629 675 705 734 785\nImprovement 0% -3.01 % -2.35% -3.42% -1.75%\nTable 6.3: Search System Evaluation – Search metric – Best Model\nModel Top 1 Top 2 Top 3 Top 5 Top 10\nBM25 685 933 1133 1482 2216\nPurely Semantic 2226 2932 3532 4332 6243\nLexical-First 2769 3485 3993 4835 6352\nLexical + Semantic 2984 3732 4292 5168 7282\nImprovement 335% 300 % 278% 248% 228%\nTable 6.4: Search System Evaluation – Discovery metric – Best Model\nOur work explored three different search system architectures, and we concluded that combining\nboth lexical and semantic capabilities is a better approach than using only technique isolated. The\nLexical + Semantic Search System, which combines the strengths of both lexical and semantic systems,\nperformed similarly to BM25 in the Search metric, with performance delta of around −2.1%. In the\nDiscovery metric, it clearly outperforms the BM25 technique, going from around 335% improvement on\nthe Top 1 results and topping at 228% improvement on the Top 10 results. Such outcomes suggest that\nthe proposed search systems can improve the decision-making process in the legal domain by providing\nrelevant and insightful information.\nWhen considering these results, we can argue that our proposed and developed Lexical + Semantic\nSearch System can provide beneficial results and insights that ease and enrich the work judges perform\nin the legal domain. Similarly, this capability is extended to other professionals and non-professionals.\nBased on our research, we believe that the utilization of such search system can improve the actual\ncourt decision process and reduce the limitation and risks of formulating a decision based on incomplete\ninformation.\n77\n78\n7\nConclusion\nContents\n7.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n79\n80\nThe work presented in this document comes from a growing necessity for providing complete infor-\nmation to judges and legal authorities. Formulating a decision or gathering insight from the vast legal\ndomain is a very complex and often time-consuming task. This work provided insights into creating a\nsystem that utilises semantic search through embeddings, exploring the immense potential of neural\nnetworks instead of using more traditional lexical techniques. We explored the capability of Elastic-\nSearch to host a semantic search system, developing the necessary language models along the way.\nWe handled different problems when treating raw text data, tackling the need to pre-process the text\nbefore performing further tasks. Working with multiple Project IRIS members, we develop a prototype of\na Semantic Search System for STJ and provided support for other related tasks.\nBeing a segment of a more significant project: Project IRIS, this thesis allowed me to develop soft\nand hard skills by interacting with outstanding professionals and doctorate students. It provided insight\ninto effectively researching daily and cooperating on a large project with multiple members.\n7.1 Contributions\nWe created initial datasets containing pre-processed text that can be utilised for future tasks and the\nnecessary scripts to recreate the results. Along the work, we created multiple language models that can\nbe fine-tuned for multiple tasks, such as Question-Answering. Legal-BERTimbau developed variants\nwere trained entirely in Portuguese and adapted to the Portuguese legal domain.\nOur Legal-BERTimbau variants represent the first publicly available Portuguese BERTs fine-tuned for\nSTS. On the same note, as mentioned previously, all the developed SBERT variants are adapted to the\nPortuguese legal domain since they suffered domain adaptation techniques. Multiple Legal-BERTimbau\nvariants surpass state-of-the-art multilingual language models on assin , and assin2 datasets, which\nPortuguese and Brazilian researchers annotated.\nAs intended with this thesis, we proposed and developed a Semantic Search System and two dif-\nferent versions of Hybrid Search Systems: Lexical-First and Lexical+Semantic Search Systems. Based\non our preliminary evaluation, we believe our developed Search System can suggest more accurately\nrelevant documents than BM25 with a Discovery performance improving over 300% in some scenarios,\nwhile still maintaining a similar ability to identify a query’s source when compared to the BM25 technique.\nWith such information retrieval systems in place, we consider that it is possible to more accurately\nprovide insights into jurisprudence, easing and improving judges’ essential work. Such Search System\ncan generate a positive impact throughout all Portuguese legal entities, helping to enrich and ease the\ndecision process inherent in each of these entities. Our Search System can play an important role in\nhelping formulate fair and essential decisions and maintaining the stability and consistency required in\napplying the law.\n81\n7.1.1 Publications\n7.1.1.A Exploring Embeddings Models for Portuguese Supreme Court Judgments Summariza-\ntion\nIn recent years, the field of legal text summarization has grown due to the increase in electronically\navailable legal documents. As part of a collaboration with project IRIS members, a scientific paper was\ndrafted, “Exploring Embeddings Models for Portuguese Supreme Court Judgments Summarization”.\nThe findings of this study will be submitted for publication in a peer-reviewed journal in the near future.\nLegal documents can differ based on language and legal system, and this paper proposes an ap-\nproach for automatically summarizing Portuguese Supreme Court judgments. Because of the unique\ncharacteristics of these judgments and the judges’ methods for summarizing them, the paper suggests\na set of pre-processing techniques to optimize the task. We implemented an extractive summarization\nsystem using the LexRank technique. To account for specific vocabulary in Portuguese that pre-trained\nmodels like BERTimbau or mBERT do not cover, we experimented with different tailored models. In\nthis work, multiple Legal-BERTimbau models originated from our research were used. The proposed\napproach achieves a ROUGE-1 score of 47.92 and a ROUGE-2 score of 22.50, better than reported\nscores for similar work in other languages.\n7.1.1.B Semantic Search System for Supremo Tribunal de Justic ¸a\nAs part of this thesis, it was drafted a paper covering the main aspects of this research and the impact\nit could have on the scientific domain. It was submitted to the EPIA Conference on Artificial Intelligence,\na well-established European conference in the field of AI, ended up being accepted.\nThe scientific paper covers the main most robust hybrid system that we present within this research,\nthe pure semantic search system, and the techniques used to train Legal-BERTimbau. The article\nalso emphasises our novel technique, Metadata Knowledge Distillation, as it brought good results in\nour environment. Such a technique can be further used in other domains and/or with other metadata\ninformation.\n7.2 Future Work\n7.2.1 Albertina\nWith the publication of Albertina PT -PT, in May 2023, there is now the possibility of training a model,\nadapted to the Portuguese jurisprudence, exclusively in European Portuguese. This implies that, inher-\nently, there should be improvements with this approach. It should translate into a more effective model\n82\nthat would better comprehend Portuguese legislation.\nAnother approach would be to train a European Portuguese model completely from scratch only on\nPortuguese Jurisprudence. Having a model only trained on a specific domain should yield improved\nresults over domain-agnostic or domain-adapted models.\n7.2.2 Dataset Annotation\nWhen developing a language model, one of the main concerns is the available data. Manually annotated\nPortuguese legal domain datasets could be produced and revised. Even though we developed datasets\nfor the Domain Adaption training step and STS, automating such can lead to biased and incoherent\ndata, especially the developed STS dataset.\nThe downside of this approach is that it requires manual work, nearly impossible to automatize. Both\nassin andassin2 were annotated manually by different groups of researchers/volunteers, oriented only\nby simple guidelines.\nNevertheless, a properly labelled and cleaned legal dataset from the Portuguese domain would be\nhelpful in future applications. Similarly, one dataset was developed and published, alongside a paper,\non 1st July 2022, entitled “Pile of Law: Learning Responsible Data Filtering from the Law and a 256 GB\nOpen-Source Legal Dataset” by Peter Henderson et al. [17]. It encompasses a large corpus of legal\nand administrative data from multiple U.S.A. entities. The Pile of Law paper also exposed the ethical\nchallenges it faced and how it was handled “biased, obscene, copyrighted, and private information”.\n7.2.3 Architecture Improvements\nIn terms of query expansion, one potential improvement to the Hybrid Search System is to incorporate\nknowledge graph embeddings. A knowledge graph is a structured representation of information, where\nentities and their relationships are represented by nodes and edges, respectively. By incorporating\nknowledge graph embeddings, the system would be able to understand the context of the query better\nand expand it with related entities and concepts, which would help in the relevant documents’ retrieval.\nAnother aspect that can be explored is the incorporation of additional information to the embeddings.\nAs explored in [11], it was inserted titles in the beginning of each sentence before generating the em-\nbedding. In [11], that approach did not provide satisfying results, but it can be interesting to explore\ninserting some metadata information into the embeddings.\nAnother approach is to use entity recognition, a technique that identifies named entities such as\npeople, organizations, and locations in a given text. By recognizing entities in the query, the system could\nexpand the query by including relevant entities, which would help retrieve more relevant documents.\nActive learning is a machine learning technique that enables the system to continuously learn from\n83\nthe user’s feedback. The system could present the user with a small set of documents, and the user\nwould provide feedback on which documents are relevant and which are not. The system would then\nuse this feedback to improve its performance over time. This approach can be especially useful in the\nlegal domain, where the relevance of a document can be challenging to determine, and expert feedback\ncan help the system improve its performance.\nImprovements in the prompts given to a GLM, such as for GPT -3, can provide a better model re-\nsponse regarding a user’s query. Having tailored prompts to either enrich the user’s query or to improve\nthe models’ response can guide to a more seamless interaction with the Search System and more\nconcise answers from the System itself.\nOverall, incorporating knowledge graph embeddings or entity recognition can lead to a more sophis-\nticated and accurate Search System that can help legal practitioners in their research and decision-\nmaking.\n84\nBibliography\n[1] A GIRRE , E., B ANEA , C., C ARDIE , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., G UO, W.,\nLOPEZ -GAZPIO , I., M ARITXALAR , M., M IHALCEA , R., R IGAU, G., U RIA, L., AND WIEBE, J.\nSemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In\nProceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) (Denver,\nColorado, 2015), ACL, pp. 252–263.\n[2] A GIRRE , E., B ANEA , C., C ARDIE , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., G UO, W.,\nMIHALCEA , R., R IGAU, G., AND WIEBE, J. SemEval-2014 task 10: Multilingual semantic textual\nsimilarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval\n2014) (Dublin, Ireland, 2014), ACL, pp. 81–91.\n[3] A GIRRE , E., B ANEA , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., M IHALCEA , R., R IGAU, G.,\nAND WIEBE, J. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual\nevaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-\n2016) (San Diego, California, 2016), pp. 497–511.\n[4] A GIRRE , E., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., AND GUO, W. *SEM 2013 shared task:\nSemantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics\n(*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual\nSimilarity (Atlanta, Georgia, USA, 2013), ACL, pp. 32–43.\n[5] A GIRRE , E., D IAB, M., C ER, D., AND GONZALEZ -AGIRRE , A. SemEval-2012 task 6: A pilot on se-\nmantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational\nSemantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2:\nProceedings of the Sixth International Workshop on Semantic Evaluation (USA, 2012), SemEval\n’12, ACL, p. 385–393.\n[6] B OOS, R., P RESTES , K., V ILLAVICENCIO , A., AND PADR´O, M. brWaC: A WaCky Corpus for Brazil-\nian Portuguese. In Computational Processing of the Portuguese Language (Cham, 2014), J. Bap-\n85\ntista, N. Mamede, S. Candeias, I. Paraboni, T. A. S. Pardo, and M. d. G. Volpe Nunes, Eds., Springer\nInternational Publishing, pp. 201–206.\n[7] B OWMAN , S. R., A NGELI , G., P OTTS , C., AND MANNING , C. D. A large annotated corpus for\nlearning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods\nin Natural Language Processing (Lisbon, Portugal, 2015), ACL, pp. 632–642.\n[8] C ER, D., D IAB, M., A GIRRE , E., L OPEZ -GAZPIO , I., AND SPECIA , L. SemEval-2017 task 1: Se-\nmantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the\n11th International Workshop on Semantic Evaluation (SemEval-2017) (Vancouver, Canada, 2017),\nAssociation for Computational Linguistics, pp. 1–14.\n[9] C ONNEAU , A., K IELA, D., S CHWENK , H., B ARRAULT , L., AND BORDES , A. Supervised learning\nof universal sentence representations from natural language inference data. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Language Processing (Copenhagen, Denmark,\n2017), Association for Computational Linguistics, pp. 670–680.\n[10] C ORDEIRO , N. NLP applied to portuguese consumer law. Master’s thesis, Instituto Superior\nT´ecnico, Universidade de Lisboa, 2022.\n[11] D AI, Z., AND CALLAN , J. Deeper text understanding for IR with contextual neural language model-\ning. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval (2019), ACM.\n[12] D EVLIN , J., C HANG , M.-W., L EE, K., ANDTOUTANOVA , K. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the ACL: Human Language Technologies, Vol 1 (Minneapolis, Minnesota,\n2019), ACL, pp. 4171–4186.\n[13] E RKAN , G., AND RADEV , D. R. Lexrank: Graph-based lexical centrality as salience in text summa-\nrization. J. Artif. Int. Res. 22 , 1 (2004), 457–479.\n[14] F ONSECA , E., S ANTOS , L., C RISCUOLO , M., AND ALUISIO , S. ASSIN: Avaliacao de similaridade\nsemantica e inferencia textual. In Computational Processing of the Portuguese Language-12th\nInternational Conference, Tomar, Portugal (2016), pp. 13–15.\n[15] G URURANGAN , S., M ARASOVI ´C, A., S WAYAMDIPTA , S., L O, K., B ELTAGY , I., D OWNEY , D., AND\nSMITH , N. A. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings\nof ACL (2020).\n[16] H E, P., L IU, X., G AO, J., AND CHEN, W. Deberta: Decoding-enhanced BERT with disentangled\nattention. In 2021 International Conference on Learning Representations (2021).\n86\n[17] H ENDERSON , P., K RASS , M. S., Z HENG , L., G UHA, N., M ANNING , C. D., J URAFSKY , D., AND\nHO, D. E. Pile of law: Learning responsible data filtering from the law and a 256GB open-source\nlegal dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (2022).\n[18] I NTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY AND EXPLORING ENGINEERING (IJITEE),\nVAISSNAVE , V., AND DEEPALAKSHMI , P. An Artificial Intelligence based Analysis in Legal domain.\n[19] K IM, M., R ABELO , J., AND GOEBEL , R. BM25 and transformer-based legal information extraction\nand entailment. In Proceedings of the COLIEE Workshop in ICAIL (2021).\n[20] K IM, S.-W., AND GIL, J.-M. Research paper classification systems based on TF-IDF and LDA\nschemes. In Human-centric Computing and Information Sciences (2019), vol. 9, p. 30.\n[21] K INGMA , D. P., ANDBA, J. Adam: A method for stochastic optimization. In 3rd International Confer-\nence on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings (2015), Y . Bengio and Y . LeCun, Eds.\n[22] M ARELLI , M., M ENINI , S., B ARONI , M., B ENTIVOGLI , L., B ERNARDI , R., AND ZAMPARELLI , R. A\nSICK cure for the evaluation of compositional distributional semantic models. In Proceedings of\nthe Ninth International Conference on Language Resources and Evaluation (LREC’14) (Reykjavik,\nIceland, 2014), European Language Resources Association (ELRA), pp. 216–223.\n[23] M AY, P. IMachine translated multilingual STS benchmark dataset. HuggingFace, stsb multi mt.\n[24] M IKOLOV , T., C HEN, K., C ORRADO , G., AND DEAN, J. Efficient estimation of word representations\nin vector space. In 1st International Conference on Learning Representations, ICLR 2013, Scotts-\ndale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings (2013), Y . Bengio and Y . LeCun,\nEds.\n[25] N GUYEN , H., V UONG , H. T., N GUYEN , P. M., D ANG, T. B., B UI, Q. M., S INH, V. T., N GUYEN ,\nC. M., T RAN, V. D., S ATOH , K., AND NGUYEN , M. L. JNLP team: Deep learning for legal process-\ning in COLIEE 2020. CoRR abs/2011.08071 (2020).\n[26] N GUYEN , T., R OSENBERG , M., S ONG, X., G AO, J., T IWARY , S., M AJUMDER , R., AND DENG, L.\nMS MARCO: A human generated machine reading comprehension dataset. CoRR abs/1611.09268\n(2016).\n[27] N IGAM , S. K., G OEL, N., AND BHATTACHARYA , A. nigam@coliee-22: Legal case retrieval and en-\ntailment using cascading of lexical and semantic-based models. In New Frontiers in Artificial Intelli-\ngence (Cham, 2023), Y . Takama, K. Y ada, K. Satoh, and S. Arai, Eds., Springer Nature Switzerland,\npp. 96–108.\n87\n[28] P ENNINGTON , J., S OCHER , R., AND MANNING , C. GloVe: Global vectors for word representation.\nInProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) (Doha, Qatar, 2014), Association for Computational Linguistics, pp. 1532–1543.\n[29] R AFFEL , C., S HAZEER , N., R OBERTS , A., L EE, K., N ARANG , S., M ATENA , M., Z HOU, Y., L I, W.,\nANDLIU, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal\nof Machine Learning Research 21 , 140 (2020), 1–67.\n[30] R EAL, L., F ONSECA , E., AND OLIVEIRA , H. G. The assin 2 shared task: a quick overview. In Inter-\nnational Conference on Computational Processing of the Portuguese Language (2020), Springer,\npp. 406–412.\n[31] R EIMERS , N., AND GUREVYCH , I. Sentence-BERT: Sentence Embeddings using Siamese BERT -\nNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing (2019), ACL.\n[32] R EIMERS , N., AND GUREVYCH , I. Making monolingual sentence embeddings multilingual using\nknowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (2020), ACL.\n[33] R EIMERS , N., AND GUREVYCH , I. Making monolingual sentence embeddings multilingual using\nknowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (2020), Association for Computational Linguistics.\n[34] R OBERTSON , S., AND ZARAGOZA , H. The Probabilistic Relevance Framework: BM25 and Beyond.\nFound. Trends Inf. Retr. 3 , 4 (2009), 333–389.\n[35] R ODRIGUES , J., G OMES , L., S ILVA, J., B RANCO , A., S ANTOS , R., C ARDOSO , H. L., AND OS´ORIO ,\nT. Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*, 2023.\n[36] R ONG, X. Word2Vec Parameter Learning Explained. CoRR abs/1411.2738 (2014).\n[37] S OUZA , F., N OGUEIRA , R., AND LOTUFO , R. BERTimbau: Pretrained BERT Models for Brazilian\nPortuguese. In Intelligent Systems (Cham, 2020), R. Cerri and R. C. Prati, Eds., Springer Interna-\ntional Publishing, pp. 403–417.\n[38] S OUZA , F., N OGUEIRA , R. F., AND DE ALENCAR LOTUFO , R. Portuguese named entity recognition\nusing BERT -CRF. CoRR abs/1909.10649 (2019).\n[39] T HAKUR , N., R EIMERS , N., R ¨UCKL ´E, A., S RIVASTAVA , A., AND GUREVYCH , I. BEIR: A heteroge-\nneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021).\n88\n[40] V ASWANI , A., S HAZEER , N., P ARMAR , N., U SZKOREIT , J., J ONES , L., G OMEZ , A. N., K AISER ,\nL.U.,ANDPOLOSUKHIN , I. Attention is all you need. In Advances in Neural Information Processing\nSystems (2017), vol. 30, Curran Associates, Inc.\n[41] V IRTANEN , P., G OMMERS , R., O LIPHANT , T. E., H ABERLAND , M., R EDDY , T., C OURNAPEAU , D.,\nBUROVSKI , E., P ETERSON , P., W ECKESSER , W., B RIGHT , J., VAN DER WALT, S. J., B RETT , M.,\nWILSON , J., M ILLMAN , K. J., M AYOROV , N., N ELSON , A. R. J., J ONES , E., K ERN, R., L ARSON ,\nE., C AREY , C. J., P OLAT,˙I., F ENG, Y., M OORE , E. W., V ANDER PLAS, J., L AXALDE , D., P ERK-\nTOLD , J., C IMRMAN , R., H ENRIKSEN , I., Q UINTERO , E. A., H ARRIS , C. R., A RCHIBALD , A. M.,\nRIBEIRO , A. H., P EDREGOSA , F., VANMULBREGT , P., AND SCIPY1.0 C ONTRIBUTORS . SciPy 1.0:\nFundamental Algorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261–272.\n[42] W ANG, K., R EIMERS , N., AND GUREVYCH , I. TSDAE: Using transformer-based sequential denois-\ning auto-encoderfor unsupervised sentence embedding learning. In Findings of the ACL: EMNLP\n2021 (Punta Cana, Dominican Republic, 2021), ACL, pp. 671–688.\n[43] W ANG, K., T HAKUR , N., R EIMERS , N., AND GUREVYCH , I. GPL: Generative pseudo labeling for\nunsupervised domain adaptation of dense retrieval. In North American Chapter of the ACL (2021).\n[44] W ILLIAMS , A., N ANGIA , N., AND BOWMAN , S. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of the 2018 Conference of the North American\nChapter of the ACL: Human Language Technologies, Vol 1 (New Orleans, Louisiana, 2018), ACL,\npp. 1112–1122.\n89\n90\n91"
  },
  "doc-941e4a1f93174b5ca998c6b60483dcd8": {
    "content": "A Semantic Search System for the Supremo Tribunal de\nJustic ¸a\nRui Filipe Coimbra Pereira de Melo\nThesis to obtain the Master of Science Degree in\nComputer Science and Engineering\nSupervisors: Prof. Pedro Alexandre Sim ˜oes dos Santos\nProf. Jo ˜ao Miguel De Sousa de Assis Dias\nExamination Committee\nChairperson: Prof. Maria Lu ´ısa Torres Ribeiro Marques da Silva Coheur\nSupervisor: Prof. Pedro Alexandre Sim ˜oes dos Santos\nMember of the Committee: Prof. Jos ´e Lu´ıs Brinquete Borbinha\nJune 2023\n\nAcknowledgments\nI want to express my sincerest gratitude and appreciation to my supervisors, Professor Pedro Alexan-\ndre Santos and Professor Jo ˜ao Dias, who guided me throughout the entirety of this work and relentlessly\noffered much-needed advice, sound counsel and honest feedback. They were infallible in being reliable\nand continuously kept in touch with the progress of this thesis. They always provided input on the many\nupdates, advancements, and setbacks that occurred, and I am genuinely grateful.\nOn the same note, I would like to thank all Project IRIS members I co-operated with in the past\nmonths. The resonating experience of partaking in a more significant project with brilliant minds will\nshape my future ventures.\nLastly, there is no way I could wholly express in words the unwavering support and unconditional love\nI received from my family. To my mother and father, from the bottom of my heart. . . thank you.\ni\n\nAbstract\nMany information retrieval systems use lexical approaches to retrieve information. Such approaches\nhave multiple limitations, and these constraints are exacerbated when tied to specific domains, such as\nthe legal one. Large language models, such as BERT, deeply understand a language and may overcome\nthe limitations of older methodologies, such as BM25.\nThis work investigated and developed a prototype of a Semantic Search System to assist the Supremo\nTribunal de Justic ¸a (Portuguese Supreme Court of Justice) in its decision-making process.\nWe built a Semantic Search System that uses specially trained BERT models (Legal-BERTimbau\nvariants) and Hybrid Search Systems that incorporate both lexical and semantic techniques by com-\nbining the capabilities of BM25 and the potential of Legal-BERTimbau. In this context, we obtained a\n335% increase on the discovery metric when compared to BM25 for the first query result. This work also\nprovides information on the most relevant techniques for training a Large Language Model adapted to\nPortuguese jurisprudence and introduces a new technique of Metadata Knowledge Distillation.\nKeywords\nArtificial Intelligence; BERT; Information Retrieval; Natural Language Processing; Jurisprudence; SBERT\niii\n\nResumo\nOs sistemas de recuperac ¸ ˜ao de informac ¸ ˜ao utilizam frequentemente abordagens lexicais para recuperar\ninformac ¸ ˜ao. Tais abordagens t ˆem m ´ultiplas limitac ¸ ˜oes, e estas limitac ¸ ˜oes s ˜ao agravadas quando ligadas\na dom ´ınios espec ´ıficos, tais como o legal. Large Language Models, como o BERT, compreendem\nprofundamente uma linguagem e podem ultrapassar as limitac ¸ ˜oes de metodologias mais antigas, como\no BM25.\nEste trabalho investigou e desenvolveu um prot ´otipo de um Sistema de Busca Sem ˆantica para assi-\nstir o Supremo Tribunal de Justic ¸a portugu ˆes no seu processo de tomada de decis ˜ao.\nConstru ´ımos um Sistema de Pesquisa Sem ˆantica que utiliza modelos BERT especialmente treina-\ndos (variantes Legal-BERTimbau) e Sistemas de Pesquisa h ´ıbrida que incorporam tanto t ´ecnicas lex-\nicais como sem ˆanticas, combinando as capacidades da BM25 e o potencial da Legal-BERTimbau.\nReportamos um aumento de desempenho de 335% na recuperac ¸ ˜ao de passagens relevantes quando\ncomparado com BM25 para o resultado da primeira consulta.\nEste trabalho tamb ´em fornece informac ¸ ˜oes sobre as t ´ecnicas mais relevantes para a formac ¸ ˜ao de\num Modelo de Grandes L ´ınguas adaptado `a jurisprud ˆencia portuguesa e introduz uma nova t ´ecnica,\nMetadata Knowledge Distillation.\nPalavras Chave\nBERT; Intelig ˆencia Artificial; Jurisprud ˆencia; Processamento de Linguagem Natural; Recuperac ¸ ˜ao de\nInformac ¸ ˜ao; SBERT\nv\n\nContents\n1 Introduction 1\n1.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2 Background Review 5\n2.1 Lexical approaches for Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.1 Term Frequency Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.2 Best Matching Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.3 Distance metrics for lexical approaches . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Neural Networks for Semantic Information Retrieval . . . . . . . . . . . . . . . . . . . . . 9\n2.2.1 Word and Sentence Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2.2 Word2Vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2.3 GloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.4 Recurrent Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2.5 Long Short-Term Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.6 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3 Semantic Search Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4 Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.4.1 LexRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3 State of the Art 25\n3.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.1.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.1.2 Fine-tuning on Downstream Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.2 BERTimbau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3 Deeper Text Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.4 Legal Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.5 NLP Applied To Portuguese Consumer Law . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nvii\n3.6 Albertina PT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4 Semantic Search System 43\n4.1 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.1.1 ElasticSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.2 The Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.2.1 Data Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.3.1 Purely Semantic Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3.2 Lexical-First Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3.3 Lexical + Semantic Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n5 Legal Language Model 53\n5.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n5.1.1 Masked Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n5.1.2 Transformer-based Sequential Denoising Auto-Encoder . . . . . . . . . . . . . . . 57\n5.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.2.1 Semantic Textual Similarity Custom Dataset . . . . . . . . . . . . . . . . . . . . . . 59\n5.3 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.4 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n5.5 Multilingual Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n5.6 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.7 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6 System Evaluation 65\n6.1 Language Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.1.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.1.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.2 Search System Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2.1 Automatic Query Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n7 Conclusion 79\n7.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.1.1 Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2.1 Albertina . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2.2 Dataset Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\nviii\n7.2.3 Architecture Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\nBibliography 85\nix\nx\nList of Figures\n2.1 Example of Word Embeddings in a 3D vector space . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Word2Vec with Common Bag Of Words (CBOW) model based on a one word context. . . 11\n2.3 Word2Vec with CBOW model based on multiple words context. . . . . . . . . . . . . . . . 11\n2.4 Word2Vec with Skip-Gram model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.5 Recurrent Network Fully Connected . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.6 RNN Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.7 LSTM Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.8 Transformer Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.9 Scaled Dot-Product Attention and Multi-Head Attention . . . . . . . . . . . . . . . . . . . . 19\n2.10 Vector space with a query embedding and multiple sentence embeddings . . . . . . . . . 20\n2.11 Bi-Encoder and Cross-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.12 LexRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.1 BERT input representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.2 Data Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.3 Masked Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.4 Transformer-based Sequential Denoising Auto-Encoder (TSDAE) Architecture . . . . . . . 31\n3.5 T5 diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.6 GenQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.7 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.8 Fine-Tuning SBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.9 Multilingual Knowledge Distillation Process . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n4.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.2 Lexical-First Search System Retrieval Method . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3 Lexical + Semantic Search System Retrieval Method . . . . . . . . . . . . . . . . . . . . . 51\n5.1 Training Tasks Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\nxi\n5.2 Masked Language Modeling (MLM) Training Loss . . . . . . . . . . . . . . . . . . . . . . . 57\n5.3 TSDAE Training Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n5.4 Metadata Knowledge Distillation Ideology . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.5 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.1 Evaluation Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2 Search System Evaluation – Search metric - Models V0 . . . . . . . . . . . . . . . . . . . 73\n6.3 Search System Evaluation – Search metric - Models V1 . . . . . . . . . . . . . . . . . . . 74\n6.4 Search System Evaluation - Discovery metric - Models V0 . . . . . . . . . . . . . . . . . . 75\n6.5 Search System Evaluation - Discovery metric - Models V1 . . . . . . . . . . . . . . . . . . 76\nxii\nList of Tables\n3.1 SBERT Spearman correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.2 Sentence-BERT (SBERT) evaluation on the Semantic Textual Similarity (STS) benchmark 36\n3.3 BERTimbau variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.4 COLIEE 2021 - Task 1 results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.5 COLIEE 2021 - Task 3 results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.1 Legal-BERTimbau variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.1 MLM average loss for legal documents in the test set . . . . . . . . . . . . . . . . . . . . . 68\n6.2 STS evaluation on Portuguese datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.3 Search System Evaluation – Search metric – Best Model . . . . . . . . . . . . . . . . . . 77\n6.4 Search System Evaluation – Discovery metric – Best Model . . . . . . . . . . . . . . . . . 77\nxiii\nxiv\nAcronyms\nAI Artificial Intelligence\nBERT Bidirectional Encoder Representations from Transformers\nBM25 Okapi BM25\nBrWaC Brazilian Web as Corpus\nCBOW Common Bag Of Words\nCOLIEE Competition on Legal Information Extraction and Entailment\nDA Domain Adaptation\nDH Distributional Hypothesis\nFCUL Faculdade de Ci ˆencias da Universidade de Lisboa\nFEUP Faculdadede Engenharia da Universidade do Porto\nGPL Generative Pseudo Labeling\nGLM Generative Language Model\nGloVe Global Vectors for Word Representation\nIDF Inverse Document Frequency\nIR Information Retrieval\nLSTM Long Short-Term Memory\nLeSSE Legal Semantic Search Engine\nMetaKD Metadata Knowledge Distillation\nMLM Masked Language Modeling\nMKD Multilingual Knowledge Distillation\nML Machine Learning\nMNR Multiple Negatives Ranking\nxv\nNLI Natural Language Inference\nNLP Natural Language Processing\nNL Natural Language\nNN Neural Network\nNSP Next Sentence Prediction\nQA Question and Answer\nRNN Recurrent Neural Network\nSBERT Sentence-BERT\nSNLI Stanford Natural Language Inference\nSTJ Supremo Tribunal de Justic ¸a\nSTS Semantic Textual Similarity\nT5 Text-to-Text Transfer Transformer\nTF-IDF Term Frequency-Inverse Document Frequency\nTF Term Frequency\nTSDAE Transformer-based Sequential Denoising Auto-Encoder\nUKP Ubiquitous Knowledge Processing\nxvi\n1\nIntroduction\nContents\n1.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1\n2\nSupremo Tribunal de Justic ¸a (STJ) serves as Portugal’s highest judiciary court, also known as the\nSupreme Court of Justice. It plays a crucial role in making well-informed, lawful, and ethical decisions\nthat have a profound impact on the specific case at hand and future cases. To arrive at a decision, a\ncomprehensive examination of the relevant jurisprudence is indispensable. This exhaustive consultation\nserves as the bedrock for a thoughtful and principled judgment.\nWhen a judge is tasked with formulating a decision, whether it involves researching specific legis-\nlation or referring to precedents from similar cases, the process of locating the necessary information\nis far from trivial, speedy, or efficient. Effectively managing and accessing such vast volumes of infor-\nmation necessitates the utilization of a robust Information Retrieval (IR) system. Such a system is an\nindispensable tool in ensuring efficient access to the required legal resources.\nThe court decision process could be improved based on a search system that would help retrieve\nthe desired information and identify that information source. Even though traditional lexical techniques\ncould be implemented to solve this need, they might raise other issues. Lexical techniques work by\nsearching for specific word matches from a given query. Using lexical techniques in a search system\ncan only retrieve so much information that might include only some cases or sections of legislation that\nare relevant to that specific instance. A search system that utilizes semantic techniques, that work by\ntrying to understand the semantic meaning of a query, and finding document passages closer in meaning\nto the query, is essential to reach a broader range of relevant documents.\nA country’s judiciary system should resist bad decisions based on incomplete or inaccurate informa-\ntion. The occurrence of such events opens the door to unjust precedents in future cases, thus leading\nto imbalances in the overall decision-making, which ultimately leads to the weakening and destabiliza-\ntion of the justice system. Establishing a reliable legal search system is necessary since it promotes\nconsistency in applying the law.\n1.1 Objectives\nThe research presented in this thesis is a contribution to the IRIS project developed by INESC-ID Lisboa\nfor STJ. Project IRIS aims to develop summarization approaches for court decisions and create a\nrepresentation able to be browsed in a way that is helpful in the court decision process.\nAs mentioned beforehand, retrieving desired information might be more complex than one wishes.\nThe first challenge someone faces when procuring information, is in the choosing of (adequate) words\nto input into the search query. Even though judges might use specific terminology terms, the system\nshould be resilient enough to access information based on Natural Language (NL). One will always find\nadversity when constructing a sophisticated and appropriate query that expresses the correct need for\nspecific information.\n3\nHaving these motivations well-defined, it is the aim of this thesis to develop a reliable search system\nthat uses semantic strategies for STJ that will help in the court decision process.\n1.2 Contributions\nThrough the making of this work, three separate types of assets were produced, and promptly deliv-\nered available to appropriate recipients, under the form of contributions. Firstly, we developed multiple\ndatasets from the various Portuguese legal documents available and published them on the Internet.\nThese datasets are publicly available in the HuggingFace platform, allowing them to be easily used\nin other scenarios. Secondly, numerous generic and fine-tuned for Semantic Textual Similarity (STS)\nlanguage models were developed and made available on the same platform. All developed large lan-\nguage models were successfully adapted to the Portuguese legal domain. Furthermore, we developed\na search system with the intent of helping judges in the court decision process. Finally, from the re-\nsults of this work, two papers were written. The first one, Exploring Embeddings Models for Portuguese\nSupreme Court Judgments Summarization, is currently on the verge of being submitted. The second\npaper, Semantic Search System for Supremo Tribunal de Justic ¸a, was submitted to the EPIA Conference\non Artificial Intelligence and it is currently under review.\n1.3 Thesis Outline\nChapter 2 provides essential elements for understanding subsequent content, while Chapter 3 show-\ncases state-of-the-art models and relevant scientific papers used in the final solution.\nIn Chapter 4 we enumerate the implementation requirements and explain the search system architec-\nture. Next, Chapter 5 explains the development of the language model used in our system. Furthermore,\nin Chapter 6, we analyse the performance of our search system and developed language models.\nFinally, Chapter 7 provides an overall retrospective pondering of the developed work and the achieved\nmilestones and gives a quick insight into future work.\n4\n2\nBackground Review\nContents\n2.1 Lexical approaches for Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Neural Networks for Semantic Information Retrieval . . . . . . . . . . . . . . . . . . . 9\n2.3 Semantic Search Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4 Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5\n6\nIn this chapter, we will present and explain some core aspects necessary to fully understand the ex-\nposed content in the following chapters, such as state-of-the-art techniques and the developed solution.\nIn the last few years, Artificial Intelligence (AI) has played an increasingly important role in almost\nevery aspect of modern society. Subsequently, Machine Learning (ML) has found itself as an alternative\nto traditional solutions in multiple fields. [18]\nThere are two main types of approaches regarding IR from documents. The classical approaches\nare based on lexical search, whereas the more recent approaches are based on semantic search.\n2.1 Lexical approaches for Information Retrieval\nTraditionally, when a system searches for document content related to a query, it uses techniques that\nsearch for documents or entries that present those exact query words. This process is called lexi-\ncal search. The basic algorithm for lexical search is Okapi BM25 (BM25) which incorporates Term\nFrequency-Inverse Document Frequency (TF-IDF). BM25 is a widely used algorithm in information re-\ntrieval and has been shown to perform well in various benchmarks and evaluations.\n2.1.1 Term Frequency Algorithm\nTF-IDF [20] is a ranking function for document search and information retrieval. It evaluates how relevant\na term tis relative to a document dbelonging to a set of documents Dwhile being based on a group of\nNdocuments. TF-IDF is defined as follows:\nTF-IDF (t, d, D ) =TF(t, d)×IDF(t, D) (2.1)\nTF-IDF is thus the product of two factors, Term Frequency (TF), and Inverse Document Frequency\n(IDF). TF represents how often a term appears in a specific document. It is calculated as follows:\nTF(t, d) := log(1 + freq(t, d)) (2.2)\nwhere\nfreq(t, d) :=number of times tappeares in d\nnumber of terms in d(2.3)\n7\nIDF represents the rarity of a term in the entire group of documents, where values near 0 show that\nterms are very common and values near 1 show that terms are rarer. IDF is defined as follows:\nIDF(t, D) := log\u0012N\ncount (d∈D:t∈d)\u0013\n(2.4)\n2.1.2 Best Matching Algorithm\nBM25 [34], in which BM stands for Best Matching, is a ranking function usually used in search engines\nlike Elasticsearch (explored later in the document) to estimate the relevance of documents given a query.\nBM25 relies on a bag-of-words logic, by which it ranks a collection of documents based on the query\nterms that appear in which document, independently of the position in that same file. The equation is as\nfollows:\nscore (d, Q) =nX\ni=1IDF (qi)·f(qi, d)·(ki+ 1)\nf(qi, d) +k1·(1−b+b·|d|\navgdl)(2.5)\nwhere Qrepresents the Query given (with q1, ..., q nbeing the keywords), IDF is defined above in (2.4)\nanddrepresents the Document. f(qi, d)represents the qifrequency in the document dthat has a |d|\nlength. Both k1andbterms are free terms that are used to optimize BM25 function.\n2.1.3 Distance metrics for lexical approaches\nA traditional way to search for similar documents is through the Jaccard Similarity measure between\ntwo sets. It evaluates the amount of shared information or content between both sets. In practice, it\nrepresents the extent of the intersection divided by the size of their union. It is defined as follows:\nSim(C1, C2) :=|C1∩C2|\n|C1∪C2|(2.6)\nwhere C1andC2represent two sets.\nThe drawbacks of lexical searches are evident when some important passages or documents are not\nretrieved because some words need to be present. For example: “I walked through the public garden. I\nenjoyed it.” We can easily understand that “it” was an underlying meaning of “public garden”. A similar\nproblem occur when the query uses a synonym to an expression existing in a document and not the\nexact word. That document will not be returned. To tackle these downsides of lexical approaches, we\n8\nexplore a more advance and robust approach that aims to retrieve information based on the intrinsic\nmeaning and not the literal word matches themselves.\n2.2 Neural Networks for Semantic Information Retrieval\nSemantic search is a technique that, instead of searching for specific words, aims to search for the\ncontextual meaning of those words. Unlike lexical search, which only searches for literal matches of\nwords, semantic search tries to understand the user’s intention and the overall sentence’s meaning.\n2.2.1 Word and Sentence Embeddings\nWord Embedding is a mathematical technique and implementation of the Distributional Hypothesis (DH).\nDH defends that if words appear in similar contexts, they must have similar meanings.\nWord embeddings provide a verification of which words are similar to each other based on an initial\ncorpus of sentences. Each word embedding represents a word in a multidimensional space. With\nmultiple multidimensional word representations, verifying which words are related to each is possible\nbased on the distance between words.\nFigure 2.1: Example of Word Embeddings in a 3D vector space\nThe distance can be calculated through the Cosine Similarity, which measures the cosine of the\nangle, θ, between two vector embeddings( AandB):\n9\nCosSim (A, B) :=cos(θ)\n=A·B\n||A||||B||\n=Pn\ni=1Ai·BipPn\ni=1A2\ni·pPn\ni=1B2\ni(2.7)\nSince embedding spaces are linear systems, it is possible to perform arithmetic operations in the em-\nbedding space. If we get a word “England” and subtract the word “London” and add the word “Portugal”,\nwe should be able to get the word “Lisbon”.\nWord (“England ”)−Word (“London ”) +Word (“Portugal ”) = Word (“Lisbon ”) (2.8)\nThere are different implementations of word embeddings, such as Word2Vec proposed by a Google\nTeam led by Tomas Mikolov in 2013 [24] or Global Vectors for Word Representation (GloVe) [28] devel-\noped at Stanford in 2014. Both make use of neural networks to create their models through unsupervised\ntraining. Both implementations have their unique advantages, but what distinguishes them mainly are\ntheir fundamentals of solution formulation.\n2.2.2 Word2Vec\nWord2Vec is a two-layer neural network proposed in 2013 by Tomas Mikolov et al., and it revolves around\nthe idea that words that appear close to one another have similar meanings.\nWord2Vec algorithm uses one of two methods that utilise neural networks [36]:\n• Common Bag Of Words (CBOW);\n• Skip-Gram.\nCBOW model, through the representation of context using the surrounding words as an input, aims\nto predict the corresponding word. Considering the example: “I walked through the public garden.”, we\ncan input the phrase without the word “public” in the Neural Network. By using this single input, it aims\nto predict the word “public” just by interpreting its surroundings.\nThe architecture of the CBOW model is defined in Figure 2.2.\nIn this example, we provide one word X={x1, ..., x V}in the form of a one-hot encoding with size\nVas the context and the network aims to predict another word. Vrepresents the vocabulary size. The\n10\nFigure 2.2: Word2Vec with CBOW model based on a one word context. Figure based on [36]\nhidden layer is composed by Nneurons, and the output layer is a vector of size Vthat represents the\npredicted word. The weight matrix Whas size V×N. The choice of hidden layer size in a Word2Vec\nmodel depends on various factors such as the size of the vocabulary, the complexity of the task, and the\navailable computational resources. Similarly to the input, the yielded word Yfrom the output layer is a\none-hot encoded vector of size V.\nIf the objective is to use multiple words for the context to predict a word, the neural network would\nneed to increase the input layer, as suggested in Figure 2.3.\nFigure 2.3: Word2Vec with CBOW model based on multiple words context. Figure based on [36]\n11\nThe Skip-Gram model’s function is to use a word as an input and generate the context of that same\nword. The architecture is shown in Figure 2.4.\nFigure 2.4: Word2Vec with Skip-Gram model. Figure based on [36]\nWord2Vec relies on local information, meaning words are only affected by words in the surroundings.\nThe technique can not associate a word as a stop-word or a word that has meaning in a phrase. Stop\nwords are everyday words that have no complex meaning. For instance, in the sentence: “The cat sat\non the mat” , Word2Vec cannot identify if the word “The” is a particular context of the words “cat” and\n“mat” or if it is just a stop-word. Nevertheless, this technique performs very well in analogy tasks.\n2.2.3 GloVe\nGloVe is an unsupervised learning algorithm developed by Stanford University researchers aiming to\nrepresent words as vectors. It focuses on the idea that it can derive semantic relationships between\nwords based on a co-occurrence matrix. Each value in the co-occurrence matrix, M, represents a pair\nof words occurring together. For example, a co-occurrence matrix entry Mijrepresents the probability\nof a word jappearing next to the word i.\n12\nThe probability of a word jco-occurring with a word iis the ratio of the number of times word j\nappears in the context of word ito the number of times any word appears in the context of word i. It is\ndefined as follows:\nPij:=P(j|i)\n=MijP\nk∈contextMik(2.9)\nGloVe’s loss function is formally defined as follows:\nJ=VX\ni,j=1f(Xij)(WT\ni˙˜Wj+bi+˜bj−log(Xij))2(2.10)\n, where f(Xij)represents the weighting function, WT\ni˜Wjis the dot product of the input vectors, bi+\n˜bjrepresents the bias, which aims to mitigate the impact of common words and stop-words, and V\nrepresents the size of the vocabulary.\nThe original paper demonstrated the co-occurrence matrix produced with probabilities for targets\nword iceandsteam , as shown in Table 2.2.3. The word iceco-occurs more frequently with the word\nsolid than it does with the word gas, whereas steam behaviours in the opposite way. Also, both are\nrelated to water and do not show a strong co-occurrence with the word fashion . The last row shows\nwhether a word relates more with ice(values much bigger than 1), steam (values much lesser than 1)\nor presents a neutral co-occurrence (close to 1).\nProbably and Ratio k=solid k =gas k =water k =fashion\nP(k—ice) 1.9×10−46.6×10−53.0×10−41.7×10−5\nP(k—steam) 2.2×10−57.8×10−42.2×10−31.8×10−5\nP(k—ice)/P(k—steam) 8.9 8 .5×10−21.36 0 .96\nIn a sense, GloVe receives a corpus of text as an input and transforms each word in that corpus into\na position in a high-dimensional space based purely on statistics through a co-occurrence matrix. With\nthe produced vectors, it is possible to retrieve related words based on the distance between vectors.\n2.2.4 Recurrent Neural Network\nRecurrent Neural Network (RNN) is a type of neural network used for processing sequential or time\nseries data, where the input has some defined order. One way to visualize RNN is by viewing the\narchitecture as multiple feed-forward neural networks that feed information from one network to another.\nIn practice, it is one network where the cells iterate over themselves for every input they acquire.\n13\nFigure 2.5: Recurrent Network Fully Connected.1\nRNN makes use of a hidden vector that has information from the last iteration. In doing so, its actual\nvector, a<t>, depends on the previous vector, a<t−1>, and the current input, x<t>. It is defined by the\nfollowing:\na<t>:=g1(a<t−1>, x<t>)\n=f(Waa·a<t−1>+Wax·x<t>+ba)(2.11)\nwhere WahandWaxrepresent the weight matrix for the hidden vectors and inputs, respectively, brep-\nresents the associated bias, and g1(and subsequently g2) representing an activation function. Usually,\nthe activation functions used for this RNN are either the logistic function (Sigmoid), Hyperbolic Tangent\n(Tanh), or Rectified Linear Unit (ReLU).\nThe output vector (prediction), y<t>, depends on the hidden state vector, a<t>.\ny<t>:=g2(Wya·a<t>+by) (2.12)\nwhere g is another activation function.\nThis architecture allows the networks to analyse any input with unspecified length while maintaining\nthe model size, considering historical information, with weights being shared across time.\nThe loss function used in this architecture has to be defined at each timestep:\nL(ˆy, y) :=TX\nt=1L(ˆyt, yt) (2.13)\nIn the training process, the RNN makes use of a gradient-based technique named Backpropagation\n1Figure based on https://stanford.edu/^shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n14\nThrough Time, calculated at each point in time. At a timestep, T, the derivative of the loss function, L,\nwith respect to the weight matrix Wis as follows:\n∂L(T)\n∂W:=TX\nt=1∂L(t)\n∂W(2.14)\nIn the Backpropagation Through Time mechanism, since it is hard to capture long-term dependen-\ncies because of the multiplicative gradient, the propagated errors might either tend to zero or increase\nexponentially (vanishing or exploding gradient phenomena). One method to help deal with the exploding\ngradient phenomena is by capping the maximum value for the gradient: Gradient clipping. On the other\nhand, several types of gates with well-defined purposes are used to deal with the vanishing gradient\nphenomena.\nOne problem raised by RNN is related to long-term dependencies. For example: “I like flowers a lot.\nToday, I walked through the public garden .”. The information that the user likes flowers should indicate\nthat he might walk through the “garden”. It is a possibility that the distance between the information and\nthe location where it is required is too great. In these circumstances, RNNs cannot learn to connect the\ninformation. Long Short-Term Memory (LSTM) solve this problem.\n2.2.5 Long Short-Term Memory\nLSTM network is a variant of RNN that is able to deal with long-term dependencies.\nStandard RNNs have a very simple structure. They can have, for instance, a single tanh layer\nrepresented by g1. (See Figure 2.6)\nFigure 2.6: RNN.2\n15\nLSTMs contain a slightly more complex structure (See Figure 2.7). They are specifically designed to\ntackle the problem with long-dependencies that ordinary RNNs would struggle with.\nFigure 2.7: LSTM.2\nThe cell state, c, flows across the entire chain, interacting linearly occasionally. If the information pro-\nvided by the cell state is barely changed, it is easy to preserve it over time. The gates can remove or\nadd information to the cell state. In Fig. 2.7, these gates are represented by σsince they use sigmoid\nfunctions. There are three main types of gates: Forget Gate, Input Gate, and Output gate.\nThe Forget Gate layer is responsible for selecting which information is staying or not in the cell state.\nIt makes use of a sigmoid function and, by looking at ht−1andxt, it outputs a number between 0 and 1\nthat represents how much information is kept:\nft:=σ(Wf· |ht−1, xt|+bf) (2.15)\nThe Input Gate layer aims to determine which values will be updated. In conjunction with a tanh layer, it\nhelps determine what information to store in the cell state.\nThe Input Gate output is as follows:\nit:=σ(Wi· |ht−1, xt|+bi) (2.16)\nwhile the output of the tanh layer, the new cell state candidate vector, ˜Ct, is given by:\n˜Ct:=tanh(WC· |ht−1, xt|+bC) (2.17)\n2Figure based on https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n16\nThis way, the cell state, Ctis defined as:\nCt:=ft∗Ct−1+it∗˜Ct (2.18)\nThe Output Gate layer is responsible for deciding what will be outputted and, in conjunction with a tanh\nlayer, deciding what will pass to the next cell. The Output Gate is defined by:\nσt:= (Wo· |ht−1, xt|+bo) (2.19)\nThis output vector originating from the Output Gate is used to determine the hidden vector, ht, that is\ngoing to be transmitted to the next cell.\nht:=σt·tanh(Ct) (2.20)\n2.2.6 Transformers\nSince it is possible to assume that RNNs are unrolled arbitrarily deep networks, they might take too\nlong to train (LSTMs even more due to their complexity). LSTMs, even though they can tackle long-\nterm dependencies, need long training times. Also, the inherent recurrence prevents the use of parallel\ncomputation (widely used in modern computer processors). With that in mind, a Google Team has\nsuggested a new Neural Network (NN) structure, Transformers [40], that introduces the self-attention\nmechanism.\nTransformers using the new mechanism of self-attention facilitate long-range dependencies, elimi-\nnate the Gradient Vanishing and Explosion phenomena, and, by using techniques that do not involve\nrecurrence, facilitate parallel computation, reducing the training time.\nThe encoder is responsible for receiving an input sequence, x= (x1, ..., x n), and map it into a\ncontextualized encoding sequence, z= (z1, ..., z n). The encoder is composed of N= 6identical layers.\nEach layer contains two sub-layers, a Multi-Head Attention Layer and a Feed-Forward Neural Network,\nthat produces outputs with 512 dimensions.\nThe Transformer Model is composed of different components. The Input Embedding is responsible\nfor transforming the inputs into a scalar vector and mapping them into space where words with similar\nmeanings are close to one another. These words might have different meanings depending on their\ncontext and position in a sentence. Positional Encoding tackles this problem.\nPositional Encoding is a vector that provides context depending on the position of a particular word\nin a sentence. In the original paper, it is used sine and cosine functions with different frequencies.\n17\nFigure 2.8: Transformer Model. Figure based on [40]\nPE(pos,2i)=sin(pos/100002i/d model) (2.21)\nPE(pos,2i+1)=cos(pos/100002i/d model) (2.22)\nwhere posis the position and iis the positional encoding dimension.\nAfter the input is passed through the Input Embedding and the Positional Embedding, it is passed\ntowards the Encoding Block. The Transformer Model contains the Multi-Head Attention Layer and Feed-\nForward Neural Network, as mentioned before.\nThe Multi-Head Attention Layer uses an attention mechanism to assign different weights that, for\nevery word, generate a vector that captures the importance between words of a specific sentence.\nThe attention function receives a query, a key, and a value and aims to pair it to an output. All the\ninputs are vectors and the output is calculated based on a weighted sum of the values.\n18\nFigure 2.9: Scaled Dot-Product Attention (left). Multi-Head Attention (right). Figure based on [40]\nAttention (Q, K, V ) :=softmax (Q·KT\n√dK)·V (2.23)\nwhere Qis a set of queries, Kis a matrix with the keys of the words from the query and Vis a matrix that\nholds the values of the words from the query. The input consists of queries and keys of dimension dk\nand values of dimension dv. Instead of running the attention function once for each set of keys, values,\nand queries, the original paper states a solution based on projecting linearly the same sets htimes with\ndifferent dk,dk, and dvdimensions, respectively. This way, the projected images of the keys, values,\nand queries are run in parallel throughout each Attention Head (Fig. 2.9).\nMultiHead (Q, K, V ) :=Concat (head 1, ..., head h)W0(2.24)\nwhere\nhead i:=Attention (QWQ\ni, KWK\ni, V WV\ni) (2.25)\nand where the projection matrices WQ\ni∈Rdmodel dk,WK\ni∈Rdmodel dk,WV\ni∈Rdmodel dVandWO\ni∈\nRhdVdmodel andh= 8parallel attention layers.\nFinally, there is the Decoder Block. The Decoder comprises a Masked Multi-Head Attention layer, a\nMulti-Head Attention layer, and a Feed Forward layer. The Multi-Head and the Feed Forward compo-\nnents are similar to the Encoder Block. When it comes to the Masked Multi-Head Attention Component,\nthere are some differences. The overall architecture was designed for translation purposes. Thus, the\n19\nMasked Attention component applies a mask to the attention scores so that the model only attends to\npositions before the current position in the input sequence, to ensure the predictions are made only on\nthe basis of past information. This mask application is applied by converting the impact words after the\ninput sequence current position to 0.\n2.3 Semantic Search Type\nSemantic search aims to improve the overall search quality by understanding the underlining query\nand sentence meaning. It achieves that by creating embeddings, which are vectorial representations of\nwords, paragraphs, or even documents, into a vector space. Both queries and sentences are embedded\ninto the same vector space, and the closest embeddings are found.\nFigure 2.10: Vector space with a query embedding and multiple sentence embeddings\nThere are two types of semantic search: Symmetric semantic search and Asymmetric semantic\nsearch. Symmetric semantic search aims to match a query input with text. For instance, if a user inputs\nin the search system “Capital crimes correspond to the worst types of crimes one can commit”, it should\nbe expected to receive retrievals that contain a similar meaning, such as “Capital crimes represent the\nworst type of crimes”. In practice, both query and result should have similar lengths.\nAsymmetric semantic search, on the other hand, provides answers to questions. Usually, the query\nis short and expects a more significant paragraph to be returned. A user might search “What are the\n20\nconsequences of robbing?” and it is expected to retrieve a sentence similar to “The consequences of\nrobbing are various from case to case. First, we need to identify the object or quantity being stolen. . . .”\nWhen applying a Symmetric semantic search, there are two approaches: Bi-Encoders and Cross-\nEncoders. Bi-Encoders generate a sentence embedding for a given sentence. Using Sentence-BERT\n(SBERT), we embed sentences AandBindependently, producing the sentence embeddings uandv,\nrespectively. These sentence embeddings can be later compared using cosine similarity. On the other\nhand, Cross-Encoders receives both sentences AandBsimultaneously and outputs a value between 0\nand1, representing the similarity of those sentences. Cross-Encoders do not return a sentence embed-\nding. It only compares the similarity between two sentences, which must be passed simultaneously.\nFigure 2.11: Bi-Encoder (left). Cross-Encoder (right).3\n2.4 Text Summarization\nText Summarization is the problem of reducing the number of sentences and words from a document,\nmaintaining its original meaning. It is the task of creating a shortened version of a given text document\nwhile retaining its most important and relevant information.\nThere are multiple techniques to extract information. These techniques can be categorized as Extrac-\ntive or Abstractive. Extractive techniques aim to retrieve the most important sentences from a document\n3Figure based on https://www.sbert.net/examples/applications/cross-encoder\n21\nwithout considering their meaning. On the other hand, Abstractive uses more complex and harder-to-\ntrain models to understand the semantics and meaning of the document text to create a proper summary.\nThis section will cover a extractive summarization technique that was used in the development of this\nwork.\nText summarization is an important field of research in Natural Language Processing (NLP), such\nas assisting search engines in generating relevant results and enabling machines to write shorter sum-\nmaries of news items or other text-based information.\n2.4.1 LexRank\nLexRank [13] is an unsupervised Extractive summarization technique. It uses a graph-based approach\nfor automatic text summarization. The score of each sentence is based on the concept of eigenvector\ncentrality in a sentence’s graph representation.\nThis algorithm has a connectivity matrix based on intra-sentence cosine similarity, which is used as\nthe adjacency matrix of the graph representation of sentences. In other words, sentences are placed\nas the graph vertices, and the edge weights are calculated using cosine similarity or Jaccard similarity.\n(See Figure 2.12)\nThe sentence similarity scores are then used to build a sentence similarity graph, in which each\nphrase is represented as a node and an edge between two nodes denotes the similarity score of the\nrespective sentences.\nThe PageRank method, which is well-known for calculating the relevance of nodes in a graph, is then\nused to generate the LexRank score of each phrase. The LexRank score is calculated by the PageRank\nalgorithm as follows:\nLexRank (i) :=d\nN+ (1−d)NX\nj=1LexRank (j)\ndeg(j)(2.26)\n, where irepresents a sentence, Nis the total number of sentences in the document, dis a damping\nfactor used to ensure that the scores converge, deg(j)is the number of sentences that are similar to\nsentence j, and the summation is over all sentences jthat are similar to sentence i.\nLexRank scores can be generated repeatedly until convergence, or by decomposing the similarity\nmatrix into eigenvalues. After that, the sentences with the highest LexRank scores are chosen for\nsummarizing.\n22\nFigure 2.12: Weighted cosine similarity graph from LexRak. Figure based on [13]\n23\n24\n3\nState of the Art\nContents\n3.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.2 BERTimbau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3 Deeper Text Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.4 Legal Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.5 NLP Applied To Portuguese Consumer Law . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.6 Albertina PT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n25\n26\nIn this chapter, we will cover some state-of-the-art models and techniques, as well as some relevant\nscientific papers that lead to the implementation of the final solution.\n3.1 BERT\nBidirectional Encoder Representations from Transformers (BERT) is a model proposed by researchers\nat Google AI Language in 2019 [12]. Just like humans tend to understand a word based on its context\nand surroundings, BERT aims to do that. Traditionally, models would look at text sequences from left\nto right or vice-versa. BERT is trained bidirectionally to obtain a higher understanding of the language\nusing the Transformers architecture. It makes use of two phases: pre-training and fine-tuning.\nThe problem when applying bidirectional unsupervised training is the trivial prediction of words. In\nunidirectional training, words can only see their left or right context. The existence of direction in training\nallows for an unbiased prediction of each subsequent word. When that does not exist, as in bidirectional\ntraining, the prediction of each word — based on both its left and right words — becomes biased, and\nwords can then predict themselves.\nOne of the tasks of using language models is to predict the next word in a sentence. BERT aims not\nonly to be able to predict that, but also to understand the language effectively. However, this trivial pro-\ncess would produce a biased model since it uses a particular direction. To tackle this phenomenon and\nreduce the bias, the original paper used two techniques in the pre-training phase while using BooksCor-\npus (800M words) and English Wikipedia (2500M words) as the datasets:\n• Masked Language Modeling (MLM);\n• Next Sentence Prediction (NSP).\nMLM is a technique that revolves around applying a [MASK] token to words, so their prediction can\nbe less biased. In the original paper, in a normal pre-training cycle, 15% of the words in each sequence\nare selected for a masking treatment. The model then aims to predict some masked words based on\nthe surrounding unmasked words. The words that were selected for the masking treatment were not\nall treated the same way. 80% of the selected words are replaced with a [MASK] token. 10% of the\nselected words are replaced with a random token, and the remaining 10% just keep the original token.\nNSP is a task in which the model aims to understand sentence relationships. Many applications like\nQuestion and Answer (QA) and Natural Language Inference (NLI) (See Section 3.1.2.A) revolve around\nthis sense of understanding. In the BERT training process, BERT receives multiple pairs of sentences\nas input. It proceeds to predict if the second sentence is a subsequent sentence of the first one. In\nthis training process, 50% of the inputs are indeed subsequent statements, and are deemed positive\n27\n(labelled as IsNext ), and the other 50% are deemed negative (labelled as NotNext ), meaning that the\nsecond sentence is completely disconnected from the first one.\nIn order to represent the input words as scalar vectors that can be read, BERT creates word embed-\ndings based on three components. Thus, the input embeddings are the sum of the token embeddings,\nthe segmentation embeddings and the position embeddings. The Token Embedding Layer is responsible\nfor assigning a value to each word based on vocabulary IDs. The second component is the Segmen-\ntation Embedding Layer, which allow distinguishing whether a word belongs to sentence A or B. The\nthird component is the Position Embedding Layer, which is responsible for indicating the position of each\nword in a sentence (See Figure 3.1).\nFigure 3.1: BERT input representation. Figure adapted from [12]\nIn the second phase, fine-tuning, one additional layer is added after the final BERT layer, and the\nentire network is trained for a few epochs with the Adam Optimizer.\nIn the original paper, the authors denoted the number of layers (i.e. Transformer blocks) as L, the\nhidden size as H, and the number of self-attention heads as A. With this architecture, two model sizes\nwere reported: BERT BASE andBERT LARGE . They have sizes L= 12 ,H= 768 ,A= 12 with Total\nParameters = 110 MandL= 24,H= 1024 ,A= 16 with Total Parameters = 340 M.\n3.1.1 Domain Adaptation\nNNs require significant amounts of data for proper training, especially labelled data. Usually, such large\nquantities of data are unavailable and training deep learning models can be very time-consuming, often\nrequiring specialized and expensive hardware. Deep learning models can perform well on a test dataset\nfrom the same domain as the training dataset. However, they tend to be less efficient with dataset from\ndifferent domains.\nTask data consists of any observable task distribution information. Data that can be used for a\nspecific task is usually non-randomly sampled from a wider distribution from a larger target domain.\n28\nNevertheless, this task data might not be present within the original data used to train a large language\nmodel. [15]\nTraining a model with data from a different domain of the one it will be used for can lead to some\nperformance degradation. This is because the model is likely to have learned patterns specific to the\noriginal domain, and these patterns may not generalize well to the target domain.\nFigure 3.2: Data Distributions. Figure based on [15]\nDomain Adaptation (DA) is a technique that aims to tackle this issue. With this technique, a model\nshould perform on a new dataset that comes from a different domain similarly to as it would on the\ntesting dataset. Usually, in order to achieve this type of result, deep learning models are re-trained\non unsupervised learning tasks. DA saves large amounts of computational resources and, by utilizing\nunsupervised learning methods, reduces the necessity of manual annotation of labelled datasets.\nSubsection 3.1.1 covers multiple DA techniques that can be used for BERT models.\n3.1.1.A Masked Language Modeling\nMLM, as mentioned in Section 3.1, is a task originally introduced by BERT. Words selected at random\nwith a 15% chance are masked from the input sentence with a predefined probability (80%), and the\nmodel aims to predict those masked words. (see Figure 3.3)\n4Figure based on https://www.sbert.net/examples/unsupervised_learning/MLM\n29\nFigure 3.3: Masked Language Modeling.4\nTo apply MLM with the intent of adapting the domain a model performs in, the model is re-trained\nover one epoch on a new dataset. The learning rate used is the same or slightly lower than the MLM\ntask performed on the model pre-training. The reasoning behind such choice is to slightly change on the\nweights used on the NN, without destabilizing the network completely.\nThe goal of the MLM task is to maximize the likelihood of predicting the correct tokens for the masked\npositions, which is equivalent to minimizing the negative log-likelihood loss function LMLM. The loss\nfunction is defined as:\nLMLM(x, y) :=−NX\nn=1wynxn, yn (3.1)\n, where xis the input, yis the target, wis the weight, and Nis the batch size.\n3.1.1.B Transformer-based Sequential Denoising Auto-Encoder\nTransformer-based Sequential Denoising Auto-Encoder (TSDAE) is an unsupervised state-of-the-art\nsentence embedding method which outperforms previous approaches, such as MLM. Firstly published\non April 14th of 2021 by Nils Reimers [42], this technique aims to improve the domain knowledge of a\nmodel. They state that TSDAE fills the gap between models that usually only perform the STS task on\na certain domain. TSDAE’s model architecture is a modified encoder-decoder Transformer, with the key\nand value of the cross-attention mechanism limited to the sentence embedding.\nIn a sense, it is similar to MLM, but instead of swapping words for [MASK] tokens, TSDAE introduces\nnoise to the sentences by deleting or swapping words. The encoder transforms the sentence into a\n30\nFigure 3.4: TSDAE Architecture. Figure based on [42]\nvector, and a decoder is supposed to reconstruct the original sentence. Formally, the training objective\nis shown in the equation 3.2.\nJTSDAE (Θ) = Ex∼D[logPΘ(x|˜x)]\n=Ex∼D[lX\nL=1logPΘ(x|˜x)]\n=Ex∼D[lX\nL=1logexp(hT\ntet)PN\ni=1exp(hT\ntei)](3.2)\nwhere Dis our training corpus, x=x1, x2...xlis the input sentence with ltokens, ˜xis the corresponding\ndamaged sentence. etis the word embedding of xt, and htrepresents the hidden state at decoding step\nt.\n3.1.1.C GenQ\nGenQ [39], published in October 2021 by the Ubiquitous Knowledge Processing Lab team, is an unsu-\npervised domain adaptation method for dense retrieval models, allowing the query generation from given\npassages. This approach aims for a semantic search system to be asymmetric (explored in Subsection\n31\n2.3), supporting question-answering scenarios simply by training with synthetically generated data.\nFirstly, GenQ requires a Text-to-Text Transfer Transformer (T5) model fine-tuned for question-answering.\nA T5 is a Transformer based architecture that uses a text-to-text approach [29]. This new model archi-\ntecture achieves state-of-the-art results on many NLP benchmarks while maintaining the ability of being\nfine-tuned for numerous downstream tasks. T5 authors found that training on in-domain unlabelled data\ncan improve performance, but using a large and diverse data set is better for generic language under-\nstanding tasks.\nFigure 3.5: T5 diagram. Figure based on [29]\nIn the original paper, they fine-tuned a T5 model on MS MARCO [26] for two epochs. After that, it\nutilised the T5 model to generate queries from original passages. The idea behind T5 models is that\nall NLP tasks can be defined as a text-to-text problem, so they are trained on numerous tasks with\nimmense amounts of data. One of these tasks is query generation. By feeding a passage, T5 models\ncan generate multiple questions the passage may answer. These generated queries might be far from\nideal. The T5 model used is one for general purposes, which can lead to noisy data with plenty of\nrandomnesses. t\nThus, a dense model, such as SBERT, can be fine-tuned with the passages and synthetically gen-\nerated query pairs using the Multiple Negatives Ranking (MNR) loss. Figure 3.6 shows the overall\narchitecture.\n3.1.1.D Generative Pseudo Labeling\nGenerative Pseudo Labeling (GPL) [43] is an improved state-of-the-art technique to perform domain\nadaptation of dense models. It comprises three phases, as shown in Figure 3.7. First, a Query Gen-\n5Figure based on https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html\n32\nFigure 3.6: GenQ5\neration step uses the GenQ approach to generate queries for multiple passages. We have access to\nqueries and passages, called positive passages, since they can answer the generated queries.\nThen, there is a Negative Mining step. Negative mining is a technique to retrieve passages that would\nnot answer the previously generated queries. Nevertheless, these negative passages are similar to the\npositive passages in the vector space. In practice, a dense index would be filled with multiple passages,\nand the closest ones to the positive passages would be retrieved and noted as negative passages.\nThe last and fundamental step is called Pseudo Labeling. In this step, a Cross-Encoder receives the\ntriplets composed of a query, a positive, and a negative passage. The Cross-Encoder calculates the\nscore margin between the negative and positive passages. The training model will use the Margin Mean\nSquared Error Loss [ ?] to identify whether these passages are also relevant to the given query. The\nloss function is calculated based on the sim(Query, Pos )−sim(Query, Neg )|and|gold sim(Q, Pos )−\ngold sim(Query, Neg )|. As stated in the original paper, the dot product was used as the default for\ncalculating the similarity between passages and queries.\n3.1.2 Fine-tuning on Downstream Tasks\n3.1.2.A Sentence-BERT\nThere was a need for representing an entire sentence in an embedding instead of only one word or\ntoken. One implementation was using the BERT model on all the tokens present in a sentence and\n33\nFigure 3.7: Generative Pseudo Labeling. Figure based on [43]\nproducing a mean pooling. Even though this technique was fast enough for the desired problem, it was\nnot very accurate (GloVe embeddings, designed in 2014, produced better results than this technique).\nSBERT [31], implemented in 2019 by a group of researchers from the Ubiquitous Knowledge Pro-\ncessing (UKP) lab, is a modification of the BERT network using siamese and triplet networks which can\ncreate sentence embeddings that are semantically meaningful. This modification of BERT allows for\nnew tasks, such as semantic similarity comparison or information retrieval via semantic search.\nReimers and Gurevych showed that SBERT is dramatically faster than BERT in comparing sentence\npairs. From 10 000 Sentences, it took BERT 65 hours to find the most similar sentence. In contrast,\nSBERT produced the embeddings in approximately 5 seconds and, using the cosine similarity, took\napproximately 0.01 seconds.\nThe siamese architecture comprises two BERT models with weights entangled between them. In\nthe training process, the sentences would be fed to both BERT models, which then would go through a\npooling operation that would transform the token embeddings of size 512×768into a vector of a fixed\nsize of 768.\nOne way of fine-tuning SBERT is through the Softmax loss approach (See Figure 3.8). Applying\na softmax classifier on top of a siamese network can improve sentence representation [9]. NLI is the\nchallenge of determining if the premise implies the hypothesis, whether they are contradictory or neutral.\nIt used a combination of the Stanford Natural Language Inference (SNLI) [7] and the Multi-Genre NLI [44]\ndatasets. The datasets contained pairs of sentences (premises and hypothesis) that could be related\nthrough a label feature. This label feature determines whether the sentences are related or not.\n• 0 – “entailment”, the premise suggests the hypothesis\n• 1 – “neutral”, the premise and hypothesis could not be related\n• 2 – “contradiction”, the premise and hypothesis contradict each other\nThis type of fine-tuning aims to train the model to identify the relationship between sentences.\n34\nFigure 3.8: Fine-Tuning SBERT. Based on [31]\nAnother task that SBERT can be fine-tuned is the STS task. The siamese architecture calculates the\ncosine similarity between uandvsentence embeddings. The researchers also tried negative Manhattan\nand Euclidean distances as similarity measures, but the results were similar. The model performance\nevaluation, in terms of Semantic Textual Similarity, was evaluated on Supervised and Unsupervised\nLearning.\nRegarding the Unsupervised Learning, the researchers used the STS tasks 2012 – 2016 by Agirre\net al. [1–5], the STS benchmark by Cer et al. [8] and the SICK-Relatedness dataset [22]. Each of these\nthree datasets contained gold labels between 0 and 5 that reflects how similar each sentence pair is.\nTable 3.1 shows the Spearman correlation ρbetween the cosine similarity of sentence representations\nand the gold labels for various Textual Similarity (STS) tasks. The performance is reported by convention\nasρx 100.\nTo evaluate Supervised learning, the researchers chose to fine-tune SBERT only with the STS bench-\nmark (STSb), which is “a popular dataset to evaluate supervised STS systems” (Reimers et al., 2019)\nand first train on NLI and then on STSb. (See Figure 3.2 .) BERT systems were trained with ten random\nseeds and four epochs. SBERT was fine-tuned on the STSb dataset, SBERT -NLI was pre-trained on\nthe NLI datasets, and then fine-tuned on the STSb dataset.\n35\nModel STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.\nAvg. GloVe embedd. 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\nAvg. BERT embedd. 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81\nBERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19\nInferSent - GloVe 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01\nUni. Sentence Enc. 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22\nSBERT -NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89\nSBERT -NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55\nSRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21\nSRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68\nTable 3.1: SBERT Spearman correlation ρresults. From [31]\nModel Spearman\nNot trained for STS\navg. GloVe embeddings 58.02\navg. BERT embeddings 46.35\nInferSent - GloVe 68.03\nUniversal Sentence Encoder 74.92\nSBERT -NLI-base 77.03\nSBERT -NLI-large 79.23\nTrained on STS benchmark dataset\nBERT -STSb-base 84.30 ±0.76\nSBERT -STSb-base 84.67 ±0.19\nSRoBERTa-STSb-base 84.92 ±0.34\nBERT -STSb-large 85.64 ±0.81\nSBERT -STSb-large 84.45 ±0.43\nSRoBERTa-STSb-large 85.02 ±0.76\nTrained on NLI data + STS benchmark dataset\nBERT -NLI-STSb-base 88.33 ±0.19\nSBERT -NLI-STSb-base 85.35 ±0.17\nSRoBERTa-NLI-STSb-base 84.79 ±0.38\nBERT -NLI-STSb-large 88.77 ±0.46\nSBERT -NLI-STSb-large 86.10 ±0.13\nSRoBERTa-NLI-STSb-large 86.15 ±0.35\nTable 3.2: SBERT evaluation on the STS benchmark test set. Retrieved from [31].\n36\n3.1.2.B Multilingual Sentence Embeddings\nOne issue raised by using pre-trained models is that the embedding models are usually monolingual\nsince they are usually trained in English. In 2020, the lead researcher from the team that published\nSBERT introduced a technique called Multilingual Knowledge Distillation (MKD) [32]. It relies on the\npremises that for a set of parallel sentences ((s1, t1), ...,(sn, tn))withtibeing the translation of siand a\nteacher Model M, a Model ˆMwould produce vectors for both siandticlose to the teacher Model M\nsentence vectors (See Figure 5.5). For a given batch of sentences β, they minimize the mean-squared\nloss as follows:\n1\n|β|X\nj∈β[(M(sj)−ˆM(sj))2+ (M(sj)−ˆM(tj))2] (3.3)\nFigure 3.9: Multilingual Knowledge Distillation Process. Figure based on [32]\nThere are two different models: a student and a teacher model. Assume that we intend for our\nstudent model to learn Portuguese and that our teacher model already knows English. Both models\nwill receive pairs of sentences with the same meaning, one sentence written in Portuguese while the\nother is in English. The teacher model will encode the English sentence for each pair, while the student\nmodel will encode both sentences. Since both sentences have the same meaning, both embeddings\nshould be similar, if not equal. Consequently, the student model embeddings will be compared to the\nteacher model embedding, which will back-propagate the error using a mean squared error approach.\nOver time, the student model embeddings will become closer to the teacher model embedding.\nMultilingual SBERT versions, such as paraphrase-multilingual-mpnet-base (768 Dimensions) or paraphrase-\nmultilingual-MiniLM-L12 (384 Dimensions), would provide relatively accurate embeddings to this context.\nmpnet model, as a larger model than MiniLM, should be able to comprehend the meaning of a text better\nthat it has yet to explicitly see, such as queries with only one or two words.\n37\n3.2 BERTimbau\nDespite the existence of multilingual BERT models trained on multiple languages, there was an effort to\ntrain monolingual BERT models on single languages. BERTimbau is a BERT model pre-trained for the\nPortuguese language.\nBoth BERT -Base ( 12layers, 768hidden dimension, 12attention heads, and 110M parameters) and\nBERT -Large ( 24layers, 1024 hidden dimension, 16attention heads and 330M parameters) variants were\ntrained with Brazilian Web as Corpus (BrWaC) [6], a large Portuguese corpus, for 1,000,000steps, using\na learning rate of 1e−4. The maximum sentence length is S= 512 tokens\nModel Arch. #Layers #Params\nneuralmind/bert-base-portuguese-cased BERT -Base 12 110M\nneuralmind/bert-large-portuguese-cased BERT -Large 24 335M\nTable 3.3: BERTimbau variants\nBrWaC corpus contained over 2.68billions tokens retrieved from across 3.53million documents,\nproviding a well diverse dataset. They utilised the HTML body, ignoring the titles and possible footnotes.\nThe researchers removed the HTML tags and fixed possible “mojibakes”, a type of text corruption that\noccurs when strings are decoded using the incorrect character encoding, producing a processed corpus\nwith17.5GB of raw text.\nThe pretraining stage was identical to BERT. BERTimbau was trained using MLM and NSP methods\n(Explained previously in Section 3.1) with the exact technique probabilities. Each pretraining example is\ngenerated by concatenating two sequences of tokens x= (x1, . . . , xn )andy= (y1, . . . , ym )separated\nby special [CLS] and [SEP] tokens as follows:\n[CLS]x1. . . x n[SEP ]y1. . . y m[SEP ] (3.4)\nFor each corpus sentence x, 50% of the time an adjacent sequence y is chosen to form a contiguous\npiece of text, and on the remaining 50% of the time, a random sentence from a completely different\ndocument from the corpus is selected as the token y.\n15% of the tokens of every example pair xandyare replaced by 1 of 3 options. Each token can\nbe replaced with a special [MASK] token with 80% probability. With 10% probability, it is replaced with\na random token from the vocabulary or, with the remaining 10% probability, the original token remains\nunchanged.\nBERTimbau serves as the foundation for our language model, Legal-BERTimbau. Even though\nBERTimbau is already a language model adapted to the Portuguese language, it was necessary to\n38\ndevelop a model for our legal domain. This fine-tuning stage is essential since we needed to ensure that\nthe model producing the embeddings could adequately understand the records.\n3.3 Deeper Text Understanding\nZhuyun Dai and Jamie Callan in [11] explore the use of a contextual neural language model, BERT, for\nad-hoc document retrieval in IR. The authors of the paper found that using BERT for text representations\nwas more effective than traditional word embeddings like Word2Vec. The contextual language model\nwas able to better leverage language structures and achieve improved performance on queries written\nin natural language. The authors also show that fine-tuning pre-trained BERT models with a limited\namount of search data could outperform strong baselines. Furthermore, they found that stopwords and\npunctuation, which are often ignored by traditional IR approaches, played a key role in understanding\nnatural language queries.\nIn [11], Zhuyun Dai and Jamie Callan studied the performance of information retrieval associated\nwith two different datasets ( Robust04 andClueWeb09-B ) and different techniques. It included testing\nthe performance with scores related to the score of the first passage (BERT -FirstP), the best passage\n(BERT -MaxP), or the sum of all passage scores (BERT -SumP). The paper showed that simply searching\nthe passage with the best score would provide better results when the dataset has well written text ( Ro-\nbust04 ) since it could understand the context and proper meaning. The results also showed that BERT\nperformed better on description queries than title queries, and that longer natural language queries are\nmore expressive than keywords.\nThey also suggest that BERT, more specifically the SBERT modification, should be applied to small\nportions since it would be “less effective on the long text”. Such implies that embedding small passages,\neither paragraphs, portions of paragraphs or only phrases, is more effective for documents. Another\nexciting approach studied in this research was adding the title to the beginning of every passage to\nprovide context, but it did not produce satisfying results.\n3.4 Legal Information Retrieval\nIn the Competition on Legal Information Extraction and Entailment (COLIEE) edition of 2021, there were\nexplored multiple techniques focused on four specific challenges in the legal domain: case law retrieval,\ncase law entailment, statute law retrieval, and statute law entailment.\nIn [19], Mi-Y oung Kim et al. discuss the use of deep learning techniques for legal information retrieval\nand question-answering tasks in the context of that same edition, focusing, especially, on the University\nof Alberta’s participation.\n39\nTeam F1-score Precision Recall\nJNLP 0.2813 0.3211 0.2502\nnigam 0.2809 0.2587 0.3072\nTable 3.4: COLIEE 2021 - Task 1 results\nTeam Return Retrieved F2 Precision Recall MAP\nOvGU 161 96 0.779 0.778 0.805 0.836\nTable 3.5: COLIEE 2021 - Task 3 results\nTwo teams achieved top COLIEE scores on the first task by combining lexical and semantic tech-\nniques. Task 1 relied on, in total, on a dataset comprised of a total of 5978 case law files. The teams\nwere provided with a labelled training set of 4415 case law files of which 900 query cases, meaning\nthere were approximately 4.9noticed cases per query case. For the third task, Statute Law Retrieval,\nthe goal was to extract a subset of Japanese Civil Code Articles from the Civil Code articles considered\nappropriate for answering legal bar exam questions.\nThe first team, OvGU, presents a two-stage TF-IDF vectorization combined with Sentence-BERT\nembeddings for the third task. Regarding the Task 1, the second team, JNLP , concentrated on dealing\nwith large articles by conducting text chunking on the supplied training data and used a self-labeled\napproach while fine-tuning pre-trained models. They began by preparing the training data using the\napproach given in 25. They employed a TF-IDF vectorizer to encode all the articles and queries into\nvectors, and then used Cosine Similarity to rank the articles. A question and an accompanying article\nare called positive training examples, whereas the converse is termed negative training examples.\nIn that same competition, a third team, nigam [27], proposed an approach where it combined transformer-\nbased and traditional IR techniques for the first task. The team made use of SBERT and Sent2Vec for\nthe semantic component and combined the scores with BM25. They first selected a pre-defined amount\nof results based on BM25, and then they proceeded to embed those documents’ sentences. The final\nresult would be based on the cosine similarity metric.\nThe Task 1 results are shown in Table 3.4 and Task 3 results are shown in Table 3.5.\n3.5 NLP Applied To Portuguese Consumer Law\nIn 2022, Nuno Cordeiro, as part of his master’s thesis [10], created a system, Legal Semantic Search\nEngine (LeSSE), that merges common document retrieval techniques with semantic search abilities on\nPortuguese consumer law. The system was developed in partnership with INESC-ID and Imprensa\n40\nNacional-Casa da Moeda, with the goal of making the Consumer Law more accessible and understand-\nable to the Portuguese citizens. The overall goal and context of his thesis are similar to the context of\nthis research. Even though Nuno’s work focuses on Portuguese Consumer Law, several state-of-the-art\ntechniques, such his usage of BERT and BM25, for Information Retrieval are relevant for our scenario.\nThe system starts by pre-processing all the law documents and the query, followed by text segmenta-\ntion and semantic and syntactic pre-processing. Embeddings are then generated from the segments and\nquery, and a search index is created. In search time, the query is processed and scores are assigned\nto the segments based on their semantic and syntactic similarity. The final stage involves reordering the\nresults using a trained semantic similarity model and presenting the results to the user. The semantic\npipeline uses BERTimbau Base (BERT -Base), a BERT model trained on the Brazilian Portuguese Web.\nThe implemented search system combines the 20 retrievals with the highest scores using BM25 with 50\nretrievals with the highest scores using the cosine similarity measure. Consequently, it orders the results\nthrough a reordering model to produce the final results.\nThe pre-processing of legal documents differs from the one needed in our context. The implemented\nsystem required tokenization to help construct the bag-of-words necessary for the BM25 algorithm,\nremoval of punctuation, and stop-words, which is unnecessary for SBERT.\nIn his work, the language model had to be fine-tuned on a corpus that included legislative jargon to\nproduce the desired results. This corpus is a Portuguese corpus with the help of annotated questions\nfrom the Official Portuguese Gazette (Di ´ario da Rep ´ublica) search database. The fine-tuning was done\nusing a machine with 2 NVIDIA GeForce RTX 3090 GPUs, each with 24 GB of memory and 10496\ncores. The hyperparameter optimization was performed using Population Based Training, which is a\ncombination of Grid search and Hand Tuning. The hyperparameter optimization was performed using\nthe Ray Tune library, which was integrated into the Trainer class function hyperparameter search. The\ntraining dataset was divided into three subsets: training set, validation set, and test set, with 80%, 10%,\nand 10% of the original dataset, respectively. The datasets were shuffled before division to ensure that\nevery training batch was representative of the dataset as a whole. This fine-tuning stage is an important\nstep, since it would help ensure that the model could create proper relationships with words not seen in\nthe pre-training stage. We implemented this approach in this thesis work, since there will be legal terms\nand jargon that the model has not seen in the pre-training phase.\n3.6 Albertina PT\nIn the closing stages of this research work, in May 2023, Jo ˜ao Rodrigues et al., shared their brand\nnew state-of-the-art model, Albertina [35]. This BERT model represents the new state-of-the-art for\nEuropean Portuguese (PT -PT) and Brazilian Portuguese (PT -BR) encoder models. It was developed\n41\nin a partnership between Faculdade de Ci ˆencias da Universidade de Lisboa (FCUL) and Faculdadede\nEngenharia da Universidade do Porto (FEUP), more concretely, NLX–Natural Language and Speech\nGroup, and Laborat ´orio de Intelig ˆencia Artificial e Ci ˆencia de Computadores, respectively.\nThe starting point was DeBERTa [16] architecture, and the pre-training was done over data sets\nof Portuguese for the PT -PT version and the BrWaC corpus for PT -BR, allowing for comparison with\nBERTimbau It has 24layers with a hidden size of 1536 and a total of 900million parameters.\nAlbertina PT -BR outperforms BERTimbau in the STS task over the assin2 dataset, as well as on the\nSTS-B dataset. Interestedly enough, Albertina PT -BR fails to match that performance, falling short of\nBERTimbau.\nNevertheless, Albertina PT -BR appears to be an improved version of BERTimbau, mainly due to its\nlarger architecture. Albertina PT -PT comes up as the first substantial BERT model completely trained\nfor European Portuguese, which is a breakthrough on its own.\n42\n4\nSemantic Search System\nContents\n4.1 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.2 The Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n43\n44\nThis thesis aimed to implement a semantic search system to help the court decision process. The\nproposed system aims to address this challenge by implementing a semantic search system that can\nefficiently retrieve and present relevant legal information.\nIn Chapter 4 of this thesis, we provide an overview of the implemented search system architecture\nand the requirements that the system is expected to meet. The Chapter delves into the different stages\nof the system and introduces the key components that make up the architecture. The developed large\nlanguage model, Legal-BERTimbau, was designed to achieve the project’s goals and is a crucial element\nof the system. Chapter 4 presents an overview of Legal-BERTimbau’s role in the overall search system.\nChapter 5 takes a more detailed look at the Legal-BERTimbau model and explores its inner workings.\nThe Chapter examines the model’s architecture, training data, and the different techniques used to\nfine-tune the model for our legal applications. Chapter 5 will provide a better understanding of how\nthe Legal-BERTimbau model works and how it contributes to the overall effectiveness of the semantic\nsearch system.\n4.1 Constraints\nAs mentioned in Chapter 1, this work is a segment of Project IRIS. Consequently, there were some\npre-defined aspects, such as which technologies the search system should be implemented in.\n4.1.1 ElasticSearch\nElasticsearch, released in 2010, is a distributed, open-source search and analytics engine built over\nApache Lucene. It works as a NO-SQL JSON document-based datastore. A user can interact with\nElasticsearch similarly to interactions with REST APIs, meaning that every request, either POSTs, GETs,\nor PUTs, is sent in a JSON format to different indices. An index is used to store documents in dedicated\ndata structures and allows the user to partition the data in a certain way within a specific namespace.\nElasticsearch uses a complex architecture to ensure the scalability and resilience of the system.\nIt comprises node clusters, where nodes are single instances of Elasticsearch. It also makes use of\nshards, which are subsets of index documents. Shards allow splitting the data from indices to maintain\na good performance and replicate information to handle failures.\nElasticSearch was a pre-defined constraint since the Project IRIS solution was based on the Elas-\nticSearch engine. To utilise ElasticSearch for our use case, it was required to understand how to utilise\nthe provided engine with embeddings.\nBy default, Elasticsearch uses BM25 to search through documents. However, it can support other\nsearch functions, such as Cosine Similarity. To utilise Cosine Similarity, ElasticSearch requires an initial\nmapping of indices, pre-defining its document structure and allowing some fields to be dense vectors.\n45\nThese dense vectors can store embeddings. This powerful edge provides an ideal solution for a se-\nmantic search system implementation, allowing to search and analyse of huge volumes of data in near\nreal-time.\n4.2 The Corpus\nLegal documents contain specific language not easily found in conventional websites or books. To\ncreate a semantic search system adapted to the Portuguese legal domain, it was required to collect\nmany records. Project IRIS members performed the collecting data process through ecli-indexer6. This\nrepository contains multiple tools to extract documents from dgsi.pt, a public database, and index them\ninto ElasticSearch. The retrieval process recovered the HTML content from multiple web domain pages\ncontaining legal documents, summing up to 31690 documents.\nThe structure of each indexed document is as follows:\n1{'_index ':'jurisprudencia .1.0 ',\n2'_id':'- B5mRoABpM44h1Fg -6QX ',\n3'_score ': 1.0,\n4'_source ':{'ECLI ':'ECLI :PT:STJ : 2022 : 251.18.1 T8CSC .L2.S1 ',\n5 'Tribunal ':'Supremo Tribunal de Justica ',\n6 'Processo ':'251/18.1 T8CSC .L2.S1 ',\n7 'Relator ': Relator 1 ',\n8 'Data ':'17/03/2022 ',\n9 'Descritores ': ['CONTRATO DE TRABALHO ',\n10 'CONTRATO DE PRESTACAO DE SERVICO '],\n11 'Sumario ':'\\n<p>I- Subjacente ao contrato de trabalho existe uma\nrelacao de dependencia necessaria ... \\n </p><p> ',\n12 'Texto ':'... <p><i>d) Deve a Re ser condenada a pagar ao Autor a\ndiferenca entre os vencimentos pagos desde julho de 2011 e o\nvencimento que venha a ser determinado nos termos dos pedidos\nformulados em b) ou c) ... ',\n13 'Tipo ':'Acordao ',\n14 'Original URL ':'http :// www . dgsi .pt/ jstj .nsf /12345 ',\n15 'Votacao ':'UNANIMIDADE ',\n16 'Meio Processual ':'REVISTA ',\n6https://github.com/diogoalmiro/ecli-indexer\n46\n17 'Seccao ':'4a SECCAO ',\n18 'Especie ': None ,\n19 'Decisao ':'<b> NEGADA A REVISTA .</b> ',\n20 'Aditamento ': None ,\n21 'Jurisprudencia ':'unknown ',\n22 'Origem ':'dgsi - indexer -STJ ',\n23 'Data do Acordao ':'17/03/2022 '}}\nThe partition of utilised data was mainly the “Texto” (Text) and “Sum ´ario” (Summary) fields. It con-\ntained the HTML content of a legal document corpus. This data needed further processing to create a\nreliable semantic search system. The Summary section reflects not a summary of the full document, but\ninstead, it is a summary of the judgement ruling decision and the newly established jurisprudence.\nThree dataset splits were generated to train, test, and validate the produced models. The percent-\nages for each split were as follows: 80% for the training dataset, 10% for the testing dataset, and 10%\nfor the validation dataset. This dataset was published to the HuggingFace platform7to facilitate model\nreproducibility and future project use. The divisions were as follows:\n• Training dataset – 26952 documents\n• Testing dataset – 3169 documents\n• Validation dataset – 3169 documents\n4.2.1 Data Processing\nWith all the documents properly indexed, it was necessary to clean the available text and split the content\ninto multiple sentences. Our search system acts on singular sentences, as explored further in Chapter\n4 and 5.\nFirstly, HTML tags needed to be removed as well as some unexpected characters, such as “&”. It\nwas required to identify and remove Roman numeration from the text and, more importantly, not take\ninto consideration sections or Subsection titles.\nTexts also contained references to other sections, such as “como referido em a) e b)”. This example\nshows a possible problem a semantic search system can face due to the difficulty of demonstrating\nto the system that there is relevant information outside that section. When the system identifies that\nthere is referred information from other sections, it is required to handle that dependency. A solution\nto this issue could start by incorporating a summarization technique to join the information in the same\n7https://huggingface.co/datasets/stjiris/portuguese-legal-sentences-v0\n47\nplace. We decided to utilise a more straightforward approach. In the data pre-processing step, these\noccurrences are removed from the text. Even though it does not contain all the related information as\napplying some summarization technique would provide, the focus was to further simplify the text. This\naimed to improve the semantic search system’s performance, since the sentences themselves would\npresent a clearer meaning without depending on other sections.\nIn our scenario, with over 30000 documents, the solution involves implementing a Bi-Encoder. We\nneed to create the embeddings independently of each other, allowing us to search later using the cosine\nsimilarity. This way, the overall search system performance is doable, whereas the search would not be\nfeasible if we utilise a Cross-Encoder.\n4.3 Architecture\nSection 4.3 provides insights on the implemented Search Systems’ architectures. Throughout this work,\nwe developed three different search systems: one semantic search and two hybrid search systems that\ncombine both semantic and lexical approaches.\nInitially, there was a pre-processing of the documents in the original dataset to split entire documents\ninto smaller units. This pre-processing was essential to separate the text into smaller passages since\nLegal-BERTimbau would be less effective on large sentences. Legal-BERTimbau’s siamese and triplet\nnetwork structures depend on the available training data, implying that the effectiveness of itself may\nvary depending on the size and nature of the input data. In our scenario, Legal-BERTimbau’s training\ndata were not very extensive, comprehending only a few tokens each time. Nevertheless, it was required\nto analyse the documents in more detail to verify if the phrases are too long, too short or if they raised\nother concerns, such as referenced in Subsection 4.2.1. On the other hand, stop-words removal was\nnot necessary, since Legal-BERTimbau, being a version of SBERT, is designed to receive meaningful\nsentences rather than isolated keywords.\nThe proposed solution architecture is illustrated in Figure 4.1.\nFor implementing the search system, we utilised Elasticsearch, which allows for scalability and fast\nretrieval of results while using the cosine similarity function to search through embeddings. Elasticsearch\nwas a requirement for this project, as stated in Subsection 4.1.1. Elasticsearch is used as a dense\nvector database where embeddings will be stored in indices. Such indices require initial mapping, which\nmandates the size of the embeddings and other extra information, such as the original document from\nwhere the sentence was retrieved.\nAfter the pre-processing described in the previous section, the next step was generating the embed-\ndings. The sentence embeddings were created by making use of the Legal-BERTimbau model hosted\n48\nFigure 4.1: System Architecture\non the Hugging Face Hub8, through the SentenceTransformers Python9library. With the embeddings\ngenerated, it is possible to populate the indexes on Elasticsearch through its Python Client.\nFor retrieving specific query results, that exact query would be transformed into an embedding by\nLegal-BERTimbau. Then the system can proceed to search similar sentences by executing a ranking\nfunction on Elasticsearch using the query embedding. Depending on which search system architecture,\nthe Ranking function varies slightly.\nFinally, the relevant passages are included in a prompt for a Generative Language Model (GLM),\nGPT3.5, providing a user-friendly response, yet based on our retrieved results. For example, with a\nquery as such: “Furto de Armas”, the system’s output is the following:\n— A passagem relevante para a quest ˜ao´e a seguinte: ”Quem, de noite e acompanhado, entra\nnuma casa museu, depois de arrombar a porta e de l ´a retira v ´arias armas pec ¸as de museu e delas se\napropria, contra vontade do dono, pratica os crimes de introduc ¸ ˜ao em lugar vedado ao p ´ublico e furto\nqualificado.” (Documento ID: 9EWRY oMBF lErWh5 w2g).\n8https://huggingface.co/\n9https://www.sbert.net/\n49\n4.3.1 Purely Semantic Search System\nThe Purely Semantic Search System makes use of only the semantic capabilities of the embedding\nmodel. The system utilises Elasticsearch and cosine similarity function to search through the embed-\ndings and retrieve relevant search results.\nWe chose to implement a symmetric semantic search system. The reasoning behind such a decision\nwas predominantly due to the need for more queries and results from pairs examples to implement a\nproper asymmetric semantic search. On the same note, when a judge interacts with a search system,\nthe judge is more likely to insert an extensive query with proper terminology than inputting a question\nthat wants to be answered.\nTo be noted, there are other distance metrics, such as the Dot Product, but Cosine Similarity is\nregarded as the most prominent one for use cases as this.\n4.3.2 Lexical-First Search System\nThis work also presents a different version of a search system, a Hybrid Search System, that combines\nthe potential of lexical search techniques and the reach of large language models. We called it Lexical-\nFirst Search System.\nThe architecture is similar to the developed Purely Semantic Search System. The pre-processing\nand usage of ElasticSearch are equal, and only the retrieval method changes slightly. Instead of yielding\nthe best matches using the cosine similarity metric, it combines the use of BM25 before evaluating the\nsimilarity of the embeddings using the cosine similarity metric.\nThe method retrieves a pre-defined number of top results using BM25 (i.e Top 20 results). Afterwards,\nit ranks the outcomes using the cosine similarity metric, as shown in Figure 4.2.\nFigure 4.2: Lexical-First Search System Retrieval Method\n50\n4.3.3 Lexical + Semantic Search System\nLexical-First Search System presents promising results, which are explored in more detail in Section 6.2.\nHowever, such hybrid system architecture relies immensely on BM25 results. In a more straightforward\noverview, the presented Hybrid Search System filters the possible results using BM25 and then verifies\nwhich results should be retrieved based on a Legal-BERTimbau model.\nThis Subsection introduces a more flexible architecture for a Hybrid Search System, designated as\nLexical + Semantic Search System. Instead of filtering the first batch of results using BM25, we propose\nto combine the scores of both lexical and semantic information retrieval methods.\nLexical + Semantic Search System utilises the scores provided by BM25 and the cosine similarity\nvalue from the Legal-BERTimbau embedding space. BM25 scores are normalised using the maximum\nscore obtained using a specific query. This implies that the highest score using BM25 will be 1.\nIn parallel, the cosine similarity between the dense vector and the query embedding is calculated.\nThe cosine similarity value does not need to be normalised.\nFinally, we sum the scores using both methods and proceed to reorder the results and present them.\nThe retrieved method is illustrated in Figure 4.3.\nFigure 4.3: Lexical + Semantic Search System Retrieval Method\n51\n52\n5\nLegal Language Model\nContents\n5.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n5.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.3 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.4 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n5.5 Multilingual Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n5.6 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.7 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n53\n54\nThe embedding’s creation required a proper language model adapted to the Portuguese language\nand, more specifically, the Portuguese legal domain. This Chapter will explain the explored approaches\nand the final implementations of Legal-BERTimbau.10Advances in language representation using Neu-\nral Networks allowed for the usage of this concept in facilitating the transfer of “learned internal states of\nlarge pre-trained language models” [37,38]. Transfer learning is a technique that allows a model trained\nfor a general task to be later fine-tuned for other specific tasks.\nBERT contains numerous parameters, reaching over 300 M on the large version. Training a BERT\nmodel from scratch, with a relatively small dataset, would cause overfitting. In our case, having nearly 30\n000 documents, the correct approach is to use a model to fulfil our needs that is already trained with a\nlarger corpus than the one available in our domain. With BERTimbau (See Section 3.2) as a base model,\nwe were able to create our own model for finding similar passages, a SBERT model: Legal-BERTimbau.\nChapter 5 provides an overview of the training involved in the generation of Legal-BERTimbau. It\nshares an overview of the different techniques used for each different version of the model. Figure 5\npresents an overview of the different training tasks through this research to attain the different model\nversions. The presented additional tasks were combined with each other to generate multiple model\nversions.\n5.1 Domain Adaptation\nDomain adaptation is a type of fine-tuning regarding language models. As the name suggests, the\ngoal is that the model can further understand a new domain. This scenario arises when a model is\npre-trained on a specific dataset but needs to be used with a different (but related) dataset. In our\ncontext, BERTimbau was trained with a large Portuguese corpus, BrWaC, but we wanted to use a\nmodel on Portuguese jurisprudence. Legal documents are very distinct from BrWaC, only sharing the\nsame language. Documents provided by official courts should contain text better structured than BrWaC\nand contain some jargon and technical language of the domain. Over the years, multiple techniques\nhave been aiming to adapt a language model to a new domain. This stage was done on the BERTimbau\nlarge variant, which can produce embeddings of 1024 dimensions. The Domain Adaptation stages were\nperformed on a NVIDIA GeForce RTX 3090 24 GB GPU. The developed variants can be easily used\nwith SentenceTransformers Python Library, TensorFlow, PyTorch, or JAX, since each model is hosted\non the HuggingFace Platform, using the HuggingFace’s Transformers library.\n10Models available on https://huggingface.co/stjiris\n55\nFigure 5.1: Training Tasks Overview\n5.1.1 Masked Language Modeling\nMLM, as mentioned in Section 3.1, is a task originally introduced by BERT. The training consisted\nin applying the traditional BERT MLM training over our training dataset. With this approach, the model\nbecame more familiarized with technical language or jargon presented in those documents. For the MLM\ntask, we defined the learning rate as 10−5. We want the learning rate in this stage to be significantly lower\nthan in the initial training stage itself. Since we are training the model with such numerous parameters\non a rather small dataset, it would easily overfit. In the same vein, the performed fine-tuning was carried\nout by employing the procedure for a single epoch. This fine-tuning stage, performed with a batch size\nof 2, generated a BERTimbau variant. The loss associated with the training process can be shown in\nthe following image:\nThe selected MLM model variant is the one obtained at the 770 training steps mark. The model\nselection was based on the best model performance within our evaluation split. The variant that was\n56\nFigure 5.2: MLM Training Loss\ncreated is:\n• stjiris/bert-large-portuguese-cased-legal-mlm\n5.1.2 Transformer-based Sequential Denoising Auto-Encoder\nAs described in Subsection 3.1.1.B, TSDAE is an unsupervised sentence embedding approach. TSDAE\nencodes damaged sentences into fixed-sized vectors during training and needs the decoder to recover\nthe original sentences from this sentence embedding.\nWhen using the TSDAE technique, we used a learning rate of 5∗10−6over our training dataset and\na batch size of 2. The loss associated with the training process can be shown in the Figure 5.1.2.\nThe selected TSDAE model variant was the one with the lowest loss value: 1300 training steps. The\nloss value was calculated using the evaluation split. The variant that was created is:\n• stjiris/bert-large-portuguese-cased-legal-tsdae\n5.2 Semantic Textual Similarity\nThe task our language model needs to perform is STS evaluation. STS is a regression task that deter-\nmines how similar two text segments are on a numeric scale, ranging from 1 to 5.\nTo adapt the generated variants to this task, we created SBERT versions of themselves and trained\nthem with four distinct datasets. We attached an independent linear layer to each Legal-BERTimbau\n57\nFigure 5.3: TSDAE Training Loss\nvariant and fine-tuned the model using a mean squared error loss. The SBERT version of Legal-\nBERTimbau-large, utilising the SentenceTransformer library, is defined as follows:\n1bertmodelname = 'rufimelo/Legal-BERTimbau-large'\n2wordembedding model = models.Transformer(bert modelname, max seqlength=256)\n3pooling model = models.Pooling(\n4 wordembedding model.get wordembedding dimension())\n5densemodel = models.Dense(\n6 infeatures=pooling model.get sentence embedding dimension(),\n7 outfeatures=256,\n8 activation function=nn.Tanh())\n9model = SentenceTransformer(modules=[word embedding model, pooling model, dense model])\nThis code snippet shows how we can create a SBERT model from scratch, using a BERT model as\nthe foundation. Since we did not add a Pooling layer in our architecture, which would lower the accuracy\nof the embeddings for the STS task, this SBERT model variant generates 1024 dimension embeddings.\nTo train the models for the STS task, the datasets assin [14] and assin2 [30] were used, as well as\nthe stsb multi mt [23] Portuguese sub-dataset. Each dataset contained pairs of sentences and a label\nvalue representing both sentences’ similarities.\nThe assin dataset contains 10 000 pairs of sentences, 5 000 of which were used for training. Similarly,\nthe assin2 dataset contains 9 448 pairs of sentences, from which 6 500 were also used for training.\nFinally, stsb multi mt Portuguese sub-dataset contains 8 628 pairs of sentences, from which we used\n5749 to fine-tune the model. In a nutshell, for the STS task, our models were trained with 20 197\n58\nPortuguese sentence pairs, allowing the model to be more familiarized with the Portuguese language.\nBoth assin and assin2 are Brazilian Portuguese datasets.\nFollowing the MLM and TSDAE domain adaptation performed on BERTimbau, we trained the large\nversion with a learning rate of 10−5, making use of the Adam optimization algorithm [21]. We trained\nwith a batch size of 8 during five epochs.\nThis type of fine-tuning, generated the following SBERT variants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-sts-v0\n5.2.1 Semantic Textual Similarity Custom Dataset\nThe solution we present implies training using the STS task. The resources available for this effect\nare slim. For this effect, there are only 3 Portuguese STS datasets: assin ,assin2 andstsb multi mt\nPortuguese sub-dataset, which is a translation from the English sub-dataset version. On top of this fact,\nthe Portuguese legal domain is unique on its own. To improve the STS task, we developed a unique\ndataset for further training our models. The dataset is publicly available at HuggingFace:\n• stjiris/IRIS sts\nThe dataset creation process was automated. The dataset, similarly to assin and assin2, contained\nrelatedness scores from 0 to 5. When training, these scores are normalized for values ranging between\n0 and 1. Sentence pairs selected randomly across our document collection were given relatedness\nvalues from 0 to 1. Values 1 to 4 were attributed to sentence pairs selected from the same summary.\nThe summaries are short, and thus, they might imply some entailment. Finally, we selected sentences\nfrom our collection and proceeded to generate their pairs using OpenAI’s GPT3 text-davinci-003 model\nAPI, publicly available since November 29th11. Such pairs received a relatedness score from 4 to 5. The\nGPT3 model received the following request:\n• ”Escreve por outras palavras: Entrada: sentence Sa´ıda:”\n, which translates to:\n• ”Write, in other words: Input: sentence Output:”\nThis custom dataset also presents NLI annotations. Sentences pairs with relatedness values above\nfour were given a “2” as the entailment label, meaning they are entailed. Pairs with relatedness scores\n11https://beta.openai.com/playground\n59\nbetween one and four were given a “1” as the entailment label, meaning those sentences have no rela-\ntionship. Finally, sentence pairs with a relatedness score below one were associated with an entailment\nlabel of “0”, meaning they are contradictory.\nSimilarly to the STS fine-tuning stage described in BERTimbau’s paper, the models were trained with\na learning rate of 10−5, also making use of the Adam optimization algorithm, but only performed on a\nbatch size of 8during five epochs.\nThis fine-tuning, with a custom dataset, generated various SBERT variants. To differentiate the\nmodels that were trained on this custom STS dataset, we denoted sts-v0 when it was trained on the\nthree original datasets and sts-v1 when a model was trained on all four datasets, including /IRIS sts.\nWe utilised this new dataset and trained the following variants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-sts-v1\n5.3 Natural Language Inference\nAccording to [31], a slight STS performance improvement when the models were subjected to NLI data\nis reported (See Section 3.1.2.A). The previously used assin and assin2 datasets also contain informa-\ntion about the relatedness between sentence pairs. Similarly to SNLI, assin contains a label feature\nindicating if a sentence entails the other (0), if they have no apparent relationship between them (1)\nor if they contradict each other (2). In the case of assin2 it contains labels 1 or 0, representing if the\nsentences are entailed or not, respectively.\nWe trained the large models on assin and assin2 NLI information with an 8 batch size for five epochs\nwith a learning rate of 10−5, using the Adam optimization algorithm. The variants produced with this\napproach outperform others that do not. (Discussed in more detail in Chapter 6)\nThis fine-tuning, combined with STS, generated 4 different SBERT variants. To denote the models\nsubjected to NLI data, we add nliin the model name.\nAdding this type of differentiation, we trained the following models.\n• stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v1\n60\n5.4 Generative Pseudo Labeling\nGPL, as mentioned in Subsection 3.1.1.D, is a state-of-the-art unsupervised technique to fine-tune ex-\nisting models in different domain. This technique allows a model to understand which sentences can\nanswer a question.\nAs explained, GPL has three different stages: Query Generation (from GenQ), Negative Mining and\nPseudo Labeling.\nIn the Query Generation step, we created 10000 Queries for 10000 legal documents. We used\na pre-trained T5 model, fine-tuned for the Portuguese Language, pierreguillou/t5-base-qa-squad-v1.1-\nportuguese12, to generate queries from each document summary. After this step, we have a collection\nof queries that each summary (positive passage) should be able to answer individually.\nIn the Negative Mining stage, we retrieved passages very similar to our initial passage, but that should\nnot be able to answer the generated queries. For this purpose, we created an index on ElasticSearch\nwhere we stored the embeddings of the other summaries used for the previous step. To reduce the bias\nin the system, we used an original BERTimbau large fine-tuned for STS. This model was fine-tuned\nfollowing the guidelines in the original paper. We used assin and assin2 datasets for five epochs, using\n3∗10−5for the learning rate.\nIn the final step, we used the same model to calculate the margin between positive and negative\npassages using the dot product. We trained our models using the created triplets (positive passage,\nnegative passage and margin score), applying the Margin Mean Squared Error Loss with a learning rate\nof2∗10−5on one epoch. The variants produced with this approach outperform the variants that were\nnot subject to this technique. (Discussed in more detail in Chapter 6)\nThis technique was applied to models that were trained using both NLI and STS data.\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1\n12https://huggingface.co/pierreguillou/t5-base-qa-squad-v1.1-portuguese\n61\n5.5 Multilingual Knowledge Distillation\nMKD is a technique developed by Neil Reimers, as stated in Subsection 3.1.2.B. This sort of technique\nallows for a model to extend its knowledge over a language, i.e. English, to different languages such as\nPortuguese. This technique is attractive, especially when we intend to create a model that should learn\na language. However, only a few datasets are available in that same language.\nIn this work, we developed a language model that utilises this technique. However, the goal was\nnot to create a multilingual model, but rather to improve the knowledge a model already has of the\nPortuguese language.\nThe dataset used was: TED 2020 – Parallel Sentences Corpus [33]. TED 2020 contains around\n4000 TED13and TED-X transcripts from July 2020. These transcripts were translated by volunteers into\nmore than 100 languages, adding up to a total of 10 544 174 sentences. All the sentences were aligned\nto generate a parallel corpus for training tasks such as this.\nWith the explained end goal, this technique was applied to Legal-BERTimbau-large . It was desig-\nnated as the student model, supporting the English Language already, and we intended for it to learn\nPortuguese. The chosen teacher model was sentence-transformers/stsb-roberta-large14. It was defined\nthat the number of warm-up steps should be 10000 . The training was performed with a 10−5learning\nrate using the Adam optimization algorithm during five epochs.\nFurthermore, after this extra training step, we fine-tuned the models for the STS regression task as\ndescribed in Section 5.2. With the integration of this different application of the technique, it was possible\nto further train a model for the Portuguese language by mimicking a teacher model that knew how to\nencode English Sentences properly.\nThe application of the MKD technique and the classical STS fine-tuning produced a different BERT\nvariants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1\n13https://www.ted.com/\n14https://huggingface.co/sentence-transformers/stsb-roberta-large\n62\n5.6 Metadata Knowledge Distillation\nThis work presents a new technique developed to improve information retrieval through dense vectors:\nMetadata Knowledge Distillation (MetaKD)\nIn our dataset, multiple documents have associated with them “Descritores”, brief tags manually\nannotated by experts. These tags intend to identify the main document subjects. These tags could\nindicate if a crime was committed with knives or even if it is related to COVID-19.\nWith such annotation, we assumed that the documents are, in a way, related to one another. Thus,\nthe sentences from each document have some trim level of entailment between each other.\nWe started by identifying the documents related to a subject, COVID-19, i.e. and we proceeded\nto encode those documents’ sentences. The generated embeddings form a cluster. We processed to\ncalculate the centroid of those embeddings and adjusted the embeddings slightly to the centroid. (1-5%)\nThis minor adjustment is based on the assumption that those sentences are related and, thus, they\nshould be closer to one another. This process is done through the tags we have available. This ideology\ncan be shown in Figure 5.6. Finally, the updated embeddings will serve as gold labels for what the\nembeddings of the same model should look like. We then applied the mean-squared error loss, similar\nto Multilingual Knowledge Distillation, to train the model. The process is illustrated in Figure 5.6.\nFigure 5.4: Metadata Knowledge Distillation Ideology\nWe used this technique with a learning rate of 10−6and a batch size of 3sentences for one epoch.\nWe adjusted the embeddings of a 1000 document sample selected at random from the training document\nsubset. The embeddings were adjusted based on each tag’s centroids, centralizing the embeddings by\n1%. These hyperparameter choices were defined based on a grid search optimization algorithm where\nwe tried to maximize the STS task evaluation. With this technique, the following models were generated:\n63\nFigure 5.5: Metadata Knowledge Distillation\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-MetaKD-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-MetaKD-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\n5.7 Overview\nTable 5.1 provides an intuitive summary of the different versions created throughout this work, where it\npoints out the different techniques used for each variant.\nModel MLM TSDAE STS NLI GPL MKD MetaKD\nmlm x\ntsdae x\nmlm-sts-v0 x x\ntsdae-sts-v0 x x\nmlm-sts-v1 x x\ntsdae-sts-v1 x x\nmlm-nli-sts-v0 x x x\nmlm-nli-sts-v1 x x x\ntsdae-nli-sts-v0 x × x\ntsdae-nli-sts-v1 x x x\nmlm-gpl-nli-sts-v0 x x x x\nmlm-gpl-nli-sts-v1 x x x x\ntsdae-gpl-nli-sts-v0 x × x x\ntsdae-gpl-nli-sts-v1 x x x x\nmlm-mkd-nli-sts-v0 x x x x\nmlm-mkd-nli-sts-v1 x x x x\ntsdae-mkd-nli-sts-v0 x × x x\ntsdae-mkd-nli-sts-v1 x x x x\nmlm-gpl-nli-sts-MetaKD-v0 x x x x x\nmlm-gpl-nli-sts-MetaKD-v1 x x x x x\ntsdae-gpl-nli-sts-MetaKD-v0 x x x x x\ntsdae-gpl-nli-sts-MetaKD-v1 x x x x x\nTable 5.1: Legal-BERTimbau variants\n64\n6\nSystem Evaluation\nContents\n6.1 Language Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.2 Search System Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n65\n66\nThis work aimed to improve the information retrieval process court professionals have to go through,\nwith the ultimate goal of helping in the court decision process. Judges would have to go through immense\nand often dispersed information to formulate a decision. This process is well known for being time-\nconsuming and complex, and there is no guarantee that all the relevant documents are retrieved for user\nconsultation. A poor-made decision based on incomplete information can open a precedent for future\nscenarios and should be avoided.\nEvaluating the performance of a search system is a critical task in information retrieval research. A\ngood evaluation methodology can provide insights into the strengths and weaknesses of the system and\nguide the development of future improvements. To realise the search system’s performance, we require\nexamples of queries and expected results. Unfortunately, the required data was not available, and\ngenerating the aforementioned subsets of data is a complex process since it requires manual annotation.\nAnd even trying to replicate those assets, can introduce bias to our evaluation system.\nChapter 6 provides insights into the project’s evaluation. Throughout this project, we explored differ-\nent search system architectures with different trained language models. These language models were\ntailored to our scenario, understanding the Portuguese legal context better than state-of-the-art generic\nmodels. Since the large language models play a critical role in our final solution, Chapter 6 also provides\ninsights into such models. As such, the evaluation is composed of two parts.\nFirstly, we evaluate the developed language models. We evaluate whether the developed models are\ntrimmed for the Portuguese legal context and their performance on their task: STS.\nThe second part will be a quantitative evaluation of the search system itself. We compare the perfor-\nmance of our solution, comparing it to traditional techniques such as BM25 on two different “dimensions”.\nSuch evaluation provides insights into the system’s ability to retrieve relevant information helpful to the\njudges.\n67\n6.1 Language Model Evaluation\nThis thesis led to the development of multiple Legal-BERTimbau versions. From fine-tuning large vari-\nants of BERTimbau to applying the Multilingual Knowledge Distillation technique to models, there were\nnumerous variants implemented. This section will provide the details on the evaluation performed on the\ndifferent Legal-BERTimbau variants. It starts by verifying if the models were successfully adapted to the\nPortuguese legal domain. Then it will verify if the variants have a good performance in the STS task.\n6.1.1 Domain Adaptation\nThe domain adaptation is a crucial technique as it helps to address the problem of model generalization,\nespecially when there is a lack of labelled data in the target domain. The importance of domain adapta-\ntion lies in its ability to improve the effectiveness of models when applied to new and unseen data (See\nSection 5.1).\nIn order to evaluate whether the domain adaptation stage was successful or not, we verified how\nwell the do Legal-BERTimbau variants handle Portuguese legal text. Such was done by comparing\nhow successful the Legal-BERTimbau could replace [MASK] tokens for the correct words in the MLM\ntask. We made use of the same loss function used in the MLM task, negative log-likelihood loss. In this\nscenario, a lower average loss value would indicate that the model performs well.\nWe utilised the testing dataset from the splits generated in Subsection 4.2 and compared the average\nloss produced (Shown in Table 6.1.1). We also submitted the models subjected to the TSDAE technique\nto this evaluation.\nAVG loss\nBERTimbau large 18.60706\nstjiris/bert-large-portuguese-cased-legal-mlm 0.02679\nstjiris/bert-large-portuguese-cased-legal-tsdae 10.51178\nTable 6.1: MLM average loss for legal documents in the test set\nRegarding our testing split, both stjiris/bert-large-portuguese-cased-legal-mlm andstjiris/bert-large-\nportuguese-cased-legal-tsdae perform better on the MLM task than BERTimbau . Both models sub-\njected to MLM and TSDAE domain adaptation techniques can predict more accurately which words\nshould replace in the [MASK] tokens. The model subjected to TSDAE presents a more significant aver-\nage loss than the model subjected to the MLM technique. This is to be expected since the model was\nnot trained on that same task. Nevertheless, despite not being trained for the MLM task, it still outper-\nformed BERTimbau for legal documents in our test set. Thus, validating our conjecture, our models are\nsuccessfully adapted to the Portuguese legal domain.\n68\n6.1.2 Semantic Textual Similarity\nThe main task of a large language model within our search system architecture is to determine how\nsemantically similar two sentences are, STS. To evaluate a model’s STS task performance, we evaluate\nif a model is able to provide accurate similarity scores for a given sentence pair. For such effect, it is\ncalculated the Pearson correlation [41] between the expected and projected similarity score between\ndifferent sentence pairs.\nThe Portuguese datasets used in the fine-tuning stage were also utilised to evaluate these mod-\nels. We proceeded to evaluate using those datasets’ test splits for the STS task. We can analyse the\nperformance of the different versions of Legal-BERTimbau against state-of-the-art multilingual models\nwhen performing the same task. These state-of-the-art multilingual models serve as a baseline for our\nevaluation. For this effect, we selected the following multilingual models: paraphrase-multilingual-mpnet-\nbase-v2 andall-mpnet-base-v2\nassin assin2 stsb multi mt Avg.\npt\nBERTimbau large Fine-tuned for STS 0.81289 0.84133 0.77958 0.81126\nparaphrase-multilingual-mpnet-base-v2 0.74373 0.83999 0.71468 0.76613\nall-mpnet-base-v2 0.56306 0.62126 0.51287 0.56573\nmlm-sts-v0 0.78509 0.81158 0.83625 0.81097\nmlm-nli-sts-v0 0.78095 0.81001 0.83684 0.80927\nmlm-gpl-nli-sts-v0 0.78119 0.81187 0.83543 0.80950\nmlm-mkd-nli-sts-v0 0.77634 0.80976 0.84779 0.81130\ntsdae-sts-v0 0.78597 0.81542 0.84424 0.81521\ntsdae-nli-sts-v0 0.78430 0.80311 0.83842 0.80861\ntsdae-gpl-nli-sts-v0 0.77862 0.80675 0.83925 0.80821\ntsdae-mkd-nli-sts-v0 0.78008 0.84145 0.85060 0.80503\nmlm-gpl-nli-sts-MetaKD-v0 0.81115 0.83634 0.78210 0.80987\ntsdae-gpl-nli-sts-MetaKD-v0 0.80743 0.84041 0.78294 0.81026\nmlm-sts-v1 0.78025 0.81479 0.83460 0.80988\nmlm-nli-sts-v1 0.77740 0.80975 0.83588 0.80768\nmlm-gpl-nli-sts-v1 0.78143 0.80964 0.83610 0.80905\nmlm-mkd-nli-sts-v1 0.77274 0.812149 0.84997 0.81162\ntsdae-sts-v1 0.78433 0.81610 0.84320 0.81454\ntsdae-nli-sts-v1 0.78251 0.80494 0.84077 0.80941\ntsdae-gpl-nli-sts-v1 0.77634 0.80673 0.83889 0.80731\ntsdae-mkd-nli-sts-v1 0.77368 0.81603 0.85495 0.81489\nmlm-gpl-nli-sts-MetaKD-v1 0.80767 0.83701 0.77955 0.80807\ntsdae-gpl-nli-sts-MetaKD-v1 0.80543 0.83467 0.77749 0.80586\nTable 6.2: STS evaluation on Portuguese datasets\nIt is possible to verify that our SBERT variants performed better than state-of-the-art multilingual mod-\nels on the STS task for both assin andassin2 datasets. Regarding the performance on the stsb multi mt\ndataset, the values obtained do not outperform multilingual models. stsb multi mtis a dataset composed\nof different multilingual translations from the original STSbenchmark dataset. Consequently, multilingual\nmodels did engage with multiple translations from the same sentence during the training process. Nev-\nertheless, the score is similar, and, more importantly, BERTimbau variants are adapted to our domain,\nas exposed previously. This quantitive evaluation aims to verify if our models still understand the Por-\n69\ntuguese Language in general and can comprehend the sentence similarities in our domain through our\ncustom STS dataset.\n6.2 Search System Evaluation\n6.2.1 Automatic Query Generation\nIn order to evaluate our search system, we needed a group of queries and expected retrieved results to\nevaluate its performance. In our scenario, there is no such group of data for this effect. For this reason,\nwe had to explore an automatic generation of examples to provide a preliminary evaluation – or at least\nto help us understand the potential performance of the system. With such queries and information about\nthe document from which they were generated, we are able to assess our system’s performance.\nOur solution to evaluate the system performance passed through creating embeddings from a col-\nlection of 1000 legal documents and store them in ElasticSearch. Then we generate queries from each\ndocument and utilise those same queries to test the system. Our assumption is that a search system\nshould be able to return as a result the original document (or passage) used to automatically build a\nparticular query. The evaluation architecture was implemented as described in Figure 6.2.1.\nFigure 6.1: Evaluation Architecture\n70\nThe first step is related to creating queries from each document summary that we use later on. For\nthis effect, we tried two different approaches.\nInitially, we utilised the LexRank summarization technique, using the original BERTimbau large\nmodel, to retrieve the sentence with the highest centrality. This approach would provide the most impor-\ntant sentence in the summary, but it would yield a sentence that contains no new words, meaning we are\nnot exploring the full capabilities of a Semantic Search System. If a user inputs a query with words that\ndo not appear within the document, the lexical techniques would not provide adequate results. Thus,\naiming to explore the full extent of this scenario, we processed all the queries through a GPT3 model\nprovided by Open AI to rewrite the sentences, whilst keeping the same meaning. Unfortunately, this ap-\nproach did not provide the query examples we were hoping for. Often it would use the exact keywords,\nonly in a different order.\nAfter further iterations, we utilised a T5 model to generate queries reducing bias. Nevertheless, this\napproach lead to queries that made use of keywords present within the summary. Subsequently, we\ntreated the queries, so that they maintained a similar meaning, but did not contain every exact key-\nword. We identified the top 20 keywords from the summary with TF-IDF and exchanged them with syn-\nonyms or similar expressions. The synonyms gathering was performed by producing multiple requests\ntohttps://www.sinonimos.com.br/ , a website that provides synonyms for Brazilian Portuguese words.\nThe selected synonyms and expressions that replaced the keywords were chosen by evaluating which\nexchange would better preserve the meaning of the queries. To do so, we used BERTimbau large and\nevaluated the different sentences using cosine similarity.\nThe second step was creating embeddings for each sentence using different versions of our devel-\noped model, Legal-BERTimbau. Each sentence embedding is stored in an ElasticSearch index. Each\nindexed document would be composed of a text data field, a dense vector data field that contained the\nsentence embedding, and an indication of the document from where the sentence was retrieved.\n6.2.2 Results\nWe retrieve the results using the cosine similarity metric for the Semantic Search System and combined\nthe cosine similarity metric with BM25 technique to replicate the proposed Hybrid Search Systems. Our\ncomparison baseline consisted on utilising BM25 searches with the same queries and other multilingual\nmodels such as “sentence-transformers/all-mpnet-base-v2” and “paraphrase-multilingual-mpnet-base-\nv2” instead of Legal-BERTimbau versions. The evaluation dataset, since it was generated automatically,\ndid not allow an overview of False or True Positives, and neither it did for False and True Negatives.\nAs such, we could not use traditional metrics such as Precision and Recall to evaluate the system’s\nperformance. We defined two alternative metrics to evaluate the system’s performance: Search and\nDiscovery.\n71\nThe Search metric allows gathering insights into the system’s ability to find which document a certain\nquery refers to. Initially, each query was generated from individual summaries. If the search system re-\nceives a query based on document x, the retrieved result should be from the document x. We evaluated\nwhether this happens within the first result or the first group of results for a specific query. If the retrieved\ndocument is the same as the one used for creating the query, it increases the evaluation score by 1.\nThe Discovery metric provides interpretability on a search system’s ability to retrieve additional doc-\numents that might be relevant to the user. The search system should retrieve documents that are\nimportant for a given query, even if they are not the original document from which the query was gen-\nerated. Each legal document has “descritores” (tags) that were manually annotated. Similarly to the\nSearch metric, if the retrieved document within a group of results contains a tag equal to the original\ndocument’s tags from which the query was generated, we increase the score. For each equal tag, the\nDiscovery metric score is increased by one, calculating the intersection between each retrieved passage\nand the original document tags. In practice, if the search system receives a query that was created from\na document ycontaining the tag “Knives”, we want to evaluate whether the same tags appear within the\ntop results.\nSearch metric results for models fine-tuned without and with the custom STS dataset are shown in\nFigures 6.2 and 6.3, respectively.\nWe evaluated the Semantic Search System performance based on the different top results sizes. We\nverified whether the queries’ original document is suggested within the first result (Top 1), the first two\nresults (Top 2), and so forth. Likewise, we evaluated the Top 1, Top 2, Top 3, Top 5, Top 10, and Top 20.\nBM25 outperforms our original Semantic Search System in the Search metric. It also shows that\na Semantic Search System using a Multilingual model would perform considerably worse than when\nusing Legal-BERTimbau. For this metric, the Lexical-First approach that we proposed performs closer to\nBM25 and the Lexical+Semantic can even occasionally surpass BM25, maintaining a tight performance\nbetween the two. This metric verifies that BM25 can identify a query source better than a Semantic\nSearch System. However, a Hybrid Search System such as the one we developed can match and even\noutperform BM25 capabilities. Search Systems using models that were fine-tuned on the custom STS\ndataset (V1 models) present a slightly lower performance than the ones fine-tuned only on pre-existing\nand manually annotated datasets (V0 models).\nSimilarly, for the Discovery metric, the system performance using models fine-tuned without and with\nthe custom STS dataset are shown in Figures 6.4 and 6.5, respectively.\n72\nFigure 6.2: Search System Evaluation – Search metric - Models V0\nWe evaluated the Semantic Search System performance based on the different top results sizes.\nWe verified whether the queries’ original document tags appeared within the first result (Top 1), the first\ntwo results (Top 2), and so forth. Likewise, we also evaluated the Top 1, Top 2, Top 3, Top 5, Top 10,\n73\nFigure 6.3: Search System Evaluation – Search metric - Models V1\nand Top 20. Search Systems using models fine-tuned on the custom STS dataset (V1 models) perform\nmarginally worse than those fine-tuned on pre-existing and manually annotated datasets (V0 models).\nThis metric shows that BM25 does not have the same capability to recommend similar documents.\nWhen searching for a specific matter, it is often beneficial for a Search System to retrieve relevant docu-\nments we were unaware of. As such, we can verify that our Semantic Search System can suggest similar\n74\nFigure 6.4: Search System Evaluation - Discovery metric - Models V0\ndocuments more regularly than BM25 or any other search system using a multilingual model. Surpris-\ningly, both our Lexical-First and Lexical+Semantic Search Systems outperform our semantic search\nsystem significantly.\n75\nFigure 6.5: Search System Evaluation - Discovery metric - Models V1\nWe averaged the Top 1, Top 2, Top 3, Top 5, Top 10, and Top 20 metric results for each metric, raising\nsome understanding of the techniques used in the research. The variations that received NLI training\nhad a 4.3%improvement in the Search metric and a 5.4%improvement in the Discovery measure.\nModels subjected to the GPL training approach show a 3.2%improvement in the Search metric and a\n1.7%improvement in the Discovery metric. Finally, models that used our novel method exhibited a 2.0%\n76\ndrop in the Search metric but a 1.7%decrease in the Discovery metric.\nWith these results, we selected our best-performing model, tsdae-gpl-nli-sts-MetaKD-v0, to analyse\nin more detail. Tables 6.3 and 6.4 provide more quantitive insights on the development.\nModel Top 1 Top 2 Top 3 Top 5 Top 10\nBM25 629 696 722 760 799\nPurely Semantic 411 471 518 559 618\nLexical-First 581 635 668 716 754\nLexical + Semantic 629 675 705 734 785\nImprovement 0% -3.01 % -2.35% -3.42% -1.75%\nTable 6.3: Search System Evaluation – Search metric – Best Model\nModel Top 1 Top 2 Top 3 Top 5 Top 10\nBM25 685 933 1133 1482 2216\nPurely Semantic 2226 2932 3532 4332 6243\nLexical-First 2769 3485 3993 4835 6352\nLexical + Semantic 2984 3732 4292 5168 7282\nImprovement 335% 300 % 278% 248% 228%\nTable 6.4: Search System Evaluation – Discovery metric – Best Model\nOur work explored three different search system architectures, and we concluded that combining\nboth lexical and semantic capabilities is a better approach than using only technique isolated. The\nLexical + Semantic Search System, which combines the strengths of both lexical and semantic systems,\nperformed similarly to BM25 in the Search metric, with performance delta of around −2.1%. In the\nDiscovery metric, it clearly outperforms the BM25 technique, going from around 335% improvement on\nthe Top 1 results and topping at 228% improvement on the Top 10 results. Such outcomes suggest that\nthe proposed search systems can improve the decision-making process in the legal domain by providing\nrelevant and insightful information.\nWhen considering these results, we can argue that our proposed and developed Lexical + Semantic\nSearch System can provide beneficial results and insights that ease and enrich the work judges perform\nin the legal domain. Similarly, this capability is extended to other professionals and non-professionals.\nBased on our research, we believe that the utilization of such search system can improve the actual\ncourt decision process and reduce the limitation and risks of formulating a decision based on incomplete\ninformation.\n77\n78\n7\nConclusion\nContents\n7.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n79\n80\nThe work presented in this document comes from a growing necessity for providing complete infor-\nmation to judges and legal authorities. Formulating a decision or gathering insight from the vast legal\ndomain is a very complex and often time-consuming task. This work provided insights into creating a\nsystem that utilises semantic search through embeddings, exploring the immense potential of neural\nnetworks instead of using more traditional lexical techniques. We explored the capability of Elastic-\nSearch to host a semantic search system, developing the necessary language models along the way.\nWe handled different problems when treating raw text data, tackling the need to pre-process the text\nbefore performing further tasks. Working with multiple Project IRIS members, we develop a prototype of\na Semantic Search System for STJ and provided support for other related tasks.\nBeing a segment of a more significant project: Project IRIS, this thesis allowed me to develop soft\nand hard skills by interacting with outstanding professionals and doctorate students. It provided insight\ninto effectively researching daily and cooperating on a large project with multiple members.\n7.1 Contributions\nWe created initial datasets containing pre-processed text that can be utilised for future tasks and the\nnecessary scripts to recreate the results. Along the work, we created multiple language models that can\nbe fine-tuned for multiple tasks, such as Question-Answering. Legal-BERTimbau developed variants\nwere trained entirely in Portuguese and adapted to the Portuguese legal domain.\nOur Legal-BERTimbau variants represent the first publicly available Portuguese BERTs fine-tuned for\nSTS. On the same note, as mentioned previously, all the developed SBERT variants are adapted to the\nPortuguese legal domain since they suffered domain adaptation techniques. Multiple Legal-BERTimbau\nvariants surpass state-of-the-art multilingual language models on assin , and assin2 datasets, which\nPortuguese and Brazilian researchers annotated.\nAs intended with this thesis, we proposed and developed a Semantic Search System and two dif-\nferent versions of Hybrid Search Systems: Lexical-First and Lexical+Semantic Search Systems. Based\non our preliminary evaluation, we believe our developed Search System can suggest more accurately\nrelevant documents than BM25 with a Discovery performance improving over 300% in some scenarios,\nwhile still maintaining a similar ability to identify a query’s source when compared to the BM25 technique.\nWith such information retrieval systems in place, we consider that it is possible to more accurately\nprovide insights into jurisprudence, easing and improving judges’ essential work. Such Search System\ncan generate a positive impact throughout all Portuguese legal entities, helping to enrich and ease the\ndecision process inherent in each of these entities. Our Search System can play an important role in\nhelping formulate fair and essential decisions and maintaining the stability and consistency required in\napplying the law.\n81\n7.1.1 Publications\n7.1.1.A Exploring Embeddings Models for Portuguese Supreme Court Judgments Summariza-\ntion\nIn recent years, the field of legal text summarization has grown due to the increase in electronically\navailable legal documents. As part of a collaboration with project IRIS members, a scientific paper was\ndrafted, “Exploring Embeddings Models for Portuguese Supreme Court Judgments Summarization”.\nThe findings of this study will be submitted for publication in a peer-reviewed journal in the near future.\nLegal documents can differ based on language and legal system, and this paper proposes an ap-\nproach for automatically summarizing Portuguese Supreme Court judgments. Because of the unique\ncharacteristics of these judgments and the judges’ methods for summarizing them, the paper suggests\na set of pre-processing techniques to optimize the task. We implemented an extractive summarization\nsystem using the LexRank technique. To account for specific vocabulary in Portuguese that pre-trained\nmodels like BERTimbau or mBERT do not cover, we experimented with different tailored models. In\nthis work, multiple Legal-BERTimbau models originated from our research were used. The proposed\napproach achieves a ROUGE-1 score of 47.92 and a ROUGE-2 score of 22.50, better than reported\nscores for similar work in other languages.\n7.1.1.B Semantic Search System for Supremo Tribunal de Justic ¸a\nAs part of this thesis, it was drafted a paper covering the main aspects of this research and the impact\nit could have on the scientific domain. It was submitted to the EPIA Conference on Artificial Intelligence,\na well-established European conference in the field of AI, ended up being accepted.\nThe scientific paper covers the main most robust hybrid system that we present within this research,\nthe pure semantic search system, and the techniques used to train Legal-BERTimbau. The article\nalso emphasises our novel technique, Metadata Knowledge Distillation, as it brought good results in\nour environment. Such a technique can be further used in other domains and/or with other metadata\ninformation.\n7.2 Future Work\n7.2.1 Albertina\nWith the publication of Albertina PT -PT, in May 2023, there is now the possibility of training a model,\nadapted to the Portuguese jurisprudence, exclusively in European Portuguese. This implies that, inher-\nently, there should be improvements with this approach. It should translate into a more effective model\n82\nthat would better comprehend Portuguese legislation.\nAnother approach would be to train a European Portuguese model completely from scratch only on\nPortuguese Jurisprudence. Having a model only trained on a specific domain should yield improved\nresults over domain-agnostic or domain-adapted models.\n7.2.2 Dataset Annotation\nWhen developing a language model, one of the main concerns is the available data. Manually annotated\nPortuguese legal domain datasets could be produced and revised. Even though we developed datasets\nfor the Domain Adaption training step and STS, automating such can lead to biased and incoherent\ndata, especially the developed STS dataset.\nThe downside of this approach is that it requires manual work, nearly impossible to automatize. Both\nassin andassin2 were annotated manually by different groups of researchers/volunteers, oriented only\nby simple guidelines.\nNevertheless, a properly labelled and cleaned legal dataset from the Portuguese domain would be\nhelpful in future applications. Similarly, one dataset was developed and published, alongside a paper,\non 1st July 2022, entitled “Pile of Law: Learning Responsible Data Filtering from the Law and a 256 GB\nOpen-Source Legal Dataset” by Peter Henderson et al. [17]. It encompasses a large corpus of legal\nand administrative data from multiple U.S.A. entities. The Pile of Law paper also exposed the ethical\nchallenges it faced and how it was handled “biased, obscene, copyrighted, and private information”.\n7.2.3 Architecture Improvements\nIn terms of query expansion, one potential improvement to the Hybrid Search System is to incorporate\nknowledge graph embeddings. A knowledge graph is a structured representation of information, where\nentities and their relationships are represented by nodes and edges, respectively. By incorporating\nknowledge graph embeddings, the system would be able to understand the context of the query better\nand expand it with related entities and concepts, which would help in the relevant documents’ retrieval.\nAnother aspect that can be explored is the incorporation of additional information to the embeddings.\nAs explored in [11], it was inserted titles in the beginning of each sentence before generating the em-\nbedding. In [11], that approach did not provide satisfying results, but it can be interesting to explore\ninserting some metadata information into the embeddings.\nAnother approach is to use entity recognition, a technique that identifies named entities such as\npeople, organizations, and locations in a given text. By recognizing entities in the query, the system could\nexpand the query by including relevant entities, which would help retrieve more relevant documents.\nActive learning is a machine learning technique that enables the system to continuously learn from\n83\nthe user’s feedback. The system could present the user with a small set of documents, and the user\nwould provide feedback on which documents are relevant and which are not. The system would then\nuse this feedback to improve its performance over time. This approach can be especially useful in the\nlegal domain, where the relevance of a document can be challenging to determine, and expert feedback\ncan help the system improve its performance.\nImprovements in the prompts given to a GLM, such as for GPT -3, can provide a better model re-\nsponse regarding a user’s query. Having tailored prompts to either enrich the user’s query or to improve\nthe models’ response can guide to a more seamless interaction with the Search System and more\nconcise answers from the System itself.\nOverall, incorporating knowledge graph embeddings or entity recognition can lead to a more sophis-\nticated and accurate Search System that can help legal practitioners in their research and decision-\nmaking.\n84\nBibliography\n[1] A GIRRE , E., B ANEA , C., C ARDIE , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., G UO, W.,\nLOPEZ -GAZPIO , I., M ARITXALAR , M., M IHALCEA , R., R IGAU, G., U RIA, L., AND WIEBE, J.\nSemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In\nProceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) (Denver,\nColorado, 2015), ACL, pp. 252–263.\n[2] A GIRRE , E., B ANEA , C., C ARDIE , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., G UO, W.,\nMIHALCEA , R., R IGAU, G., AND WIEBE, J. SemEval-2014 task 10: Multilingual semantic textual\nsimilarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval\n2014) (Dublin, Ireland, 2014), ACL, pp. 81–91.\n[3] A GIRRE , E., B ANEA , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., M IHALCEA , R., R IGAU, G.,\nAND WIEBE, J. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual\nevaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-\n2016) (San Diego, California, 2016), pp. 497–511.\n[4] A GIRRE , E., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., AND GUO, W. *SEM 2013 shared task:\nSemantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics\n(*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual\nSimilarity (Atlanta, Georgia, USA, 2013), ACL, pp. 32–43.\n[5] A GIRRE , E., D IAB, M., C ER, D., AND GONZALEZ -AGIRRE , A. SemEval-2012 task 6: A pilot on se-\nmantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational\nSemantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2:\nProceedings of the Sixth International Workshop on Semantic Evaluation (USA, 2012), SemEval\n’12, ACL, p. 385–393.\n[6] B OOS, R., P RESTES , K., V ILLAVICENCIO , A., AND PADR´O, M. brWaC: A WaCky Corpus for Brazil-\nian Portuguese. In Computational Processing of the Portuguese Language (Cham, 2014), J. Bap-\n85\ntista, N. Mamede, S. Candeias, I. Paraboni, T. A. S. Pardo, and M. d. G. Volpe Nunes, Eds., Springer\nInternational Publishing, pp. 201–206.\n[7] B OWMAN , S. R., A NGELI , G., P OTTS , C., AND MANNING , C. D. A large annotated corpus for\nlearning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods\nin Natural Language Processing (Lisbon, Portugal, 2015), ACL, pp. 632–642.\n[8] C ER, D., D IAB, M., A GIRRE , E., L OPEZ -GAZPIO , I., AND SPECIA , L. SemEval-2017 task 1: Se-\nmantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the\n11th International Workshop on Semantic Evaluation (SemEval-2017) (Vancouver, Canada, 2017),\nAssociation for Computational Linguistics, pp. 1–14.\n[9] C ONNEAU , A., K IELA, D., S CHWENK , H., B ARRAULT , L., AND BORDES , A. Supervised learning\nof universal sentence representations from natural language inference data. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Language Processing (Copenhagen, Denmark,\n2017), Association for Computational Linguistics, pp. 670–680.\n[10] C ORDEIRO , N. NLP applied to portuguese consumer law. Master’s thesis, Instituto Superior\nT´ecnico, Universidade de Lisboa, 2022.\n[11] D AI, Z., AND CALLAN , J. Deeper text understanding for IR with contextual neural language model-\ning. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval (2019), ACM.\n[12] D EVLIN , J., C HANG , M.-W., L EE, K., ANDTOUTANOVA , K. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the ACL: Human Language Technologies, Vol 1 (Minneapolis, Minnesota,\n2019), ACL, pp. 4171–4186.\n[13] E RKAN , G., AND RADEV , D. R. Lexrank: Graph-based lexical centrality as salience in text summa-\nrization. J. Artif. Int. Res. 22 , 1 (2004), 457–479.\n[14] F ONSECA , E., S ANTOS , L., C RISCUOLO , M., AND ALUISIO , S. ASSIN: Avaliacao de similaridade\nsemantica e inferencia textual. In Computational Processing of the Portuguese Language-12th\nInternational Conference, Tomar, Portugal (2016), pp. 13–15.\n[15] G URURANGAN , S., M ARASOVI ´C, A., S WAYAMDIPTA , S., L O, K., B ELTAGY , I., D OWNEY , D., AND\nSMITH , N. A. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings\nof ACL (2020).\n[16] H E, P., L IU, X., G AO, J., AND CHEN, W. Deberta: Decoding-enhanced BERT with disentangled\nattention. In 2021 International Conference on Learning Representations (2021).\n86\n[17] H ENDERSON , P., K RASS , M. S., Z HENG , L., G UHA, N., M ANNING , C. D., J URAFSKY , D., AND\nHO, D. E. Pile of law: Learning responsible data filtering from the law and a 256GB open-source\nlegal dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (2022).\n[18] I NTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY AND EXPLORING ENGINEERING (IJITEE),\nVAISSNAVE , V., AND DEEPALAKSHMI , P. An Artificial Intelligence based Analysis in Legal domain.\n[19] K IM, M., R ABELO , J., AND GOEBEL , R. BM25 and transformer-based legal information extraction\nand entailment. In Proceedings of the COLIEE Workshop in ICAIL (2021).\n[20] K IM, S.-W., AND GIL, J.-M. Research paper classification systems based on TF-IDF and LDA\nschemes. In Human-centric Computing and Information Sciences (2019), vol. 9, p. 30.\n[21] K INGMA , D. P., ANDBA, J. Adam: A method for stochastic optimization. In 3rd International Confer-\nence on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings (2015), Y . Bengio and Y . LeCun, Eds.\n[22] M ARELLI , M., M ENINI , S., B ARONI , M., B ENTIVOGLI , L., B ERNARDI , R., AND ZAMPARELLI , R. A\nSICK cure for the evaluation of compositional distributional semantic models. In Proceedings of\nthe Ninth International Conference on Language Resources and Evaluation (LREC’14) (Reykjavik,\nIceland, 2014), European Language Resources Association (ELRA), pp. 216–223.\n[23] M AY, P. IMachine translated multilingual STS benchmark dataset. HuggingFace, stsb multi mt.\n[24] M IKOLOV , T., C HEN, K., C ORRADO , G., AND DEAN, J. Efficient estimation of word representations\nin vector space. In 1st International Conference on Learning Representations, ICLR 2013, Scotts-\ndale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings (2013), Y . Bengio and Y . LeCun,\nEds.\n[25] N GUYEN , H., V UONG , H. T., N GUYEN , P. M., D ANG, T. B., B UI, Q. M., S INH, V. T., N GUYEN ,\nC. M., T RAN, V. D., S ATOH , K., AND NGUYEN , M. L. JNLP team: Deep learning for legal process-\ning in COLIEE 2020. CoRR abs/2011.08071 (2020).\n[26] N GUYEN , T., R OSENBERG , M., S ONG, X., G AO, J., T IWARY , S., M AJUMDER , R., AND DENG, L.\nMS MARCO: A human generated machine reading comprehension dataset. CoRR abs/1611.09268\n(2016).\n[27] N IGAM , S. K., G OEL, N., AND BHATTACHARYA , A. nigam@coliee-22: Legal case retrieval and en-\ntailment using cascading of lexical and semantic-based models. In New Frontiers in Artificial Intelli-\ngence (Cham, 2023), Y . Takama, K. Y ada, K. Satoh, and S. Arai, Eds., Springer Nature Switzerland,\npp. 96–108.\n87\n[28] P ENNINGTON , J., S OCHER , R., AND MANNING , C. GloVe: Global vectors for word representation.\nInProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) (Doha, Qatar, 2014), Association for Computational Linguistics, pp. 1532–1543.\n[29] R AFFEL , C., S HAZEER , N., R OBERTS , A., L EE, K., N ARANG , S., M ATENA , M., Z HOU, Y., L I, W.,\nANDLIU, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal\nof Machine Learning Research 21 , 140 (2020), 1–67.\n[30] R EAL, L., F ONSECA , E., AND OLIVEIRA , H. G. The assin 2 shared task: a quick overview. In Inter-\nnational Conference on Computational Processing of the Portuguese Language (2020), Springer,\npp. 406–412.\n[31] R EIMERS , N., AND GUREVYCH , I. Sentence-BERT: Sentence Embeddings using Siamese BERT -\nNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing (2019), ACL.\n[32] R EIMERS , N., AND GUREVYCH , I. Making monolingual sentence embeddings multilingual using\nknowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (2020), ACL.\n[33] R EIMERS , N., AND GUREVYCH , I. Making monolingual sentence embeddings multilingual using\nknowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (2020), Association for Computational Linguistics.\n[34] R OBERTSON , S., AND ZARAGOZA , H. The Probabilistic Relevance Framework: BM25 and Beyond.\nFound. Trends Inf. Retr. 3 , 4 (2009), 333–389.\n[35] R ODRIGUES , J., G OMES , L., S ILVA, J., B RANCO , A., S ANTOS , R., C ARDOSO , H. L., AND OS´ORIO ,\nT. Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*, 2023.\n[36] R ONG, X. Word2Vec Parameter Learning Explained. CoRR abs/1411.2738 (2014).\n[37] S OUZA , F., N OGUEIRA , R., AND LOTUFO , R. BERTimbau: Pretrained BERT Models for Brazilian\nPortuguese. In Intelligent Systems (Cham, 2020), R. Cerri and R. C. Prati, Eds., Springer Interna-\ntional Publishing, pp. 403–417.\n[38] S OUZA , F., N OGUEIRA , R. F., AND DE ALENCAR LOTUFO , R. Portuguese named entity recognition\nusing BERT -CRF. CoRR abs/1909.10649 (2019).\n[39] T HAKUR , N., R EIMERS , N., R ¨UCKL ´E, A., S RIVASTAVA , A., AND GUREVYCH , I. BEIR: A heteroge-\nneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021).\n88\n[40] V ASWANI , A., S HAZEER , N., P ARMAR , N., U SZKOREIT , J., J ONES , L., G OMEZ , A. N., K AISER ,\nL.U.,ANDPOLOSUKHIN , I. Attention is all you need. In Advances in Neural Information Processing\nSystems (2017), vol. 30, Curran Associates, Inc.\n[41] V IRTANEN , P., G OMMERS , R., O LIPHANT , T. E., H ABERLAND , M., R EDDY , T., C OURNAPEAU , D.,\nBUROVSKI , E., P ETERSON , P., W ECKESSER , W., B RIGHT , J., VAN DER WALT, S. J., B RETT , M.,\nWILSON , J., M ILLMAN , K. J., M AYOROV , N., N ELSON , A. R. J., J ONES , E., K ERN, R., L ARSON ,\nE., C AREY , C. J., P OLAT,˙I., F ENG, Y., M OORE , E. W., V ANDER PLAS, J., L AXALDE , D., P ERK-\nTOLD , J., C IMRMAN , R., H ENRIKSEN , I., Q UINTERO , E. A., H ARRIS , C. R., A RCHIBALD , A. M.,\nRIBEIRO , A. H., P EDREGOSA , F., VANMULBREGT , P., AND SCIPY1.0 C ONTRIBUTORS . SciPy 1.0:\nFundamental Algorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261–272.\n[42] W ANG, K., R EIMERS , N., AND GUREVYCH , I. TSDAE: Using transformer-based sequential denois-\ning auto-encoderfor unsupervised sentence embedding learning. In Findings of the ACL: EMNLP\n2021 (Punta Cana, Dominican Republic, 2021), ACL, pp. 671–688.\n[43] W ANG, K., T HAKUR , N., R EIMERS , N., AND GUREVYCH , I. GPL: Generative pseudo labeling for\nunsupervised domain adaptation of dense retrieval. In North American Chapter of the ACL (2021).\n[44] W ILLIAMS , A., N ANGIA , N., AND BOWMAN , S. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of the 2018 Conference of the North American\nChapter of the ACL: Human Language Technologies, Vol 1 (New Orleans, Louisiana, 2018), ACL,\npp. 1112–1122.\n89\n90\n91"
  },
  "doc-08b6204e6ca3b057973880cf4f424b6a": {
    "content": "A Semantic Search System for the Supremo Tribunal de\nJustic ¸a\nRui Filipe Coimbra Pereira de Melo\nThesis to obtain the Master of Science Degree in\nComputer Science and Engineering\nSupervisors: Prof. Pedro Alexandre Sim ˜oes dos Santos\nProf. Jo ˜ao Miguel De Sousa de Assis Dias\nExamination Committee\nChairperson: Prof. Maria Lu ´ısa Torres Ribeiro Marques da Silva Coheur\nSupervisor: Prof. Pedro Alexandre Sim ˜oes dos Santos\nMember of the Committee: Prof. Jos ´e Lu´ıs Brinquete Borbinha\nJune 2023\n\nAcknowledgments\nI want to express my sincerest gratitude and appreciation to my supervisors, Professor Pedro Alexan-\ndre Santos and Professor Jo ˜ao Dias, who guided me throughout the entirety of this work and relentlessly\noffered much-needed advice, sound counsel and honest feedback. They were infallible in being reliable\nand continuously kept in touch with the progress of this thesis. They always provided input on the many\nupdates, advancements, and setbacks that occurred, and I am genuinely grateful.\nOn the same note, I would like to thank all Project IRIS members I co-operated with in the past\nmonths. The resonating experience of partaking in a more significant project with brilliant minds will\nshape my future ventures.\nLastly, there is no way I could wholly express in words the unwavering support and unconditional love\nI received from my family. To my mother and father, from the bottom of my heart. . . thank you.\ni\n\nAbstract\nMany information retrieval systems use lexical approaches to retrieve information. Such approaches\nhave multiple limitations, and these constraints are exacerbated when tied to specific domains, such as\nthe legal one. Large language models, such as BERT, deeply understand a language and may overcome\nthe limitations of older methodologies, such as BM25.\nThis work investigated and developed a prototype of a Semantic Search System to assist the Supremo\nTribunal de Justic ¸a (Portuguese Supreme Court of Justice) in its decision-making process.\nWe built a Semantic Search System that uses specially trained BERT models (Legal-BERTimbau\nvariants) and Hybrid Search Systems that incorporate both lexical and semantic techniques by com-\nbining the capabilities of BM25 and the potential of Legal-BERTimbau. In this context, we obtained a\n335% increase on the discovery metric when compared to BM25 for the first query result. This work also\nprovides information on the most relevant techniques for training a Large Language Model adapted to\nPortuguese jurisprudence and introduces a new technique of Metadata Knowledge Distillation.\nKeywords\nArtificial Intelligence; BERT; Information Retrieval; Natural Language Processing; Jurisprudence; SBERT\niii\n\nResumo\nOs sistemas de recuperac ¸ ˜ao de informac ¸ ˜ao utilizam frequentemente abordagens lexicais para recuperar\ninformac ¸ ˜ao. Tais abordagens t ˆem m ´ultiplas limitac ¸ ˜oes, e estas limitac ¸ ˜oes s ˜ao agravadas quando ligadas\na dom ´ınios espec ´ıficos, tais como o legal. Large Language Models, como o BERT, compreendem\nprofundamente uma linguagem e podem ultrapassar as limitac ¸ ˜oes de metodologias mais antigas, como\no BM25.\nEste trabalho investigou e desenvolveu um prot ´otipo de um Sistema de Busca Sem ˆantica para assi-\nstir o Supremo Tribunal de Justic ¸a portugu ˆes no seu processo de tomada de decis ˜ao.\nConstru ´ımos um Sistema de Pesquisa Sem ˆantica que utiliza modelos BERT especialmente treina-\ndos (variantes Legal-BERTimbau) e Sistemas de Pesquisa h ´ıbrida que incorporam tanto t ´ecnicas lex-\nicais como sem ˆanticas, combinando as capacidades da BM25 e o potencial da Legal-BERTimbau.\nReportamos um aumento de desempenho de 335% na recuperac ¸ ˜ao de passagens relevantes quando\ncomparado com BM25 para o resultado da primeira consulta.\nEste trabalho tamb ´em fornece informac ¸ ˜oes sobre as t ´ecnicas mais relevantes para a formac ¸ ˜ao de\num Modelo de Grandes L ´ınguas adaptado `a jurisprud ˆencia portuguesa e introduz uma nova t ´ecnica,\nMetadata Knowledge Distillation.\nPalavras Chave\nBERT; Intelig ˆencia Artificial; Jurisprud ˆencia; Processamento de Linguagem Natural; Recuperac ¸ ˜ao de\nInformac ¸ ˜ao; SBERT\nv\n\nContents\n1 Introduction 1\n1.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2 Background Review 5\n2.1 Lexical approaches for Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.1 Term Frequency Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.2 Best Matching Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.3 Distance metrics for lexical approaches . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Neural Networks for Semantic Information Retrieval . . . . . . . . . . . . . . . . . . . . . 9\n2.2.1 Word and Sentence Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2.2 Word2Vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2.3 GloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.4 Recurrent Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2.5 Long Short-Term Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.6 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3 Semantic Search Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4 Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.4.1 LexRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3 State of the Art 25\n3.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.1.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.1.2 Fine-tuning on Downstream Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.2 BERTimbau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3 Deeper Text Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.4 Legal Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.5 NLP Applied To Portuguese Consumer Law . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nvii\n3.6 Albertina PT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4 Semantic Search System 43\n4.1 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.1.1 ElasticSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.2 The Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.2.1 Data Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.3.1 Purely Semantic Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3.2 Lexical-First Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3.3 Lexical + Semantic Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n5 Legal Language Model 53\n5.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n5.1.1 Masked Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n5.1.2 Transformer-based Sequential Denoising Auto-Encoder . . . . . . . . . . . . . . . 57\n5.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.2.1 Semantic Textual Similarity Custom Dataset . . . . . . . . . . . . . . . . . . . . . . 59\n5.3 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.4 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n5.5 Multilingual Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n5.6 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.7 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6 System Evaluation 65\n6.1 Language Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.1.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.1.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.2 Search System Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2.1 Automatic Query Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n7 Conclusion 79\n7.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.1.1 Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2.1 Albertina . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.2.2 Dataset Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\nviii\n7.2.3 Architecture Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\nBibliography 85\nix\nx\nList of Figures\n2.1 Example of Word Embeddings in a 3D vector space . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Word2Vec with Common Bag Of Words (CBOW) model based on a one word context. . . 11\n2.3 Word2Vec with CBOW model based on multiple words context. . . . . . . . . . . . . . . . 11\n2.4 Word2Vec with Skip-Gram model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.5 Recurrent Network Fully Connected . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.6 RNN Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.7 LSTM Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.8 Transformer Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.9 Scaled Dot-Product Attention and Multi-Head Attention . . . . . . . . . . . . . . . . . . . . 19\n2.10 Vector space with a query embedding and multiple sentence embeddings . . . . . . . . . 20\n2.11 Bi-Encoder and Cross-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.12 LexRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.1 BERT input representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.2 Data Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.3 Masked Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.4 Transformer-based Sequential Denoising Auto-Encoder (TSDAE) Architecture . . . . . . . 31\n3.5 T5 diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.6 GenQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.7 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.8 Fine-Tuning SBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.9 Multilingual Knowledge Distillation Process . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n4.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.2 Lexical-First Search System Retrieval Method . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3 Lexical + Semantic Search System Retrieval Method . . . . . . . . . . . . . . . . . . . . . 51\n5.1 Training Tasks Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\nxi\n5.2 Masked Language Modeling (MLM) Training Loss . . . . . . . . . . . . . . . . . . . . . . . 57\n5.3 TSDAE Training Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n5.4 Metadata Knowledge Distillation Ideology . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.5 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.1 Evaluation Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.2 Search System Evaluation – Search metric - Models V0 . . . . . . . . . . . . . . . . . . . 73\n6.3 Search System Evaluation – Search metric - Models V1 . . . . . . . . . . . . . . . . . . . 74\n6.4 Search System Evaluation - Discovery metric - Models V0 . . . . . . . . . . . . . . . . . . 75\n6.5 Search System Evaluation - Discovery metric - Models V1 . . . . . . . . . . . . . . . . . . 76\nxii\nList of Tables\n3.1 SBERT Spearman correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.2 Sentence-BERT (SBERT) evaluation on the Semantic Textual Similarity (STS) benchmark 36\n3.3 BERTimbau variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.4 COLIEE 2021 - Task 1 results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.5 COLIEE 2021 - Task 3 results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.1 Legal-BERTimbau variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.1 MLM average loss for legal documents in the test set . . . . . . . . . . . . . . . . . . . . . 68\n6.2 STS evaluation on Portuguese datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.3 Search System Evaluation – Search metric – Best Model . . . . . . . . . . . . . . . . . . 77\n6.4 Search System Evaluation – Discovery metric – Best Model . . . . . . . . . . . . . . . . . 77\nxiii\nxiv\nAcronyms\nAI Artificial Intelligence\nBERT Bidirectional Encoder Representations from Transformers\nBM25 Okapi BM25\nBrWaC Brazilian Web as Corpus\nCBOW Common Bag Of Words\nCOLIEE Competition on Legal Information Extraction and Entailment\nDA Domain Adaptation\nDH Distributional Hypothesis\nFCUL Faculdade de Ci ˆencias da Universidade de Lisboa\nFEUP Faculdadede Engenharia da Universidade do Porto\nGPL Generative Pseudo Labeling\nGLM Generative Language Model\nGloVe Global Vectors for Word Representation\nIDF Inverse Document Frequency\nIR Information Retrieval\nLSTM Long Short-Term Memory\nLeSSE Legal Semantic Search Engine\nMetaKD Metadata Knowledge Distillation\nMLM Masked Language Modeling\nMKD Multilingual Knowledge Distillation\nML Machine Learning\nMNR Multiple Negatives Ranking\nxv\nNLI Natural Language Inference\nNLP Natural Language Processing\nNL Natural Language\nNN Neural Network\nNSP Next Sentence Prediction\nQA Question and Answer\nRNN Recurrent Neural Network\nSBERT Sentence-BERT\nSNLI Stanford Natural Language Inference\nSTJ Supremo Tribunal de Justic ¸a\nSTS Semantic Textual Similarity\nT5 Text-to-Text Transfer Transformer\nTF-IDF Term Frequency-Inverse Document Frequency\nTF Term Frequency\nTSDAE Transformer-based Sequential Denoising Auto-Encoder\nUKP Ubiquitous Knowledge Processing\nxvi\n1\nIntroduction\nContents\n1.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1\n2\nSupremo Tribunal de Justic ¸a (STJ) serves as Portugal’s highest judiciary court, also known as the\nSupreme Court of Justice. It plays a crucial role in making well-informed, lawful, and ethical decisions\nthat have a profound impact on the specific case at hand and future cases. To arrive at a decision, a\ncomprehensive examination of the relevant jurisprudence is indispensable. This exhaustive consultation\nserves as the bedrock for a thoughtful and principled judgment.\nWhen a judge is tasked with formulating a decision, whether it involves researching specific legis-\nlation or referring to precedents from similar cases, the process of locating the necessary information\nis far from trivial, speedy, or efficient. Effectively managing and accessing such vast volumes of infor-\nmation necessitates the utilization of a robust Information Retrieval (IR) system. Such a system is an\nindispensable tool in ensuring efficient access to the required legal resources.\nThe court decision process could be improved based on a search system that would help retrieve\nthe desired information and identify that information source. Even though traditional lexical techniques\ncould be implemented to solve this need, they might raise other issues. Lexical techniques work by\nsearching for specific word matches from a given query. Using lexical techniques in a search system\ncan only retrieve so much information that might include only some cases or sections of legislation that\nare relevant to that specific instance. A search system that utilizes semantic techniques, that work by\ntrying to understand the semantic meaning of a query, and finding document passages closer in meaning\nto the query, is essential to reach a broader range of relevant documents.\nA country’s judiciary system should resist bad decisions based on incomplete or inaccurate informa-\ntion. The occurrence of such events opens the door to unjust precedents in future cases, thus leading\nto imbalances in the overall decision-making, which ultimately leads to the weakening and destabiliza-\ntion of the justice system. Establishing a reliable legal search system is necessary since it promotes\nconsistency in applying the law.\n1.1 Objectives\nThe research presented in this thesis is a contribution to the IRIS project developed by INESC-ID Lisboa\nfor STJ. Project IRIS aims to develop summarization approaches for court decisions and create a\nrepresentation able to be browsed in a way that is helpful in the court decision process.\nAs mentioned beforehand, retrieving desired information might be more complex than one wishes.\nThe first challenge someone faces when procuring information, is in the choosing of (adequate) words\nto input into the search query. Even though judges might use specific terminology terms, the system\nshould be resilient enough to access information based on Natural Language (NL). One will always find\nadversity when constructing a sophisticated and appropriate query that expresses the correct need for\nspecific information.\n3\nHaving these motivations well-defined, it is the aim of this thesis to develop a reliable search system\nthat uses semantic strategies for STJ that will help in the court decision process.\n1.2 Contributions\nThrough the making of this work, three separate types of assets were produced, and promptly deliv-\nered available to appropriate recipients, under the form of contributions. Firstly, we developed multiple\ndatasets from the various Portuguese legal documents available and published them on the Internet.\nThese datasets are publicly available in the HuggingFace platform, allowing them to be easily used\nin other scenarios. Secondly, numerous generic and fine-tuned for Semantic Textual Similarity (STS)\nlanguage models were developed and made available on the same platform. All developed large lan-\nguage models were successfully adapted to the Portuguese legal domain. Furthermore, we developed\na search system with the intent of helping judges in the court decision process. Finally, from the re-\nsults of this work, two papers were written. The first one, Exploring Embeddings Models for Portuguese\nSupreme Court Judgments Summarization, is currently on the verge of being submitted. The second\npaper, Semantic Search System for Supremo Tribunal de Justic ¸a, was submitted to the EPIA Conference\non Artificial Intelligence and it is currently under review.\n1.3 Thesis Outline\nChapter 2 provides essential elements for understanding subsequent content, while Chapter 3 show-\ncases state-of-the-art models and relevant scientific papers used in the final solution.\nIn Chapter 4 we enumerate the implementation requirements and explain the search system architec-\nture. Next, Chapter 5 explains the development of the language model used in our system. Furthermore,\nin Chapter 6, we analyse the performance of our search system and developed language models.\nFinally, Chapter 7 provides an overall retrospective pondering of the developed work and the achieved\nmilestones and gives a quick insight into future work.\n4\n2\nBackground Review\nContents\n2.1 Lexical approaches for Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Neural Networks for Semantic Information Retrieval . . . . . . . . . . . . . . . . . . . 9\n2.3 Semantic Search Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4 Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5\n6\nIn this chapter, we will present and explain some core aspects necessary to fully understand the ex-\nposed content in the following chapters, such as state-of-the-art techniques and the developed solution.\nIn the last few years, Artificial Intelligence (AI) has played an increasingly important role in almost\nevery aspect of modern society. Subsequently, Machine Learning (ML) has found itself as an alternative\nto traditional solutions in multiple fields. [18]\nThere are two main types of approaches regarding IR from documents. The classical approaches\nare based on lexical search, whereas the more recent approaches are based on semantic search.\n2.1 Lexical approaches for Information Retrieval\nTraditionally, when a system searches for document content related to a query, it uses techniques that\nsearch for documents or entries that present those exact query words. This process is called lexi-\ncal search. The basic algorithm for lexical search is Okapi BM25 (BM25) which incorporates Term\nFrequency-Inverse Document Frequency (TF-IDF). BM25 is a widely used algorithm in information re-\ntrieval and has been shown to perform well in various benchmarks and evaluations.\n2.1.1 Term Frequency Algorithm\nTF-IDF [20] is a ranking function for document search and information retrieval. It evaluates how relevant\na term tis relative to a document dbelonging to a set of documents Dwhile being based on a group of\nNdocuments. TF-IDF is defined as follows:\nTF-IDF (t, d, D ) =TF(t, d)×IDF(t, D) (2.1)\nTF-IDF is thus the product of two factors, Term Frequency (TF), and Inverse Document Frequency\n(IDF). TF represents how often a term appears in a specific document. It is calculated as follows:\nTF(t, d) := log(1 + freq(t, d)) (2.2)\nwhere\nfreq(t, d) :=number of times tappeares in d\nnumber of terms in d(2.3)\n7\nIDF represents the rarity of a term in the entire group of documents, where values near 0 show that\nterms are very common and values near 1 show that terms are rarer. IDF is defined as follows:\nIDF(t, D) := log\u0012N\ncount (d∈D:t∈d)\u0013\n(2.4)\n2.1.2 Best Matching Algorithm\nBM25 [34], in which BM stands for Best Matching, is a ranking function usually used in search engines\nlike Elasticsearch (explored later in the document) to estimate the relevance of documents given a query.\nBM25 relies on a bag-of-words logic, by which it ranks a collection of documents based on the query\nterms that appear in which document, independently of the position in that same file. The equation is as\nfollows:\nscore (d, Q) =nX\ni=1IDF (qi)·f(qi, d)·(ki+ 1)\nf(qi, d) +k1·(1−b+b·|d|\navgdl)(2.5)\nwhere Qrepresents the Query given (with q1, ..., q nbeing the keywords), IDF is defined above in (2.4)\nanddrepresents the Document. f(qi, d)represents the qifrequency in the document dthat has a |d|\nlength. Both k1andbterms are free terms that are used to optimize BM25 function.\n2.1.3 Distance metrics for lexical approaches\nA traditional way to search for similar documents is through the Jaccard Similarity measure between\ntwo sets. It evaluates the amount of shared information or content between both sets. In practice, it\nrepresents the extent of the intersection divided by the size of their union. It is defined as follows:\nSim(C1, C2) :=|C1∩C2|\n|C1∪C2|(2.6)\nwhere C1andC2represent two sets.\nThe drawbacks of lexical searches are evident when some important passages or documents are not\nretrieved because some words need to be present. For example: “I walked through the public garden. I\nenjoyed it.” We can easily understand that “it” was an underlying meaning of “public garden”. A similar\nproblem occur when the query uses a synonym to an expression existing in a document and not the\nexact word. That document will not be returned. To tackle these downsides of lexical approaches, we\n8\nexplore a more advance and robust approach that aims to retrieve information based on the intrinsic\nmeaning and not the literal word matches themselves.\n2.2 Neural Networks for Semantic Information Retrieval\nSemantic search is a technique that, instead of searching for specific words, aims to search for the\ncontextual meaning of those words. Unlike lexical search, which only searches for literal matches of\nwords, semantic search tries to understand the user’s intention and the overall sentence’s meaning.\n2.2.1 Word and Sentence Embeddings\nWord Embedding is a mathematical technique and implementation of the Distributional Hypothesis (DH).\nDH defends that if words appear in similar contexts, they must have similar meanings.\nWord embeddings provide a verification of which words are similar to each other based on an initial\ncorpus of sentences. Each word embedding represents a word in a multidimensional space. With\nmultiple multidimensional word representations, verifying which words are related to each is possible\nbased on the distance between words.\nFigure 2.1: Example of Word Embeddings in a 3D vector space\nThe distance can be calculated through the Cosine Similarity, which measures the cosine of the\nangle, θ, between two vector embeddings( AandB):\n9\nCosSim (A, B) :=cos(θ)\n=A·B\n||A||||B||\n=Pn\ni=1Ai·BipPn\ni=1A2\ni·pPn\ni=1B2\ni(2.7)\nSince embedding spaces are linear systems, it is possible to perform arithmetic operations in the em-\nbedding space. If we get a word “England” and subtract the word “London” and add the word “Portugal”,\nwe should be able to get the word “Lisbon”.\nWord (“England ”)−Word (“London ”) +Word (“Portugal ”) = Word (“Lisbon ”) (2.8)\nThere are different implementations of word embeddings, such as Word2Vec proposed by a Google\nTeam led by Tomas Mikolov in 2013 [24] or Global Vectors for Word Representation (GloVe) [28] devel-\noped at Stanford in 2014. Both make use of neural networks to create their models through unsupervised\ntraining. Both implementations have their unique advantages, but what distinguishes them mainly are\ntheir fundamentals of solution formulation.\n2.2.2 Word2Vec\nWord2Vec is a two-layer neural network proposed in 2013 by Tomas Mikolov et al., and it revolves around\nthe idea that words that appear close to one another have similar meanings.\nWord2Vec algorithm uses one of two methods that utilise neural networks [36]:\n• Common Bag Of Words (CBOW);\n• Skip-Gram.\nCBOW model, through the representation of context using the surrounding words as an input, aims\nto predict the corresponding word. Considering the example: “I walked through the public garden.”, we\ncan input the phrase without the word “public” in the Neural Network. By using this single input, it aims\nto predict the word “public” just by interpreting its surroundings.\nThe architecture of the CBOW model is defined in Figure 2.2.\nIn this example, we provide one word X={x1, ..., x V}in the form of a one-hot encoding with size\nVas the context and the network aims to predict another word. Vrepresents the vocabulary size. The\n10\nFigure 2.2: Word2Vec with CBOW model based on a one word context. Figure based on [36]\nhidden layer is composed by Nneurons, and the output layer is a vector of size Vthat represents the\npredicted word. The weight matrix Whas size V×N. The choice of hidden layer size in a Word2Vec\nmodel depends on various factors such as the size of the vocabulary, the complexity of the task, and the\navailable computational resources. Similarly to the input, the yielded word Yfrom the output layer is a\none-hot encoded vector of size V.\nIf the objective is to use multiple words for the context to predict a word, the neural network would\nneed to increase the input layer, as suggested in Figure 2.3.\nFigure 2.3: Word2Vec with CBOW model based on multiple words context. Figure based on [36]\n11\nThe Skip-Gram model’s function is to use a word as an input and generate the context of that same\nword. The architecture is shown in Figure 2.4.\nFigure 2.4: Word2Vec with Skip-Gram model. Figure based on [36]\nWord2Vec relies on local information, meaning words are only affected by words in the surroundings.\nThe technique can not associate a word as a stop-word or a word that has meaning in a phrase. Stop\nwords are everyday words that have no complex meaning. For instance, in the sentence: “The cat sat\non the mat” , Word2Vec cannot identify if the word “The” is a particular context of the words “cat” and\n“mat” or if it is just a stop-word. Nevertheless, this technique performs very well in analogy tasks.\n2.2.3 GloVe\nGloVe is an unsupervised learning algorithm developed by Stanford University researchers aiming to\nrepresent words as vectors. It focuses on the idea that it can derive semantic relationships between\nwords based on a co-occurrence matrix. Each value in the co-occurrence matrix, M, represents a pair\nof words occurring together. For example, a co-occurrence matrix entry Mijrepresents the probability\nof a word jappearing next to the word i.\n12\nThe probability of a word jco-occurring with a word iis the ratio of the number of times word j\nappears in the context of word ito the number of times any word appears in the context of word i. It is\ndefined as follows:\nPij:=P(j|i)\n=MijP\nk∈contextMik(2.9)\nGloVe’s loss function is formally defined as follows:\nJ=VX\ni,j=1f(Xij)(WT\ni˙˜Wj+bi+˜bj−log(Xij))2(2.10)\n, where f(Xij)represents the weighting function, WT\ni˜Wjis the dot product of the input vectors, bi+\n˜bjrepresents the bias, which aims to mitigate the impact of common words and stop-words, and V\nrepresents the size of the vocabulary.\nThe original paper demonstrated the co-occurrence matrix produced with probabilities for targets\nword iceandsteam , as shown in Table 2.2.3. The word iceco-occurs more frequently with the word\nsolid than it does with the word gas, whereas steam behaviours in the opposite way. Also, both are\nrelated to water and do not show a strong co-occurrence with the word fashion . The last row shows\nwhether a word relates more with ice(values much bigger than 1), steam (values much lesser than 1)\nor presents a neutral co-occurrence (close to 1).\nProbably and Ratio k=solid k =gas k =water k =fashion\nP(k—ice) 1.9×10−46.6×10−53.0×10−41.7×10−5\nP(k—steam) 2.2×10−57.8×10−42.2×10−31.8×10−5\nP(k—ice)/P(k—steam) 8.9 8 .5×10−21.36 0 .96\nIn a sense, GloVe receives a corpus of text as an input and transforms each word in that corpus into\na position in a high-dimensional space based purely on statistics through a co-occurrence matrix. With\nthe produced vectors, it is possible to retrieve related words based on the distance between vectors.\n2.2.4 Recurrent Neural Network\nRecurrent Neural Network (RNN) is a type of neural network used for processing sequential or time\nseries data, where the input has some defined order. One way to visualize RNN is by viewing the\narchitecture as multiple feed-forward neural networks that feed information from one network to another.\nIn practice, it is one network where the cells iterate over themselves for every input they acquire.\n13\nFigure 2.5: Recurrent Network Fully Connected.1\nRNN makes use of a hidden vector that has information from the last iteration. In doing so, its actual\nvector, a<t>, depends on the previous vector, a<t−1>, and the current input, x<t>. It is defined by the\nfollowing:\na<t>:=g1(a<t−1>, x<t>)\n=f(Waa·a<t−1>+Wax·x<t>+ba)(2.11)\nwhere WahandWaxrepresent the weight matrix for the hidden vectors and inputs, respectively, brep-\nresents the associated bias, and g1(and subsequently g2) representing an activation function. Usually,\nthe activation functions used for this RNN are either the logistic function (Sigmoid), Hyperbolic Tangent\n(Tanh), or Rectified Linear Unit (ReLU).\nThe output vector (prediction), y<t>, depends on the hidden state vector, a<t>.\ny<t>:=g2(Wya·a<t>+by) (2.12)\nwhere g is another activation function.\nThis architecture allows the networks to analyse any input with unspecified length while maintaining\nthe model size, considering historical information, with weights being shared across time.\nThe loss function used in this architecture has to be defined at each timestep:\nL(ˆy, y) :=TX\nt=1L(ˆyt, yt) (2.13)\nIn the training process, the RNN makes use of a gradient-based technique named Backpropagation\n1Figure based on https://stanford.edu/^shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n14\nThrough Time, calculated at each point in time. At a timestep, T, the derivative of the loss function, L,\nwith respect to the weight matrix Wis as follows:\n∂L(T)\n∂W:=TX\nt=1∂L(t)\n∂W(2.14)\nIn the Backpropagation Through Time mechanism, since it is hard to capture long-term dependen-\ncies because of the multiplicative gradient, the propagated errors might either tend to zero or increase\nexponentially (vanishing or exploding gradient phenomena). One method to help deal with the exploding\ngradient phenomena is by capping the maximum value for the gradient: Gradient clipping. On the other\nhand, several types of gates with well-defined purposes are used to deal with the vanishing gradient\nphenomena.\nOne problem raised by RNN is related to long-term dependencies. For example: “I like flowers a lot.\nToday, I walked through the public garden .”. The information that the user likes flowers should indicate\nthat he might walk through the “garden”. It is a possibility that the distance between the information and\nthe location where it is required is too great. In these circumstances, RNNs cannot learn to connect the\ninformation. Long Short-Term Memory (LSTM) solve this problem.\n2.2.5 Long Short-Term Memory\nLSTM network is a variant of RNN that is able to deal with long-term dependencies.\nStandard RNNs have a very simple structure. They can have, for instance, a single tanh layer\nrepresented by g1. (See Figure 2.6)\nFigure 2.6: RNN.2\n15\nLSTMs contain a slightly more complex structure (See Figure 2.7). They are specifically designed to\ntackle the problem with long-dependencies that ordinary RNNs would struggle with.\nFigure 2.7: LSTM.2\nThe cell state, c, flows across the entire chain, interacting linearly occasionally. If the information pro-\nvided by the cell state is barely changed, it is easy to preserve it over time. The gates can remove or\nadd information to the cell state. In Fig. 2.7, these gates are represented by σsince they use sigmoid\nfunctions. There are three main types of gates: Forget Gate, Input Gate, and Output gate.\nThe Forget Gate layer is responsible for selecting which information is staying or not in the cell state.\nIt makes use of a sigmoid function and, by looking at ht−1andxt, it outputs a number between 0 and 1\nthat represents how much information is kept:\nft:=σ(Wf· |ht−1, xt|+bf) (2.15)\nThe Input Gate layer aims to determine which values will be updated. In conjunction with a tanh layer, it\nhelps determine what information to store in the cell state.\nThe Input Gate output is as follows:\nit:=σ(Wi· |ht−1, xt|+bi) (2.16)\nwhile the output of the tanh layer, the new cell state candidate vector, ˜Ct, is given by:\n˜Ct:=tanh(WC· |ht−1, xt|+bC) (2.17)\n2Figure based on https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n16\nThis way, the cell state, Ctis defined as:\nCt:=ft∗Ct−1+it∗˜Ct (2.18)\nThe Output Gate layer is responsible for deciding what will be outputted and, in conjunction with a tanh\nlayer, deciding what will pass to the next cell. The Output Gate is defined by:\nσt:= (Wo· |ht−1, xt|+bo) (2.19)\nThis output vector originating from the Output Gate is used to determine the hidden vector, ht, that is\ngoing to be transmitted to the next cell.\nht:=σt·tanh(Ct) (2.20)\n2.2.6 Transformers\nSince it is possible to assume that RNNs are unrolled arbitrarily deep networks, they might take too\nlong to train (LSTMs even more due to their complexity). LSTMs, even though they can tackle long-\nterm dependencies, need long training times. Also, the inherent recurrence prevents the use of parallel\ncomputation (widely used in modern computer processors). With that in mind, a Google Team has\nsuggested a new Neural Network (NN) structure, Transformers [40], that introduces the self-attention\nmechanism.\nTransformers using the new mechanism of self-attention facilitate long-range dependencies, elimi-\nnate the Gradient Vanishing and Explosion phenomena, and, by using techniques that do not involve\nrecurrence, facilitate parallel computation, reducing the training time.\nThe encoder is responsible for receiving an input sequence, x= (x1, ..., x n), and map it into a\ncontextualized encoding sequence, z= (z1, ..., z n). The encoder is composed of N= 6identical layers.\nEach layer contains two sub-layers, a Multi-Head Attention Layer and a Feed-Forward Neural Network,\nthat produces outputs with 512 dimensions.\nThe Transformer Model is composed of different components. The Input Embedding is responsible\nfor transforming the inputs into a scalar vector and mapping them into space where words with similar\nmeanings are close to one another. These words might have different meanings depending on their\ncontext and position in a sentence. Positional Encoding tackles this problem.\nPositional Encoding is a vector that provides context depending on the position of a particular word\nin a sentence. In the original paper, it is used sine and cosine functions with different frequencies.\n17\nFigure 2.8: Transformer Model. Figure based on [40]\nPE(pos,2i)=sin(pos/100002i/d model) (2.21)\nPE(pos,2i+1)=cos(pos/100002i/d model) (2.22)\nwhere posis the position and iis the positional encoding dimension.\nAfter the input is passed through the Input Embedding and the Positional Embedding, it is passed\ntowards the Encoding Block. The Transformer Model contains the Multi-Head Attention Layer and Feed-\nForward Neural Network, as mentioned before.\nThe Multi-Head Attention Layer uses an attention mechanism to assign different weights that, for\nevery word, generate a vector that captures the importance between words of a specific sentence.\nThe attention function receives a query, a key, and a value and aims to pair it to an output. All the\ninputs are vectors and the output is calculated based on a weighted sum of the values.\n18\nFigure 2.9: Scaled Dot-Product Attention (left). Multi-Head Attention (right). Figure based on [40]\nAttention (Q, K, V ) :=softmax (Q·KT\n√dK)·V (2.23)\nwhere Qis a set of queries, Kis a matrix with the keys of the words from the query and Vis a matrix that\nholds the values of the words from the query. The input consists of queries and keys of dimension dk\nand values of dimension dv. Instead of running the attention function once for each set of keys, values,\nand queries, the original paper states a solution based on projecting linearly the same sets htimes with\ndifferent dk,dk, and dvdimensions, respectively. This way, the projected images of the keys, values,\nand queries are run in parallel throughout each Attention Head (Fig. 2.9).\nMultiHead (Q, K, V ) :=Concat (head 1, ..., head h)W0(2.24)\nwhere\nhead i:=Attention (QWQ\ni, KWK\ni, V WV\ni) (2.25)\nand where the projection matrices WQ\ni∈Rdmodel dk,WK\ni∈Rdmodel dk,WV\ni∈Rdmodel dVandWO\ni∈\nRhdVdmodel andh= 8parallel attention layers.\nFinally, there is the Decoder Block. The Decoder comprises a Masked Multi-Head Attention layer, a\nMulti-Head Attention layer, and a Feed Forward layer. The Multi-Head and the Feed Forward compo-\nnents are similar to the Encoder Block. When it comes to the Masked Multi-Head Attention Component,\nthere are some differences. The overall architecture was designed for translation purposes. Thus, the\n19\nMasked Attention component applies a mask to the attention scores so that the model only attends to\npositions before the current position in the input sequence, to ensure the predictions are made only on\nthe basis of past information. This mask application is applied by converting the impact words after the\ninput sequence current position to 0.\n2.3 Semantic Search Type\nSemantic search aims to improve the overall search quality by understanding the underlining query\nand sentence meaning. It achieves that by creating embeddings, which are vectorial representations of\nwords, paragraphs, or even documents, into a vector space. Both queries and sentences are embedded\ninto the same vector space, and the closest embeddings are found.\nFigure 2.10: Vector space with a query embedding and multiple sentence embeddings\nThere are two types of semantic search: Symmetric semantic search and Asymmetric semantic\nsearch. Symmetric semantic search aims to match a query input with text. For instance, if a user inputs\nin the search system “Capital crimes correspond to the worst types of crimes one can commit”, it should\nbe expected to receive retrievals that contain a similar meaning, such as “Capital crimes represent the\nworst type of crimes”. In practice, both query and result should have similar lengths.\nAsymmetric semantic search, on the other hand, provides answers to questions. Usually, the query\nis short and expects a more significant paragraph to be returned. A user might search “What are the\n20\nconsequences of robbing?” and it is expected to retrieve a sentence similar to “The consequences of\nrobbing are various from case to case. First, we need to identify the object or quantity being stolen. . . .”\nWhen applying a Symmetric semantic search, there are two approaches: Bi-Encoders and Cross-\nEncoders. Bi-Encoders generate a sentence embedding for a given sentence. Using Sentence-BERT\n(SBERT), we embed sentences AandBindependently, producing the sentence embeddings uandv,\nrespectively. These sentence embeddings can be later compared using cosine similarity. On the other\nhand, Cross-Encoders receives both sentences AandBsimultaneously and outputs a value between 0\nand1, representing the similarity of those sentences. Cross-Encoders do not return a sentence embed-\nding. It only compares the similarity between two sentences, which must be passed simultaneously.\nFigure 2.11: Bi-Encoder (left). Cross-Encoder (right).3\n2.4 Text Summarization\nText Summarization is the problem of reducing the number of sentences and words from a document,\nmaintaining its original meaning. It is the task of creating a shortened version of a given text document\nwhile retaining its most important and relevant information.\nThere are multiple techniques to extract information. These techniques can be categorized as Extrac-\ntive or Abstractive. Extractive techniques aim to retrieve the most important sentences from a document\n3Figure based on https://www.sbert.net/examples/applications/cross-encoder\n21\nwithout considering their meaning. On the other hand, Abstractive uses more complex and harder-to-\ntrain models to understand the semantics and meaning of the document text to create a proper summary.\nThis section will cover a extractive summarization technique that was used in the development of this\nwork.\nText summarization is an important field of research in Natural Language Processing (NLP), such\nas assisting search engines in generating relevant results and enabling machines to write shorter sum-\nmaries of news items or other text-based information.\n2.4.1 LexRank\nLexRank [13] is an unsupervised Extractive summarization technique. It uses a graph-based approach\nfor automatic text summarization. The score of each sentence is based on the concept of eigenvector\ncentrality in a sentence’s graph representation.\nThis algorithm has a connectivity matrix based on intra-sentence cosine similarity, which is used as\nthe adjacency matrix of the graph representation of sentences. In other words, sentences are placed\nas the graph vertices, and the edge weights are calculated using cosine similarity or Jaccard similarity.\n(See Figure 2.12)\nThe sentence similarity scores are then used to build a sentence similarity graph, in which each\nphrase is represented as a node and an edge between two nodes denotes the similarity score of the\nrespective sentences.\nThe PageRank method, which is well-known for calculating the relevance of nodes in a graph, is then\nused to generate the LexRank score of each phrase. The LexRank score is calculated by the PageRank\nalgorithm as follows:\nLexRank (i) :=d\nN+ (1−d)NX\nj=1LexRank (j)\ndeg(j)(2.26)\n, where irepresents a sentence, Nis the total number of sentences in the document, dis a damping\nfactor used to ensure that the scores converge, deg(j)is the number of sentences that are similar to\nsentence j, and the summation is over all sentences jthat are similar to sentence i.\nLexRank scores can be generated repeatedly until convergence, or by decomposing the similarity\nmatrix into eigenvalues. After that, the sentences with the highest LexRank scores are chosen for\nsummarizing.\n22\nFigure 2.12: Weighted cosine similarity graph from LexRak. Figure based on [13]\n23\n24\n3\nState of the Art\nContents\n3.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.2 BERTimbau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3 Deeper Text Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.4 Legal Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.5 NLP Applied To Portuguese Consumer Law . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.6 Albertina PT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n25\n26\nIn this chapter, we will cover some state-of-the-art models and techniques, as well as some relevant\nscientific papers that lead to the implementation of the final solution.\n3.1 BERT\nBidirectional Encoder Representations from Transformers (BERT) is a model proposed by researchers\nat Google AI Language in 2019 [12]. Just like humans tend to understand a word based on its context\nand surroundings, BERT aims to do that. Traditionally, models would look at text sequences from left\nto right or vice-versa. BERT is trained bidirectionally to obtain a higher understanding of the language\nusing the Transformers architecture. It makes use of two phases: pre-training and fine-tuning.\nThe problem when applying bidirectional unsupervised training is the trivial prediction of words. In\nunidirectional training, words can only see their left or right context. The existence of direction in training\nallows for an unbiased prediction of each subsequent word. When that does not exist, as in bidirectional\ntraining, the prediction of each word — based on both its left and right words — becomes biased, and\nwords can then predict themselves.\nOne of the tasks of using language models is to predict the next word in a sentence. BERT aims not\nonly to be able to predict that, but also to understand the language effectively. However, this trivial pro-\ncess would produce a biased model since it uses a particular direction. To tackle this phenomenon and\nreduce the bias, the original paper used two techniques in the pre-training phase while using BooksCor-\npus (800M words) and English Wikipedia (2500M words) as the datasets:\n• Masked Language Modeling (MLM);\n• Next Sentence Prediction (NSP).\nMLM is a technique that revolves around applying a [MASK] token to words, so their prediction can\nbe less biased. In the original paper, in a normal pre-training cycle, 15% of the words in each sequence\nare selected for a masking treatment. The model then aims to predict some masked words based on\nthe surrounding unmasked words. The words that were selected for the masking treatment were not\nall treated the same way. 80% of the selected words are replaced with a [MASK] token. 10% of the\nselected words are replaced with a random token, and the remaining 10% just keep the original token.\nNSP is a task in which the model aims to understand sentence relationships. Many applications like\nQuestion and Answer (QA) and Natural Language Inference (NLI) (See Section 3.1.2.A) revolve around\nthis sense of understanding. In the BERT training process, BERT receives multiple pairs of sentences\nas input. It proceeds to predict if the second sentence is a subsequent sentence of the first one. In\nthis training process, 50% of the inputs are indeed subsequent statements, and are deemed positive\n27\n(labelled as IsNext ), and the other 50% are deemed negative (labelled as NotNext ), meaning that the\nsecond sentence is completely disconnected from the first one.\nIn order to represent the input words as scalar vectors that can be read, BERT creates word embed-\ndings based on three components. Thus, the input embeddings are the sum of the token embeddings,\nthe segmentation embeddings and the position embeddings. The Token Embedding Layer is responsible\nfor assigning a value to each word based on vocabulary IDs. The second component is the Segmen-\ntation Embedding Layer, which allow distinguishing whether a word belongs to sentence A or B. The\nthird component is the Position Embedding Layer, which is responsible for indicating the position of each\nword in a sentence (See Figure 3.1).\nFigure 3.1: BERT input representation. Figure adapted from [12]\nIn the second phase, fine-tuning, one additional layer is added after the final BERT layer, and the\nentire network is trained for a few epochs with the Adam Optimizer.\nIn the original paper, the authors denoted the number of layers (i.e. Transformer blocks) as L, the\nhidden size as H, and the number of self-attention heads as A. With this architecture, two model sizes\nwere reported: BERT BASE andBERT LARGE . They have sizes L= 12 ,H= 768 ,A= 12 with Total\nParameters = 110 MandL= 24,H= 1024 ,A= 16 with Total Parameters = 340 M.\n3.1.1 Domain Adaptation\nNNs require significant amounts of data for proper training, especially labelled data. Usually, such large\nquantities of data are unavailable and training deep learning models can be very time-consuming, often\nrequiring specialized and expensive hardware. Deep learning models can perform well on a test dataset\nfrom the same domain as the training dataset. However, they tend to be less efficient with dataset from\ndifferent domains.\nTask data consists of any observable task distribution information. Data that can be used for a\nspecific task is usually non-randomly sampled from a wider distribution from a larger target domain.\n28\nNevertheless, this task data might not be present within the original data used to train a large language\nmodel. [15]\nTraining a model with data from a different domain of the one it will be used for can lead to some\nperformance degradation. This is because the model is likely to have learned patterns specific to the\noriginal domain, and these patterns may not generalize well to the target domain.\nFigure 3.2: Data Distributions. Figure based on [15]\nDomain Adaptation (DA) is a technique that aims to tackle this issue. With this technique, a model\nshould perform on a new dataset that comes from a different domain similarly to as it would on the\ntesting dataset. Usually, in order to achieve this type of result, deep learning models are re-trained\non unsupervised learning tasks. DA saves large amounts of computational resources and, by utilizing\nunsupervised learning methods, reduces the necessity of manual annotation of labelled datasets.\nSubsection 3.1.1 covers multiple DA techniques that can be used for BERT models.\n3.1.1.A Masked Language Modeling\nMLM, as mentioned in Section 3.1, is a task originally introduced by BERT. Words selected at random\nwith a 15% chance are masked from the input sentence with a predefined probability (80%), and the\nmodel aims to predict those masked words. (see Figure 3.3)\n4Figure based on https://www.sbert.net/examples/unsupervised_learning/MLM\n29\nFigure 3.3: Masked Language Modeling.4\nTo apply MLM with the intent of adapting the domain a model performs in, the model is re-trained\nover one epoch on a new dataset. The learning rate used is the same or slightly lower than the MLM\ntask performed on the model pre-training. The reasoning behind such choice is to slightly change on the\nweights used on the NN, without destabilizing the network completely.\nThe goal of the MLM task is to maximize the likelihood of predicting the correct tokens for the masked\npositions, which is equivalent to minimizing the negative log-likelihood loss function LMLM. The loss\nfunction is defined as:\nLMLM(x, y) :=−NX\nn=1wynxn, yn (3.1)\n, where xis the input, yis the target, wis the weight, and Nis the batch size.\n3.1.1.B Transformer-based Sequential Denoising Auto-Encoder\nTransformer-based Sequential Denoising Auto-Encoder (TSDAE) is an unsupervised state-of-the-art\nsentence embedding method which outperforms previous approaches, such as MLM. Firstly published\non April 14th of 2021 by Nils Reimers [42], this technique aims to improve the domain knowledge of a\nmodel. They state that TSDAE fills the gap between models that usually only perform the STS task on\na certain domain. TSDAE’s model architecture is a modified encoder-decoder Transformer, with the key\nand value of the cross-attention mechanism limited to the sentence embedding.\nIn a sense, it is similar to MLM, but instead of swapping words for [MASK] tokens, TSDAE introduces\nnoise to the sentences by deleting or swapping words. The encoder transforms the sentence into a\n30\nFigure 3.4: TSDAE Architecture. Figure based on [42]\nvector, and a decoder is supposed to reconstruct the original sentence. Formally, the training objective\nis shown in the equation 3.2.\nJTSDAE (Θ) = Ex∼D[logPΘ(x|˜x)]\n=Ex∼D[lX\nL=1logPΘ(x|˜x)]\n=Ex∼D[lX\nL=1logexp(hT\ntet)PN\ni=1exp(hT\ntei)](3.2)\nwhere Dis our training corpus, x=x1, x2...xlis the input sentence with ltokens, ˜xis the corresponding\ndamaged sentence. etis the word embedding of xt, and htrepresents the hidden state at decoding step\nt.\n3.1.1.C GenQ\nGenQ [39], published in October 2021 by the Ubiquitous Knowledge Processing Lab team, is an unsu-\npervised domain adaptation method for dense retrieval models, allowing the query generation from given\npassages. This approach aims for a semantic search system to be asymmetric (explored in Subsection\n31\n2.3), supporting question-answering scenarios simply by training with synthetically generated data.\nFirstly, GenQ requires a Text-to-Text Transfer Transformer (T5) model fine-tuned for question-answering.\nA T5 is a Transformer based architecture that uses a text-to-text approach [29]. This new model archi-\ntecture achieves state-of-the-art results on many NLP benchmarks while maintaining the ability of being\nfine-tuned for numerous downstream tasks. T5 authors found that training on in-domain unlabelled data\ncan improve performance, but using a large and diverse data set is better for generic language under-\nstanding tasks.\nFigure 3.5: T5 diagram. Figure based on [29]\nIn the original paper, they fine-tuned a T5 model on MS MARCO [26] for two epochs. After that, it\nutilised the T5 model to generate queries from original passages. The idea behind T5 models is that\nall NLP tasks can be defined as a text-to-text problem, so they are trained on numerous tasks with\nimmense amounts of data. One of these tasks is query generation. By feeding a passage, T5 models\ncan generate multiple questions the passage may answer. These generated queries might be far from\nideal. The T5 model used is one for general purposes, which can lead to noisy data with plenty of\nrandomnesses. t\nThus, a dense model, such as SBERT, can be fine-tuned with the passages and synthetically gen-\nerated query pairs using the Multiple Negatives Ranking (MNR) loss. Figure 3.6 shows the overall\narchitecture.\n3.1.1.D Generative Pseudo Labeling\nGenerative Pseudo Labeling (GPL) [43] is an improved state-of-the-art technique to perform domain\nadaptation of dense models. It comprises three phases, as shown in Figure 3.7. First, a Query Gen-\n5Figure based on https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html\n32\nFigure 3.6: GenQ5\neration step uses the GenQ approach to generate queries for multiple passages. We have access to\nqueries and passages, called positive passages, since they can answer the generated queries.\nThen, there is a Negative Mining step. Negative mining is a technique to retrieve passages that would\nnot answer the previously generated queries. Nevertheless, these negative passages are similar to the\npositive passages in the vector space. In practice, a dense index would be filled with multiple passages,\nand the closest ones to the positive passages would be retrieved and noted as negative passages.\nThe last and fundamental step is called Pseudo Labeling. In this step, a Cross-Encoder receives the\ntriplets composed of a query, a positive, and a negative passage. The Cross-Encoder calculates the\nscore margin between the negative and positive passages. The training model will use the Margin Mean\nSquared Error Loss [ ?] to identify whether these passages are also relevant to the given query. The\nloss function is calculated based on the sim(Query, Pos )−sim(Query, Neg )|and|gold sim(Q, Pos )−\ngold sim(Query, Neg )|. As stated in the original paper, the dot product was used as the default for\ncalculating the similarity between passages and queries.\n3.1.2 Fine-tuning on Downstream Tasks\n3.1.2.A Sentence-BERT\nThere was a need for representing an entire sentence in an embedding instead of only one word or\ntoken. One implementation was using the BERT model on all the tokens present in a sentence and\n33\nFigure 3.7: Generative Pseudo Labeling. Figure based on [43]\nproducing a mean pooling. Even though this technique was fast enough for the desired problem, it was\nnot very accurate (GloVe embeddings, designed in 2014, produced better results than this technique).\nSBERT [31], implemented in 2019 by a group of researchers from the Ubiquitous Knowledge Pro-\ncessing (UKP) lab, is a modification of the BERT network using siamese and triplet networks which can\ncreate sentence embeddings that are semantically meaningful. This modification of BERT allows for\nnew tasks, such as semantic similarity comparison or information retrieval via semantic search.\nReimers and Gurevych showed that SBERT is dramatically faster than BERT in comparing sentence\npairs. From 10 000 Sentences, it took BERT 65 hours to find the most similar sentence. In contrast,\nSBERT produced the embeddings in approximately 5 seconds and, using the cosine similarity, took\napproximately 0.01 seconds.\nThe siamese architecture comprises two BERT models with weights entangled between them. In\nthe training process, the sentences would be fed to both BERT models, which then would go through a\npooling operation that would transform the token embeddings of size 512×768into a vector of a fixed\nsize of 768.\nOne way of fine-tuning SBERT is through the Softmax loss approach (See Figure 3.8). Applying\na softmax classifier on top of a siamese network can improve sentence representation [9]. NLI is the\nchallenge of determining if the premise implies the hypothesis, whether they are contradictory or neutral.\nIt used a combination of the Stanford Natural Language Inference (SNLI) [7] and the Multi-Genre NLI [44]\ndatasets. The datasets contained pairs of sentences (premises and hypothesis) that could be related\nthrough a label feature. This label feature determines whether the sentences are related or not.\n• 0 – “entailment”, the premise suggests the hypothesis\n• 1 – “neutral”, the premise and hypothesis could not be related\n• 2 – “contradiction”, the premise and hypothesis contradict each other\nThis type of fine-tuning aims to train the model to identify the relationship between sentences.\n34\nFigure 3.8: Fine-Tuning SBERT. Based on [31]\nAnother task that SBERT can be fine-tuned is the STS task. The siamese architecture calculates the\ncosine similarity between uandvsentence embeddings. The researchers also tried negative Manhattan\nand Euclidean distances as similarity measures, but the results were similar. The model performance\nevaluation, in terms of Semantic Textual Similarity, was evaluated on Supervised and Unsupervised\nLearning.\nRegarding the Unsupervised Learning, the researchers used the STS tasks 2012 – 2016 by Agirre\net al. [1–5], the STS benchmark by Cer et al. [8] and the SICK-Relatedness dataset [22]. Each of these\nthree datasets contained gold labels between 0 and 5 that reflects how similar each sentence pair is.\nTable 3.1 shows the Spearman correlation ρbetween the cosine similarity of sentence representations\nand the gold labels for various Textual Similarity (STS) tasks. The performance is reported by convention\nasρx 100.\nTo evaluate Supervised learning, the researchers chose to fine-tune SBERT only with the STS bench-\nmark (STSb), which is “a popular dataset to evaluate supervised STS systems” (Reimers et al., 2019)\nand first train on NLI and then on STSb. (See Figure 3.2 .) BERT systems were trained with ten random\nseeds and four epochs. SBERT was fine-tuned on the STSb dataset, SBERT -NLI was pre-trained on\nthe NLI datasets, and then fine-tuned on the STSb dataset.\n35\nModel STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.\nAvg. GloVe embedd. 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\nAvg. BERT embedd. 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81\nBERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19\nInferSent - GloVe 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01\nUni. Sentence Enc. 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22\nSBERT -NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89\nSBERT -NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55\nSRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21\nSRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68\nTable 3.1: SBERT Spearman correlation ρresults. From [31]\nModel Spearman\nNot trained for STS\navg. GloVe embeddings 58.02\navg. BERT embeddings 46.35\nInferSent - GloVe 68.03\nUniversal Sentence Encoder 74.92\nSBERT -NLI-base 77.03\nSBERT -NLI-large 79.23\nTrained on STS benchmark dataset\nBERT -STSb-base 84.30 ±0.76\nSBERT -STSb-base 84.67 ±0.19\nSRoBERTa-STSb-base 84.92 ±0.34\nBERT -STSb-large 85.64 ±0.81\nSBERT -STSb-large 84.45 ±0.43\nSRoBERTa-STSb-large 85.02 ±0.76\nTrained on NLI data + STS benchmark dataset\nBERT -NLI-STSb-base 88.33 ±0.19\nSBERT -NLI-STSb-base 85.35 ±0.17\nSRoBERTa-NLI-STSb-base 84.79 ±0.38\nBERT -NLI-STSb-large 88.77 ±0.46\nSBERT -NLI-STSb-large 86.10 ±0.13\nSRoBERTa-NLI-STSb-large 86.15 ±0.35\nTable 3.2: SBERT evaluation on the STS benchmark test set. Retrieved from [31].\n36\n3.1.2.B Multilingual Sentence Embeddings\nOne issue raised by using pre-trained models is that the embedding models are usually monolingual\nsince they are usually trained in English. In 2020, the lead researcher from the team that published\nSBERT introduced a technique called Multilingual Knowledge Distillation (MKD) [32]. It relies on the\npremises that for a set of parallel sentences ((s1, t1), ...,(sn, tn))withtibeing the translation of siand a\nteacher Model M, a Model ˆMwould produce vectors for both siandticlose to the teacher Model M\nsentence vectors (See Figure 5.5). For a given batch of sentences β, they minimize the mean-squared\nloss as follows:\n1\n|β|X\nj∈β[(M(sj)−ˆM(sj))2+ (M(sj)−ˆM(tj))2] (3.3)\nFigure 3.9: Multilingual Knowledge Distillation Process. Figure based on [32]\nThere are two different models: a student and a teacher model. Assume that we intend for our\nstudent model to learn Portuguese and that our teacher model already knows English. Both models\nwill receive pairs of sentences with the same meaning, one sentence written in Portuguese while the\nother is in English. The teacher model will encode the English sentence for each pair, while the student\nmodel will encode both sentences. Since both sentences have the same meaning, both embeddings\nshould be similar, if not equal. Consequently, the student model embeddings will be compared to the\nteacher model embedding, which will back-propagate the error using a mean squared error approach.\nOver time, the student model embeddings will become closer to the teacher model embedding.\nMultilingual SBERT versions, such as paraphrase-multilingual-mpnet-base (768 Dimensions) or paraphrase-\nmultilingual-MiniLM-L12 (384 Dimensions), would provide relatively accurate embeddings to this context.\nmpnet model, as a larger model than MiniLM, should be able to comprehend the meaning of a text better\nthat it has yet to explicitly see, such as queries with only one or two words.\n37\n3.2 BERTimbau\nDespite the existence of multilingual BERT models trained on multiple languages, there was an effort to\ntrain monolingual BERT models on single languages. BERTimbau is a BERT model pre-trained for the\nPortuguese language.\nBoth BERT -Base ( 12layers, 768hidden dimension, 12attention heads, and 110M parameters) and\nBERT -Large ( 24layers, 1024 hidden dimension, 16attention heads and 330M parameters) variants were\ntrained with Brazilian Web as Corpus (BrWaC) [6], a large Portuguese corpus, for 1,000,000steps, using\na learning rate of 1e−4. The maximum sentence length is S= 512 tokens\nModel Arch. #Layers #Params\nneuralmind/bert-base-portuguese-cased BERT -Base 12 110M\nneuralmind/bert-large-portuguese-cased BERT -Large 24 335M\nTable 3.3: BERTimbau variants\nBrWaC corpus contained over 2.68billions tokens retrieved from across 3.53million documents,\nproviding a well diverse dataset. They utilised the HTML body, ignoring the titles and possible footnotes.\nThe researchers removed the HTML tags and fixed possible “mojibakes”, a type of text corruption that\noccurs when strings are decoded using the incorrect character encoding, producing a processed corpus\nwith17.5GB of raw text.\nThe pretraining stage was identical to BERT. BERTimbau was trained using MLM and NSP methods\n(Explained previously in Section 3.1) with the exact technique probabilities. Each pretraining example is\ngenerated by concatenating two sequences of tokens x= (x1, . . . , xn )andy= (y1, . . . , ym )separated\nby special [CLS] and [SEP] tokens as follows:\n[CLS]x1. . . x n[SEP ]y1. . . y m[SEP ] (3.4)\nFor each corpus sentence x, 50% of the time an adjacent sequence y is chosen to form a contiguous\npiece of text, and on the remaining 50% of the time, a random sentence from a completely different\ndocument from the corpus is selected as the token y.\n15% of the tokens of every example pair xandyare replaced by 1 of 3 options. Each token can\nbe replaced with a special [MASK] token with 80% probability. With 10% probability, it is replaced with\na random token from the vocabulary or, with the remaining 10% probability, the original token remains\nunchanged.\nBERTimbau serves as the foundation for our language model, Legal-BERTimbau. Even though\nBERTimbau is already a language model adapted to the Portuguese language, it was necessary to\n38\ndevelop a model for our legal domain. This fine-tuning stage is essential since we needed to ensure that\nthe model producing the embeddings could adequately understand the records.\n3.3 Deeper Text Understanding\nZhuyun Dai and Jamie Callan in [11] explore the use of a contextual neural language model, BERT, for\nad-hoc document retrieval in IR. The authors of the paper found that using BERT for text representations\nwas more effective than traditional word embeddings like Word2Vec. The contextual language model\nwas able to better leverage language structures and achieve improved performance on queries written\nin natural language. The authors also show that fine-tuning pre-trained BERT models with a limited\namount of search data could outperform strong baselines. Furthermore, they found that stopwords and\npunctuation, which are often ignored by traditional IR approaches, played a key role in understanding\nnatural language queries.\nIn [11], Zhuyun Dai and Jamie Callan studied the performance of information retrieval associated\nwith two different datasets ( Robust04 andClueWeb09-B ) and different techniques. It included testing\nthe performance with scores related to the score of the first passage (BERT -FirstP), the best passage\n(BERT -MaxP), or the sum of all passage scores (BERT -SumP). The paper showed that simply searching\nthe passage with the best score would provide better results when the dataset has well written text ( Ro-\nbust04 ) since it could understand the context and proper meaning. The results also showed that BERT\nperformed better on description queries than title queries, and that longer natural language queries are\nmore expressive than keywords.\nThey also suggest that BERT, more specifically the SBERT modification, should be applied to small\nportions since it would be “less effective on the long text”. Such implies that embedding small passages,\neither paragraphs, portions of paragraphs or only phrases, is more effective for documents. Another\nexciting approach studied in this research was adding the title to the beginning of every passage to\nprovide context, but it did not produce satisfying results.\n3.4 Legal Information Retrieval\nIn the Competition on Legal Information Extraction and Entailment (COLIEE) edition of 2021, there were\nexplored multiple techniques focused on four specific challenges in the legal domain: case law retrieval,\ncase law entailment, statute law retrieval, and statute law entailment.\nIn [19], Mi-Y oung Kim et al. discuss the use of deep learning techniques for legal information retrieval\nand question-answering tasks in the context of that same edition, focusing, especially, on the University\nof Alberta’s participation.\n39\nTeam F1-score Precision Recall\nJNLP 0.2813 0.3211 0.2502\nnigam 0.2809 0.2587 0.3072\nTable 3.4: COLIEE 2021 - Task 1 results\nTeam Return Retrieved F2 Precision Recall MAP\nOvGU 161 96 0.779 0.778 0.805 0.836\nTable 3.5: COLIEE 2021 - Task 3 results\nTwo teams achieved top COLIEE scores on the first task by combining lexical and semantic tech-\nniques. Task 1 relied on, in total, on a dataset comprised of a total of 5978 case law files. The teams\nwere provided with a labelled training set of 4415 case law files of which 900 query cases, meaning\nthere were approximately 4.9noticed cases per query case. For the third task, Statute Law Retrieval,\nthe goal was to extract a subset of Japanese Civil Code Articles from the Civil Code articles considered\nappropriate for answering legal bar exam questions.\nThe first team, OvGU, presents a two-stage TF-IDF vectorization combined with Sentence-BERT\nembeddings for the third task. Regarding the Task 1, the second team, JNLP , concentrated on dealing\nwith large articles by conducting text chunking on the supplied training data and used a self-labeled\napproach while fine-tuning pre-trained models. They began by preparing the training data using the\napproach given in 25. They employed a TF-IDF vectorizer to encode all the articles and queries into\nvectors, and then used Cosine Similarity to rank the articles. A question and an accompanying article\nare called positive training examples, whereas the converse is termed negative training examples.\nIn that same competition, a third team, nigam [27], proposed an approach where it combined transformer-\nbased and traditional IR techniques for the first task. The team made use of SBERT and Sent2Vec for\nthe semantic component and combined the scores with BM25. They first selected a pre-defined amount\nof results based on BM25, and then they proceeded to embed those documents’ sentences. The final\nresult would be based on the cosine similarity metric.\nThe Task 1 results are shown in Table 3.4 and Task 3 results are shown in Table 3.5.\n3.5 NLP Applied To Portuguese Consumer Law\nIn 2022, Nuno Cordeiro, as part of his master’s thesis [10], created a system, Legal Semantic Search\nEngine (LeSSE), that merges common document retrieval techniques with semantic search abilities on\nPortuguese consumer law. The system was developed in partnership with INESC-ID and Imprensa\n40\nNacional-Casa da Moeda, with the goal of making the Consumer Law more accessible and understand-\nable to the Portuguese citizens. The overall goal and context of his thesis are similar to the context of\nthis research. Even though Nuno’s work focuses on Portuguese Consumer Law, several state-of-the-art\ntechniques, such his usage of BERT and BM25, for Information Retrieval are relevant for our scenario.\nThe system starts by pre-processing all the law documents and the query, followed by text segmenta-\ntion and semantic and syntactic pre-processing. Embeddings are then generated from the segments and\nquery, and a search index is created. In search time, the query is processed and scores are assigned\nto the segments based on their semantic and syntactic similarity. The final stage involves reordering the\nresults using a trained semantic similarity model and presenting the results to the user. The semantic\npipeline uses BERTimbau Base (BERT -Base), a BERT model trained on the Brazilian Portuguese Web.\nThe implemented search system combines the 20 retrievals with the highest scores using BM25 with 50\nretrievals with the highest scores using the cosine similarity measure. Consequently, it orders the results\nthrough a reordering model to produce the final results.\nThe pre-processing of legal documents differs from the one needed in our context. The implemented\nsystem required tokenization to help construct the bag-of-words necessary for the BM25 algorithm,\nremoval of punctuation, and stop-words, which is unnecessary for SBERT.\nIn his work, the language model had to be fine-tuned on a corpus that included legislative jargon to\nproduce the desired results. This corpus is a Portuguese corpus with the help of annotated questions\nfrom the Official Portuguese Gazette (Di ´ario da Rep ´ublica) search database. The fine-tuning was done\nusing a machine with 2 NVIDIA GeForce RTX 3090 GPUs, each with 24 GB of memory and 10496\ncores. The hyperparameter optimization was performed using Population Based Training, which is a\ncombination of Grid search and Hand Tuning. The hyperparameter optimization was performed using\nthe Ray Tune library, which was integrated into the Trainer class function hyperparameter search. The\ntraining dataset was divided into three subsets: training set, validation set, and test set, with 80%, 10%,\nand 10% of the original dataset, respectively. The datasets were shuffled before division to ensure that\nevery training batch was representative of the dataset as a whole. This fine-tuning stage is an important\nstep, since it would help ensure that the model could create proper relationships with words not seen in\nthe pre-training stage. We implemented this approach in this thesis work, since there will be legal terms\nand jargon that the model has not seen in the pre-training phase.\n3.6 Albertina PT\nIn the closing stages of this research work, in May 2023, Jo ˜ao Rodrigues et al., shared their brand\nnew state-of-the-art model, Albertina [35]. This BERT model represents the new state-of-the-art for\nEuropean Portuguese (PT -PT) and Brazilian Portuguese (PT -BR) encoder models. It was developed\n41\nin a partnership between Faculdade de Ci ˆencias da Universidade de Lisboa (FCUL) and Faculdadede\nEngenharia da Universidade do Porto (FEUP), more concretely, NLX–Natural Language and Speech\nGroup, and Laborat ´orio de Intelig ˆencia Artificial e Ci ˆencia de Computadores, respectively.\nThe starting point was DeBERTa [16] architecture, and the pre-training was done over data sets\nof Portuguese for the PT -PT version and the BrWaC corpus for PT -BR, allowing for comparison with\nBERTimbau It has 24layers with a hidden size of 1536 and a total of 900million parameters.\nAlbertina PT -BR outperforms BERTimbau in the STS task over the assin2 dataset, as well as on the\nSTS-B dataset. Interestedly enough, Albertina PT -BR fails to match that performance, falling short of\nBERTimbau.\nNevertheless, Albertina PT -BR appears to be an improved version of BERTimbau, mainly due to its\nlarger architecture. Albertina PT -PT comes up as the first substantial BERT model completely trained\nfor European Portuguese, which is a breakthrough on its own.\n42\n4\nSemantic Search System\nContents\n4.1 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.2 The Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n43\n44\nThis thesis aimed to implement a semantic search system to help the court decision process. The\nproposed system aims to address this challenge by implementing a semantic search system that can\nefficiently retrieve and present relevant legal information.\nIn Chapter 4 of this thesis, we provide an overview of the implemented search system architecture\nand the requirements that the system is expected to meet. The Chapter delves into the different stages\nof the system and introduces the key components that make up the architecture. The developed large\nlanguage model, Legal-BERTimbau, was designed to achieve the project’s goals and is a crucial element\nof the system. Chapter 4 presents an overview of Legal-BERTimbau’s role in the overall search system.\nChapter 5 takes a more detailed look at the Legal-BERTimbau model and explores its inner workings.\nThe Chapter examines the model’s architecture, training data, and the different techniques used to\nfine-tune the model for our legal applications. Chapter 5 will provide a better understanding of how\nthe Legal-BERTimbau model works and how it contributes to the overall effectiveness of the semantic\nsearch system.\n4.1 Constraints\nAs mentioned in Chapter 1, this work is a segment of Project IRIS. Consequently, there were some\npre-defined aspects, such as which technologies the search system should be implemented in.\n4.1.1 ElasticSearch\nElasticsearch, released in 2010, is a distributed, open-source search and analytics engine built over\nApache Lucene. It works as a NO-SQL JSON document-based datastore. A user can interact with\nElasticsearch similarly to interactions with REST APIs, meaning that every request, either POSTs, GETs,\nor PUTs, is sent in a JSON format to different indices. An index is used to store documents in dedicated\ndata structures and allows the user to partition the data in a certain way within a specific namespace.\nElasticsearch uses a complex architecture to ensure the scalability and resilience of the system.\nIt comprises node clusters, where nodes are single instances of Elasticsearch. It also makes use of\nshards, which are subsets of index documents. Shards allow splitting the data from indices to maintain\na good performance and replicate information to handle failures.\nElasticSearch was a pre-defined constraint since the Project IRIS solution was based on the Elas-\nticSearch engine. To utilise ElasticSearch for our use case, it was required to understand how to utilise\nthe provided engine with embeddings.\nBy default, Elasticsearch uses BM25 to search through documents. However, it can support other\nsearch functions, such as Cosine Similarity. To utilise Cosine Similarity, ElasticSearch requires an initial\nmapping of indices, pre-defining its document structure and allowing some fields to be dense vectors.\n45\nThese dense vectors can store embeddings. This powerful edge provides an ideal solution for a se-\nmantic search system implementation, allowing to search and analyse of huge volumes of data in near\nreal-time.\n4.2 The Corpus\nLegal documents contain specific language not easily found in conventional websites or books. To\ncreate a semantic search system adapted to the Portuguese legal domain, it was required to collect\nmany records. Project IRIS members performed the collecting data process through ecli-indexer6. This\nrepository contains multiple tools to extract documents from dgsi.pt, a public database, and index them\ninto ElasticSearch. The retrieval process recovered the HTML content from multiple web domain pages\ncontaining legal documents, summing up to 31690 documents.\nThe structure of each indexed document is as follows:\n1{'_index ':'jurisprudencia .1.0 ',\n2'_id':'- B5mRoABpM44h1Fg -6QX ',\n3'_score ': 1.0,\n4'_source ':{'ECLI ':'ECLI :PT:STJ : 2022 : 251.18.1 T8CSC .L2.S1 ',\n5 'Tribunal ':'Supremo Tribunal de Justica ',\n6 'Processo ':'251/18.1 T8CSC .L2.S1 ',\n7 'Relator ': Relator 1 ',\n8 'Data ':'17/03/2022 ',\n9 'Descritores ': ['CONTRATO DE TRABALHO ',\n10 'CONTRATO DE PRESTACAO DE SERVICO '],\n11 'Sumario ':'\\n<p>I- Subjacente ao contrato de trabalho existe uma\nrelacao de dependencia necessaria ... \\n </p><p> ',\n12 'Texto ':'... <p><i>d) Deve a Re ser condenada a pagar ao Autor a\ndiferenca entre os vencimentos pagos desde julho de 2011 e o\nvencimento que venha a ser determinado nos termos dos pedidos\nformulados em b) ou c) ... ',\n13 'Tipo ':'Acordao ',\n14 'Original URL ':'http :// www . dgsi .pt/ jstj .nsf /12345 ',\n15 'Votacao ':'UNANIMIDADE ',\n16 'Meio Processual ':'REVISTA ',\n6https://github.com/diogoalmiro/ecli-indexer\n46\n17 'Seccao ':'4a SECCAO ',\n18 'Especie ': None ,\n19 'Decisao ':'<b> NEGADA A REVISTA .</b> ',\n20 'Aditamento ': None ,\n21 'Jurisprudencia ':'unknown ',\n22 'Origem ':'dgsi - indexer -STJ ',\n23 'Data do Acordao ':'17/03/2022 '}}\nThe partition of utilised data was mainly the “Texto” (Text) and “Sum ´ario” (Summary) fields. It con-\ntained the HTML content of a legal document corpus. This data needed further processing to create a\nreliable semantic search system. The Summary section reflects not a summary of the full document, but\ninstead, it is a summary of the judgement ruling decision and the newly established jurisprudence.\nThree dataset splits were generated to train, test, and validate the produced models. The percent-\nages for each split were as follows: 80% for the training dataset, 10% for the testing dataset, and 10%\nfor the validation dataset. This dataset was published to the HuggingFace platform7to facilitate model\nreproducibility and future project use. The divisions were as follows:\n• Training dataset – 26952 documents\n• Testing dataset – 3169 documents\n• Validation dataset – 3169 documents\n4.2.1 Data Processing\nWith all the documents properly indexed, it was necessary to clean the available text and split the content\ninto multiple sentences. Our search system acts on singular sentences, as explored further in Chapter\n4 and 5.\nFirstly, HTML tags needed to be removed as well as some unexpected characters, such as “&”. It\nwas required to identify and remove Roman numeration from the text and, more importantly, not take\ninto consideration sections or Subsection titles.\nTexts also contained references to other sections, such as “como referido em a) e b)”. This example\nshows a possible problem a semantic search system can face due to the difficulty of demonstrating\nto the system that there is relevant information outside that section. When the system identifies that\nthere is referred information from other sections, it is required to handle that dependency. A solution\nto this issue could start by incorporating a summarization technique to join the information in the same\n7https://huggingface.co/datasets/stjiris/portuguese-legal-sentences-v0\n47\nplace. We decided to utilise a more straightforward approach. In the data pre-processing step, these\noccurrences are removed from the text. Even though it does not contain all the related information as\napplying some summarization technique would provide, the focus was to further simplify the text. This\naimed to improve the semantic search system’s performance, since the sentences themselves would\npresent a clearer meaning without depending on other sections.\nIn our scenario, with over 30000 documents, the solution involves implementing a Bi-Encoder. We\nneed to create the embeddings independently of each other, allowing us to search later using the cosine\nsimilarity. This way, the overall search system performance is doable, whereas the search would not be\nfeasible if we utilise a Cross-Encoder.\n4.3 Architecture\nSection 4.3 provides insights on the implemented Search Systems’ architectures. Throughout this work,\nwe developed three different search systems: one semantic search and two hybrid search systems that\ncombine both semantic and lexical approaches.\nInitially, there was a pre-processing of the documents in the original dataset to split entire documents\ninto smaller units. This pre-processing was essential to separate the text into smaller passages since\nLegal-BERTimbau would be less effective on large sentences. Legal-BERTimbau’s siamese and triplet\nnetwork structures depend on the available training data, implying that the effectiveness of itself may\nvary depending on the size and nature of the input data. In our scenario, Legal-BERTimbau’s training\ndata were not very extensive, comprehending only a few tokens each time. Nevertheless, it was required\nto analyse the documents in more detail to verify if the phrases are too long, too short or if they raised\nother concerns, such as referenced in Subsection 4.2.1. On the other hand, stop-words removal was\nnot necessary, since Legal-BERTimbau, being a version of SBERT, is designed to receive meaningful\nsentences rather than isolated keywords.\nThe proposed solution architecture is illustrated in Figure 4.1.\nFor implementing the search system, we utilised Elasticsearch, which allows for scalability and fast\nretrieval of results while using the cosine similarity function to search through embeddings. Elasticsearch\nwas a requirement for this project, as stated in Subsection 4.1.1. Elasticsearch is used as a dense\nvector database where embeddings will be stored in indices. Such indices require initial mapping, which\nmandates the size of the embeddings and other extra information, such as the original document from\nwhere the sentence was retrieved.\nAfter the pre-processing described in the previous section, the next step was generating the embed-\ndings. The sentence embeddings were created by making use of the Legal-BERTimbau model hosted\n48\nFigure 4.1: System Architecture\non the Hugging Face Hub8, through the SentenceTransformers Python9library. With the embeddings\ngenerated, it is possible to populate the indexes on Elasticsearch through its Python Client.\nFor retrieving specific query results, that exact query would be transformed into an embedding by\nLegal-BERTimbau. Then the system can proceed to search similar sentences by executing a ranking\nfunction on Elasticsearch using the query embedding. Depending on which search system architecture,\nthe Ranking function varies slightly.\nFinally, the relevant passages are included in a prompt for a Generative Language Model (GLM),\nGPT3.5, providing a user-friendly response, yet based on our retrieved results. For example, with a\nquery as such: “Furto de Armas”, the system’s output is the following:\n— A passagem relevante para a quest ˜ao´e a seguinte: ”Quem, de noite e acompanhado, entra\nnuma casa museu, depois de arrombar a porta e de l ´a retira v ´arias armas pec ¸as de museu e delas se\napropria, contra vontade do dono, pratica os crimes de introduc ¸ ˜ao em lugar vedado ao p ´ublico e furto\nqualificado.” (Documento ID: 9EWRY oMBF lErWh5 w2g).\n8https://huggingface.co/\n9https://www.sbert.net/\n49\n4.3.1 Purely Semantic Search System\nThe Purely Semantic Search System makes use of only the semantic capabilities of the embedding\nmodel. The system utilises Elasticsearch and cosine similarity function to search through the embed-\ndings and retrieve relevant search results.\nWe chose to implement a symmetric semantic search system. The reasoning behind such a decision\nwas predominantly due to the need for more queries and results from pairs examples to implement a\nproper asymmetric semantic search. On the same note, when a judge interacts with a search system,\nthe judge is more likely to insert an extensive query with proper terminology than inputting a question\nthat wants to be answered.\nTo be noted, there are other distance metrics, such as the Dot Product, but Cosine Similarity is\nregarded as the most prominent one for use cases as this.\n4.3.2 Lexical-First Search System\nThis work also presents a different version of a search system, a Hybrid Search System, that combines\nthe potential of lexical search techniques and the reach of large language models. We called it Lexical-\nFirst Search System.\nThe architecture is similar to the developed Purely Semantic Search System. The pre-processing\nand usage of ElasticSearch are equal, and only the retrieval method changes slightly. Instead of yielding\nthe best matches using the cosine similarity metric, it combines the use of BM25 before evaluating the\nsimilarity of the embeddings using the cosine similarity metric.\nThe method retrieves a pre-defined number of top results using BM25 (i.e Top 20 results). Afterwards,\nit ranks the outcomes using the cosine similarity metric, as shown in Figure 4.2.\nFigure 4.2: Lexical-First Search System Retrieval Method\n50\n4.3.3 Lexical + Semantic Search System\nLexical-First Search System presents promising results, which are explored in more detail in Section 6.2.\nHowever, such hybrid system architecture relies immensely on BM25 results. In a more straightforward\noverview, the presented Hybrid Search System filters the possible results using BM25 and then verifies\nwhich results should be retrieved based on a Legal-BERTimbau model.\nThis Subsection introduces a more flexible architecture for a Hybrid Search System, designated as\nLexical + Semantic Search System. Instead of filtering the first batch of results using BM25, we propose\nto combine the scores of both lexical and semantic information retrieval methods.\nLexical + Semantic Search System utilises the scores provided by BM25 and the cosine similarity\nvalue from the Legal-BERTimbau embedding space. BM25 scores are normalised using the maximum\nscore obtained using a specific query. This implies that the highest score using BM25 will be 1.\nIn parallel, the cosine similarity between the dense vector and the query embedding is calculated.\nThe cosine similarity value does not need to be normalised.\nFinally, we sum the scores using both methods and proceed to reorder the results and present them.\nThe retrieved method is illustrated in Figure 4.3.\nFigure 4.3: Lexical + Semantic Search System Retrieval Method\n51\n52\n5\nLegal Language Model\nContents\n5.1 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n5.2 Semantic Textual Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.3 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.4 Generative Pseudo Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n5.5 Multilingual Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n5.6 Metadata Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.7 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n53\n54\nThe embedding’s creation required a proper language model adapted to the Portuguese language\nand, more specifically, the Portuguese legal domain. This Chapter will explain the explored approaches\nand the final implementations of Legal-BERTimbau.10Advances in language representation using Neu-\nral Networks allowed for the usage of this concept in facilitating the transfer of “learned internal states of\nlarge pre-trained language models” [37,38]. Transfer learning is a technique that allows a model trained\nfor a general task to be later fine-tuned for other specific tasks.\nBERT contains numerous parameters, reaching over 300 M on the large version. Training a BERT\nmodel from scratch, with a relatively small dataset, would cause overfitting. In our case, having nearly 30\n000 documents, the correct approach is to use a model to fulfil our needs that is already trained with a\nlarger corpus than the one available in our domain. With BERTimbau (See Section 3.2) as a base model,\nwe were able to create our own model for finding similar passages, a SBERT model: Legal-BERTimbau.\nChapter 5 provides an overview of the training involved in the generation of Legal-BERTimbau. It\nshares an overview of the different techniques used for each different version of the model. Figure 5\npresents an overview of the different training tasks through this research to attain the different model\nversions. The presented additional tasks were combined with each other to generate multiple model\nversions.\n5.1 Domain Adaptation\nDomain adaptation is a type of fine-tuning regarding language models. As the name suggests, the\ngoal is that the model can further understand a new domain. This scenario arises when a model is\npre-trained on a specific dataset but needs to be used with a different (but related) dataset. In our\ncontext, BERTimbau was trained with a large Portuguese corpus, BrWaC, but we wanted to use a\nmodel on Portuguese jurisprudence. Legal documents are very distinct from BrWaC, only sharing the\nsame language. Documents provided by official courts should contain text better structured than BrWaC\nand contain some jargon and technical language of the domain. Over the years, multiple techniques\nhave been aiming to adapt a language model to a new domain. This stage was done on the BERTimbau\nlarge variant, which can produce embeddings of 1024 dimensions. The Domain Adaptation stages were\nperformed on a NVIDIA GeForce RTX 3090 24 GB GPU. The developed variants can be easily used\nwith SentenceTransformers Python Library, TensorFlow, PyTorch, or JAX, since each model is hosted\non the HuggingFace Platform, using the HuggingFace’s Transformers library.\n10Models available on https://huggingface.co/stjiris\n55\nFigure 5.1: Training Tasks Overview\n5.1.1 Masked Language Modeling\nMLM, as mentioned in Section 3.1, is a task originally introduced by BERT. The training consisted\nin applying the traditional BERT MLM training over our training dataset. With this approach, the model\nbecame more familiarized with technical language or jargon presented in those documents. For the MLM\ntask, we defined the learning rate as 10−5. We want the learning rate in this stage to be significantly lower\nthan in the initial training stage itself. Since we are training the model with such numerous parameters\non a rather small dataset, it would easily overfit. In the same vein, the performed fine-tuning was carried\nout by employing the procedure for a single epoch. This fine-tuning stage, performed with a batch size\nof 2, generated a BERTimbau variant. The loss associated with the training process can be shown in\nthe following image:\nThe selected MLM model variant is the one obtained at the 770 training steps mark. The model\nselection was based on the best model performance within our evaluation split. The variant that was\n56\nFigure 5.2: MLM Training Loss\ncreated is:\n• stjiris/bert-large-portuguese-cased-legal-mlm\n5.1.2 Transformer-based Sequential Denoising Auto-Encoder\nAs described in Subsection 3.1.1.B, TSDAE is an unsupervised sentence embedding approach. TSDAE\nencodes damaged sentences into fixed-sized vectors during training and needs the decoder to recover\nthe original sentences from this sentence embedding.\nWhen using the TSDAE technique, we used a learning rate of 5∗10−6over our training dataset and\na batch size of 2. The loss associated with the training process can be shown in the Figure 5.1.2.\nThe selected TSDAE model variant was the one with the lowest loss value: 1300 training steps. The\nloss value was calculated using the evaluation split. The variant that was created is:\n• stjiris/bert-large-portuguese-cased-legal-tsdae\n5.2 Semantic Textual Similarity\nThe task our language model needs to perform is STS evaluation. STS is a regression task that deter-\nmines how similar two text segments are on a numeric scale, ranging from 1 to 5.\nTo adapt the generated variants to this task, we created SBERT versions of themselves and trained\nthem with four distinct datasets. We attached an independent linear layer to each Legal-BERTimbau\n57\nFigure 5.3: TSDAE Training Loss\nvariant and fine-tuned the model using a mean squared error loss. The SBERT version of Legal-\nBERTimbau-large, utilising the SentenceTransformer library, is defined as follows:\n1bertmodelname = 'rufimelo/Legal-BERTimbau-large'\n2wordembedding model = models.Transformer(bert modelname, max seqlength=256)\n3pooling model = models.Pooling(\n4 wordembedding model.get wordembedding dimension())\n5densemodel = models.Dense(\n6 infeatures=pooling model.get sentence embedding dimension(),\n7 outfeatures=256,\n8 activation function=nn.Tanh())\n9model = SentenceTransformer(modules=[word embedding model, pooling model, dense model])\nThis code snippet shows how we can create a SBERT model from scratch, using a BERT model as\nthe foundation. Since we did not add a Pooling layer in our architecture, which would lower the accuracy\nof the embeddings for the STS task, this SBERT model variant generates 1024 dimension embeddings.\nTo train the models for the STS task, the datasets assin [14] and assin2 [30] were used, as well as\nthe stsb multi mt [23] Portuguese sub-dataset. Each dataset contained pairs of sentences and a label\nvalue representing both sentences’ similarities.\nThe assin dataset contains 10 000 pairs of sentences, 5 000 of which were used for training. Similarly,\nthe assin2 dataset contains 9 448 pairs of sentences, from which 6 500 were also used for training.\nFinally, stsb multi mt Portuguese sub-dataset contains 8 628 pairs of sentences, from which we used\n5749 to fine-tune the model. In a nutshell, for the STS task, our models were trained with 20 197\n58\nPortuguese sentence pairs, allowing the model to be more familiarized with the Portuguese language.\nBoth assin and assin2 are Brazilian Portuguese datasets.\nFollowing the MLM and TSDAE domain adaptation performed on BERTimbau, we trained the large\nversion with a learning rate of 10−5, making use of the Adam optimization algorithm [21]. We trained\nwith a batch size of 8 during five epochs.\nThis type of fine-tuning, generated the following SBERT variants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-sts-v0\n5.2.1 Semantic Textual Similarity Custom Dataset\nThe solution we present implies training using the STS task. The resources available for this effect\nare slim. For this effect, there are only 3 Portuguese STS datasets: assin ,assin2 andstsb multi mt\nPortuguese sub-dataset, which is a translation from the English sub-dataset version. On top of this fact,\nthe Portuguese legal domain is unique on its own. To improve the STS task, we developed a unique\ndataset for further training our models. The dataset is publicly available at HuggingFace:\n• stjiris/IRIS sts\nThe dataset creation process was automated. The dataset, similarly to assin and assin2, contained\nrelatedness scores from 0 to 5. When training, these scores are normalized for values ranging between\n0 and 1. Sentence pairs selected randomly across our document collection were given relatedness\nvalues from 0 to 1. Values 1 to 4 were attributed to sentence pairs selected from the same summary.\nThe summaries are short, and thus, they might imply some entailment. Finally, we selected sentences\nfrom our collection and proceeded to generate their pairs using OpenAI’s GPT3 text-davinci-003 model\nAPI, publicly available since November 29th11. Such pairs received a relatedness score from 4 to 5. The\nGPT3 model received the following request:\n• ”Escreve por outras palavras: Entrada: sentence Sa´ıda:”\n, which translates to:\n• ”Write, in other words: Input: sentence Output:”\nThis custom dataset also presents NLI annotations. Sentences pairs with relatedness values above\nfour were given a “2” as the entailment label, meaning they are entailed. Pairs with relatedness scores\n11https://beta.openai.com/playground\n59\nbetween one and four were given a “1” as the entailment label, meaning those sentences have no rela-\ntionship. Finally, sentence pairs with a relatedness score below one were associated with an entailment\nlabel of “0”, meaning they are contradictory.\nSimilarly to the STS fine-tuning stage described in BERTimbau’s paper, the models were trained with\na learning rate of 10−5, also making use of the Adam optimization algorithm, but only performed on a\nbatch size of 8during five epochs.\nThis fine-tuning, with a custom dataset, generated various SBERT variants. To differentiate the\nmodels that were trained on this custom STS dataset, we denoted sts-v0 when it was trained on the\nthree original datasets and sts-v1 when a model was trained on all four datasets, including /IRIS sts.\nWe utilised this new dataset and trained the following variants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-sts-v1\n5.3 Natural Language Inference\nAccording to [31], a slight STS performance improvement when the models were subjected to NLI data\nis reported (See Section 3.1.2.A). The previously used assin and assin2 datasets also contain informa-\ntion about the relatedness between sentence pairs. Similarly to SNLI, assin contains a label feature\nindicating if a sentence entails the other (0), if they have no apparent relationship between them (1)\nor if they contradict each other (2). In the case of assin2 it contains labels 1 or 0, representing if the\nsentences are entailed or not, respectively.\nWe trained the large models on assin and assin2 NLI information with an 8 batch size for five epochs\nwith a learning rate of 10−5, using the Adam optimization algorithm. The variants produced with this\napproach outperform others that do not. (Discussed in more detail in Chapter 6)\nThis fine-tuning, combined with STS, generated 4 different SBERT variants. To denote the models\nsubjected to NLI data, we add nliin the model name.\nAdding this type of differentiation, we trained the following models.\n• stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v1\n60\n5.4 Generative Pseudo Labeling\nGPL, as mentioned in Subsection 3.1.1.D, is a state-of-the-art unsupervised technique to fine-tune ex-\nisting models in different domain. This technique allows a model to understand which sentences can\nanswer a question.\nAs explained, GPL has three different stages: Query Generation (from GenQ), Negative Mining and\nPseudo Labeling.\nIn the Query Generation step, we created 10000 Queries for 10000 legal documents. We used\na pre-trained T5 model, fine-tuned for the Portuguese Language, pierreguillou/t5-base-qa-squad-v1.1-\nportuguese12, to generate queries from each document summary. After this step, we have a collection\nof queries that each summary (positive passage) should be able to answer individually.\nIn the Negative Mining stage, we retrieved passages very similar to our initial passage, but that should\nnot be able to answer the generated queries. For this purpose, we created an index on ElasticSearch\nwhere we stored the embeddings of the other summaries used for the previous step. To reduce the bias\nin the system, we used an original BERTimbau large fine-tuned for STS. This model was fine-tuned\nfollowing the guidelines in the original paper. We used assin and assin2 datasets for five epochs, using\n3∗10−5for the learning rate.\nIn the final step, we used the same model to calculate the margin between positive and negative\npassages using the dot product. We trained our models using the created triplets (positive passage,\nnegative passage and margin score), applying the Margin Mean Squared Error Loss with a learning rate\nof2∗10−5on one epoch. The variants produced with this approach outperform the variants that were\nnot subject to this technique. (Discussed in more detail in Chapter 6)\nThis technique was applied to models that were trained using both NLI and STS data.\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1\n12https://huggingface.co/pierreguillou/t5-base-qa-squad-v1.1-portuguese\n61\n5.5 Multilingual Knowledge Distillation\nMKD is a technique developed by Neil Reimers, as stated in Subsection 3.1.2.B. This sort of technique\nallows for a model to extend its knowledge over a language, i.e. English, to different languages such as\nPortuguese. This technique is attractive, especially when we intend to create a model that should learn\na language. However, only a few datasets are available in that same language.\nIn this work, we developed a language model that utilises this technique. However, the goal was\nnot to create a multilingual model, but rather to improve the knowledge a model already has of the\nPortuguese language.\nThe dataset used was: TED 2020 – Parallel Sentences Corpus [33]. TED 2020 contains around\n4000 TED13and TED-X transcripts from July 2020. These transcripts were translated by volunteers into\nmore than 100 languages, adding up to a total of 10 544 174 sentences. All the sentences were aligned\nto generate a parallel corpus for training tasks such as this.\nWith the explained end goal, this technique was applied to Legal-BERTimbau-large . It was desig-\nnated as the student model, supporting the English Language already, and we intended for it to learn\nPortuguese. The chosen teacher model was sentence-transformers/stsb-roberta-large14. It was defined\nthat the number of warm-up steps should be 10000 . The training was performed with a 10−5learning\nrate using the Adam optimization algorithm during five epochs.\nFurthermore, after this extra training step, we fine-tuned the models for the STS regression task as\ndescribed in Section 5.2. With the integration of this different application of the technique, it was possible\nto further train a model for the Portuguese language by mimicking a teacher model that knew how to\nencode English Sentences properly.\nThe application of the MKD technique and the classical STS fine-tuning produced a different BERT\nvariants:\n• stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1\n13https://www.ted.com/\n14https://huggingface.co/sentence-transformers/stsb-roberta-large\n62\n5.6 Metadata Knowledge Distillation\nThis work presents a new technique developed to improve information retrieval through dense vectors:\nMetadata Knowledge Distillation (MetaKD)\nIn our dataset, multiple documents have associated with them “Descritores”, brief tags manually\nannotated by experts. These tags intend to identify the main document subjects. These tags could\nindicate if a crime was committed with knives or even if it is related to COVID-19.\nWith such annotation, we assumed that the documents are, in a way, related to one another. Thus,\nthe sentences from each document have some trim level of entailment between each other.\nWe started by identifying the documents related to a subject, COVID-19, i.e. and we proceeded\nto encode those documents’ sentences. The generated embeddings form a cluster. We processed to\ncalculate the centroid of those embeddings and adjusted the embeddings slightly to the centroid. (1-5%)\nThis minor adjustment is based on the assumption that those sentences are related and, thus, they\nshould be closer to one another. This process is done through the tags we have available. This ideology\ncan be shown in Figure 5.6. Finally, the updated embeddings will serve as gold labels for what the\nembeddings of the same model should look like. We then applied the mean-squared error loss, similar\nto Multilingual Knowledge Distillation, to train the model. The process is illustrated in Figure 5.6.\nFigure 5.4: Metadata Knowledge Distillation Ideology\nWe used this technique with a learning rate of 10−6and a batch size of 3sentences for one epoch.\nWe adjusted the embeddings of a 1000 document sample selected at random from the training document\nsubset. The embeddings were adjusted based on each tag’s centroids, centralizing the embeddings by\n1%. These hyperparameter choices were defined based on a grid search optimization algorithm where\nwe tried to maximize the STS task evaluation. With this technique, the following models were generated:\n63\nFigure 5.5: Metadata Knowledge Distillation\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-MetaKD-v0\n• stjiris/bert-large-portuguese-cased-legal-mlm-gpl-nli-sts-MetaKD-v1\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v0\n• stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-MetaKD-v1\n5.7 Overview\nTable 5.1 provides an intuitive summary of the different versions created throughout this work, where it\npoints out the different techniques used for each variant.\nModel MLM TSDAE STS NLI GPL MKD MetaKD\nmlm x\ntsdae x\nmlm-sts-v0 x x\ntsdae-sts-v0 x x\nmlm-sts-v1 x x\ntsdae-sts-v1 x x\nmlm-nli-sts-v0 x x x\nmlm-nli-sts-v1 x x x\ntsdae-nli-sts-v0 x × x\ntsdae-nli-sts-v1 x x x\nmlm-gpl-nli-sts-v0 x x x x\nmlm-gpl-nli-sts-v1 x x x x\ntsdae-gpl-nli-sts-v0 x × x x\ntsdae-gpl-nli-sts-v1 x x x x\nmlm-mkd-nli-sts-v0 x x x x\nmlm-mkd-nli-sts-v1 x x x x\ntsdae-mkd-nli-sts-v0 x × x x\ntsdae-mkd-nli-sts-v1 x x x x\nmlm-gpl-nli-sts-MetaKD-v0 x x x x x\nmlm-gpl-nli-sts-MetaKD-v1 x x x x x\ntsdae-gpl-nli-sts-MetaKD-v0 x x x x x\ntsdae-gpl-nli-sts-MetaKD-v1 x x x x x\nTable 5.1: Legal-BERTimbau variants\n64\n6\nSystem Evaluation\nContents\n6.1 Language Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n6.2 Search System Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n65\n66\nThis work aimed to improve the information retrieval process court professionals have to go through,\nwith the ultimate goal of helping in the court decision process. Judges would have to go through immense\nand often dispersed information to formulate a decision. This process is well known for being time-\nconsuming and complex, and there is no guarantee that all the relevant documents are retrieved for user\nconsultation. A poor-made decision based on incomplete information can open a precedent for future\nscenarios and should be avoided.\nEvaluating the performance of a search system is a critical task in information retrieval research. A\ngood evaluation methodology can provide insights into the strengths and weaknesses of the system and\nguide the development of future improvements. To realise the search system’s performance, we require\nexamples of queries and expected results. Unfortunately, the required data was not available, and\ngenerating the aforementioned subsets of data is a complex process since it requires manual annotation.\nAnd even trying to replicate those assets, can introduce bias to our evaluation system.\nChapter 6 provides insights into the project’s evaluation. Throughout this project, we explored differ-\nent search system architectures with different trained language models. These language models were\ntailored to our scenario, understanding the Portuguese legal context better than state-of-the-art generic\nmodels. Since the large language models play a critical role in our final solution, Chapter 6 also provides\ninsights into such models. As such, the evaluation is composed of two parts.\nFirstly, we evaluate the developed language models. We evaluate whether the developed models are\ntrimmed for the Portuguese legal context and their performance on their task: STS.\nThe second part will be a quantitative evaluation of the search system itself. We compare the perfor-\nmance of our solution, comparing it to traditional techniques such as BM25 on two different “dimensions”.\nSuch evaluation provides insights into the system’s ability to retrieve relevant information helpful to the\njudges.\n67\n6.1 Language Model Evaluation\nThis thesis led to the development of multiple Legal-BERTimbau versions. From fine-tuning large vari-\nants of BERTimbau to applying the Multilingual Knowledge Distillation technique to models, there were\nnumerous variants implemented. This section will provide the details on the evaluation performed on the\ndifferent Legal-BERTimbau variants. It starts by verifying if the models were successfully adapted to the\nPortuguese legal domain. Then it will verify if the variants have a good performance in the STS task.\n6.1.1 Domain Adaptation\nThe domain adaptation is a crucial technique as it helps to address the problem of model generalization,\nespecially when there is a lack of labelled data in the target domain. The importance of domain adapta-\ntion lies in its ability to improve the effectiveness of models when applied to new and unseen data (See\nSection 5.1).\nIn order to evaluate whether the domain adaptation stage was successful or not, we verified how\nwell the do Legal-BERTimbau variants handle Portuguese legal text. Such was done by comparing\nhow successful the Legal-BERTimbau could replace [MASK] tokens for the correct words in the MLM\ntask. We made use of the same loss function used in the MLM task, negative log-likelihood loss. In this\nscenario, a lower average loss value would indicate that the model performs well.\nWe utilised the testing dataset from the splits generated in Subsection 4.2 and compared the average\nloss produced (Shown in Table 6.1.1). We also submitted the models subjected to the TSDAE technique\nto this evaluation.\nAVG loss\nBERTimbau large 18.60706\nstjiris/bert-large-portuguese-cased-legal-mlm 0.02679\nstjiris/bert-large-portuguese-cased-legal-tsdae 10.51178\nTable 6.1: MLM average loss for legal documents in the test set\nRegarding our testing split, both stjiris/bert-large-portuguese-cased-legal-mlm andstjiris/bert-large-\nportuguese-cased-legal-tsdae perform better on the MLM task than BERTimbau . Both models sub-\njected to MLM and TSDAE domain adaptation techniques can predict more accurately which words\nshould replace in the [MASK] tokens. The model subjected to TSDAE presents a more significant aver-\nage loss than the model subjected to the MLM technique. This is to be expected since the model was\nnot trained on that same task. Nevertheless, despite not being trained for the MLM task, it still outper-\nformed BERTimbau for legal documents in our test set. Thus, validating our conjecture, our models are\nsuccessfully adapted to the Portuguese legal domain.\n68\n6.1.2 Semantic Textual Similarity\nThe main task of a large language model within our search system architecture is to determine how\nsemantically similar two sentences are, STS. To evaluate a model’s STS task performance, we evaluate\nif a model is able to provide accurate similarity scores for a given sentence pair. For such effect, it is\ncalculated the Pearson correlation [41] between the expected and projected similarity score between\ndifferent sentence pairs.\nThe Portuguese datasets used in the fine-tuning stage were also utilised to evaluate these mod-\nels. We proceeded to evaluate using those datasets’ test splits for the STS task. We can analyse the\nperformance of the different versions of Legal-BERTimbau against state-of-the-art multilingual models\nwhen performing the same task. These state-of-the-art multilingual models serve as a baseline for our\nevaluation. For this effect, we selected the following multilingual models: paraphrase-multilingual-mpnet-\nbase-v2 andall-mpnet-base-v2\nassin assin2 stsb multi mt Avg.\npt\nBERTimbau large Fine-tuned for STS 0.81289 0.84133 0.77958 0.81126\nparaphrase-multilingual-mpnet-base-v2 0.74373 0.83999 0.71468 0.76613\nall-mpnet-base-v2 0.56306 0.62126 0.51287 0.56573\nmlm-sts-v0 0.78509 0.81158 0.83625 0.81097\nmlm-nli-sts-v0 0.78095 0.81001 0.83684 0.80927\nmlm-gpl-nli-sts-v0 0.78119 0.81187 0.83543 0.80950\nmlm-mkd-nli-sts-v0 0.77634 0.80976 0.84779 0.81130\ntsdae-sts-v0 0.78597 0.81542 0.84424 0.81521\ntsdae-nli-sts-v0 0.78430 0.80311 0.83842 0.80861\ntsdae-gpl-nli-sts-v0 0.77862 0.80675 0.83925 0.80821\ntsdae-mkd-nli-sts-v0 0.78008 0.84145 0.85060 0.80503\nmlm-gpl-nli-sts-MetaKD-v0 0.81115 0.83634 0.78210 0.80987\ntsdae-gpl-nli-sts-MetaKD-v0 0.80743 0.84041 0.78294 0.81026\nmlm-sts-v1 0.78025 0.81479 0.83460 0.80988\nmlm-nli-sts-v1 0.77740 0.80975 0.83588 0.80768\nmlm-gpl-nli-sts-v1 0.78143 0.80964 0.83610 0.80905\nmlm-mkd-nli-sts-v1 0.77274 0.812149 0.84997 0.81162\ntsdae-sts-v1 0.78433 0.81610 0.84320 0.81454\ntsdae-nli-sts-v1 0.78251 0.80494 0.84077 0.80941\ntsdae-gpl-nli-sts-v1 0.77634 0.80673 0.83889 0.80731\ntsdae-mkd-nli-sts-v1 0.77368 0.81603 0.85495 0.81489\nmlm-gpl-nli-sts-MetaKD-v1 0.80767 0.83701 0.77955 0.80807\ntsdae-gpl-nli-sts-MetaKD-v1 0.80543 0.83467 0.77749 0.80586\nTable 6.2: STS evaluation on Portuguese datasets\nIt is possible to verify that our SBERT variants performed better than state-of-the-art multilingual mod-\nels on the STS task for both assin andassin2 datasets. Regarding the performance on the stsb multi mt\ndataset, the values obtained do not outperform multilingual models. stsb multi mtis a dataset composed\nof different multilingual translations from the original STSbenchmark dataset. Consequently, multilingual\nmodels did engage with multiple translations from the same sentence during the training process. Nev-\nertheless, the score is similar, and, more importantly, BERTimbau variants are adapted to our domain,\nas exposed previously. This quantitive evaluation aims to verify if our models still understand the Por-\n69\ntuguese Language in general and can comprehend the sentence similarities in our domain through our\ncustom STS dataset.\n6.2 Search System Evaluation\n6.2.1 Automatic Query Generation\nIn order to evaluate our search system, we needed a group of queries and expected retrieved results to\nevaluate its performance. In our scenario, there is no such group of data for this effect. For this reason,\nwe had to explore an automatic generation of examples to provide a preliminary evaluation – or at least\nto help us understand the potential performance of the system. With such queries and information about\nthe document from which they were generated, we are able to assess our system’s performance.\nOur solution to evaluate the system performance passed through creating embeddings from a col-\nlection of 1000 legal documents and store them in ElasticSearch. Then we generate queries from each\ndocument and utilise those same queries to test the system. Our assumption is that a search system\nshould be able to return as a result the original document (or passage) used to automatically build a\nparticular query. The evaluation architecture was implemented as described in Figure 6.2.1.\nFigure 6.1: Evaluation Architecture\n70\nThe first step is related to creating queries from each document summary that we use later on. For\nthis effect, we tried two different approaches.\nInitially, we utilised the LexRank summarization technique, using the original BERTimbau large\nmodel, to retrieve the sentence with the highest centrality. This approach would provide the most impor-\ntant sentence in the summary, but it would yield a sentence that contains no new words, meaning we are\nnot exploring the full capabilities of a Semantic Search System. If a user inputs a query with words that\ndo not appear within the document, the lexical techniques would not provide adequate results. Thus,\naiming to explore the full extent of this scenario, we processed all the queries through a GPT3 model\nprovided by Open AI to rewrite the sentences, whilst keeping the same meaning. Unfortunately, this ap-\nproach did not provide the query examples we were hoping for. Often it would use the exact keywords,\nonly in a different order.\nAfter further iterations, we utilised a T5 model to generate queries reducing bias. Nevertheless, this\napproach lead to queries that made use of keywords present within the summary. Subsequently, we\ntreated the queries, so that they maintained a similar meaning, but did not contain every exact key-\nword. We identified the top 20 keywords from the summary with TF-IDF and exchanged them with syn-\nonyms or similar expressions. The synonyms gathering was performed by producing multiple requests\ntohttps://www.sinonimos.com.br/ , a website that provides synonyms for Brazilian Portuguese words.\nThe selected synonyms and expressions that replaced the keywords were chosen by evaluating which\nexchange would better preserve the meaning of the queries. To do so, we used BERTimbau large and\nevaluated the different sentences using cosine similarity.\nThe second step was creating embeddings for each sentence using different versions of our devel-\noped model, Legal-BERTimbau. Each sentence embedding is stored in an ElasticSearch index. Each\nindexed document would be composed of a text data field, a dense vector data field that contained the\nsentence embedding, and an indication of the document from where the sentence was retrieved.\n6.2.2 Results\nWe retrieve the results using the cosine similarity metric for the Semantic Search System and combined\nthe cosine similarity metric with BM25 technique to replicate the proposed Hybrid Search Systems. Our\ncomparison baseline consisted on utilising BM25 searches with the same queries and other multilingual\nmodels such as “sentence-transformers/all-mpnet-base-v2” and “paraphrase-multilingual-mpnet-base-\nv2” instead of Legal-BERTimbau versions. The evaluation dataset, since it was generated automatically,\ndid not allow an overview of False or True Positives, and neither it did for False and True Negatives.\nAs such, we could not use traditional metrics such as Precision and Recall to evaluate the system’s\nperformance. We defined two alternative metrics to evaluate the system’s performance: Search and\nDiscovery.\n71\nThe Search metric allows gathering insights into the system’s ability to find which document a certain\nquery refers to. Initially, each query was generated from individual summaries. If the search system re-\nceives a query based on document x, the retrieved result should be from the document x. We evaluated\nwhether this happens within the first result or the first group of results for a specific query. If the retrieved\ndocument is the same as the one used for creating the query, it increases the evaluation score by 1.\nThe Discovery metric provides interpretability on a search system’s ability to retrieve additional doc-\numents that might be relevant to the user. The search system should retrieve documents that are\nimportant for a given query, even if they are not the original document from which the query was gen-\nerated. Each legal document has “descritores” (tags) that were manually annotated. Similarly to the\nSearch metric, if the retrieved document within a group of results contains a tag equal to the original\ndocument’s tags from which the query was generated, we increase the score. For each equal tag, the\nDiscovery metric score is increased by one, calculating the intersection between each retrieved passage\nand the original document tags. In practice, if the search system receives a query that was created from\na document ycontaining the tag “Knives”, we want to evaluate whether the same tags appear within the\ntop results.\nSearch metric results for models fine-tuned without and with the custom STS dataset are shown in\nFigures 6.2 and 6.3, respectively.\nWe evaluated the Semantic Search System performance based on the different top results sizes. We\nverified whether the queries’ original document is suggested within the first result (Top 1), the first two\nresults (Top 2), and so forth. Likewise, we evaluated the Top 1, Top 2, Top 3, Top 5, Top 10, and Top 20.\nBM25 outperforms our original Semantic Search System in the Search metric. It also shows that\na Semantic Search System using a Multilingual model would perform considerably worse than when\nusing Legal-BERTimbau. For this metric, the Lexical-First approach that we proposed performs closer to\nBM25 and the Lexical+Semantic can even occasionally surpass BM25, maintaining a tight performance\nbetween the two. This metric verifies that BM25 can identify a query source better than a Semantic\nSearch System. However, a Hybrid Search System such as the one we developed can match and even\noutperform BM25 capabilities. Search Systems using models that were fine-tuned on the custom STS\ndataset (V1 models) present a slightly lower performance than the ones fine-tuned only on pre-existing\nand manually annotated datasets (V0 models).\nSimilarly, for the Discovery metric, the system performance using models fine-tuned without and with\nthe custom STS dataset are shown in Figures 6.4 and 6.5, respectively.\n72\nFigure 6.2: Search System Evaluation – Search metric - Models V0\nWe evaluated the Semantic Search System performance based on the different top results sizes.\nWe verified whether the queries’ original document tags appeared within the first result (Top 1), the first\ntwo results (Top 2), and so forth. Likewise, we also evaluated the Top 1, Top 2, Top 3, Top 5, Top 10,\n73\nFigure 6.3: Search System Evaluation – Search metric - Models V1\nand Top 20. Search Systems using models fine-tuned on the custom STS dataset (V1 models) perform\nmarginally worse than those fine-tuned on pre-existing and manually annotated datasets (V0 models).\nThis metric shows that BM25 does not have the same capability to recommend similar documents.\nWhen searching for a specific matter, it is often beneficial for a Search System to retrieve relevant docu-\nments we were unaware of. As such, we can verify that our Semantic Search System can suggest similar\n74\nFigure 6.4: Search System Evaluation - Discovery metric - Models V0\ndocuments more regularly than BM25 or any other search system using a multilingual model. Surpris-\ningly, both our Lexical-First and Lexical+Semantic Search Systems outperform our semantic search\nsystem significantly.\n75\nFigure 6.5: Search System Evaluation - Discovery metric - Models V1\nWe averaged the Top 1, Top 2, Top 3, Top 5, Top 10, and Top 20 metric results for each metric, raising\nsome understanding of the techniques used in the research. The variations that received NLI training\nhad a 4.3%improvement in the Search metric and a 5.4%improvement in the Discovery measure.\nModels subjected to the GPL training approach show a 3.2%improvement in the Search metric and a\n1.7%improvement in the Discovery metric. Finally, models that used our novel method exhibited a 2.0%\n76\ndrop in the Search metric but a 1.7%decrease in the Discovery metric.\nWith these results, we selected our best-performing model, tsdae-gpl-nli-sts-MetaKD-v0, to analyse\nin more detail. Tables 6.3 and 6.4 provide more quantitive insights on the development.\nModel Top 1 Top 2 Top 3 Top 5 Top 10\nBM25 629 696 722 760 799\nPurely Semantic 411 471 518 559 618\nLexical-First 581 635 668 716 754\nLexical + Semantic 629 675 705 734 785\nImprovement 0% -3.01 % -2.35% -3.42% -1.75%\nTable 6.3: Search System Evaluation – Search metric – Best Model\nModel Top 1 Top 2 Top 3 Top 5 Top 10\nBM25 685 933 1133 1482 2216\nPurely Semantic 2226 2932 3532 4332 6243\nLexical-First 2769 3485 3993 4835 6352\nLexical + Semantic 2984 3732 4292 5168 7282\nImprovement 335% 300 % 278% 248% 228%\nTable 6.4: Search System Evaluation – Discovery metric – Best Model\nOur work explored three different search system architectures, and we concluded that combining\nboth lexical and semantic capabilities is a better approach than using only technique isolated. The\nLexical + Semantic Search System, which combines the strengths of both lexical and semantic systems,\nperformed similarly to BM25 in the Search metric, with performance delta of around −2.1%. In the\nDiscovery metric, it clearly outperforms the BM25 technique, going from around 335% improvement on\nthe Top 1 results and topping at 228% improvement on the Top 10 results. Such outcomes suggest that\nthe proposed search systems can improve the decision-making process in the legal domain by providing\nrelevant and insightful information.\nWhen considering these results, we can argue that our proposed and developed Lexical + Semantic\nSearch System can provide beneficial results and insights that ease and enrich the work judges perform\nin the legal domain. Similarly, this capability is extended to other professionals and non-professionals.\nBased on our research, we believe that the utilization of such search system can improve the actual\ncourt decision process and reduce the limitation and risks of formulating a decision based on incomplete\ninformation.\n77\n78\n7\nConclusion\nContents\n7.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n79\n80\nThe work presented in this document comes from a growing necessity for providing complete infor-\nmation to judges and legal authorities. Formulating a decision or gathering insight from the vast legal\ndomain is a very complex and often time-consuming task. This work provided insights into creating a\nsystem that utilises semantic search through embeddings, exploring the immense potential of neural\nnetworks instead of using more traditional lexical techniques. We explored the capability of Elastic-\nSearch to host a semantic search system, developing the necessary language models along the way.\nWe handled different problems when treating raw text data, tackling the need to pre-process the text\nbefore performing further tasks. Working with multiple Project IRIS members, we develop a prototype of\na Semantic Search System for STJ and provided support for other related tasks.\nBeing a segment of a more significant project: Project IRIS, this thesis allowed me to develop soft\nand hard skills by interacting with outstanding professionals and doctorate students. It provided insight\ninto effectively researching daily and cooperating on a large project with multiple members.\n7.1 Contributions\nWe created initial datasets containing pre-processed text that can be utilised for future tasks and the\nnecessary scripts to recreate the results. Along the work, we created multiple language models that can\nbe fine-tuned for multiple tasks, such as Question-Answering. Legal-BERTimbau developed variants\nwere trained entirely in Portuguese and adapted to the Portuguese legal domain.\nOur Legal-BERTimbau variants represent the first publicly available Portuguese BERTs fine-tuned for\nSTS. On the same note, as mentioned previously, all the developed SBERT variants are adapted to the\nPortuguese legal domain since they suffered domain adaptation techniques. Multiple Legal-BERTimbau\nvariants surpass state-of-the-art multilingual language models on assin , and assin2 datasets, which\nPortuguese and Brazilian researchers annotated.\nAs intended with this thesis, we proposed and developed a Semantic Search System and two dif-\nferent versions of Hybrid Search Systems: Lexical-First and Lexical+Semantic Search Systems. Based\non our preliminary evaluation, we believe our developed Search System can suggest more accurately\nrelevant documents than BM25 with a Discovery performance improving over 300% in some scenarios,\nwhile still maintaining a similar ability to identify a query’s source when compared to the BM25 technique.\nWith such information retrieval systems in place, we consider that it is possible to more accurately\nprovide insights into jurisprudence, easing and improving judges’ essential work. Such Search System\ncan generate a positive impact throughout all Portuguese legal entities, helping to enrich and ease the\ndecision process inherent in each of these entities. Our Search System can play an important role in\nhelping formulate fair and essential decisions and maintaining the stability and consistency required in\napplying the law.\n81\n7.1.1 Publications\n7.1.1.A Exploring Embeddings Models for Portuguese Supreme Court Judgments Summariza-\ntion\nIn recent years, the field of legal text summarization has grown due to the increase in electronically\navailable legal documents. As part of a collaboration with project IRIS members, a scientific paper was\ndrafted, “Exploring Embeddings Models for Portuguese Supreme Court Judgments Summarization”.\nThe findings of this study will be submitted for publication in a peer-reviewed journal in the near future.\nLegal documents can differ based on language and legal system, and this paper proposes an ap-\nproach for automatically summarizing Portuguese Supreme Court judgments. Because of the unique\ncharacteristics of these judgments and the judges’ methods for summarizing them, the paper suggests\na set of pre-processing techniques to optimize the task. We implemented an extractive summarization\nsystem using the LexRank technique. To account for specific vocabulary in Portuguese that pre-trained\nmodels like BERTimbau or mBERT do not cover, we experimented with different tailored models. In\nthis work, multiple Legal-BERTimbau models originated from our research were used. The proposed\napproach achieves a ROUGE-1 score of 47.92 and a ROUGE-2 score of 22.50, better than reported\nscores for similar work in other languages.\n7.1.1.B Semantic Search System for Supremo Tribunal de Justic ¸a\nAs part of this thesis, it was drafted a paper covering the main aspects of this research and the impact\nit could have on the scientific domain. It was submitted to the EPIA Conference on Artificial Intelligence,\na well-established European conference in the field of AI, ended up being accepted.\nThe scientific paper covers the main most robust hybrid system that we present within this research,\nthe pure semantic search system, and the techniques used to train Legal-BERTimbau. The article\nalso emphasises our novel technique, Metadata Knowledge Distillation, as it brought good results in\nour environment. Such a technique can be further used in other domains and/or with other metadata\ninformation.\n7.2 Future Work\n7.2.1 Albertina\nWith the publication of Albertina PT -PT, in May 2023, there is now the possibility of training a model,\nadapted to the Portuguese jurisprudence, exclusively in European Portuguese. This implies that, inher-\nently, there should be improvements with this approach. It should translate into a more effective model\n82\nthat would better comprehend Portuguese legislation.\nAnother approach would be to train a European Portuguese model completely from scratch only on\nPortuguese Jurisprudence. Having a model only trained on a specific domain should yield improved\nresults over domain-agnostic or domain-adapted models.\n7.2.2 Dataset Annotation\nWhen developing a language model, one of the main concerns is the available data. Manually annotated\nPortuguese legal domain datasets could be produced and revised. Even though we developed datasets\nfor the Domain Adaption training step and STS, automating such can lead to biased and incoherent\ndata, especially the developed STS dataset.\nThe downside of this approach is that it requires manual work, nearly impossible to automatize. Both\nassin andassin2 were annotated manually by different groups of researchers/volunteers, oriented only\nby simple guidelines.\nNevertheless, a properly labelled and cleaned legal dataset from the Portuguese domain would be\nhelpful in future applications. Similarly, one dataset was developed and published, alongside a paper,\non 1st July 2022, entitled “Pile of Law: Learning Responsible Data Filtering from the Law and a 256 GB\nOpen-Source Legal Dataset” by Peter Henderson et al. [17]. It encompasses a large corpus of legal\nand administrative data from multiple U.S.A. entities. The Pile of Law paper also exposed the ethical\nchallenges it faced and how it was handled “biased, obscene, copyrighted, and private information”.\n7.2.3 Architecture Improvements\nIn terms of query expansion, one potential improvement to the Hybrid Search System is to incorporate\nknowledge graph embeddings. A knowledge graph is a structured representation of information, where\nentities and their relationships are represented by nodes and edges, respectively. By incorporating\nknowledge graph embeddings, the system would be able to understand the context of the query better\nand expand it with related entities and concepts, which would help in the relevant documents’ retrieval.\nAnother aspect that can be explored is the incorporation of additional information to the embeddings.\nAs explored in [11], it was inserted titles in the beginning of each sentence before generating the em-\nbedding. In [11], that approach did not provide satisfying results, but it can be interesting to explore\ninserting some metadata information into the embeddings.\nAnother approach is to use entity recognition, a technique that identifies named entities such as\npeople, organizations, and locations in a given text. By recognizing entities in the query, the system could\nexpand the query by including relevant entities, which would help retrieve more relevant documents.\nActive learning is a machine learning technique that enables the system to continuously learn from\n83\nthe user’s feedback. The system could present the user with a small set of documents, and the user\nwould provide feedback on which documents are relevant and which are not. The system would then\nuse this feedback to improve its performance over time. This approach can be especially useful in the\nlegal domain, where the relevance of a document can be challenging to determine, and expert feedback\ncan help the system improve its performance.\nImprovements in the prompts given to a GLM, such as for GPT -3, can provide a better model re-\nsponse regarding a user’s query. Having tailored prompts to either enrich the user’s query or to improve\nthe models’ response can guide to a more seamless interaction with the Search System and more\nconcise answers from the System itself.\nOverall, incorporating knowledge graph embeddings or entity recognition can lead to a more sophis-\nticated and accurate Search System that can help legal practitioners in their research and decision-\nmaking.\n84\nBibliography\n[1] A GIRRE , E., B ANEA , C., C ARDIE , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., G UO, W.,\nLOPEZ -GAZPIO , I., M ARITXALAR , M., M IHALCEA , R., R IGAU, G., U RIA, L., AND WIEBE, J.\nSemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In\nProceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) (Denver,\nColorado, 2015), ACL, pp. 252–263.\n[2] A GIRRE , E., B ANEA , C., C ARDIE , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., G UO, W.,\nMIHALCEA , R., R IGAU, G., AND WIEBE, J. SemEval-2014 task 10: Multilingual semantic textual\nsimilarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval\n2014) (Dublin, Ireland, 2014), ACL, pp. 81–91.\n[3] A GIRRE , E., B ANEA , C., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., M IHALCEA , R., R IGAU, G.,\nAND WIEBE, J. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual\nevaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-\n2016) (San Diego, California, 2016), pp. 497–511.\n[4] A GIRRE , E., C ER, D., D IAB, M., G ONZALEZ -AGIRRE , A., AND GUO, W. *SEM 2013 shared task:\nSemantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics\n(*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual\nSimilarity (Atlanta, Georgia, USA, 2013), ACL, pp. 32–43.\n[5] A GIRRE , E., D IAB, M., C ER, D., AND GONZALEZ -AGIRRE , A. SemEval-2012 task 6: A pilot on se-\nmantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational\nSemantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2:\nProceedings of the Sixth International Workshop on Semantic Evaluation (USA, 2012), SemEval\n’12, ACL, p. 385–393.\n[6] B OOS, R., P RESTES , K., V ILLAVICENCIO , A., AND PADR´O, M. brWaC: A WaCky Corpus for Brazil-\nian Portuguese. In Computational Processing of the Portuguese Language (Cham, 2014), J. Bap-\n85\ntista, N. Mamede, S. Candeias, I. Paraboni, T. A. S. Pardo, and M. d. G. Volpe Nunes, Eds., Springer\nInternational Publishing, pp. 201–206.\n[7] B OWMAN , S. R., A NGELI , G., P OTTS , C., AND MANNING , C. D. A large annotated corpus for\nlearning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods\nin Natural Language Processing (Lisbon, Portugal, 2015), ACL, pp. 632–642.\n[8] C ER, D., D IAB, M., A GIRRE , E., L OPEZ -GAZPIO , I., AND SPECIA , L. SemEval-2017 task 1: Se-\nmantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the\n11th International Workshop on Semantic Evaluation (SemEval-2017) (Vancouver, Canada, 2017),\nAssociation for Computational Linguistics, pp. 1–14.\n[9] C ONNEAU , A., K IELA, D., S CHWENK , H., B ARRAULT , L., AND BORDES , A. Supervised learning\nof universal sentence representations from natural language inference data. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Language Processing (Copenhagen, Denmark,\n2017), Association for Computational Linguistics, pp. 670–680.\n[10] C ORDEIRO , N. NLP applied to portuguese consumer law. Master’s thesis, Instituto Superior\nT´ecnico, Universidade de Lisboa, 2022.\n[11] D AI, Z., AND CALLAN , J. Deeper text understanding for IR with contextual neural language model-\ning. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval (2019), ACM.\n[12] D EVLIN , J., C HANG , M.-W., L EE, K., ANDTOUTANOVA , K. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the ACL: Human Language Technologies, Vol 1 (Minneapolis, Minnesota,\n2019), ACL, pp. 4171–4186.\n[13] E RKAN , G., AND RADEV , D. R. Lexrank: Graph-based lexical centrality as salience in text summa-\nrization. J. Artif. Int. Res. 22 , 1 (2004), 457–479.\n[14] F ONSECA , E., S ANTOS , L., C RISCUOLO , M., AND ALUISIO , S. ASSIN: Avaliacao de similaridade\nsemantica e inferencia textual. In Computational Processing of the Portuguese Language-12th\nInternational Conference, Tomar, Portugal (2016), pp. 13–15.\n[15] G URURANGAN , S., M ARASOVI ´C, A., S WAYAMDIPTA , S., L O, K., B ELTAGY , I., D OWNEY , D., AND\nSMITH , N. A. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings\nof ACL (2020).\n[16] H E, P., L IU, X., G AO, J., AND CHEN, W. Deberta: Decoding-enhanced BERT with disentangled\nattention. In 2021 International Conference on Learning Representations (2021).\n86\n[17] H ENDERSON , P., K RASS , M. S., Z HENG , L., G UHA, N., M ANNING , C. D., J URAFSKY , D., AND\nHO, D. E. Pile of law: Learning responsible data filtering from the law and a 256GB open-source\nlegal dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (2022).\n[18] I NTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY AND EXPLORING ENGINEERING (IJITEE),\nVAISSNAVE , V., AND DEEPALAKSHMI , P. An Artificial Intelligence based Analysis in Legal domain.\n[19] K IM, M., R ABELO , J., AND GOEBEL , R. BM25 and transformer-based legal information extraction\nand entailment. In Proceedings of the COLIEE Workshop in ICAIL (2021).\n[20] K IM, S.-W., AND GIL, J.-M. Research paper classification systems based on TF-IDF and LDA\nschemes. In Human-centric Computing and Information Sciences (2019), vol. 9, p. 30.\n[21] K INGMA , D. P., ANDBA, J. Adam: A method for stochastic optimization. In 3rd International Confer-\nence on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings (2015), Y . Bengio and Y . LeCun, Eds.\n[22] M ARELLI , M., M ENINI , S., B ARONI , M., B ENTIVOGLI , L., B ERNARDI , R., AND ZAMPARELLI , R. A\nSICK cure for the evaluation of compositional distributional semantic models. In Proceedings of\nthe Ninth International Conference on Language Resources and Evaluation (LREC’14) (Reykjavik,\nIceland, 2014), European Language Resources Association (ELRA), pp. 216–223.\n[23] M AY, P. IMachine translated multilingual STS benchmark dataset. HuggingFace, stsb multi mt.\n[24] M IKOLOV , T., C HEN, K., C ORRADO , G., AND DEAN, J. Efficient estimation of word representations\nin vector space. In 1st International Conference on Learning Representations, ICLR 2013, Scotts-\ndale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings (2013), Y . Bengio and Y . LeCun,\nEds.\n[25] N GUYEN , H., V UONG , H. T., N GUYEN , P. M., D ANG, T. B., B UI, Q. M., S INH, V. T., N GUYEN ,\nC. M., T RAN, V. D., S ATOH , K., AND NGUYEN , M. L. JNLP team: Deep learning for legal process-\ning in COLIEE 2020. CoRR abs/2011.08071 (2020).\n[26] N GUYEN , T., R OSENBERG , M., S ONG, X., G AO, J., T IWARY , S., M AJUMDER , R., AND DENG, L.\nMS MARCO: A human generated machine reading comprehension dataset. CoRR abs/1611.09268\n(2016).\n[27] N IGAM , S. K., G OEL, N., AND BHATTACHARYA , A. nigam@coliee-22: Legal case retrieval and en-\ntailment using cascading of lexical and semantic-based models. In New Frontiers in Artificial Intelli-\ngence (Cham, 2023), Y . Takama, K. Y ada, K. Satoh, and S. Arai, Eds., Springer Nature Switzerland,\npp. 96–108.\n87\n[28] P ENNINGTON , J., S OCHER , R., AND MANNING , C. GloVe: Global vectors for word representation.\nInProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) (Doha, Qatar, 2014), Association for Computational Linguistics, pp. 1532–1543.\n[29] R AFFEL , C., S HAZEER , N., R OBERTS , A., L EE, K., N ARANG , S., M ATENA , M., Z HOU, Y., L I, W.,\nANDLIU, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal\nof Machine Learning Research 21 , 140 (2020), 1–67.\n[30] R EAL, L., F ONSECA , E., AND OLIVEIRA , H. G. The assin 2 shared task: a quick overview. In Inter-\nnational Conference on Computational Processing of the Portuguese Language (2020), Springer,\npp. 406–412.\n[31] R EIMERS , N., AND GUREVYCH , I. Sentence-BERT: Sentence Embeddings using Siamese BERT -\nNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing (2019), ACL.\n[32] R EIMERS , N., AND GUREVYCH , I. Making monolingual sentence embeddings multilingual using\nknowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (2020), ACL.\n[33] R EIMERS , N., AND GUREVYCH , I. Making monolingual sentence embeddings multilingual using\nknowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (2020), Association for Computational Linguistics.\n[34] R OBERTSON , S., AND ZARAGOZA , H. The Probabilistic Relevance Framework: BM25 and Beyond.\nFound. Trends Inf. Retr. 3 , 4 (2009), 333–389.\n[35] R ODRIGUES , J., G OMES , L., S ILVA, J., B RANCO , A., S ANTOS , R., C ARDOSO , H. L., AND OS´ORIO ,\nT. Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*, 2023.\n[36] R ONG, X. Word2Vec Parameter Learning Explained. CoRR abs/1411.2738 (2014).\n[37] S OUZA , F., N OGUEIRA , R., AND LOTUFO , R. BERTimbau: Pretrained BERT Models for Brazilian\nPortuguese. In Intelligent Systems (Cham, 2020), R. Cerri and R. C. Prati, Eds., Springer Interna-\ntional Publishing, pp. 403–417.\n[38] S OUZA , F., N OGUEIRA , R. F., AND DE ALENCAR LOTUFO , R. Portuguese named entity recognition\nusing BERT -CRF. CoRR abs/1909.10649 (2019).\n[39] T HAKUR , N., R EIMERS , N., R ¨UCKL ´E, A., S RIVASTAVA , A., AND GUREVYCH , I. BEIR: A heteroge-\nneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021).\n88\n[40] V ASWANI , A., S HAZEER , N., P ARMAR , N., U SZKOREIT , J., J ONES , L., G OMEZ , A. N., K AISER ,\nL.U.,ANDPOLOSUKHIN , I. Attention is all you need. In Advances in Neural Information Processing\nSystems (2017), vol. 30, Curran Associates, Inc.\n[41] V IRTANEN , P., G OMMERS , R., O LIPHANT , T. E., H ABERLAND , M., R EDDY , T., C OURNAPEAU , D.,\nBUROVSKI , E., P ETERSON , P., W ECKESSER , W., B RIGHT , J., VAN DER WALT, S. J., B RETT , M.,\nWILSON , J., M ILLMAN , K. J., M AYOROV , N., N ELSON , A. R. J., J ONES , E., K ERN, R., L ARSON ,\nE., C AREY , C. J., P OLAT,˙I., F ENG, Y., M OORE , E. W., V ANDER PLAS, J., L AXALDE , D., P ERK-\nTOLD , J., C IMRMAN , R., H ENRIKSEN , I., Q UINTERO , E. A., H ARRIS , C. R., A RCHIBALD , A. M.,\nRIBEIRO , A. H., P EDREGOSA , F., VANMULBREGT , P., AND SCIPY1.0 C ONTRIBUTORS . SciPy 1.0:\nFundamental Algorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261–272.\n[42] W ANG, K., R EIMERS , N., AND GUREVYCH , I. TSDAE: Using transformer-based sequential denois-\ning auto-encoderfor unsupervised sentence embedding learning. In Findings of the ACL: EMNLP\n2021 (Punta Cana, Dominican Republic, 2021), ACL, pp. 671–688.\n[43] W ANG, K., T HAKUR , N., R EIMERS , N., AND GUREVYCH , I. GPL: Generative pseudo labeling for\nunsupervised domain adaptation of dense retrieval. In North American Chapter of the ACL (2021).\n[44] W ILLIAMS , A., N ANGIA , N., AND BOWMAN , S. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of the 2018 Conference of the North American\nChapter of the ACL: Human Language Technologies, Vol 1 (New Orleans, Louisiana, 2018), ACL,\npp. 1112–1122.\n89\n90\n91"
  }
}