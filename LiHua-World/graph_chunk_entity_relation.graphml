<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d6" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d5" for="edge" attr.name="keywords" attr.type="string"/>
<key id="d4" for="edge" attr.name="description" attr.type="string"/>
<key id="d3" for="edge" attr.name="weight" attr.type="double"/>
<key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Supremo Tribunal de Justiça is a judicial organization responsible for significant legal decisions and oversight in its jurisdiction."&lt;SEP&gt;"The Supremo Tribunal de Justiça is a legal institution responsible for judicial decisions and oversight in Portugal."&lt;SEP&gt;"Supremo Tribunal de Justiça is a location or institution related to legal proceedings and decisions."&lt;SEP&gt;"Supremo Tribunal de Justiça is the main organization discussed, focusing on using the Sistema de Busca Semântica for decision support."&lt;SEP&gt;"The Supremo Tribunal de Justiça (STJ) is a key legal entity for which the Semantic Search System was developed."&lt;SEP&gt;"The Supremo Tribunal de Justica (Supreme Court) is referenced as the tribunal that issued the document."&lt;SEP&gt;"The Supremo Tribunal de Justiça is a legal institution that might benefit from or use Legal-BERTimbau for legal searches and data retrieval systems."&lt;SEP&gt;"The Supremo Tribunal de Justiça is the Portuguese Supreme Court of Justice, where the Semantic Search System was developed to assist in decision-making processes."&lt;SEP&gt;"This is a location where potential applications of legal search systems might be relevant."&lt;SEP&gt;"The Supremo Tribunal de Justiça is the Portuguese Supreme Court of Justice, the main focus of the research and the organization that benefits from the developed Semantic Search System."&lt;SEP&gt;"The Supremo Tribunal de Justiça is the Portuguese Supreme Court of Justice, where the Semantic Search System is being developed to assist in decision-making processes."&lt;SEP&gt;"This is a legal institution where research could have an impact, mentioned as part of the thesis work."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86&lt;SEP&gt;chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-2b8710310791165f23bcc4f86fda6d82&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Rui Filipe Coimbra Pereira de Melo is a master's degree candidate who conducted research and development for the Semantic Search System."&lt;SEP&gt;"Rui Filipe Coimbra Pereira de Melo is the author of a semantic search system aimed at improving information retrieval within the context of legal institutions."&lt;SEP&gt;"Rui Filipe Coimbra Pereira de Melo is the author or researcher associated with developing a semantic search system for the Supremo Tribunal de Justiça."&lt;SEP&gt;"Rui Filipe Coimbra Pereira de Melo is the author of the thesis and has been working on developing a semantic search system for the Supreme Court of Justice."&lt;SEP&gt;"Rui Filipe Coimbra Pereira de Melo is the author of this thesis and the developer of the Semantic Search System."&lt;SEP&gt;"Rui Filipe Coimbra Pereira de Melo is the author of the thesis and a Master's degree candidate working on developing a Semantic Search System."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86&lt;SEP&gt;chunk-2b8710310791165f23bcc4f86fda6d82</data>
</node>
<node id="&quot;SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A Semantic Search System is a type of information retrieval system designed to understand and provide relevant results based on natural language queries."&lt;SEP&gt;"A search system utilizing semantic models, which performs worse than BM25 in certain metrics but shows better performance when using Legal-BERTimbau."&lt;SEP&gt;"The Semantic Search System involves implementing a Bi-Encoder to create embeddings and later search using cosine similarity."&lt;SEP&gt;"The Semantic Search System is a key component developed as part of the project, combining lexical and semantic capabilities."&lt;SEP&gt;"The Semantic Search System is a part of the research that aims to improve query expansion in legal searches."&lt;SEP&gt;"The Semantic Search System is a system developed for STJ that can suggest more accurately relevant documents than BM25, improving Discovery performance over 300% in some scenarios."&lt;SEP&gt;"The Semantic Search System is a tool developed by Rui Filipe Coimbra Pereira de Melo to improve information retrieval within legal institutions."&lt;SEP&gt;"The Semantic Search System is an innovative technology developed by Rui Filipe Coimbra Pereira de Melo to improve information retrieval within the judicial organization."&lt;SEP&gt;"The Semantic Search System is an information retrieval system designed to find relevant documents based on queries in Brazilian Portuguese, using Legal-BERTimbau and other models for embeddings and cosine similarity."&lt;SEP&gt;"A type of search system, including a multilingual model and Legal-BERTimbau, which perform worse than BM25 for certain metrics."&lt;SEP&gt;"The Semantic Search System is a system developed for retrieving relevant documents based on user queries."&lt;SEP&gt;"The Semantic Search System is designed to provide more accurate relevant document suggestions than traditional methods like BM25, with significant improvements in discovery performance."&lt;SEP&gt;"The Semantic Search System refers to various types of search architectures described in the document, focusing on semantic and lexical processing."&lt;SEP&gt;"The semantic search system is a key component that leverages the Legal-BERTimbau model for effective searching of legal documents."&lt;SEP&gt;"A general term for a type of search system used to find relevant documents. It includes both Legal-BERTimbau and V0 models that are fine-tuned on pre-existing datasets."&lt;SEP&gt;"A system developed to provide more accurate relevant documents in jurisprudence for judges."&lt;SEP&gt;"Semantic Search System is a solution that involves implementing a Bi-Encoder to create embeddings independently for semantic search."&lt;SEP&gt;"Semantic Search System is the system developed for performing semantic searches on legal documents, using BERTimbau large embeddings stored in an ElasticSearch index."&lt;SEP&gt;"The Semantic Search System refers to an information retrieval system that uses semantic analysis for searching and retrieving documents based on their meaning rather than just keyword matching."&lt;SEP&gt;"Semantic Search System refers to a specific system discussed in the text, dealing with various search architectures and data processing methods."&lt;SEP&gt;"Semantic Search System refers to a system designed for retrieving relevant documents based on semantic analysis and search queries."&lt;SEP&gt;"The Semantic Search System includes models fine-tuned on different datasets and performs differently based on the dataset used."&lt;SEP&gt;"The Semantic Search System is designed to accurately suggest relevant documents and improve decision-making processes in legal contexts."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-2b8710310791165f23bcc4f86fda6d82&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac&lt;SEP&gt;chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-23944cdf583b2c2b5fcf537fea3f8421&lt;SEP&gt;chunk-486e9fdbc67025b64b42032778600c9c&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;SUPREMO TRIBUNAL DE JUSTIÇA SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Supremo Tribunal de Justiça Semantic Search System is a tool developed to enhance information retrieval within this judicial institution."</data>
  <data key="d2">chunk-2b8710310791165f23bcc4f86fda6d82</data>
</node>
<node id="&quot;SUPREMO TRIBUNAL DE JUSTIÇ A&quot;">
  <data key="d2">chunk-2b8710310791165f23bcc4f86fda6d82</data>
  <data key="d1">"Rui Filipe Coimbra Pereira de Melo developed a semantic search system for this judicial institution, indicating an effort to enhance its operational efficiency."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SUPREMO TRIBUNAL DE JUSTIC¸A&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Supremo Tribunal de Justiça is the Portuguese Supreme Court of Justice, which this research aims to assist in its decision-making process."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Pedro Alexandre Simões dos Santos is a supervisor who guided Rui Filipe Coimbra Pereira de Melo throughout the research process."&lt;SEP&gt;"Prof. Pedro Alexandre Simões dos Santos is one of the supervisors who guided Rui Filipe Coimbra Pereira de Melo throughout his thesis work."&lt;SEP&gt;"Pedro Alexandre Simões dos Santos is one of the supervisors who guided Rui throughout the thesis and provided critical advice."&lt;SEP&gt;"Pedro Alexandre Simões dos Santos is one of the supervisors of Rui Filipe Coimbra Pereira de Melo, guiding him throughout his thesis work."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;PROF. JOÃO MIGUEL DE SOUSA DE ASSIS DIAS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"João Miguel De Sousa de Assis Dias is another supervisor of Rui Filipe Coimbra Pereira de Melo, providing guidance and support for the thesis."&lt;SEP&gt;"João Miguel de Sousa de Assis Dias is another supervisor involved in guiding and advising Rui Filipe Coimbra Pereira de Melo during his thesis work."&lt;SEP&gt;"Prof. João Miguel De Sousa de Assis Dias is also a supervisor for the thesis, offering guidance and advice to Rui Filipe Coimbra Pereira de Melo."&lt;SEP&gt;"João Miguel de Sousa de Assis Dias is another supervisor who supported Rui in his research."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;PROF. MARIA LUÍSA TORRES RIBEIRO MARQUES DA SILVA COHEUR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Maria Luísa Torres Ribeiro Marques da Silva Coheur is the chairperson of the examination committee for Rui Filipe Coimbra Pereira de Melo's thesis."&lt;SEP&gt;"Prof. Maria Luísa Torres Ribeiro Marques da Silva Coheur is the chairperson of the examination committee and has supervised parts of the thesis work."&lt;SEP&gt;"Maria Luísa Torres Ribeiro Marques da Silva Coheur is the chairperson of the examination committee, overseeing the process and providing direction."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;PROF. JOSÉ LUÍS BRINQUETE BORBINHA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"José Luís Brinquete Borbinha is a member of the examination committee who contributed to the evaluation process of the thesis."&lt;SEP&gt;"Prof. José Luís Brinquete Borbinha is a member of the examination committee, contributing to the supervision and evaluation process."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;PROJECT IRIS MEMBERS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Project IRIS members are individuals working on the project, contributing to developing the Semantic Search System.")&lt;SEP&gt;"Project IRIS members are involved in developing and supporting the Semantic Search System for STJ."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"The Project IRIS members are colleagues with whom Rui collaborated, contributing to his project experience."&lt;SEP&gt;"The Project IRIS members are the collaborative group that Rui Filipe Coimbra Pereira de Melo has worked with in developing the semantic search system."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;THESIS TO OBTAIN THE MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The event is Rui Filipe Coimbra Pereira de Melo obtaining his Master of Science Degree in Computer Science and Engineering through the development of a Semantic Search System."&lt;SEP&gt;"The thesis is a significant event in Rui Filipe Coimbra Pereira de Melo's academic career aimed at obtaining a Master of Science Degree in Computer Science and Engineering."&lt;SEP&gt;"This event refers to Rui Filipe Coimbra Pereira de Melo obtaining his Master's degree in Computer Science and Engineering."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;JUNE 2023&quot;">
  <data key="d0">"DATE"</data>
  <data key="d1">"June 2023 is the date when the thesis was submitted for examination."&lt;SEP&gt;"June 2023 marks the date of submission and defense of the thesis."&lt;SEP&gt;"The location here is June 2023, when the thesis was submitted or defended."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;LARGE LANGUAGE MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Large Language Models like BERT are used in the semantic search system for understanding natural language better."&lt;SEP&gt;"Large language models are advanced machine learning models used for tasks such as STS, providing the core functionality of the developed system."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;BERT&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERT (Bidirectional Encoder Representations from Transformers) is a large language model used to understand and process natural language."&lt;SEP&gt;"BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art pre-trained model for understanding and generating natural language."&lt;SEP&gt;"BERT (Bidirectional Encoder Representations from Transformers) is an existing language representation model that SBERT improves upon in terms of speed and efficiency."&lt;SEP&gt;"BERT is a bidirectionally trained language model that uses the Transformers architecture for pre-training and fine-tuning."&lt;SEP&gt;"BERT is a deep learning-based natural language processing model used for text classification and other NLP tasks."&lt;SEP&gt;"BERT is a language model developed by Google aimed at understanding context through bidirectional training and using the Transformers architecture."&lt;SEP&gt;"BERT is a pre-trained language model architecture used to train BERTimbau variants."&lt;SEP&gt;"BERT is a specific type of large language model used in the semantic search system."&lt;SEP&gt;"BERT is a transformer-based model known for its performance in natural language processing tasks such as description queries and title queries."&lt;SEP&gt;"BERT is the base model that SBERT improves, used as a reference in comparing its performance on tasks such as semantic similarity and natural language inference."&lt;SEP&gt;"BERT is a language model that aims to understand natural language more effectively using bidirectional training and specific techniques like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP)."&lt;SEP&gt;"BERT is a large language model used in the development of the Semantic Search System, enhancing its capabilities over traditional methods like BM25."&lt;SEP&gt;"BERT is a model proposed by Google AI Language in 2019 that aims to understand text based on its context and surroundings."&lt;SEP&gt;"BERT is a pre-trained language representation model that SBERT compares with in terms of performance."&lt;SEP&gt;"BERT refers to the BERT model used in natural language processing tasks."&lt;SEP&gt;"BERT is a state-of-the-art model for natural language understanding and processing."&lt;SEP&gt;"BERT is a language model that aims to understand the language effectively through bidirectional training, pre-training and fine-tuning phases."&lt;SEP&gt;"BERT is a pre-trained model used as the basis for SBERT, known for its token embeddings and siamese architecture."&lt;SEP&gt;"BERT is an NLP model known for its effectiveness in natural language understanding tasks."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86&lt;SEP&gt;chunk-813c546217d2864adf1fc0789841ad36&lt;SEP&gt;chunk-abe3791c0e495b4a72310f8a36c50056&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-b6270162d82d1fef624d494a11c5caca&lt;SEP&gt;chunk-4178cfa608054c267be41d058b830af4&lt;SEP&gt;chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;BM25&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BM25 is a model used for evaluation in search systems, showing specific performance metrics in both Search and Discovery metrics."&lt;SEP&gt;"BM25 is a search algorithm that outperforms the original Semantic Search System in terms of query source identification and metric performance."&lt;SEP&gt;"BM25 is a technique used as a baseline comparison for the Semantic Search System, which maintains similar query identification ability but performs less accurately in some scenarios."&lt;SEP&gt;"BM25 is a traditional Information Retrieval technique used as a baseline comparison to measure the performance of the Sistema de Pesquisa Semântica."&lt;SEP&gt;"BM25 is a traditional information retrieval technique that will be compared against the developed search system in terms of performance."&lt;SEP&gt;"BM25 is an older information retrieval methodology that the research compares and improves upon."&lt;SEP&gt;"BM25 is a ranking function used in information retrieval that scores relevance of documents based on keyword frequency, typically used as a baseline in the evaluation."&lt;SEP&gt;"BM25 is a search algorithm that outperforms the original Semantic Search System in terms of identifying query sources and maintaining performance metrics."&lt;SEP&gt;"BM25 is an information retrieval model utilized as part of the hybrid search system developed for the Portuguese Supreme Court."&lt;SEP&gt;"BM25 is the default search function used by Elasticsearch for document searching."&lt;SEP&gt;"BM25 is a search algorithm that outperforms the original Semantic Search System in the Search metric, and can identify query sources better than semantic systems."&lt;SEP&gt;"BM25 is a search algorithm used for ranking documents based on their relevance to a query."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"BM25 is a traditional information retrieval technique that will be compared with the developed language models in the search system evaluation."&lt;SEP&gt;"BM25 is a traditional information retrieval technique used as a baseline comparison with the Semantic Search System in performance evaluation."&lt;SEP&gt;"BM25 is used in the Lexical-First Search System to retrieve top results before cosine similarity evaluation, and in the Lexical + Semantic Search System as a pre-filtering step."&lt;SEP&gt;"BM25 is a search algorithm that outperforms the original Semantic Search System in the Search metric."&lt;SEP&gt;"BM25 is a search system architecture used for evaluation in Table 6.3 and Table 6.4, providing top-ranking performance metrics."&lt;SEP&gt;"BM25 is a technique used for lexical first search, ranking results based on term frequency and inverse document frequency."&lt;SEP&gt;"BM25 is a traditional information retrieval model used as a benchmark for comparison in the development of Semantic Search Systems."&lt;SEP&gt;"BM25 is an information retrieval technique used alongside BERT and cosine similarity to score segments based on semantic and syntactic similarity."&lt;SEP&gt;"BM25 is the default search algorithm used by Elasticsearch for searching through documents, but it can support other functions like Cosine Similarity."&lt;SEP&gt;"The BM25 algorithm was used as a baseline for comparison with the enhanced semantic search system."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-23944cdf583b2c2b5fcf537fea3f8421&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87&lt;SEP&gt;chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;SUPREMO TRIBUNAL DE JUSTIÇA PORTUGUÊS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Supremo Tribunal de Justiça português is a judicial body in Portugal that the System of Semantic Search assists with decision-making processes."&lt;SEP&gt;"The Supremo Tribunal de Justiça português is the Portuguese Supreme Court involved in decision-making processes."&lt;SEP&gt;"The Supremo Tribunal de Justiça português is the organization using the developed semantic search system."&lt;SEP&gt;"Supremo Tribunal de Justiça português is a legal institution involved in decision-making processes."&lt;SEP&gt;"The Supremo Tribunal de Justiça português is the object of a decision-making support system developed in this work."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;SISTEMA DE PESQUISA SEMÂNTICA&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Sistema de Pesquisa Semântica is a prototype developed to assist the Supremo Tribunal de Justiça português, utilizing advanced language models and hybrid search systems."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;LEGAL-BERTIMBAU&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A variant of BERTimbau specifically adapted for the Portuguese legal domain and used in various language models for STJ judgments summarization."&lt;SEP&gt;"An enhanced semantic search model that performs better than a Multilingual model but still shows reduced performance compared to BM25."&lt;SEP&gt;"Legal-BERTimbau is a large language model developed for the thesis to support legal information retrieval and semantic search systems."&lt;SEP&gt;"Legal-BERTimbau is a specifically trained variant of BERT used in the Sistema de Pesquisa Semântica for legal text processing."&lt;SEP&gt;"Legal-BERTimbau is a version of SBERT used for processing legal documents, specifically in the context of splitting large sentences into smaller units."&lt;SEP&gt;"Legal-BERTimbau is the final implementation of a SBERT model designed for finding similar passages in the legal domain. It was trained using BERTimbau as the base model and Portuguese legal documents."&lt;SEP&gt;"Legal-BERTimbau variants are language models developed in Portuguese, fine-tuned for STJ and surpassing state-of-the-art multilingual models on assin and assin2 datasets."&lt;SEP&gt;"Legal-BERTimbau is a model created for finding similar passages in legal documents, based on BERTimbau as its base model."&lt;SEP&gt;"Legal-BERTimbau is a model derived from research, used for semantic search systems and achieving specific ROUGE scores."&lt;SEP&gt;"Legal-BERTimbau is a specific model used in Semantic Search Systems that performs poorly compared to BM25."&lt;SEP&gt;"Legal-BERTimbau is a variant of BERT models specifically fine-tuned for the Portuguese legal domain and used in creating multiple language models for different tasks such as question-answering and semantic search."&lt;SEP&gt;"Legal-BERTimbau is a version of SBERT used for generating sentence embeddings, optimized for legal text processing."&lt;SEP&gt;"Legal-BERTimbau is an entity involved in developing models for semantic search and legal document retrieval."&lt;SEP&gt;"Legal-BERTimbau is a large language model designed for legal applications and plays a crucial role in the semantic search system developed as part of this thesis."&lt;SEP&gt;"Legal-BERTimbau is a multilingual model mentioned as performing worse than BM25 in the Search metric."&lt;SEP&gt;"Legal-BERTimbau is a specific version of BERTimbau tailored for legal documents used in creating sentence embeddings."&lt;SEP&gt;"Legal-BERTimbau is a specifically trained BERT variant used in the Semantic Search System to improve performance."&lt;SEP&gt;"Legal-BERTimbau is an AI model used in creating embeddings for legal documents and performing semantic searches."&lt;SEP&gt;"Legal-BERTimbau is an SBERT model created for finding similar passages in the Portuguese legal domain using transfer learning from BERTimbau. It has been fine-tuned on a large corpus of documents."&lt;SEP&gt;"Legal-BERTimbau is an adaptation of BERTimbau specifically designed for the legal domain, aiming to understand legal records better."&lt;SEP&gt;"Legal-BERTimbau is employed in the Lexical + Semantic Search System for verification of retrieved results based on semantic information retrieval."&lt;SEP&gt;"Legal-BERTimbau is a large language model developed for legal applications within this research work."&lt;SEP&gt;"Legal-BERTimbau is a model used in the Lexical + Semantic Search System for verifying which results should be retrieved."&lt;SEP&gt;"Legal-BERTimbau is a model utilized for generating sentence embeddings, hosted on Hugging Face Hub and part of the proposed solution architecture."&lt;SEP&gt;"Legal-BERTimbau is a specific implementation of BERT tailored for the Portuguese legal domain, created using transfer learning techniques to find similar passages."&lt;SEP&gt;"Legal-BERTimbau is a specific model for legal applications developed within Project IRIS to enhance the effectiveness of semantic search systems."&lt;SEP&gt;"Legal-BERTimbau is a specific multilingual model that outperforms other multilingual models in the Semantic Search System."&lt;SEP&gt;"Legal-BERTimbau is a variant of BERT fine-tuned for Portuguese legal domain, surpassing state-of-the-art multilingual models on STS tasks."&lt;SEP&gt;"Legal-BERTimbau is an organization or model developed for processing legal documents in Brazilian Portuguese."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6&lt;SEP&gt;chunk-813c546217d2864adf1fc0789841ad36&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-23944cdf583b2c2b5fcf537fea3f8421&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-486e9fdbc67025b64b42032778600c9c&lt;SEP&gt;chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A Semantic Search System was developed to assist the Supreme Court of Justice of Portugal in its decision-making process, utilizing BERT models and hybrid search systems."&lt;SEP&gt;"Sistema de Busca Semântica refers to a semantic search system developed for the Portuguese Supreme Court."&lt;SEP&gt;"The Sistema de Busca Semântica is a prototype developed to assist the Supremo Tribunal de Justiça português in its decision-making processes."&lt;SEP&gt;"A semantic search prototype for supporting the decision-making process at the Supremo Tribunal de Justiça português was developed in this work."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;PORTUGAL&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Portugal is the country where the Supremo Tribunal de Justiça operates and where the System of Semantic Search is developed to assist in legal processes."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;SUPREME COURT&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Supreme Court is another term used for the Supremo Tribunal de Justiça, indicating its high judicial status."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;PROCESS OF DECISION MAKING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The process of decision making at the Supreme Court involves using advanced search systems to assist in legal judgments and rulings."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;INTELIGÊNCIA ARTIFICIAL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Inteligência Artificial is mentioned as part of the technology stack utilized by the Sistema de Pesquisa Semântica for processing and understanding legal text."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;GLOVE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm developed by Stanford University researchers for representing words as vectors based on co-occurrence matrices."&lt;SEP&gt;"GloVe is a machine learning model used for word embedding, based on co-occurrence matrices and neural network architectures."&lt;SEP&gt;"GloVe is a method for learning word embeddings from co-occurrence statistics."&lt;SEP&gt;"GloVe is a model for learning word embeddings from raw text, contributing to the field of Natural Language Processing."&lt;SEP&gt;"GloVe is an unsupervised learning algorithm developed to represent words as vectors based on co-occurrence matrices."&lt;SEP&gt;"GloVe stands for Global Vectors for Word Representation, developed at Stanford University in 2014 by researchers including Tomas Mikolov."&lt;SEP&gt;"GloVe is a method for learning word embeddings, often used in Natural Language Processing tasks."&lt;SEP&gt;"GloVe is an unsupervised learning algorithm developed for representing words as vectors based on their co-occurrence in text data."&lt;SEP&gt;"Global Vectors for Word Representation (GloVe) is a method developed at Stanford University in 2014 that also uses unsupervised learning to generate word embeddings, focusing on co-occurrence statistics."&lt;SEP&gt;"GloVe is a technique for word representation used in natural language processing."&lt;SEP&gt;"GloVe is a word embedding model developed at Stanford, distinct from Word2Vec in its approach but similar in goal."&lt;SEP&gt;"GloVe is an unsupervised learning algorithm that creates vector representations for words based on their co-occurrence matrix, focusing on semantic relationships."&lt;SEP&gt;"GloVe is another word embedding technique discussed as part of neural networks for semantic information retrieval methods."&lt;SEP&gt;"GloVe is a method for learning word embeddings, focusing on co-occurrence probabilities and statistical transformations of text corpora."&lt;SEP&gt;"GloVe is a method used for word embedding, often part of the broader NLP toolkit."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b&lt;SEP&gt;chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Recurrent Neural Network (RNN) is described as a type of neural network used for processing sequential or time series data, where the input has some defined order."&lt;SEP&gt;"Recurrent Neural Network is a type of neural network that processes sequences of inputs, useful in various NLP applications."&lt;SEP&gt;"Recurrent Neural Network is a type of neural network used in sequence prediction and other tasks involving sequential data."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;LONG SHORT-TERM MEMORY (LSTM)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"LSTM refers to a variant of RNNs that are designed to handle long-term dependencies more effectively than standard RNNs."&lt;SEP&gt;"Long Short-Term Memory is an architecture within Recurrent Neural Networks designed to address the vanishing gradient problem, enabling better handling of long-term dependencies."&lt;SEP&gt;"Long Short-Term Memory is an architecture within the RNN family known for its ability to handle long-term dependencies effectively."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;TRANSFORMERS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A neural network structure suggested by a Google Team that introduces self-attention mechanisms for improved performance over LSTMs."&lt;SEP&gt;"Transformers are a model architecture used in sequence-to-sequence tasks and have been influential in advancing machine translation and text summarization."&lt;SEP&gt;"Transformers are a type of neural network architecture that has revolutionized the field of NLP, offering significant improvements over previous models like RNN and LSTM."&lt;SEP&gt;"Transformers are a model architecture used in NLP tasks, often involving attention mechanisms for efficient and effective processing."&lt;SEP&gt;"Transformers are a new neural network structure introduced by Google to address issues with RNNs and LSTMs, such as handling long-range dependencies and facilitating parallel computation."&lt;SEP&gt;"Transformers refer to a novel approach in machine learning, particularly in NLP, which has gained popularity for its efficiency and effectiveness."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;LEXRANK&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"LexRank is a graph-based method for ranking sentences in a document based on their similarity to other sentences, often used in text summarization tasks."&lt;SEP&gt;"LexRank is a technique for extractive summarization that uses PageRank to rank sentences based on their semantic similarity, contributing to the field of text summarization."&lt;SEP&gt;"LexRank is a method for extracting summaries from text by using a graph-based ranking algorithm similar to PageRank."&lt;SEP&gt;"LexRank is a text summarization technique based on PageRank, designed to identify important sentences in a document."&lt;SEP&gt;"LexRank is an unsupervised extractive summarization technique using a graph-based approach where sentence scores are based on eigenvector centrality, with cosine similarity serving as the adjacency matrix for sentences."&lt;SEP&gt;"LexRank is an algorithm for text summarization that uses PageRank to identify the most important sentences in a document."&lt;SEP&gt;"LexRank is an unsupervised extractive summarization technique that uses a graph-based approach for automatic text summarization. The score of each sentence is based on eigenvector centrality in its graph representation."&lt;SEP&gt;"LexRank is an algorithm used for text summarization, leveraging the PageRank technique to rank sentences based on their similarity."&lt;SEP&gt;"LexRank is mentioned without additional details, suggesting a text summarization method based on eigenvector centrality."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;BERT (BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERT is a pre-trained model developed by Google for natural language understanding and has been widely used in various downstream tasks such as question answering and text classification."&lt;SEP&gt;"BERT is an NLP model pre-trained using the masked language modeling task and next sentence prediction, achieving state-of-the-art performance across a wide range of tasks."&lt;SEP&gt;"BERT is an AI model proposed by Google AI Language, designed for natural language processing tasks and context-based word understanding."&lt;SEP&gt;"BERT is an AI model developed by Google AI Language, designed to understand context through bidirectional training and transformer architecture."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;SEMANTIC SEARCH TYPE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Semantic Search Type refers to the techniques or methods used in semantic search, contributing to information retrieval systems."&lt;SEP&gt;"Semantic Search refers to a type of search that aims to improve query understanding by embedding words and sentences into vector spaces."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;TEXT SUMMARIZATION&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Text Summarization is a field that focuses on reducing text documents while maintaining their original meaning. It involves creating an abstractive or extractive summary based on the document’s content."&lt;SEP&gt;"Text Summarization is a process where technologies like LexRank and BERT are applied to generate concise summaries of text documents."&lt;SEP&gt;"Text summarization is the process of reducing the number of sentences and words from a document while maintaining its original meaning, creating a shortened version that retains important information."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;STATE OF THE ART&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"State of the Art refers to the current best practices or leading-edge technologies in a specific field such as NLP."&lt;SEP&gt;"State of the Art refers to the current state or latest developments in a field such as Natural Language Processing."&lt;SEP&gt;"The State of the Art section discusses current advancements in NLP technologies."&lt;SEP&gt;"The State of the Art refers to the current cutting-edge methods in NLP, such as BERT, BERTimbau, etc."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;BERTIMBAU&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERTimbau is a BERT model, which serves as a comparison point for Albertina's performance and architecture."&lt;SEP&gt;"BERTimbau is a base model used as a foundation for developing Legal-BERTimbau variants, tailored to better understand the Portuguese legal domain."&lt;SEP&gt;"BERTimbau is a comparison model mentioned for its performance on STS tasks, with which Albertina PT-BR fails to match in some aspects."&lt;SEP&gt;"BERTimbau is a pre-trained language model used as the base for Legal-BERTimbau, specifically trained on a large Portuguese corpus named BrWaC."&lt;SEP&gt;"BERTimbau is an extension or variant of BERT designed for specific tasks in Brazilian Portuguese language processing."&lt;SEP&gt;"BERTimbau refers to a large BERT variant that has been fine-tuned for STS tasks, showing its importance in handling similar tasks as the models mentioned above."&lt;SEP&gt;"BERTimbau is a language model developed by ERTimbau and trained using MLM and NSP methods on the BrWaC corpus."&lt;SEP&gt;"BERTimbau is a model trained with BrWaC, a large Portuguese corpus, and later adapted for legal domain."&lt;SEP&gt;"BERTimbau is a specifically trained variant of BERT used in the development of the Sistema de Busca Semântica."&lt;SEP&gt;"BERTimbau is an adaptation of BERT specifically tailored for the Brazilian Portuguese language."&lt;SEP&gt;"BERTimbau is another BERT model that Albertina PT either outperforms or falls short of, depending on the task."&lt;SEP&gt;"BERTimbau is the base model used to create Legal-BERTimbau and was pre-trained with a large Portuguese corpus BrWaC."&lt;SEP&gt;"BERTimbau is a language model variant that uses pretraining methods similar to BERT but adapted for Portuguese language."&lt;SEP&gt;"BERTimbau is an adaptation of BERT for Portuguese, designed specifically for the language and tasks related to it."&lt;SEP&gt;"BERTimbau is the base model used to create Legal-BERTimbau, trained with a large Portuguese corpus, BrWaC."&lt;SEP&gt;"BERTimbau is a large language model trained with BrWaC, a Portuguese corpus, and later adapted for use in legal domains."&lt;SEP&gt;"BERTimbau is a model that Albertina PT-BR outperformed or underperformed depending on the task, being compared to it."&lt;SEP&gt;"BERTimbau is a pre-trained language model used as the base for Legal-BERTimbau, providing it with a large corpus of data to fine-tune on."&lt;SEP&gt;"BERTimbau is a variant or extension of BERT, focusing on specific domains like Portuguese consumer law."&lt;SEP&gt;"Bertimbau is the name of a language model variant that was pre-trained using specific techniques and methods."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6&lt;SEP&gt;chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-813c546217d2864adf1fc0789841ad36&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681&lt;SEP&gt;chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;DEEPER TEXT UNDERSTANDING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Deeper Text Understanding refers to enhancing the comprehension and analysis of text data, often through advanced NLP techniques and models."&lt;SEP&gt;"Deeper Text Understanding refers to the advancements in understanding text at a deeper level using technologies like BERT and Transformers."&lt;SEP&gt;"Deeper Text Understanding refers to the idea of enhancing text understanding capabilities in NLP models."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;LEGAL INFORMATION RETRIEVAL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Legal Information Retrieval is a specific application of NLP aimed at retrieving and understanding legal information."&lt;SEP&gt;"Legal Information Retrieval is an application of NLP for retrieving legal information from documents or databases."&lt;SEP&gt;"Legal Information Retrieval is the process of extracting relevant legal information from unstructured or semi-structured text sources."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;NLP APPLIED TO PORTUGUESE CONSUMER LAW&quot;">
  <data key="d0">"APPLICATION"</data>
  <data key="d1">"NLP Applied To Portuguese Consumer Law refers to the practical use of NLP in analyzing and processing Portuguese consumer law documents."&lt;SEP&gt;"NLP Applied To Portuguese Consumer Law refers to the use of NLP techniques in understanding and processing Portuguese consumer law texts."&lt;SEP&gt;"This application refers to using NLP techniques for understanding and processing consumer law documents, likely focusing on Brazilian Portuguese texts."&lt;SEP&gt;"This master's thesis explored applying NLP techniques specifically to Portuguese consumer law."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;ALBERTINA PT&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Albertina PT is a BERT model for European Portuguese (PT-PT) and Brazilian Portuguese (PT-BR), representing the state-of-the-art."&lt;SEP&gt;"Albertina PT is a BERT model for Portuguese (both European and Brazilian), developed by a partnership between FCUL and FEUP. It outperforms BERTimbau on certain STS tasks but falls short in others."&lt;SEP&gt;"Albertina PT is a company or organization mentioned in the document, likely related to semantic search systems."&lt;SEP&gt;"Albertina PT is a reference to an organization, possibly the primary subject or contributor of some text."&lt;SEP&gt;"Albertina PT is a state-of-the-art BERT model for European Portuguese (PT-PT) and Brazilian Portuguese (PT-BR), built on DeBERTa architecture with 24 layers and 900 million parameters."&lt;SEP&gt;"Albertina PT is a technology or application related to Portuguese language processing, possibly an NLP tool or model."&lt;SEP&gt;"Albertina PT is an organization involved in legal information retrieval tasks specifically tailored for the Brazilian Portuguese language."&lt;SEP&gt;"Albertina PT is a Portuguese language model that has been adapted for various legal and consumer law applications."&lt;SEP&gt;"Albertina PT is a brand new state-of-the-art BERT model developed for European Portuguese (PT-PT) and Brazilian Portuguese (PT-BR)."&lt;SEP&gt;"Albertina PT is a company or organization referenced as a point of departure in the text, possibly related to previous work or context."&lt;SEP&gt;"Albertina PT is a specific application or implementation of NLP designed for Portuguese text understanding."&lt;SEP&gt;"Albertina PT is a state-of-the-art model developed by João Rodrigues et al. for European and Brazilian Portuguese."&lt;SEP&gt;"Albertina PT is an organization referenced in the document, likely involved in some project or system."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;4.1 CONSTRAINTS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"4.1 Constraints refers to a specific section or event within the document that outlines certain limitations or conditions."</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;GENERATIVE PSEUDO LABELING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Generative Pseudo Labeling is a method for generating pseudo labels, discussed beginning on page 34."&lt;SEP&gt;"Generative Pseudo Labeling is a technique for generating pseudo-labels by training models to predict labels for unlabeled data, which can then be used for semi-supervised learning."&lt;SEP&gt;"Generative Pseudo Labeling is a technique used for labeling data by generating pseudo labels, often employed in semi-supervised learning scenarios."&lt;SEP&gt;"Generative Pseudo Labeling is a technique used for training models where pseudo labels are generated using another model, often a pre-trained one."&lt;SEP&gt;"Generative Pseudo Labeling is an event or technique described in the document, likely part of a larger system evaluation or research process."&lt;SEP&gt;"Generative Pseudo Labeling is an event related to the development of language models, focusing on pseudo labeling techniques."&lt;SEP&gt;"Generative Pseudo Labeling is an event or process described at the beginning of the document, possibly a method for data processing."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Multilingual Knowledge Distillation involves transferring knowledge from multilingual models to monolingual ones, often used in improving the performance of language models across different languages."&lt;SEP&gt;"Multilingual Knowledge Distillation is an event focused on knowledge distillation in multiple languages, aiming to improve model performance across different linguistic contexts."&lt;SEP&gt;"Multilingual Knowledge Distillation is another event or technique described, related to evaluating language models and systems."&lt;SEP&gt;"Multilingual Knowledge Distillation is another event or process mentioned in section 5.5, likely involving methods to improve language models across multiple languages."&lt;SEP&gt;"The Multilingual Knowledge Distillation technique was applied to model variants, aiding their adaptation and performance improvement."&lt;SEP&gt;"Multilingual Knowledge Distillation is a technique used to transfer knowledge from one model to another across different languages or tasks."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-427843b4c7ba44f1dcc8f571081e36ae&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;METADATA KNOWLEDGE DISTILLATION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A new technique called Metadata Knowledge Distillation was introduced in this work."&lt;SEP&gt;"Metadata Knowledge Distillation involves the process of transferring knowledge from existing models to new ones, leveraging metadata for better efficiency and effectiveness."&lt;SEP&gt;"Metadata Knowledge Distillation is a new technique introduced in this work for adapting large language models to Portuguese jurisprudence."&lt;SEP&gt;"Metadata Knowledge Distillation is a process where knowledge from large pre-trained models is transferred to smaller, fine-tuned models using metadata as a guide."&lt;SEP&gt;"Metadata Knowledge Distillation is a technique used to train models by adjusting embeddings based on centroids."&lt;SEP&gt;"Metadata Knowledge Distillation is a third event or technique mentioned in the document, part of the system evaluation process."&lt;SEP&gt;"Metadata Knowledge Distillation is a new technique introduced, potentially enhancing the performance of the Semantic Search System."&lt;SEP&gt;"Metadata Knowledge Distillation refers to a specific event or process discussed in section 5.6, probably related to improving model performance using metadata."&lt;SEP&gt;"Metadata Knowledge Distillation refers to the process of distilling or extracting relevant information from metadata for use in training or improving machine learning models."&lt;SEP&gt;"Metadata Knowledge Distillation is a process that involves transferring knowledge through metadata, detailed starting from page 64."&lt;SEP&gt;"Metadata Knowledge Distillation is a technique used to adjust embeddings based on the tags and centroids of similar documents, improving model performance."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9&lt;SEP&gt;chunk-86921910d0aba929bef9ae955431a4fa&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;OVERVIEW&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Overview is a summary or review event that provides an overall understanding of the system or methodology discussed in the document."&lt;SEP&gt;"Overview provides an overall summary of sections discussed, setting context for subsequent detailed evaluations and discussions."&lt;SEP&gt;"Overview is provided in section 5.7, summarizing the previous sections and likely setting up for further details."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;SYSTEM EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"System Evaluation encompasses both Language Model Evaluation and Search System Evaluation, providing insights into the effectiveness of the developed solutions."&lt;SEP&gt;"System Evaluation encompasses various testing phases to assess the performance of language models and search systems, focusing on specific tasks like domain adaptation and query generation."&lt;SEP&gt;"System Evaluation introduces a broader category or topic that encompasses multiple sub-events such as Language Model Evaluation and Search System Evaluation."&lt;SEP&gt;"System Evaluation is a critical task to assess the performance of the developed search system."&lt;SEP&gt;"System Evaluation starts from section 6, evaluating various aspects of language models and search systems, detailing different methodologies and results."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;LANGUAGE MODEL EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Language Model Evaluation involves assessing the effectiveness of different language models through semantic textual similarity tests and other domain-specific adaptations."&lt;SEP&gt;"Language Model Evaluation is a specific event within the larger category of system evaluation, focusing on evaluating language models in terms of domain adaptation and semantic textual similarity."&lt;SEP&gt;"Language Model Evaluation is the evaluation of different language models, specifically Legal-BERTimbau variants, for their suitability and performance in the Portuguese legal context."&lt;SEP&gt;"Language Model Evaluation is a specific event or process described in section 6.1, focusing on the evaluation of language models across different domains and semantic tasks."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;SEARCH SYSTEM EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Search System Evaluation includes automatic query generation and result analysis to measure the search system's performance."&lt;SEP&gt;"Search System Evaluation is an ongoing process involving the evaluation of different metrics and models to improve search systems, particularly focusing on the discovery metric."&lt;SEP&gt;"Search System Evaluation is a process to evaluate search metrics and models, highlighting key findings in the document."&lt;SEP&gt;"Search System Evaluation is another sub-event under the broader topic of system evaluation, involving automatic query generation and result analysis."&lt;SEP&gt;"Search System Evaluation refers to the process of evaluating the performance of a search system used in court settings, aimed at helping judges with their decision-making."&lt;SEP&gt;"Search System Evaluation is an ongoing process of evaluating the performance metrics of search systems."&lt;SEP&gt;"Search System Evaluation is another specific event or process mentioned under the System Evaluation, focusing on evaluating search systems and their results."&lt;SEP&gt;"Search System Evaluation is an ongoing process to assess various metrics and models of a search system."&lt;SEP&gt;"The search system evaluation involves assessing the effectiveness and performance of the developed search system in retrieving relevant information for judges."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-f66becb00b4d98284bacd25a49e26a3e&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;CONCLUSION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Conclusion is a summary of contributions, future work, and key takeaways from the research, providing insights for further advancements in the field."&lt;SEP&gt;"Conclusion provides a summary of contributions made in the research, setting the stage for future work and publications."&lt;SEP&gt;"Conclusion provides a summary of contributions in section 7.1 and outlines future work in sections 7.2 and beyond."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;68&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Page 68 refers to a section within the document discussing Language Model Evaluation and Domain Adaptation."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;69&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Page 69 refers to a section within the document discussing Semantic Textual Similarity evaluations."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;70&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Page 70 refers to a section within the document discussing Search System Evaluation and Automatic Query Generation."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;71&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Page 71 refers to a section within the document discussing the results of search system evaluations."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;81&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Page 81 refers to a section within the document summarizing contributions and future work."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;82&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Pages 82-83 refer to sections discussing contributions, publications, and future work."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;83&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Page 83 refers to a section within the document discussing architecture improvements for the system."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;WORD EMBEDDINGS&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Word Embeddings are described in a 3D vector space, indicating their use in representing words numerically."&lt;SEP&gt;"Word Embeddings refer to the process of converting words into numerical vectors in a high-dimensional space, used extensively for natural language processing tasks."&lt;SEP&gt;"Word embeddings are mathematical techniques representing words in multidimensional spaces based on their context. They allow verification of semantic similarity between words."&lt;SEP&gt;"Word embeddings are a mathematical technique representing words as vectors based on their context within sentences, allowing for understanding of semantic relationships between words."&lt;SEP&gt;"Word Embeddings represent each word in a multidimensional space, allowing for the verification of which words are similar based on proximity in this space."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;CBOW MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"CBOW model is explained with one-word context and multiple words context, highlighting its variability based on the surrounding words."&lt;SEP&gt;"The CBOW (Continuous Bag Of Words) model is a neural network architecture that predicts a word from a context of surrounding words. It's often used in word2vec embeddings."&lt;SEP&gt;"The CBOW model is a part of Word2Vec that uses context words to predict the target word."&lt;SEP&gt;"The Common Bag Of Words (CBOW) model in Word2Vec predicts a target word using its context by interpreting surroundings. It uses one-hot encoding for input and output words."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;SKIP-GRAM MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Skip-Gram model is detailed in the text, showing an alternative approach to word embedding."&lt;SEP&gt;"The Skip-Gram model aims to predict words given a target word by its context, focusing on global relationships rather than local ones."&lt;SEP&gt;"The Skip-Gram model is another neural network architecture for word2vec, which does the opposite of CBOW by predicting context words given a target word."&lt;SEP&gt;"The Skip-Gram model is another part of Word2Vec that generates context from a given word."&lt;SEP&gt;"The Skip-Gram model in Word2Vec is designed to predict the context of a given word, also utilizing neural networks and one-hot encoded vectors as inputs and outputs."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;RECURRENT NETWORK FULLY CONNECTED&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A Recurrent Network Fully Connected layer refers to a type of neural network structure used in sequence modeling tasks."&lt;SEP&gt;"This describes the architecture of a fully connected recurrent neural network, used for sequential data processing."&lt;SEP&gt;"Recurrent Network Fully Connected is mentioned but not expanded upon, suggesting a structure related to recurrent neural networks."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;RNN STRUCTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"RNN (Recurrent Neural Network) structure is a fundamental framework for processing sequential data, allowing information from previous time steps to influence the current step."&lt;SEP&gt;"The RNN structure is presented here as part of the architectural improvements discussed in this document."&lt;SEP&gt;"RNN Structure is described, implying its role in handling sequential data."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;LSTM STRUCTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"LSTM (Long Short-Term Memory) Structure is an improvement over RNNs that better retains long-term dependencies in sequences by using cell states and gates."&lt;SEP&gt;"Long Short-Term Memory (LSTM) architecture, an improvement over traditional RNNs for better handling long-term dependencies."&lt;SEP&gt;"LSTM Structure is outlined, indicating Long Short-Term Memory's importance for managing long-term dependencies."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;TRANSFORMER MODEL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Transformer Model refers to a neural network architecture used for tasks such as translation and understanding natural language, characterized by its use of multi-head attention and feed-forward layers."&lt;SEP&gt;"The Transformer model is a novel architecture for sequence-to-sequence tasks, particularly useful in natural language processing. It relies on self-attention mechanisms to process inputs efficiently without the need for recurrent layers."&lt;SEP&gt;"The Transformer Model refers to a specific model architecture used in natural language processing, characterized by its use of attention mechanisms and positional embeddings."&lt;SEP&gt;"The Transformer model is introduced as a novel approach to sequential data processing using self-attention mechanisms."&lt;SEP&gt;"Transformer Model refers to the architecture described, which uses Positional Embedding and other layers like Multi-Head Attention and Feed-Forward Neural Network."&lt;SEP&gt;"The Transformer Model, as described, involves Positional Embedding, Input Embedding, Multi-Head Attention Layer, and Feed-Forward Neural Network components for processing input data."&lt;SEP&gt;"Transformer Model is introduced with a diagram, signifying its key role in processing sequential data efficiently."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;SCALED DOT-PRODUCT ATTENTION AND MULTI-HEAD ATTENTION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"These are key attention mechanisms in the Transformer model, enabling efficient and effective alignment of input features across different positions."&lt;SEP&gt;"These are key components of the Transformer model that allow it to weigh the importance of different words in a sentence, improving its ability to capture context."&lt;SEP&gt;"These concepts are discussed, emphasizing their importance in attention mechanisms within Transformer models."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;VECTOR SPACE WITH QUERY EMBEDDING AND MULTIPLE SENTENCE EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This describes the use of vector spaces for representing queries and sentences as points in high-dimensional space, facilitating similarity searches and information retrieval tasks."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;BI-ENCODER AND CROSS-ENCODER&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"These are frameworks used in natural language processing to encode pairs of documents or questions and answers. The Bi-Encoder uses two encoders for this task, while the Cross-Encoder is more focused on capturing the relationship between them."&lt;SEP&gt;"These are introduced as methods used in information retrieval and natural language understanding tasks, focusing on comparing sentence pairs."&lt;SEP&gt;"These encoders are introduced but not detailed, indicating their role in comparing pairs of texts or queries."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;BERT INPUT REPRESENTATION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"BERT (Bidirectional Encoder Representations from Transformers) uses specific tokenizations and input representations like segment embeddings and special tokens for handling various NLP tasks."&lt;SEP&gt;"The specific way BERT processes input, including tokenization and embedding techniques, crucial for its effectiveness in NLP tasks."&lt;SEP&gt;"This describes the specific way BERT processes inputs, including tokenization and embedding techniques."&lt;SEP&gt;"BERT's input representation is introduced but not explained in detail, indicating its importance for handling natural language inputs."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;DATA DISTRIBUTIONS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Data Distributions are referenced, suggesting an analysis or visualization of data distribution within a dataset."&lt;SEP&gt;"Data Distributions refer to the statistical properties of datasets, including how they are sampled and their relationship with larger target domains."&lt;SEP&gt;"Data distributions refer to the distribution of observable task information used for specific tasks."&lt;SEP&gt;"Describes the statistical distribution of data used in machine learning models, indicating how data is spread out or concentrated."&lt;SEP&gt;"These refer to the distributions of data used in various models or experiments, important for understanding model performance under different conditions."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;MASKED LANGUAGE MODELING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A technique where parts of a text are masked out and the model is trained to predict these missing words, enhancing general language understanding."&lt;SEP&gt;"Masked Language Modeling is a technique used in the Legal Language Model and Domain Adaptation process."&lt;SEP&gt;"Masked Language Modeling is a training objective for natural language processing tasks where the model predicts some of the words based on the context provided by other masked words."&lt;SEP&gt;"Masked Language Modeling is a technique used in natural language processing where parts of the input are masked and the model learns to predict them based on the context."&lt;SEP&gt;"Masked Language Modeling is a technique within Legal Language Model's domain adaptation process."&lt;SEP&gt;"Masked Language Modeling is mentioned, highlighting its use in training models to predict words from context."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"An architecture that uses transformers for denoising auto-encoders, focusing on sequential data cleaning and reconstruction."&lt;SEP&gt;"The TSDAE architecture combines autoencoder techniques with Transformer models to handle sequential data denoising tasks efficiently."&lt;SEP&gt;"This is an architecture that uses transformers for denoising auto-encoders, focusing on sequential data cleaning and reconstruction."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;T5 DIAGRAM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A diagram of T5 is referenced, indicating its role in text-to-text transformations and summarization tasks."&lt;SEP&gt;"T5 (Text-to-Text Transfer Transform) is a model that can be used for any text-based NLP task by treating all of them as translation problems, providing a unified architecture across different tasks."&lt;SEP&gt;"The T5 diagram represents a visual representation of the T5 model architecture, which is likely another type of transformer model used in various tasks."&lt;SEP&gt;"The T5 model's structure is illustrated here as part of the discussed improvements in sequence-to-sequence learning tasks."&lt;SEP&gt;"T5 diagram is a specific visual representation related to T5 model, a type of Transformer-based architecture used for training purposes."&lt;SEP&gt;"T5 diagram depicts the structure of T5, another model discussed beginning on page 32."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;GENQ&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A specific query generation technique or tool mentioned in the text."&lt;SEP&gt;"GenQ is a method for dense retrieval models developed by the Ubiquitous Knowledge Processing Lab team that aims to generate questions from given passages."&lt;SEP&gt;"GenQ is a method for dense retrieval models that aims to generate questions from given passages."&lt;SEP&gt;"GenQ is an unspecified technology or method possibly related to generation quality testing or evaluation."&lt;SEP&gt;"GenQ is mentioned in the context of query generation, which is part of the GPL technique's stages. It refers to a specific stage where queries are generated from document summaries."&lt;SEP&gt;"GenQ refers to a generative question generation framework, which creates questions from given contexts or answers."&lt;SEP&gt;"This term likely refers to a specific generation or question-answering system, though it is not fully defined in this context."&lt;SEP&gt;"GenQ refers to a generative pseudo labeling process mentioned in the text, which is part of the system's training and evaluation methods."&lt;SEP&gt;"GenQ stands for a method developed by the Ubiquitous Knowledge Processing Lab team in October 2021. It is an unsupervised domain adaptation method that generates queries from given passages, aiming to support semantic search and question-answering scenarios."&lt;SEP&gt;"GenQ is an unsupervised domain adaptation method for dense retrieval models, allowing the query generation from given passages. It was published by the Ubiquitous Knowledge Processing Lab team in October 2021."&lt;SEP&gt;"GenQ refers to a generative model or question generation system described starting from page 33."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208&lt;SEP&gt;chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;ALEX&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Alex is a character who acknowledges the need for a more sophisticated approach in dealing with an unknown intelligence."&lt;SEP&gt;"Alex is involved in developing the Semantic Search System and is part of Project IRIS members."&lt;SEP&gt;"Alex is involved in developing the Semantic Search System as part of Project IRIS."&lt;SEP&gt;"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task."&lt;SEP&gt;"Alex is the leader of a team working on developing a semantic search system for the Supremo Tribunal de Justiça português."&lt;SEP&gt;"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task."&lt;SEP&gt;"Alex is involved in developing a Semantic Search System and handling raw text data preprocessing tasks."&lt;SEP&gt;"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-90c5c9d551d02ddad6bd6b872760fc23&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;SAM RIVERA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Sam Rivera contributes to the development of Legal-BERTimbau and participates in discussions about first contact with unknown intelligence."&lt;SEP&gt;"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety."&lt;SEP&gt;"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety."&lt;SEP&gt;"Sam Rivera is a member of the team working on developing a semantic search system for the Supremo Tribunal de Justiça português."&lt;SEP&gt;"Sam Rivera is a member of the team working on the Semantic Search System, contributing to language models and pre-processing tasks."&lt;SEP&gt;"Sam Rivera is a member of the team showing youthful energy and awe regarding the communication with the unknown intelligence."&lt;SEP&gt;"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety."&lt;SEP&gt;"Sam Rivera is part of Project IRIS and contributes to the development of the Semantic Search System, showing youthful energy and a mix of awe and anxiety."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;MASKED LANGUAGE MODELING (MLM) TRAINING LOSS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Masked Language Modeling (MLM) Training Loss is a specific type of loss function used in training, detailed starting from page 57."&lt;SEP&gt;"Masked Language Modeling Training Loss is mentioned as part of the training process, indicating its importance in measuring performance during training."&lt;SEP&gt;"The Masked Language Modeling (MLM&lt;SEP&gt;"The Masked Language Modeling (MLM) Training Loss is a specific loss function used during the MLM training phase to measure how well the model predicts masked tokens in the input sequence."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;MODELS V1&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Models V1 are versions of a model used in Search System Evaluation, specifically mentioned for both Search metric and Discovery metric evaluations."&lt;SEP&gt;"Models V1 represent advanced or refined versions of models used in evaluations, showing progress and improvements."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;DISCOVERY METRIC&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Discovery metric evaluates the Semantic Search System’s capability to retrieve additional relevant documents that may not be directly related but are still valuable for the user’s needs."&lt;SEP&gt;"Discovery metric is a specific aspect evaluated within the Search System Evaluation, focusing on aspects related to discovery capabilities."&lt;SEP&gt;"The Discovery metric assesses the ability of a search system to uncover relevant information, often focusing on higher rank results like Top 1 and Top 10, indicating its potential in complex information retrieval tasks."&lt;SEP&gt;"The Discovery metric is an aspect of the evaluation process for search systems, indicating how well they can discover relevant information."&lt;SEP&gt;"The Discovery metric assesses the system’s ability to retrieve additional documents that might be relevant to the user, even if not from the original document used for creating the query."&lt;SEP&gt;"The Discovery metric assesses the ability of a search system to discover relevant information, particularly useful for finding key documents or insights."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"The Discovery metric evaluates how well search systems can recommend similar documents, with Lexical+Semantic approaches outperforming others."::</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-23944cdf583b2c2b5fcf537fea3f8421&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac&lt;SEP&gt;chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;MODELS V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Models V0 are earlier versions of a model used in Search System Evaluation, specifically mentioned in relation to the Discovery metric."&lt;SEP&gt;"Models V0 are early versions of models used for evaluation, indicating initial stages of development."&lt;SEP&gt;"Models V0 are earlier versions of a model used in search system evaluations."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;SBERT SPEARMAN CORRELATION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SBERT Spearman correlation is a specific evaluation metric used to measure the relationship between two variables in Search System Evaluation."&lt;SEP&gt;"SBERT Spearman correlation refers to a specific evaluation metric used in the context of search system evaluations."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;SENTENCE-BERT (SBERT)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Sentence-BERT (SBERT) is a model variant used for evaluating search systems, particularly on semantic textual similarity benchmarks."&lt;SEP&gt;"Sentence-BERT is a technology used for evaluating semantic textual similarity and is part of the broader context of search system evaluations."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;BERTIMBAU VARIANTS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERTimbau Variants are domain-specific adaptations of BERT models designed for legal documents and similar texts, performing well on specific tasks like STS evaluation."&lt;SEP&gt;"BERTimbau is a language model tailored for Portuguese, with different versions or 'variants' used for performance testing."&lt;SEP&gt;"BERTimbau is a variant of BERT adapted for the Portuguese language, used to perform better on STS tasks specific to Portuguese."&lt;SEP&gt;"BERTimbau variants are described as being adapted to the specific domain and language context, highlighting their relevance in understanding Portuguese language sentence similarities."&lt;SEP&gt;"BERTimbau variants are different versions or adaptations of the BERT model used in various evaluations and tasks."&lt;SEP&gt;"BERTimbau variants are different versions or modifications of BERT models used in evaluations."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;COLIEE 2021 - TASK 1 RESULTS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"COLIEE 2021 - Task 1 results refer to specific outcomes from a task within the competition on Legal Information Extraction and Entailment."&lt;SEP&gt;"COLIEE 2021 - Task 1 results represent specific outcomes from the COLIEE competition related to Task 1."&lt;SEP&gt;"The COLIEE 2021 - Task 1 results refer to the outcomes of a specific task within the competition on Legal Information Extraction and Entailment."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;COLIEE 2021 - TASK 3 RESULTS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"COLIEE 2021 - Task 3 results are specific outcomes from the COLIEE competition related to Task 3."&lt;SEP&gt;"The COLIEE 2021 - Task 3 results refer to another outcome of a specific task within the competition on Legal Information Extraction and Entailment."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;LEGAL-BERTIMBAU VARIANTS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Legal-BERTimbau variants are adaptations or versions of BERTimbau specifically tailored for legal document processing tasks."&lt;SEP&gt;"These are variations of a tailored language model aimed at improving information retrieval for court professionals in the Portuguese legal context."&lt;SEP&gt;"Legal-BERTimbau variants are different versions of language models tailored for the Portuguese legal context, developed to improve information retrieval in court scenarios."&lt;SEP&gt;"Legal-BERTimbau variants are legal-specific modifications or versions of BERT models used in evaluations."&lt;SEP&gt;"These are different versions of a language model tailored for the Portuguese legal context."&lt;SEP&gt;"Legal-BERTimbau variants are models designed for handling Portuguese legal text and were evaluated using MLM tasks."&lt;SEP&gt;"Legal-BERTimbau variants refer to different models developed for Portuguese legal context with improvements in STS task performance."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-f66becb00b4d98284bacd25a49e26a3e&lt;SEP&gt;chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;MLM AVERAGE LOSS FOR LEGAL DOCUMENTS IN THE TEST SET&quot;">
  <data key="d0">"METRIC"</data>
  <data key="d1">"MLM (Masked Language Modeling) average loss for legal documents in the test set is a metric used to evaluate model performance on legal text data."&lt;SEP&gt;"MLM average loss is a specific metric calculated for legal documents in the test set, indicating model performance."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;STS EVALUATION ON PORTUGUESE DATASETS&quot;">
  <data key="d0">"EVALUATION"</data>
  <data key="d1">"STS evaluation on Portuguese datasets is an assessment of models' ability to handle semantic textual similarity tasks in Portuguese language data."&lt;SEP&gt;"The STS (Semantic Textual Similarity) evaluation on Portuguese datasets measures how well models can understand and compare semantic similarities in Portuguese texts."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;SUPREMO TRIBUNAL DE JUSTIÇA (STJ)&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"STJ is Portugal's highest judiciary court and plays a crucial role in making well-informed, lawful, and ethical decisions that impact specific cases and future ones. It heavily relies on comprehensive jurisprudence for decision-making."&lt;SEP&gt;"STJ is a key location and organization involved in the IRIS project, focusing on court decisions."&lt;SEP&gt;"STJ refers to Portugal’s highest judiciary court, also known as the Supreme Court of Justice. It is crucial for making well-informed decisions."&lt;SEP&gt;"Supremo Tribunal de Justiça is Portugal’s highest judiciary court and plays a crucial role in making well-informed, lawful, and ethical decisions that impact specific cases and future cases."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;INESC-ID LISBOA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"INESC-ID Lisboa is an organization involved in the development of summarization approaches for court decisions as part of the IRIS project."&lt;SEP&gt;"INESC-ID Lisboa is involved in the IRIS project which aims to develop summarization approaches for court decisions and create a representation helpful in the court decision process."&lt;SEP&gt;"INESC-ID Lisboa is the organization behind the IRIS project, involved in developing summarization and semantic strategies."&lt;SEP&gt;"INESC-ID Lisboa is the organization developing the IRIS project and contributing to its goals."&lt;SEP&gt;"INESC-ID Lisboa is involved in the IRIS project aimed at developing summarization approaches for court decisions and creating a helpful representation for the court decision process."&lt;SEP&gt;"INESC-ID Lisboa is the organization responsible for developing the IRIS project."&lt;SEP&gt;"INESC-ID Lisboa is an organization that developed Project IRIS for STJ."&lt;SEP&gt;"INESC-ID Lisboa is the organization responsible for developing the IRIS project."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"NLI is a task in natural language processing where models are trained to determine the logical relationship between two given sentences.")&lt;SEP&gt;"Natural Language Inference is a concept where entailment labels are assigned to sentence pairs."&lt;SEP&gt;"Natural Language Inference is a task in NLP that involves determining whether a hypothesis sentence is true given the premise sentence."&lt;SEP&gt;"Natural Language Inference is a task that involves determining whether a hypothesis sentence can be entailed by a premise sentence."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;NLP&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"NLP stands for Natural Language Processing, which deals with the interaction between humans and computers using natural language."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;NL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"NL stands for Natural Language, a term used in NLP to refer to human languages processed by machines."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;NN&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"NN stands for Neural Network, a type of machine learning model that mimics the structure and function of neurons in the brain."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;NSP&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"NSP stands for Next Sentence Prediction, an NLP task where the model predicts the next sentence given the current one."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;QA&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"QA stands for Question and Answer, a type of NLP task where the system generates answers to questions based on input text."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;RNN&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"RNN refers to Recurrent Neural Networks, which are used for sequence prediction tasks and maintaining historical information."&lt;SEP&gt;"RNN stands for Recurrent Neural Network, a neural network model used in sequence data processing."&lt;SEP&gt;"Recurrent Neural Network (RNN) refers to a class of neural networks where connections between nodes form a directed graph along a temporal sequence. The architecture allows the network to maintain an internal state, which can be used to process sequences of inputs."&lt;SEP&gt;"Recurrent Neural Network (RNN) is a type of neural network used for processing sequential data."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;SBERT&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SBERT (Sentence-BERT) is a variant of BERT designed for sentence-level tasks, such as semantic search and similarity."&lt;SEP&gt;"SBERT is a library used for creating sentence embeddings and performing semantic textural similarity tasks."&lt;SEP&gt;"SBERT is a model designed for semantic similarity comparison and information retrieval, outperforming BERT in processing speed."&lt;SEP&gt;"SBERT is a model used for semantic similarity comparison and information retrieval, faster than BERT in processing sentence pairs."&lt;SEP&gt;"SBERT is a modification of BERT designed for sentence embeddings and semantic similarity."&lt;SEP&gt;"SBERT is a modification of BERT specifically tailored for semantic text similarity tasks, noted to be less effective on long texts."&lt;SEP&gt;"SBERT is a system designed for semantic similarity comparison and information retrieval, improving upon BERT in terms of speed and efficiency."&lt;SEP&gt;"SBERT is mentioned as a version of Legal-BERTimbau that is designed to receive meaningful sentences rather than isolated keywords."&lt;SEP&gt;"SBERT stands for Sentence-BERT, a model that uses BERT to perform sentence-level NLP tasks."&lt;SEP&gt;"SBERT is a sentence embedding generation method that uses siamese and triplet networks to create semantically meaningful embeddings."&lt;SEP&gt;"SBERT is a specific model used for evaluation, part of the Sentence-BERT family."&lt;SEP&gt;"SBERT is a variant of BERT specifically adapted for Portuguese, enhancing the system's ability to handle linguistic nuances in jurisprudence."&lt;SEP&gt;"SBERT is mentioned as an alternative approach that does not require certain pre-processing steps like tokenization and stop-words removal."&lt;SEP&gt;"SBERT refers to a model used for evaluating sentence embeddings on benchmarks like STS."&lt;SEP&gt;"SBERT is a modification of BERT that focuses on sentence embeddings, useful for tasks requiring understanding of sentence-level semantics."&lt;SEP&gt;"SBERT is a specific type of model or framework used in the evaluation process."&lt;SEP&gt;"SBERT is a method for semantic similarity comparison and information retrieval, faster than BERT in sentence pair comparisons."&lt;SEP&gt;"SBERT is an evaluation system used on the STS benchmark test set."&lt;SEP&gt;"SBERT, a modification of BERT, is used for embedding smaller text passages to improve efficiency on long texts."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6eeb6febf5ce46ec96655d84dc54cd2f&lt;SEP&gt;chunk-f66becb00b4d98284bacd25a49e26a3e&lt;SEP&gt;chunk-493aab369fd44529a838be55e938c506&lt;SEP&gt;chunk-b6270162d82d1fef624d494a11c5caca&lt;SEP&gt;chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-67e58f1a8dccff23d808e5fa663753db&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681&lt;SEP&gt;chunk-4178cfa608054c267be41d058b830af4&lt;SEP&gt;chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;SNLI&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SNLI stands for Stanford Natural Language Inference, a dataset used in NLP tasks."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;T5&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"T5 is a Transformer-based architecture that uses a text-to-text approach and is fine-tuned for question-answering tasks."&lt;SEP&gt;"T5 stands for Text-to-Text Transfer Transformer, an advanced model for text generation tasks."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;TF-IDF&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of words in documents, commonly applied in information retrieval and text mining."&lt;SEP&gt;"TF-IDF stands for Term Frequency-Inverse Document Frequency, a statistical measure used to evaluate the importance of a word in a document collection."&lt;SEP&gt;"Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate how important a word is to a document in a collection of documents."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;TF&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TF stands for Term Frequency, a metric that reflects how often a term appears in a text."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;TSDAE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"TSDAE (Transformer-based Sequential Denoising Auto-Encoder) is a method published by Nils Reimers that aims to improve domain knowledge through sentence embedding."&lt;SEP&gt;"TSDAE stands for Transformer-based Sequential Denoising Auto-Encoder, a model used for sequential data denoising and encoding tasks."&lt;SEP&gt;"TSDAE stands for Transformer-based Sequential Denoising Auto-Encoder, an unsupervised state-of-the-art sentence embedding method published on April 14th, 2021 by Nils Reimers. It aims to improve the domain knowledge of a model and is distinct from MLM in its approach to adding noise through word deletion or swapping."&lt;SEP&gt;"TSDAE stands for Transformer-based Sequential Denoising Auto-Encoder, an unsupervised state-of-the-art sentence embedding method published on April 14th, 2021 by Nils Reimers. It aims to improve the domain knowledge of a model and is distinct from MLM in its approach to adding noise through word deletion or swapping.")&lt;SEP&gt;"TSDAE is a Transformer-based Sequential Denoising Auto-Encoder method that aims to improve the domain knowledge of a model by introducing noise into sentences, similar to MLM but with different techniques. It was first published on April 14th, 2021."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;UKP&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"UKP stands for Ubiquitous Knowledge Processing, an organization focused on NLP and related technologies."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;IRIS PROJECT&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"IRIS is a project developed by INESC-ID Lisboa for STJ, aiming to develop summarization approaches and create browsable representations of court decisions."&lt;SEP&gt;"IRIS is a project developed by INESC-ID Lisboa for STJ, focusing on summarization approaches for court decisions."&lt;SEP&gt;"IRIS is an event or project aimed at developing summarization approaches and creating representations that assist in the court decision process."&lt;SEP&gt;"IRIS project is a contribution developed by INESC-ID Lisboa for STJ to develop summarization approaches for court decisions."&lt;SEP&gt;"IRIS project is a development initiative by INESC-ID Lisboa for STJ aimed at summarizing court decisions."&lt;SEP&gt;"The IRIS Project, developed by INESC-ID Lisboa, focuses on developing summarization techniques for court decisions to aid in the decision-making process of STJ."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;STJ&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"STJ likely refers to Superior Tribunal de Justiça (Superior Court of Justice in Brazil), where the developed search system might be applied or tested."&lt;SEP&gt;"STJ stands for Supremo Tribunal de Justiça and is the target legal entity for the Semantic Search System."&lt;SEP&gt;"STJ stands for Supremo Tribunal de Justiça, a legal institution where the project aims to aid with information retrieval and decision-making processes."&lt;SEP&gt;"STJ refers to Supremo Tribunal de Justica, which is a Portuguese legal entity benefiting from the developed search system."&lt;SEP&gt;"STJ refers to Supremo Tribunal de Justiça, a Portuguese legal institution involved in the IRIS project."&lt;SEP&gt;"STJ refers to a specific legal authority or institution where the developed search system might be implemented for judicial use."&lt;SEP&gt;"STJ stands for Supremo Tribunal de Justiça, Portugal's highest judiciary court that plays a crucial role in making well-informed and lawful decisions."&lt;SEP&gt;"STJ refers to Superior Tribunal de Justiça, a major court in Brazil involved in legal decision-making."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"STJ refers to Supremo Tribunal de Justiça, a Portuguese court system involved in the IRIS project."&lt;SEP&gt;"STJ refers to Supremo Tribunal de Justiça, an important legal entity in Portuguese legal system."&lt;SEP&gt;"STJ likely refers to Superior Tribunal de Justiça, a legal institution mentioned in the context of providing support for other related tasks."&lt;SEP&gt;"STJ stands for Supreme Tribunal de Justica, a judicial body involved in the IRIS project."&lt;SEP&gt;"STJ stands for Supremo Tribunal de Justiça, which the prototype of the Semantic Search System was designed to support."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;NATURAL LANGUAGE (NL)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Natural Language is a concept referring to human language in its natural state, used by the system for querying and information retrieval."&lt;SEP&gt;"Natural Language is mentioned as a key aspect of the system's resilience and search capability."&lt;SEP&gt;"Natural Language refers to the use of human language in system queries, highlighting its role in accessibility for judges and users."&lt;SEP&gt;"Natural Language refers to the system's capability to access information based on human language input, contrasting with specific terminology terms used by judges."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;SEMANTIC TEXTUAL SIMILARITY (STS)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"STS is a regression task used to evaluate how similar two text segments are on a numeric scale, ranging from 1 to 5."&lt;SEP&gt;"STS is a task that involves evaluating how similar two sentences are, used in model performance evaluation."&lt;SEP&gt;"Semantic Textual Similarity is a technology used in developing language models that can understand the meaning behind words and sentences."&lt;SEP&gt;"Semantic Textual Similarity models are developed for use in the project, indicating their importance in language processing tasks."&lt;SEP&gt;"Semantic Textual Similarity is a technology that measures the similarity between two texts based on their meaning."&lt;SEP&gt;"STS is a task that involves determining how similar two sentences are, often used for evaluating model performance."&lt;SEP&gt;"Semantic Textual Similarity is a type of language model developed for this thesis that helps in understanding and processing natural language inputs."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;EXPLORING EMBEDDINGS MODELS FOR PORTUGUESE SUPREME COURT JUDGMENTS SUMMARIZATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This event involves writing a paper on developing embeddings models specifically for summarizing Portuguese Supreme Court judgments."&lt;SEP&gt;"This event involves writing a paper on embedding models for summarizing Portuguese Supreme Court judgments, currently submitted for review."&lt;SEP&gt;"This paper explores embeddings models for summarizing Portuguese Supreme Court judgments."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;SEMANTIC SEARCH SYSTEM FOR SUPREMO TRIBUNAL DE JUSTIÇA&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A semantic search system was developed to support the court decision process at STJ, as documented in this paper."&lt;SEP&gt;"The development and submission of this system are part of the thesis contributions, aimed at aiding judges in the court decision process."&lt;SEP&gt;"The development of this system aims to aid judges in the decision-making process through advanced search functionalities."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;INFORMATION RETRIEVAL&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Information Retrieval describes the process and techniques used to develop the Semantic Search System for the Supremo Tribunal de Justiça."&lt;SEP&gt;"Information Retrieval is the process of extracting and organizing information from a collection of documents based on user queries."&lt;SEP&gt;"Information Retrieval refers to the process of searching for specific information within legal documents."&lt;SEP&gt;"Information Retrieval refers to the process of retrieving desired information from legal documents."&lt;SEP&gt;"Information Retrieval refers to the process of searching for information using various techniques, including lexical and semantic approaches."&lt;SEP&gt;"The work focuses on improving Information Retrieval methods, particularly through semantic approaches and hybrid systems."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86&lt;SEP&gt;chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;MACHINE LEARNING (ML)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Machine Learning is a subset of Artificial Intelligence focused on developing algorithms that enable machines to learn from data without being explicitly programmed."&lt;SEP&gt;"Machine Learning is used in developing solutions that can adapt and improve over time, particularly in natural language processing tasks."&lt;SEP&gt;"Machine Learning is an approach derived from Artificial Intelligence that offers solutions in various fields, acting as an alternative to traditional methods."&lt;SEP&gt;"Machine Learning is an alternative approach to traditional solutions, finding application in various fields and integrated with AI for information retrieval."&lt;SEP&gt;"Machine Learning is an alternative solution used in various fields by the IRIS project."&lt;SEP&gt;"Machine Learning is mentioned as an alternative to traditional solutions in multiple fields, especially in conjunction with AI."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;LEGAL DOCUMENTS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Legal documents are part of the collection used to create embeddings for evaluation purposes, indicating their importance in the domain-specific context."&lt;SEP&gt;"Legal documents are the primary sources of information for judges and researchers, containing critical data needed for legal decisions."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;STATE-OF-THE-ART TECHNIQUES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"State-of-the-art techniques refer to advanced methods and approaches currently leading in their field, such as those used for information retrieval."&lt;SEP&gt;"State-of-the-art techniques represent the most advanced methods in AI and ML currently available for developing search systems."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;HUGGINGFACE PLATFORM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"HuggingFace Platform is the platform where the variants of BERTimbau are hosted, using HuggingFace’s Transformers library."&lt;SEP&gt;"HuggingFace is a platform where developed language models and datasets were published, making them publicly available for use in other scenarios."&lt;SEP&gt;"HuggingFace is a platform where developed models, datasets, and language models are published and shared publicly."&lt;SEP&gt;"The HuggingFace platform is used to publish datasets and language models developed as part of the IRIS project."&lt;SEP&gt;"The HuggingFace platform hosts publicly available datasets and models developed by the thesis work."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;ARTIFICIAL INTELLIGENCE (AI)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Artificial Intelligence (AI) is a field that has increasingly played an important role in modern society and has seen the development of Machine Learning techniques."&lt;SEP&gt;"Artificial Intelligence is a field that has become increasingly important in modern society and has led to the development of Machine Learning (ML) as an alternative solution."&lt;SEP&gt;"Artificial Intelligence refers to a broad area of computer science that focuses on creating systems that can perform tasks requiring human-like intelligence such as learning, reasoning, and self-correction."&lt;SEP&gt;"Artificial Intelligence is a field that has become increasingly important in modern society, playing a key role in information retrieval and other aspects."&lt;SEP&gt;"Artificial Intelligence is a key component of the IRIS project, contributing significantly to modern society."&lt;SEP&gt;"Artificial Intelligence is highlighted as playing an increasingly important role in almost every aspect of modern society, particularly in areas like machine learning and semantic search."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;OKAPI BM25&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Okapi BM25 is an algorithm used in information retrieval for ranking and scoring documents based on relevance to the query, incorporating Term Frequency-Inverse Document Frequency (TF-IDF)."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TF-IDF is a ranking function that evaluates the relevance of terms in documents, using factors such as term frequency and inverse document frequency."&lt;SEP&gt;"TF-IDF is a statistical measure used to evaluate how relevant a term is in relation to a document and its overall corpus."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;LEXICAL SEARCH&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical search focuses on finding literal matches of words in documents for similarity."&lt;SEP&gt;"Lexical search refers to the process of searching for documents containing exact query words, typically using algorithms like Okapi BM25."&lt;SEP&gt;"Lexical search is a traditional technique used in Information Retrieval where documents are searched based on exact query words, with Okapi BM25 being a widely used algorithm for this process."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07&lt;SEP&gt;chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;ELASTICSEARCH&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"ElasticSearch is a distributed, open-source search and analytics engine used as the backbone of the Project IRIS solution."&lt;SEP&gt;"ElasticSearch is a specific type of technology used within the Semantic Search System for data processing and querying."&lt;SEP&gt;"ElasticSearch is mentioned as a storage system used for embedding collections of documents, showcasing its role in managing and querying data efficiently."&lt;SEP&gt;"ElasticSearch is used as an index to store embeddings of summaries, indicating its role in the negative mining stage."&lt;SEP&gt;"Elasticsearch is a distributed, open-source search and analytics engine built over Apache Lucene, used as a pre-defined constraint in Project IRIS."&lt;SEP&gt;"Elasticsearch is an open-source search engine that provides full-text search, analytics, and graph databases. It supports various ranking functions including BM25 for information retrieval tasks."&lt;SEP&gt;"Elasticsearch is described as a dense vector database where embeddings will be stored and indexed for fast retrieval."&lt;SEP&gt;"Elasticsearch is mentioned as one of the technologies to be implemented in the search system, though specific details are not provided."&lt;SEP&gt;"Elasticsearch is a dense vector database where embeddings are stored, allowing for scalable retrieval of results through cosine similarity searches."&lt;SEP&gt;"Elasticsearch is a technology that was considered for implementing the semantic search system but not used in the final implementation."&lt;SEP&gt;"Elasticsearch is used by both Purely Semantic Search System and Lexical-First Search System for document retrieval."&lt;SEP&gt;"ElasticSearch is a search engine based on the Lucene library, used in the context of the Semantic Search System for data indexing and searching."&lt;SEP&gt;"ElasticSearch is a search engine used for storing sentence embeddings generated from legal documents for querying purposes."&lt;SEP&gt;"Elasticsearch is used in the Purely Semantic Search System to search through embeddings and retrieve relevant results based on cosine similarity."&lt;SEP&gt;"ElasticSearch is a search engine technology used to store and query sentence embeddings generated from the 1000 legal documents."&lt;SEP&gt;"ElasticSearch is a technology mentioned under constraints of Semantic Search System, used for certain functionalities within the described systems."&lt;SEP&gt;"ElasticSearch is an open-source distributed search and analytics engine that provides scalability and resilience through its complex architecture, including nodes and shards. It was a pre-defined constraint for Project IRIS and used in the implementation of the Legal-BERTimbau model."&lt;SEP&gt;"Elasticsearch is a technology used in the Purely Semantic Search System for embedding and cosine similarity functions."&lt;SEP&gt;"Elasticsearch is mentioned but not detailed in the text. It's a technology that might be used within the search system."&lt;SEP&gt;"Elasticsearch is used as the dense vector database and allows scalability and fast retrieval of results through cosine similarity function."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-469ff73d10b043e9b3515dc8baa15494&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6&lt;SEP&gt;chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;DEVELOPED SOLUTION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Developed solution refers to the specific approach or system being presented in this chapter, likely incorporating state-of-the-art techniques."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;CLASSICAL APPROACHES&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Classical approaches refer to traditional methods of information retrieval that are based on lexical search."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;SEMANTIC SEARCH&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Semantic Search is an approach that aims to improve search quality by understanding query and sentence meanings through embeddings. It includes Symmetric Semantic Search and Asymmetric Semantic Search."&lt;SEP&gt;"Semantic search aims to improve search quality by understanding the underlying meaning and context of queries and sentences through vector representations in a shared space."&lt;SEP&gt;"Semantic search refers to an approach aimed at improving the quality of search by understanding the underlying meaning and context of queries and sentences through embeddings."&lt;SEP&gt;"Semantic search refers to the newer approach in information retrieval, focusing on understanding the meaning and context rather than just exact word matches."&lt;SEP&gt;"Semantic search focuses on understanding the contextual meaning of words instead of their literal presence, aiming to retrieve information based on intrinsic meaning."&lt;SEP&gt;"Semantic search is a method that aims to improve overall search quality by understanding the underlying query and sentence meaning through creating embeddings of words or documents into vector spaces."&lt;SEP&gt;"Semantic search aims to understand user intent and the overall context of a sentence, going beyond literal word matches to provide more meaningful results."&lt;SEP&gt;"Semantic search refers to more recent approaches in information retrieval that go beyond simple keyword matching, focusing on meaning and context."&lt;SEP&gt;"Semantic search is a method that searches for contextual meaning rather than specific word matches, aiming to understand user intention and sentence context."&lt;SEP&gt;"Semantic search is an approach aimed at improving overall search quality by understanding the underlying query and sentence meaning through embeddings."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-8dc9bc0243bc2842874f8fe5a91e0b07&lt;SEP&gt;chunk-469ff73d10b043e9b3515dc8baa15494&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;JACCARD SIMILARITY MEASURE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Jaccard Similarity is a metric to evaluate the shared content between two sets, defined as the size of their intersection divided by the size of their union."&lt;SEP&gt;"The Jaccard Similarity measure evaluates the shared information between two sets, specifically through their intersection and union."&lt;SEP&gt;"The Jaccard Similarity measure is a traditional method used to evaluate the shared information between two sets, defined as the ratio of the intersection over the union."&lt;SEP&gt;"The Jaccard Similarity measure evaluates the similarity between two sets by dividing their intersection's size by the union's size."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;SEMANTICS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Semantics refers to the contextual meaning behind words, focusing on user intention and overall sentence's meaning rather than exact word matches."&lt;SEP&gt;"Semantics refers to the study of meaning in language, which aims to understand user intention and context rather than just matching words literally."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;WORD EMBEDDING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Word embedding is a mathematical technique that represents words in a multidimensional space based on their context."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;DISTRIBUTIONAL HYPOTHESIS (DH)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The Distributional Hypothesis is a theory that states words appearing in similar contexts tend to have similar meanings, which underlies word embeddings."&lt;SEP&gt;"The Distributional Hypothesis suggests that words appearing in similar contexts have similar meanings, forming the basis for word embeddings."&lt;SEP&gt;"The Distributional Hypothesis suggests that words appearing in similar contexts tend to have similar meanings, forming the basis for word embeddings."&lt;SEP&gt;"The Distributional Hypothesis proposes that words appearing in similar contexts have similar meanings."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;COSINE SIMILARITY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Cosine Similarity is a measure to determine the cosine angle between two vector embeddings, used to calculate the similarity of word vectors in multidimensional space."&lt;SEP&gt;"Cosine Similarity is a measure used to determine how similar two documents are by calculating the cosine of the angle between their vector representations in high-dimensional space."&lt;SEP&gt;"Cosine Similarity is a measure used to determine how similar two non-zero vectors are, often calculated as the cosine of the angle between them."&lt;SEP&gt;"Cosine Similarity is another supported search function in Elasticsearch that requires specific index mappings and dense vector fields."&lt;SEP&gt;"Cosine Similarity is used to calculate the similarity between dense vector and query embedding without normalization."&lt;SEP&gt;"Cosine Similarity measures the angle between two vector embeddings to determine their similarity."&lt;SEP&gt;"Cosine similarity is a measure of similarity between two non-zero vectors and is used to calculate edge weights in the graph representation."&lt;SEP&gt;"Cosine similarity is a measure used in semantic search and information retrieval tasks to determine the cosine of the angle between two non-zero vectors projected onto a hyperplane."&lt;SEP&gt;"Cosine Similarity is a metric utilized by both the Purely Semantic and Lexical-First Search Systems for evaluating embedding similarities."&lt;SEP&gt;"Cosine similarity is a measure used to evaluate the similarity between sentences generated by different models during the evaluation process."&lt;SEP&gt;"Cosine Similarity is a method used in search systems to measure semantic similarities between embeddings."&lt;SEP&gt;"Cosine Similarity is a search function supported by Elasticsearch that requires initial mapping of indices and dense vectors to store embeddings. It is an ideal solution for semantic search systems implementation."&lt;SEP&gt;"Cosine Similarity measures the cosine of the angle between two vector embeddings, used for calculating distances in embedding spaces."&lt;SEP&gt;"Cosine similarity is a measure used to determine how similar two sentences are by comparing the angle between them in multi-dimensional space."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07&lt;SEP&gt;chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-abe3791c0e495b4a72310f8a36c50056&lt;SEP&gt;chunk-6eeb6febf5ce46ec96655d84dc54cd2f&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;WORD2VEC&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Word2Vec is a neural network implementation proposed by Tomas Mikolov et al. in 2013, designed for word embeddings."&lt;SEP&gt;"Word2Vec is a two-layer neural network that uses the Common Bag Of Words (CBOW) and Skip-Gram methods to predict words based on their context."&lt;SEP&gt;"Word2Vec is a two-layer neural network model that uses context words to predict the target word, focusing on co-occurring words to infer their meanings."&lt;SEP&gt;"Word2Vec is a two-layer neural network proposed by Tomas Mikolov et al. in 2013, which uses unsupervised training to create vector representations of words based on their context."&lt;SEP&gt;"Word2Vec is a neural network model for generating word embeddings, proposed by Tomas Mikolov and his team at Google in 2013, focusing on context-based meaning representation."&lt;SEP&gt;"Word2Vec is a two-layer neural network that uses the context of surrounding words to predict a corresponding word, based on methods such as CBOW and Skip-Gram."&lt;SEP&gt;"Word2Vec is a word embedding technique used in the context of neural networks for semantic information retrieval methods."&lt;SEP&gt;"Word2Vec is a two-layer neural network model proposed by Tomas Mikolov et al. in 2013, focusing on predicting words based on context."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-8dc9bc0243bc2842874f8fe5a91e0b07&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;PUBLIC GARDEN&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Public Garden is a location mentioned in the example sentence where someone walks through it."&lt;SEP&gt;"Public garden is a specific location mentioned where activities take place."&lt;SEP&gt;"Public garden refers to an open space in a public area where people can relax and enjoy nature. In this example, it's used as part of a sentence to illustrate lexical search limitations."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07&lt;SEP&gt;chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;BM25 FUNCTION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BM25 function is a scoring function used in information retrieval, optimizing based on k1 and b terms."&lt;SEP&gt;"BM25 function is an optimization technique used in information retrieval to score relevance of documents."&lt;SEP&gt;"BM25 is a scoring function used in information retrieval to assess the relevance of documents, taking into account term frequency and document length."&lt;SEP&gt;"BM25 is a ranking algorithm used in information retrieval, focusing on term frequency-inverse document frequency (TF-IDF) and other factors for optimizing search results."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;TOMAS MIKOLOV&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tomas Mikolov is a key figure associated with the development of Word2Vec and GloVe, contributing significantly to neural network-based word embeddings."&lt;SEP&gt;"Tomas Mikolov is an author associated with the development of Word2Vec."&lt;SEP&gt;"Tomas Mikolov is a key figure in the development of Word2Vec, leading the Google team that proposed it."&lt;SEP&gt;"Tomas Mikolov is the author of Word2Vec, contributing significantly to the field of natural language processing."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;STANFORD UNIVERSITY&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Stanford University developed Word2Vec and GloVe, focusing on creating models through unsupervised training using neural networks."&lt;SEP&gt;"Stanford University developed Word2Vec and GloVe, focusing on neural network-based solutions for natural language processing."&lt;SEP&gt;"Stanford University is an organization known for developing machine learning technologies, specifically neural network models like Word2Vec and GloVe."&lt;SEP&gt;"Stanford University is referenced as the source of an image related to RNNs."&lt;SEP&gt;"Stanford University developed Word2Vec and GloVe, indicating its significant role in the field of neural network models."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;COMMON BAG OF WORDS (CBOW)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The CBOW method predicts a word using its surrounding context, as demonstrated in an example sentence: 'I walked through the public garden.'"</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;SKIP-GRAM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Skip-Gram is another method within Word2Vec where a single word is used as input and generates context words, focusing on local information."&lt;SEP&gt;"The Skip-Gram method generates the context of a given word based on inputting the word into a neural network."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;TOMAS MIKOLOV ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tomas Mikolov and his team are the creators of Word2Vec, proposing it in 2013 with a focus on context-based word prediction."&lt;SEP&gt;"Tomas Mikolov et al. proposed Word2Vec in 2013."&lt;SEP&gt;"Tomas Mikolov et al. proposed Word2Vec in 2013."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;CO-OCCURRENCE MATRIX&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Co-occurrence matrix refers to the probability of a word appearing next to another word, which is central to GloVe's model."&lt;SEP&gt;"Co-occurrence matrix represents the probability of words appearing in context with each other, used to define GloVe's loss function."&lt;SEP&gt;"The co-occurrence matrix is a key component of GloVe, used to derive semantic relationships between words."&lt;SEP&gt;"Co-occurrence matrix represents the probability of words appearing in context with each other, used as a basis for GloVe's loss function."&lt;SEP&gt;"Co-occurrence matrix refers to the probability of one word appearing in the context of another, used as a basis for GloVe's vector learning process."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;NEURAL NETWORK&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Neural networks are the foundation for both Word2Vec and GloVe, providing unsupervised learning methods for natural language processing tasks."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;PROBABILITY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Probability is used to measure the co-occurrence frequency between words in the context of another word, essential for GloVe’s operations."&lt;SEP&gt;"Probability refers to the measure of how likely it is for one event to occur relative to others. In this context, it's about the likelihood of words co-occurring."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;F(XIJ)&quot;">
  <data key="d0">"FUNCTION"</data>
  <data key="d1">"f(Xij) represents a weighting function applied to co-occurrence counts, determining the relative importance of co-occurrences."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;WTI˜WJ&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"WTi˜Wj is the dot product of input vectors representing words in GloVe’s model."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;BIAS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Bias (bi+˜bj) aims to mitigate common and stop-words' impact, ensuring that word embeddings reflect more meaningful relationships."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;VOCABULARY SIZE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"V represents the size of the vocabulary in GloVe’s model, determining the dimensionality of word vectors."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;WORD ICE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"In this context, 'ice' is used as a specific example of a word in the co-occurrence matrix. It does not refer to a person but rather a concept."&lt;SEP&gt;"Word 'ice' refers to a specific word in the context of co-occurrence analysis and GloVe's operations."&lt;SEP&gt;"Word 'ice' is a key concept in the GloVe's co-occurrence matrix, showing its frequency of co-occurrence with various words."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;WORD STEAM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Similarly, 'steam' is used as an example of another word in the co-occurrence matrix and does not refer to a specific person."&lt;SEP&gt;"Word 'steam' refers to another specific word used for comparison with 'ice' in GloVe’s model."&lt;SEP&gt;"Word 'steam' is another key concept in the GloVe's co-occurrence matrix, demonstrating its relationship to other concepts like gas and water."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;SOLID&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Solid is a concept co-occurring more frequently with the word ice than with steam, indicating their semantic relationship."&lt;SEP&gt;"Solid is a concept compared against ice in terms of co-occurrence frequency with the word 'ice' in the context of GloVe’s learning process."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;GAS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Gas is a concept co-occurring less frequently with the word ice compared to solid, showing a different semantic relation."&lt;SEP&gt;"Gas is another concept used for comparison, showing lower co-occurrence with 'ice' than 'solid'."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;WATER&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Water is a related concept to both ice and steam, indicating shared semantic relationships in the context of GloVe’s learning process."&lt;SEP&gt;"Water is related to both ice and steam but not strongly with fashion, indicating broader thematic connections."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;FASHION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Fashion is a concept that does not show strong co-occurrence with either ice or steam, highlighting distinct semantic spaces."&lt;SEP&gt;"Fashion serves as an unrelated comparison, showing minimal co-occurrence with words like 'ice' or 'steam'."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;HIDDEN VECTOR A&lt;T&gt;&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The hidden vector \(a_t\) is crucial for RNN, storing information from previous iterations to influence current states."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;ACTIVATION FUNCTION G1&quot;">
  <data key="d0">"FUNCTION"</data>
  <data key="d1">"Activation functions like Sigmoid, Tanh, and ReLU are used in the RNN model to introduce non-linearity."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;LOGISTIC FUNCTION (SIGMOID)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The logistic function is an activation function that squashes input values into a range between 0 and 1."&lt;SEP&gt;"The logistic function or Sigmoid is mentioned as an activation function used in RNNs."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;HYPERBOLIC TANGENT (TANH)&quot;">
  <data key="d0">"FUNCTION"</data>
  <data key="d1">"Hyperbolic tangent is another activation function used in RNN, mapping inputs to the range [-1, 1]."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;RECTIFIED LINEAR UNIT (RELU)&quot;">
  <data key="d0">"FUNCTION"</data>
  <data key="d1">"ReLU is an activation function that introduces non-linearity by outputting the input directly if it is positive."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;BACKPROPAGATION THROUGH TIME&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Backpropagation Through Time is a training technique used in RNNs to handle the issues of vanishing or exploding gradients over time."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;GRADIENT CLIPPING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Gradient clipping is a method to handle the exploding gradient phenomena in RNNs and LSTMs."&lt;SEP&gt;"Gradient clipping is a method used in Backpropagation Through Time to prevent the gradient from becoming too large, thus avoiding the exploding gradient problem."&lt;SEP&gt;"Gradient clipping is a method to prevent exploding gradients in training neural networks."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;CELL STATE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The Cell State is a component of LSTM networks that stores information over time."&lt;SEP&gt;"The Cell State is a component that stores information over time and is updated by the Forget and Input Gates." &lt;|"information storage, update mechanism"&lt;SEP&gt;"The cell state is a key component in LSTMs that allows for the preservation of information over time."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f&lt;SEP&gt;chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;FORGET GATE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Forget Gate is a layer in the LSTM network responsible for selecting which information should stay or not in the cell state."&lt;SEP&gt;"Forget Gate is a layer in the Long Short-Term Memory (LSTM) network responsible for deciding which information to keep from the cell state."&lt;SEP&gt;"Forget Gate is a type of gate in the Long Short-Term Memory network responsible for deciding which information should be kept or discarded from the cell state."&lt;SEP&gt;"The forget gate in an LSTM is responsible for deciding which information to keep or discard from the cell state."&lt;SEP&gt;"The Forget Gate is a layer responsible for selecting which information stays in the cell state using a sigmoid function."&lt;SEP&gt;"The forget gate is a component in LSTM that decides what information to discard from the cell state."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;INPUT GATE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Input Gate is a layer in the LSTM network that determines what values should be updated and stored in the cell state, working with a tanh layer to create a new candidate vector."&lt;SEP&gt;"The Input Gate aims to determine which values will be updated, working with a tanh layer to store new information in the cell state. It uses a sigmoid function for its output."&lt;SEP&gt;"The input gate in an LSTM controls the flow of new information into the cell state and updates it appropriately."&lt;SEP&gt;"The input gate in LSTM adds new information to the cell state."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;OUTPUT GATE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Output Gate is responsible for deciding what information will be outputted from the LSTM model and passed to the next cell, often combined with a tanh layer to produce the hidden state."&lt;SEP&gt;"The Output Gate decides what is outputted and combines with a tanh layer to pass information to the next cell, also using a sigmoid function."&lt;SEP&gt;"The output gate in an LSTM determines what part of the cell state is visible as the next prediction or output."&lt;SEP&gt;"The output gate in LSTM determines which part of the cell state will be outputted as the hidden state."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;MULTI-HEAD ATTENTION LAYER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A sub-layer in each of the six identical layers of the encoder, it facilitates long-range dependencies using self-attention mechanism."&lt;SEP&gt;"Multi-Head Attention is an architectural component in the Transformer model that uses multiple attention mechanisms to generate weighted vectors for each word based on different key-value pairs."&lt;SEP&gt;"The Multi-Head Attention Layer is a sub-layer in the Transformer model used to facilitate long-range dependencies and improve the network's ability to understand context."&lt;SEP&gt;"The Multi-Head Attention Layer is part of the Transformer Model that uses an attention mechanism to assign different weights to words based on their context within a sentence."&lt;SEP&gt;"Multi-Head Attention Layer is a component in the Transformer Model used for assigning different weights to words based on their importance in a sentence."&lt;SEP&gt;"The Multi-Head Attention Layer is a key component in Transformers that allows the model to focus on different positions of the input sequence."&lt;SEP&gt;"The Multi-Head Attention Layer is part of the Transformer Model that uses an attention mechanism to assign different weights to vectors representing words in a sentence. These vectors capture the importance between words and are generated by the position and query keys."&lt;SEP&gt;"The Multi-Head Attention Layer is part of the Transformer model, designed to handle self-attention mechanisms for long-range dependencies."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;INPUT EMBEDDING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Input Embedding transforms inputs into scalar vectors to map words with similar meanings close in space, addressing issues related to word context and position."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;SIGMA&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The sigmoid function σ is used to process information in LSTM gates, such as Forget Gate, Input Gate, and Output Gate."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;POSITIONAL ENCODING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Positional Encoding is a method that provides context based on the position of words in a sentence. It uses sine and cosine functions with different frequencies to encode positions."&lt;SEP&gt;"Positional Encoding is a method that provides context to words based on their position in a sentence."&lt;SEP&gt;"Positional Encoding is a method that provides context to words based on their position in the sentence, using sine and cosine functions with different frequencies."&lt;SEP&gt;"Positional Encoding is a technique that provides context based on the position of words in a sentence, using sine and cosine functions with different frequencies."&lt;SEP&gt;"Positional Encoding provides context to words based on their position in a sentence, helping the model understand the order of inputs."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;FEED-FORWARD NEURAL NETWORK&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A Feed-Forward Neural Network processes data through layers without any feedback connections, used in Transformers alongside attention mechanisms."&lt;SEP&gt;"A feed-forward neural network is a type of artificial neural network where information moves in only one direction, forward. It is used as part of the RNN architecture."&lt;SEP&gt;"The Feed-Forward Neural Network is a component of the Transformer architecture, designed to process and transform information after the multi-head attention mechanism."&lt;SEP&gt;"The Feed-Forward Neural Network is another component of the Transformer Model, used in conjunction with the Multi-Head Attention Layer to process and transform input data."&lt;SEP&gt;"This sub-layer in each of the encoder's six identical layers helps produce outputs with 512 dimensions and is part of the Transformer model."&lt;SEP&gt;"Feed-Forward Neural Network is another component of the Transformer Model that processes information through multiple layers of neurons."&lt;SEP&gt;"The Feed-Forward Neural Network is another component of the Transformer Model that processes input data along with Positional Embedding and Input Embedding, contributing to the overall model architecture."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f&lt;SEP&gt;chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;MASKED MULTI-HEAD ATTENTION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Masked Multi-Head Attention is a variant of the attention mechanism used in the decoder block, which ensures that predictions are based on past information only by masking future positions."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;SINE AND COSINE FUNCTIONS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Sine and Cosine Functions are mathematical functions used to generate the Positional Encoding vectors, providing positional information for words in a sentence."&lt;SEP&gt;"Sine and Cosine functions are mathematical techniques used in Positional Encoding to represent positional context within a sentence."&lt;SEP&gt;"Sine and Cosine functions are used with different frequencies to create Positional Embedding vectors."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;QUERY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A query is a search term or phrase used to retrieve information from a database or knowledge base."&lt;SEP&gt;"A query is a set of words or phrases that users input into a search engine or a model, which the system aims to understand and respond to appropriately."&lt;SEP&gt;"Queries are the search terms or keywords used to retrieve relevant documents from a set of indexed documents."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;KEY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A key in the context of Multi-Head Attention refers to a vector derived from the input word, used to determine its relevance relative to other words."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;VALUE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A value in the context of Multi-Head Attention refers to a vector that holds information related to the importance and meaning of each word in the sentence."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;INPUT SEQUENCE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Input Sequence is the order of words or tokens passed into the model for processing, which helps in determining the position-based embeddings."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;ATTENTION SCORES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Attention Scores are numerical values that represent the relevance and importance of each word in relation to other words within a sentence during the attention mechanism process."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;OUTPUT&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The output generated by the attention function is a vector or set of vectors that capture the weighted sum of the values, representing the contextually important information for a specific word."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;NATURAL LANGUAGE PROCESSING (NLP)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Natural Language Processing is a field of research that focuses on making computers understand human language, encompassing techniques like semantic search and text summarization."&lt;SEP&gt;"Natural Language Processing is a field of study that deals with the interaction between computers and human language, including tasks like text summarization and semantic search."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;EIGENVECTOR CENTRALITY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Eigenvector Centrality is used in LexRank to score sentences based on their importance within the document's graph representation."&lt;SEP&gt;"Eigenvector centrality in a sentence's graph representation refers to the importance of nodes based on the weighted sum of their neighbors."&lt;SEP&gt;"Eigenvector centrality refers to a measure used in graph theory where nodes are ranked based on the influence of connected nodes. In this context, it is applied to sentences within a document."&lt;SEP&gt;"Eigenvector centrality is a concept used in graph theory to measure the importance of nodes based on their connections with other important nodes."&lt;SEP&gt;"Eigenvector centrality is a measure used in graph theory to identify important nodes within a network, specifically in the context of sentence graphs where sentences are represented as vertices and similarity scores as edges."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;MASKED LANGUAGE MODELING (MLM)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Masked Language Modeling (MLM) is a task where words are masked with a 15% chance and the model aims to predict those masked words. It can be used in domain adaptation by re-training models on new datasets."&lt;SEP&gt;"Masked Language Modeling (MLM) is a technique used in BERT's pre-training phase where [MASK] tokens are applied to words to reduce prediction bias."&lt;SEP&gt;"Masked Language Modeling is a task originally introduced by BERT where some words are masked to predict them from the input sentence."&lt;SEP&gt;"Masked Language Modeling is a technique used in pre-training where certain words are masked to reduce bias during word prediction."&lt;SEP&gt;"Masked Language Modeling is a technique used in pre-training where [MASK] tokens are applied to words, allowing the model to predict masked words based on surrounding context."&lt;SEP&gt;"Masked Language Modeling is a task used to fine-tune BERTimbau for understanding technical language in legal documents."&lt;SEP&gt;"Masked Language Modeling is a technique used in BERT's pre-training phase to reduce bias by masking words and predicting them based on context."&lt;SEP&gt;"Masked Language Modeling is a training approach used to familiarize the model with technical language and jargon."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681&lt;SEP&gt;chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;NEXT SENTENCE PREDICTION (NSP)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Next Sentence Prediction (NSP) is a task in BERT's training process that aims to understand sentence relationships, particularly useful for applications like Question and Answering and Natural Language Inference."&lt;SEP&gt;"Next Sentence Prediction is a task aimed at understanding sentence relationships during pre-training by predicting if the second sentence is a subsequent one to the first."&lt;SEP&gt;"Next Sentence Prediction is a task that helps the model understand sentence relationships, often used in tasks like Question and Answering and Natural Language Inference."&lt;SEP&gt;"Next Sentence Prediction is a task in BERT's pre-training phase that helps the model understand sentence relationships, essential for applications like QA and NLI."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;BOOKSCORPUS (800M WORDS)&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"BooksCorpus is a dataset of 800 million words used in BERT's pre-training process to provide a large amount of text data for training."&lt;SEP&gt;"BooksCorpus is a dataset used for pre-training BERT containing 800 million words."&lt;SEP&gt;"BooksCorpus is a large dataset containing 800 million words used for pre-training BERT."&lt;SEP&gt;"BooksCorpus is a dataset used in the pre-training phase of BERT, containing 800 million words from books."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;ENGLISH WIKIPEDIA (2500M WORDS)&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"English Wikipedia is another dataset used for pre-training BERT, containing approximately 2.5 billion words."&lt;SEP&gt;"English Wikipedia provides another large dataset containing approximately 2.5 billion words for pre-training BERT."&lt;SEP&gt;"English Wikipedia, containing about 2.5 billion words, serves as another dataset for BERT's pre-training phase."&lt;SEP&gt;"English Wikipedia is another dataset used in the pre-training phase of BERT, consisting of 2.5 billion words from Wikipedia articles."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;TOKEN EMBEDDING LAYER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Token Embedding Layer assigns a value to each word based on vocabulary IDs, crucial for encoding words in BERT."&lt;SEP&gt;"The Token Embedding Layer assigns a value to each word based on vocabulary IDs."&lt;SEP&gt;"The Token Embedding Layer assigns values to each word based on vocabulary IDs, forming part of the input embeddings for BERT."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;SEGMENTATION EMBEDDING LAYER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Segmentation Embedding Layer distinguishes whether a word belongs to sentence A or B, aiding in understanding context during processing."&lt;SEP&gt;"The Segmentation Embedding Layer distinguishes whether a word belongs to sentence A or B."&lt;SEP&gt;"The Segmentation Embedding Layer helps distinguish whether a word belongs to sentence A or B, essential for understanding context."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;POSITION EMBEDDING LAYER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Position Embedding Layer indicates the position of each word in a sentence, aiding in maintaining the sequence information during processing."&lt;SEP&gt;"The Position Embedding Layer indicates the position of each word in a sentence, contributing to more accurate contextual understanding by the model."&lt;SEP&gt;"The Position Embedding Layer indicates the position of each word in a sentence."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;ADAM OPTIMIZER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Adam Optimizer is an algorithm used in the fine-tuning phase to train the entire network for a few epochs, optimizing the model parameters."&lt;SEP&gt;"Adam Optimizer is an algorithm used to train the model in the fine-tuning phase, optimizing weights of the neural network."&lt;SEP&gt;"Adam Optimizer is an optimization algorithm used in fine-tuning phases for training models."&lt;SEP&gt;"Adam Optimizer is used during the fine-tuning phase to train the entire network for several epochs."&lt;SEP&gt;"Adam Optimizer is an optimizer used for training the model."&lt;SEP&gt;"The Adam Optimizer is an algorithm used during the fine-tuning phase to train BERT effectively."&lt;SEP&gt;"The Adam Optimizer is used in the fine-tuning phase of BERT to train the entire network for a few epochs."&lt;SEP&gt;"Adam Optimizer is used in the fine-tuning phase of BERT to train the entire network for a few epochs."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1&lt;SEP&gt;chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;L, H, A&quot;">
  <data key="d0">"VARIABLE"</data>
  <data key="d1">"L, H, and A are variables used to denote the architecture of BERT: L for layers, H for hidden size, and A for self-attention heads."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;UNIDIRECTIONAL TRAINING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A traditional method where text sequences are processed either from left to right or vice versa, without considering bidirectionality."&lt;SEP&gt;"Unidirectional training refers to models that look at text sequences either from left to right or vice-versa."&lt;SEP&gt;"Unidirectional training is a traditional method where words can only see their left or right context, leading to biased predictions."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056&lt;SEP&gt;chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;BIDIRECTIONAL TRAINING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Bidirectional training allows the model to consider both contexts of a word, its left and right words, for better understanding."&lt;SEP&gt;"Bidirectional training is a method where BERT is trained to understand words based on their context from both left and right."&lt;SEP&gt;"Bidirectional training in BERT allows each word to be predicted based on its left and right context, reducing bias compared to unidirectional training."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;TRANSFORMER BLOCKS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Transformer blocks are components of the BERT architecture responsible for self-attention mechanisms."&lt;SEP&gt;"Transformer blocks are components within BERT architecture used for processing input data and forming the network structure."&lt;SEP&gt;"Transformer blocks are components of the BERT architecture that enable bidirectional training and attention mechanisms."&lt;SEP&gt;"Transformer blocks are components within the BERT architecture, represented by L and contributing to the model's layers."&lt;SEP&gt;"Transformer blocks are components within the Transformer architecture that handle self-attention mechanisms, with L representing the number of layers."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1&lt;SEP&gt;chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;FINE-TUNING PHASE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Fine-tuning is a phase in the BERT model training where an additional layer is added and the network is trained for a few epochs with Adam Optimizer."&lt;SEP&gt;"The fine-tuning phase involves additional training on specific tasks using labeled data to enhance performance for particular applications."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95&lt;SEP&gt;chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;BERT BASE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERT BASE refers to a specific model size with 12 layers, hidden size of 768, 12 self-attention heads, and total parameters of 110 million."&lt;SEP&gt;"BERT BASE refers to one of the model sizes with specific parameters (L=12, H=768, A=12)."&lt;SEP&gt;"BERT BASE refers to one of two model sizes reported, having 12 layers, hidden size of 768, 12 self-attention heads, and total parameters of 110M."&lt;SEP&gt;"BERT BASE refers to one of the two model sizes reported, characterized by L=12, H=768, A=12, and 110M total parameters."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;BERT LARGE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERT LARGE is another model size reported in the original paper with 24 layers, a hidden size of 1024, 16 self-attention heads, and total parameters of 340 million."&lt;SEP&gt;"BERT LARGE is another reported model size with different parameters (L=24, H=1024, A=16)."&lt;SEP&gt;"BERT LARGE is another model size reported, with 24 layers, a hidden size of 1024, 16 self-attention heads, and total parameters of 340M."&lt;SEP&gt;"BERT LARGE is another model size reported, characterized by L=24, H=1024, A=16, and 340M total parameters."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;DOMAIN ADAPTATION (DA)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Domain Adaptation (DA) is a technique aimed at improving the performance of deep learning models when applied to data from different domains than those they were trained on."&lt;SEP&gt;"Domain Adaptation (DA) is a technique to improve performance on a new dataset from a different domain."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;ORIGINAL PAPER AUTHORS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"The original paper authors denoted key parameters of BERT models such as layers (L), hidden size (H), self-attention heads (A)."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;NNS (NEURAL NETWORKS)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"NNs are mentioned in the text and require significant amounts of data for training."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;LABELLED DATA&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Labelled data is a type of data used in training deep learning models, which can be expensive to obtain."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;UNLABELED DATA (UNSUPERVISED LEARNING TASKS)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Unsupervised learning tasks are mentioned as a way to retrain models on different domains without extensive labeled data."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;TESTING DATASET&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Testing dataset is another subset of documents used to evaluate the performance of trained models on unseen data."&lt;SEP&gt;"The Testing dataset is a separate event in the dataset split, aimed at evaluating model performance."&lt;SEP&gt;"The testing dataset is where the performance of deep learning models is evaluated."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95&lt;SEP&gt;chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TSDAE is a method published by Nils Reimers that aims to improve domain knowledge through sentence embedding."&lt;SEP&gt;"TSDAE is an unsupervised state-of-the-art sentence embedding method that was first published on April 14th, 2021 by Nils Reimers. It aims to improve the domain knowledge of a model and operates through a modified encoder-decoder Transformer architecture."&lt;SEP&gt;"Transformer-based Sequential Denoising Auto-Encoder (TSDAE) is a domain adaptation technique used to help models perform better on new datasets from different domains."&lt;SEP&gt;"TSDAE is an unsupervised sentence embedding technique used for encoding damaged sentences into fixed-sized vectors."&lt;SEP&gt;"Transformer-based Sequential Denoising Auto-Encoder is a technique used in domain adaptation that helps models adapt to new domains by removing noise from sequential data."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;NILS REIMERS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Nils Reimers is the author of the first publication of TSDAE on April 14th, 2021."&lt;SEP&gt;"Nils Reimers is the author who first published TSDAE on April 14th, 2021. He is associated with advancements in unsupervised learning techniques for natural language processing."&lt;SEP&gt;"Nils Reimers is the author who published TSDAE on April 14th, 2021, as an improvement over MLM methods."&lt;SEP&gt;"Nils Reimers published TSDAE on April 14th, 2021, as an improvement over MLM methods."&lt;SEP&gt;"Nils Reimers is the author who first published the TSDAE method on April 14th, 2021."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;TEXT-TO-TEXT TRANSFER TRANSFORMER (T5)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"T5 is a Transformer-based architecture used in GenQ and is fine-tuned for question-answering tasks. It achieves state-of-the-art results on many NLP benchmarks while maintaining the ability to be fine-tuned for numerous downstream tasks."&lt;SEP&gt;"T5 is a specific type of Transformer model used in the GenQ method, designed to handle text-to-text tasks effectively."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;GENQ [39]&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"GenQ is an unsupervised domain adaptation method for dense retrieval models, published in October 2021 by the Ubiquitous Knowledge Processing Lab team. It aims to support question-answering scenarios through synthetically generated data."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;UBIQUITOUS KNOWLEDGE PROCESSING LAB TEAM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Ubiquitous Knowledge Processing Lab team developed GenQ, an unsupervised domain adaptation method for dense retrieval models."&lt;SEP&gt;"The Ubiquitous Knowledge Processing Lab team is responsible for publishing GenQ in October 2021, focusing on unsupervised domain adaptation methods for dense retrieval models."&lt;SEP&gt;"The Ubiquitous Knowledge Processing Lab team developed the GenQ method. Their approach focuses on generating queries for semantic searches using synthetically generated data."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;NEGATIVE LOG-LIKELIHOOD LOSS FUNCTION (LMLM)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The negative log-likelihood loss function is defined as LMLM(x, y) :=−NX n=1 wynxn, yn. It measures the performance of a model in predicting target values based on input data."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;MLM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"MLM refers to Masked Language Model, a technique used for pre-training language models but not as advanced or state-of-the-art as TSDAE. It is mentioned as a previous approach that TSDAE outperforms."&lt;SEP&gt;"MLM, Masked Language Modeling, is an original task introduced by BERT that aims to predict masked words in input sentences."&lt;SEP&gt;"MLM, or Masked Language Model, is a previous approach used for language modeling before the introduction of TSDAE."&lt;SEP&gt;"MLM refers to Masked Language Model, an earlier method that TSDAE outperforms. It involves masking tokens in sentences for training purposes."&lt;SEP&gt;"MLM refers to Masked Language Modeling, a previous approach that TSDAE outperforms in unsupervised sentence embedding methods."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;MS MARCO [26]&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"MS MARCO is a dataset used for fine-tuning T5 models. It provides unlabelled data for training purposes."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;CONTROL&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;INTELLIGENCE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;FIRST CONTACT&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"First Contact is the potential initial communication between humanity and an unknown intelligence."&lt;SEP&gt;"First Contact is the potential initial communication between humanity and an unknown intelligence."&lt;SEP&gt;"First Contact is the potential initial communication between humanity and an unknown intelligence."&lt;SEP&gt;"First Contact refers to the potential initial communication between humanity and an unknown intelligence."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;HUMANITY'S RESPONSE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Humanity's Response is the collective action taken by Alex's team in response to a message from an unknown intelligence."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;FIGURE 3.4: TSDAE ARCHITECTURE&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 3.4 illustrates the architecture of TSDAE as described by [42]."&lt;SEP&gt;"This is a figure reference in the text."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;FIGURE 3.5: T5 DIAGRAM&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Another figure reference in the text."&lt;SEP&gt;"Figure 3.5 illustrates the architecture of T5 as described by [29]."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;[42]&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"[42] refers to the publication where TSDAE was first published by Nils Reimers."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;[29]&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"[29] refers to the publication where T5 was introduced by its authors."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;[26]&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"MS MARCO [26] provides unlabelled data for fine-tuning T5 models."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;T5 MODEL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"T5 Model is a transformer-based model from Google designed for text generation tasks, employed in the second phase of query generation."&lt;SEP&gt;"T5 is a Transformer based architecture that uses a text-to-text approach [29]. This new model architecture achieves state-of-the-art results on many NLP benchmarks while maintaining the ability of being fine-tuned for numerous downstream tasks. T5 authors found that training on in-domain unlabelled data can improve performance, but using a large and diverse data set is better for generic language understanding tasks."&lt;SEP&gt;"T5 is a transformer-based model developed by Google that can generate text based on given inputs, often used for tasks requiring natural language transformation and generation."&lt;SEP&gt;"T5 is another type of transformer-based model used for text generation and rewriting, which was explored in the process of generating queries."&lt;SEP&gt;"T5 model generated queries with reduced bias but still made use of some keywords from the summary, indicating an ongoing challenge in fully anonymizing and rephrasing content for evaluation purposes."&lt;SEP&gt;"T5 model refers to a specific type of neural network model used for training on various NLP tasks."&lt;SEP&gt;"T5 model was used for generating queries with reduced bias but still relied on some summary keywords, highlighting ongoing challenges in anonymizing content."&lt;SEP&gt;"T5 model is a neural network language model used for training on various NLP tasks."&lt;SEP&gt;"T5 model is a type of pre-trained neural network designed to handle various NLP tasks as text-to-text problems, trained on vast data sets."&lt;SEP&gt;"The T5 model was used to generate queries for the Query Generation stage in GPL technique."&lt;SEP&gt;"T5 model refers to a type of generative pre-trained transformer model used in natural language processing tasks."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-6eeb6febf5ce46ec96655d84dc54cd2f&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;BERT MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERT model is a general term for the BERT architecture developed by Google AI, used across various NLP tasks including sentence embedding creation."&lt;SEP&gt;"The BERT model is a transformer-based architecture used for various NLP tasks."&lt;SEP&gt;"BERT model is a pre-trained language representation model used in various NLP tasks."&lt;SEP&gt;"The BERT model is a deep learning technique used for natural language processing tasks, specifically for creating sentence embeddings."</data>
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</node>
<node id="&quot;REIMERS AND GUREVYCH&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Reimers and Gurevych are researchers who conducted experiments on SBERT, showing its speed advantages over BERT."&lt;SEP&gt;"Reimers and Gurevych are researchers who demonstrated that SBERT outperforms BERT in sentence pair comparison."&lt;SEP&gt;"Reimers and Gurevych are researchers who demonstrated the benefits of using SBERT over BERT for sentence pair comparisons."&lt;SEP&gt;"Reimers and Gurevych are researchers who demonstrated the superiority of SBERT over BERT in semantic similarity tasks."&lt;SEP&gt;"Reimers and Gurevych are researchers who demonstrated the efficiency of SBERT compared to BERT in sentence pair comparisons."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca&lt;SEP&gt;chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</node>
<node id="&quot;SIAMESE ARCHITECTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Siamese architecture refers to a neural network design that can process pairs of inputs and output their similarities or dissimilarities."&lt;SEP&gt;"The Siamese architecture uses two BERT models with entangled weights, comparing sentences through a pooling operation to transform token embeddings into fixed-size vectors."&lt;SEP&gt;"The Siamese architecture involves two BERT models sharing weights, used in comparing sentence pairs."&lt;SEP&gt;"The Siamese architecture involves two BERT models with entangled weights, used to compare sentence pairs efficiently."</data>
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f&lt;SEP&gt;chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;MARGIN MEAN SQUARED ERROR LOSS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A specific loss function used in the final model training step."&lt;SEP&gt;"Margin Mean Squared Error Loss is a training loss function used in the Pseudo Labeling step to identify relevant passages based on their similarity scores."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</node>
<node id="&quot;UKP LAB&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The UKP lab implemented SBERT in 2019 and is associated with the research behind the model."&lt;SEP&gt;"The UKP lab is a research group that developed SBERT, focusing on tasks such as semantic similarity comparison and information retrieval via semantic search."</data>
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</node>
<node id="&quot;NLP TASKS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"NLP tasks refer to various natural language processing activities and applications."</data>
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</node>
<node id="&quot;SOFTMAX LOSS APPROACH&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A method for fine-tuning SBERT by applying a softmax classifier on top of the siamese network, improving sentence representation."&lt;SEP&gt;"The Softmax loss approach is used for fine-tuning SBERT on sentence pairs, improving the model's ability to identify relationships between sentences."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;NLI (NATURAL LANGUAGE INFERENCE)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"NLI involves determining the logical relationship between a premise and hypothesis, including entailment, neutral, or contradiction."&lt;SEP&gt;"NLI is a challenge involving determining if the premise implies the hypothesis or not, including labels like entailment, neutral, and contradiction."&lt;SEP&gt;"NLI is another task that some models are optimized for."&lt;SEP&gt;"NLI is an NLP task where models determine if the premise implies the hypothesis or if they are contradictory or neutral."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca&lt;SEP&gt;chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;SNLI (STANFORD NATURAL LANGUAGE INFERENCE)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SNLI is one of the datasets used for NLI tasks, containing pairs of sentences labeled as entailment, neutral, or contradiction."&lt;SEP&gt;"SNLI is referenced as a similar dataset to assin, which provides entailment labels."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d&lt;SEP&gt;chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;MULTI-GENRE NLI DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"A diverse dataset used in the NLI challenge to test models on various genres of text."&lt;SEP&gt;"The Multi-Genre NLI dataset is another resource used in the NLI task, providing a variety of sentence pairs and their relationships."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STS (SEMANTIC TEXTUAL SIMILARITY)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"STS (Semantic Textual Similarity) is a task that evaluates how similar two pieces of text are, used to assess the performance of language models."&lt;SEP&gt;"STS tasks involve evaluating how similar two sentences are based on gold labels ranging from 0 to 5."&lt;SEP&gt;"The STS task involves evaluating how similar sentence pairs are, with labels ranging from entailment, neutral, to contradiction."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca&lt;SEP&gt;chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;AGIRRE ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Agirre et al. are researchers who contributed to the STS benchmark datasets, providing valuable resources for model evaluation."&lt;SEP&gt;"Agirre et al. are researchers who contributed to the STS benchmark datasets."&lt;SEP&gt;"Agirre et al. are researchers who created the STS tasks from 2012-2016."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;CER ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Cer et al. are researchers who also contributed to the STS benchmark and other relevant datasets."&lt;SEP&gt;"Cer et al. are researchers who created the STS benchmark dataset."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;SICK-RELATEDNESS DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"A dataset containing sentence pairs and their relatedness scores used in evaluating semantic textual similarity models."&lt;SEP&gt;"The SICK-Relatedness dataset is a resource used for semantic textual similarity tasks, providing sentence pairs with gold labels between 0 and 5."&lt;SEP&gt;"The SICK-Relatedness dataset is a resource used in evaluating semantic textual similarity, providing gold labels for sentence pairs."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;SUPERVISED LEARNING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Supervised Learning refers to training models on labeled data, specifically using the STS benchmark dataset."&lt;SEP&gt;"Supervised learning involves training models on labeled data to perform specific tasks like STS or NLI."&lt;SEP&gt;"This involves training models on labeled data such as STS benchmark to improve their performance in specific tasks."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;UNSUPERVISED LEARNING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"This refers to the training of models without explicit supervision, using datasets like Agirre et al.'s and SICK-Relatedness for evaluating semantic textual similarity."&lt;SEP&gt;"Unsupervised Learning involves training models without explicit labels, utilizing datasets like STS tasks 2012-2016 and SICK-Relatedness."&lt;SEP&gt;"Unsupervised learning refers to training models without explicit labels, using techniques from datasets like SICK-Relatedness and STS benchmark."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STSB DATASET&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The STS-B dataset is a benchmark for evaluating semantic similarity tasks in natural language processing. Albertina PT BR performs well on this dataset but does not match BERTimbau's performance."&lt;SEP&gt;"The STSb dataset is a benchmark dataset used in natural language processing tasks, particularly in sentence similarity and paraphrase identification. It has multiple versions such as STS12 to STS16 and STSb."&lt;SEP&gt;"The STSb dataset is a benchmark used to evaluate sentence embeddings, particularly focusing on similarity judgments between pairs of sentences."&lt;SEP&gt;"The STSb dataset is a benchmark for sentence similarity and textual entailment tasks used in evaluating various models."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;BERT -STSB-BASE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERT -STSb-base refers to a BERT model fine-tuned on the STS benchmark dataset for sentence similarity tasks."&lt;SEP&gt;"BERT -STSb-base refers to a model trained on the STS benchmark dataset, specifically using the base version of BERT for sentence embeddings. It achieved a score of 84.30 ± 0.76."&lt;SEP&gt;"BERT -STSb-base uses a base version of the pre-trained BERT model fine-tuned on STS benchmark tasks, showing performance measures in evaluations."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SBERT -STSB-BASE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SBERT -STSb-base is a variant using SBERT models fine-tuned on STS benchmark tasks, with specific performance measures provided."&lt;SEP&gt;"SBERT -STSb-base is another model trained on the STS benchmark dataset, specifically using SBERT with the base version for sentence embeddings. It achieved a score of 84.67 ± 0.19."&lt;SEP&gt;"SBERT -STSb-base is an SBERT variant fine-tuned specifically on the STS benchmark dataset."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SROBERTA-STSB-BASE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SRoBERTa-STSb-base is yet another model trained on the STS benchmark dataset, specifically using SRoBERTa with the base version for sentence embeddings. It achieved a score of 84.92 ± 0.34."&lt;SEP&gt;"SRoBERTa-STSb-base utilizes the RoBERTa model fine-tuned on STS benchmark tasks, showing detailed performance metrics in evaluations."&lt;SEP&gt;"SRoBERTa-STSb-base refers to a RoBERTa model fine-tuned on both NLI and STS datasets, providing robust sentence embeddings."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;BERT -STSB-LARGE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERT -STSb-large employs a larger version of the pre-trained BERT model fine-tuned on STS benchmark tasks, with specific performance measures provided."&lt;SEP&gt;"BERT -STSb-large is a model trained on the STS benchmark dataset using the large version of BERT for sentence embeddings, achieving a score of 85.64 ± 0.81."&lt;SEP&gt;"BERT -STSb-large is a larger BERT variant specifically trained for the STS benchmark dataset."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SBERT -STSB-LARGE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SBERT -STSb-large is an enhanced SBERT model fine-tuned on the larger STS benchmark dataset."&lt;SEP&gt;"SBERT -STSb-large refers to SBERT trained on the STS benchmark dataset using the large version for sentence embeddings, achieving a score of 84.45 ± 0.43."&lt;SEP&gt;"SBERT -STSb-large uses SBERT models fine-tuned on large-scale STS benchmarks, showing detailed performance metrics in evaluations."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SROBERTA-STSB-LARGE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SRoBERTa-STSb-large is another model trained on the STS benchmark dataset using SRoBERTa with the large version for sentence embeddings, achieving a score of 85.02 ± 0.76."&lt;SEP&gt;"SRoBERTa-STSb-large utilizes a larger version of the RoBERTa model fine-tuned on STS benchmark tasks, with specific performance measures provided."&lt;SEP&gt;"SRoBERTa-STSb-large refers to a large RoBERTa model fine-tuned on both NLI and STS datasets, providing advanced sentence embeddings."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;INFERSENT - GLOVE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"InferSent - GloVe combines InferSent and GloVe embeddings, with performance measures provided for various models."&lt;SEP&gt;"InferSent - GloVe is a model that uses GloVe embeddings for sentence embeddings, achieving a score of 68.03."&lt;SEP&gt;"InferSent using GloVe embeddings involves a model that generates sentence encodings based on static word vectors."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;UNIVERSAL SENTENCE ENCODER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Universal Sentence Encoder is a model developed by Google to generate fixed-length numerical features from text."&lt;SEP&gt;"The Universal Sentence Encoder is another model used in the evaluation, achieving a score of 74.92."&lt;SEP&gt;"The Universal Sentence Encoder is a pre-trained model that provides dense vector representations for text, trained to maximize the cosine similarity of sentences with similar meanings."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;BERT -NLI-BASE&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"BERT -NLI-base refers to BERT trained on NLI data, achieving a score of 85.35 ± 0.17."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SBERT -NLI-STSB-BASE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SBERT -NLI-STSb-base is an SBERT variant fine-tuned on both NLI and STS benchmark datasets."&lt;SEP&gt;"SBERT -NLI-STSb-base is another model that uses SBERT with the base version and NLI data, achieving a score of 86.10 ± 0.13."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SROBERTA-NLI-STSB-BASE&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"SRoBERTa-NLI-STSb-base is yet another model using SRoBERTa with the base version and NLI data, achieving a score of 84.79 ± 0.38."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Multilingual Knowledge Distillation (MKD) is a technique introduced by the lead researcher from the SBERT team to address monolingual embedding models, aiming to train models in multiple languages."&lt;SEP&gt;"Multilingual Knowledge Distillation (MKD) is a technique introduced by the lead researcher in 2020 to enhance multilingual embedding models."&lt;SEP&gt;"Multilingual Knowledge Distillation (MKD) is a technique introduced to address issues related to monolingual models and involves transferring knowledge across different languages."&lt;SEP&gt;"Multilingual Knowledge Distillation is a technique introduced by lead researchers from a team that published SBERT, aimed at creating multilingual embedding models."&lt;SEP&gt;"Multilingual Knowledge Distillation is a technique that enables model knowledge transfer from one language to another, specifically from English to Portuguese in this case."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7&lt;SEP&gt;chunk-53db1ddb7040c30dab02f6b0bd355f81&lt;SEP&gt;chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;SBERT EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SBERT evaluation on the STS benchmark test set is a process to assess the performance of SBERT model."&lt;SEP&gt;"SBERT evaluation on the STS benchmark test set is an event where SBERT model performances are assessed using the STS benchmark test set."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;STS BENCHMARK TEST SET&quot;">
  <data key="d0">"BENCHMARK"</data>
  <data key="d1">"STS benchmark test set is a dataset used for evaluating SBERT models based on semantic text similarity."&lt;SEP&gt;"The STS benchmark test set is a dataset used to evaluate sentence embedding models like SBERT."&lt;SEP&gt;"The STS benchmark test set is used to evaluate SBERT model performances in terms of semantic textual similarity."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;ERTIMBAU VARIANTS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"ERTimbau is a set of language model variants developed from the BERT architecture, focusing on Portuguese language."&lt;SEP&gt;"ERTimbau variants refers to the variant models of ERTimbau, which are used in a pretraining stage similar to BERT."&lt;SEP&gt;"ERTimbau variants refers to the specific language model variants derived from ERTimbau, used in pretraining stages."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;NIGAM [27] &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Nigam refers to the team or individual involved in the application of NLP techniques including SBERT and Sent2Vec with BM25 scoring."&lt;SEP&gt;"nigam is a third team that proposed an approach combining transformer-based and traditional IR techniques for the first task of COLIEE 2021."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;OVGU&quot; &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"OvGU is a team that achieved top scores on COLIEE 2021 by combining lexical and semantic techniques, using Sentence-BERT embeddings."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;JNLP&quot; &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"JNLP is another team that focused on large article text chunking and self-labeled approaches for dealing with training data in the context of COLIEE 2021."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;MI-YOUNG KIM ET AL.&quot; &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mi-Young Kim et al. are researchers involved in discussing deep learning techniques for legal information retrieval and question-answering tasks."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;COLIEE 2021&quot; &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The COLIEE 2021 is an event where multiple teams competed on specific challenges related to legal information extraction and entailment."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;UNIVERSITY OF ALBERTA&quot; &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The University of Alberta participated in the COLIEE 2021, focusing specifically on deep learning techniques for legal tasks."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;NIGAM [27]&quot; &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Nigam refers to a team that proposed an approach combining transformer-based and traditional IR techniques for case law retrieval in the COLIEE 2021 competition."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;TEAM F1-SCORE PRECISION RECALL MAP&quot; &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This refers to performance metrics used by teams in the COLIEE 2021, specifically showing their effectiveness in different tasks such as case law and statute law retrieval."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;NUNO CORDEIRO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Nuno Cordeiro developed the Legal Semantic Search Engine (LeSSE&lt;SEP&gt;"Nuno Cordeiro is mentioned as part of his master’s thesis, creating the Legal Semantic Search Engine (LeSSE)."&lt;SEP&gt;"Nuno Cordeiro is the author of Legal Semantic Search Engine (LeSSE), a system that merges document retrieval techniques with semantic search abilities on Portuguese consumer law."&lt;SEP&gt;"Nuno Cordeiro is the author of the Legal Semantic Search Engine (LeSSE) and part of his master’s thesis focused on Portuguese consumer law."&lt;SEP&gt;"Nuno Cordeiro is the author of the master's thesis that created Legal Semantic Search Engine (LeSSE), focusing on Portuguese consumer law."&lt;SEP&gt;"Nuno Cordeiro is the creator of the Legal Semantic Search Engine (LeSSE) as part of his master’s thesis."&lt;SEP&gt;"Nuno Cordeiro is the creator of Legal Semantic Search Engine (LeSSE) as part of his master’s thesis in 2022."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;LEGAL SEMANTIC SEARCH ENGINE (LESSE)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"LeSSE is a system developed for legal document retrieval and semantic search, combining common document retrieval techniques with semantic capabilities."&lt;SEP&gt;"LeSSE is a system that merges document retrieval techniques with semantic search abilities for Portuguese consumer law, developed in partnership with INESC-ID and Imprensa Nacional-Casa da Moeda."&lt;SEP&gt;"LeSSE is an NLP system created by Nuno Cordeiro, in partnership with INESC-ID and Imprensa Nacional-Casa da Moeda, to make Portuguese consumer law more accessible and understandable."&lt;SEP&gt;"LeSSE is a system that merges common document retrieval techniques with semantic search abilities for Portuguese consumer law."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;INESC-ID&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"INESC-ID is a research institute that collaborated with Nuno Cordeiro on the development of LeSSE for making Portuguese consumer law more accessible and understandable."&lt;SEP&gt;"INESC-ID is an organization involved in the development of LeSSE along with Nuno Cordeiro and Imprensa Nacional-Casa da Moeda."&lt;SEP&gt;"INESC-ID was involved in the development of LeSSE as a partner institution along with Imprensa Nacional-Casa da Moeda."&lt;SEP&gt;"INESC-ID was in partnership with Nuno Cordeiro to develop the Legal Semantic Search Engine (LeSSE)."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;IMPRENSA NACIONAL-CASA DA MOEDA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Imprensa Nacional-Casa da Moeda also partnered with INESC-ID and Nuno Cordeiro on the development of LeSSE."&lt;SEP&gt;"Imprensa Nacional-Casa da Moeda is an organization that also partnered with Nuno Cordeiro to develop LeSSE, contributing towards the accessibility of Portuguese consumer law."&lt;SEP&gt;"Imprensa Nacional-Casa da Moeda is an organization that partnered with INESC-ID to develop the Legal Semantic Search Engine (LeSSE)."&lt;SEP&gt;"Imprensa Nacional-Casa da Moeda is another organization that partnered to develop LeSSE for Portuguese consumer law accessibility."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;2022&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The event in 2022 marks the year when Nuno Cordeiro created LeSSE as part of his master's thesis."&lt;SEP&gt;"The year 2022 marks the creation of Legal Semantic Search Engine (LeSSE) by Nuno Cordeiro as part of his master’s thesis."&lt;SEP&gt;"The year 2022 is mentioned as when Nuno Cordeiro created LeSSE."&lt;SEP&gt;"The year 2022 is when Nuno Cordeiro created the Legal Semantic Search Engine (LeSSE) as part of his master’s thesis."&lt;SEP&gt;"The year 2022 is when Nuno Cordeiro created the Legal Semantic Search Engine (LeSSE) as part of his master’s thesis."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;2022 MASTER’S THESIS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Nuno Cordeiro's master's thesis in 2022 focused on creating a system for Portuguese consumer law accessibility and understanding."&lt;SEP&gt;"The master's thesis by Nuno Cordeiro in 2022 focused on creating a system for Portuguese consumer law accessibility and understanding."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;BERTIMBAU BASE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERTimbau Base is a BERT model used in Nuno’s work for Portuguese legal documents."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;JO ˜ÃO RODRIGUES ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jo ˜ão Rodrigues and his team shared the Albertina model in May 2023 as a state-of-the-art BERT model for European Portuguese (PT-PT) and Brazilian Portuguese (PT-BR)."&lt;SEP&gt;"Jo ˜ão Rodrigues et al. are the researchers who developed Albertina [35]."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;ALBERTINA [35] MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Albertina is a new BERT model developed by Jo ˜ão Rodrigues et al., representing the state-of-the-art for European and Brazilian Portuguese encoder models."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;TABLE 3.4&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Table 3.4 is a reference to a specific table showing Task 1 results."&lt;SEP&gt;"Table 3.4 is a reference to a specific table showing Task 1 results."&lt;SEP&gt;"Table 3.4 is a reference to a table showing Task 1 results, indicating its importance in the document structure."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;TABLE 3.5&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Table 3.5 is a reference to a specific table showing Task 3 results."&lt;SEP&gt;"Table 3.5 is a reference to a table showing Task 3 results, indicating its importance in the document structure."&lt;SEP&gt;"Table 3.5 is a reference to another specific table showing Task 3 results."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;COSINE SIMILARITY MEASURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Cosine similarity measure is used to score the segments based on their semantic and syntactic similarity during search time."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;BM25 ALGORITHM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BM25 algorithm is a method used for information retrieval, combined with cosine similarity in the system developed by Nuno Cordeiro."&lt;SEP&gt;"The BM25 algorithm is used in pre-processing legal documents to generate scores based on relevance."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;JOÃO RODRIGUES ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"João Rodrigues et al. are the authors of Albertina, a state-of-the-art BERT model for European and Brazilian Portuguese."&lt;SEP&gt;"João Rodrigues et al. are the creators of the Albertina model, which was shared in May 2023 as a new state-of-the-art model for Portuguese languages."&lt;SEP&gt;"João Rodrigues et al. are the researchers who shared their brand-new state-of-the-art model, Albertina."&lt;SEP&gt;"João Rodrigues et al. introduced Albertina, a state-of-the-art BERT model for European and Brazilian Portuguese encoders."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;FACULDADE DE CIÊNCIAS DA UNIVERSIDADE DE LISBOA (FCUL)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"FCUL is a Portuguese university involved in the development of the Albertina PT model."&lt;SEP&gt;"FCUL is a partner in developing the Albertina PT-PT version and belongs to the NLX group, focusing on natural language and speech research."&lt;SEP&gt;"FCUL is a partner in the development of the Albertina model and more specifically the NLX–Natural Language and Speech Group."&lt;SEP&gt;"FCUL is a partner in developing Albertina, along with FEUP."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;FACULDADE DE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"FEUP is another Portuguese university that partnered with FCUL to develop the Albertina PT model."&lt;SEP&gt;"FEUP is another partner involved in creating the Albertina model, specifically through its Laboratório de Inteligência Artificial e Ciência de Computadores."&lt;SEP&gt;"FEUP is another partner in the development of the Albertina model, particularly through their Laboratório de Inteligência Artificial e Ciência de Computadores."&lt;SEP&gt;"FEUP is another partner in the development of Albertina alongside FCUL."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;DEBERTA&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"DeBERTa is an architecture used as a starting point for developing the Albertina model."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;BRWAC CORPUS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"BrWaC Corpus refers to a large Portuguese language corpus that was used to pre-train BERTimbau."&lt;SEP&gt;"BrWaC corpus is a dataset containing over 2.68 billion tokens from 3.53 million documents, providing a diverse set of text for training models."&lt;SEP&gt;"BrWaC corpus is one of the data sets used in the pre-training of the PT-BR version of Albertina."&lt;SEP&gt;"The BrWaC corpus is a large dataset containing over 2.68 billion tokens from 3.53 million documents, providing a diverse text corpus for research."&lt;SEP&gt;"The BrWaC corpus is used in the pre-training of the Albertina PT-BR version, providing data specific to Brazilian Portuguese."&lt;SEP&gt;"BrWaC corpus is a dataset containing documents retrieved from across 3.53 million documents, providing a diverse text sample."&lt;SEP&gt;"The BrWaC corpus was used in the pre-training of Albertina PT-BR."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36&lt;SEP&gt;chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;PROJECT IRIS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Project IRIS aims to develop summarization approaches for court decisions and create a representation able to be browsed in a way that is helpful in the court decision process."&lt;SEP&gt;"Project IRIS is a broader project that this thesis is a part of, defining certain technological constraints for the search system implementation."&lt;SEP&gt;"Project IRIS is a larger project where the Semantic Search System was developed as part of the thesis work."&lt;SEP&gt;"Project IRIS is an ongoing project focused on developing a semantic search system, specifically for the Portuguese legal domain."&lt;SEP&gt;"Project IRIS is an organization or project that this research is part of, involving multiple members who worked on developing a Semantic Search System for STJ."&lt;SEP&gt;"Project IRIS is a collaborative project involving multiple members and contributing to the development of various systems including the Semantic Search System."&lt;SEP&gt;"Project IRIS is mentioned as a project context for this work, which influenced some aspects of the search system implementation."&lt;SEP&gt;"Project IRIS is the overarching project that defines the implementation constraints and objectives for the legal search system."&lt;SEP&gt;"Project IRIS is a larger project involving multiple members and contributing to the development of the Semantic Search System."&lt;SEP&gt;"Project IRIS is a segment where this work is a part of and has some predefined aspects for the search system implementation."&lt;SEP&gt;"Project IRIS is the larger project of which this work is a part, involving multiple members working on developing a Semantic Search System."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"Project IRIS is the overarching project that defines constraints and objectives for developing a semantic search system, specifically using Elasticsearch."&lt;SEP&gt;"Project IRIS is a significant project where the Semantic Search System was developed, involving multiple members working on different tasks."&lt;SEP&gt;"Project IRIS is an ongoing project that involves the development of semantic search systems, particularly focusing on legal applications and the Legal-BERTimbau model."&lt;SEP&gt;"Project IRIS is the larger project within which this work is a segment."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac&lt;SEP&gt;chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;CHAPTER 1&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Chapter 1 introduces the work's context within Project IRIS, mentioning predefined aspects of the project."&lt;SEP&gt;"Chapter 1 provides initial information and context about the project, including pre-defined aspects like Elasticsearch as a constraint."&lt;SEP&gt;"Chapter 1 is a part of the document that mentions Project IRIS and its constraints."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;CHAPTER 5&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Chapter 5 explains how the Legal-BERTimbau model works and its contribution to the semantic search system."&lt;SEP&gt;"Chapter 5 provides detailed information on how the Legal-BERTimbau model works and its contributions to the semantic search system."&lt;SEP&gt;"Chapter 5 is a part of the document that provides details about the Legal-BERTimbau model and its contributions to semantic search. "&lt;SEP&gt;"Chapter 5 explains the development of the language model used in our system."&lt;SEP&gt;"Chapter 5 provides detailed insights into the Legal-BERTimbau model and its architecture."&lt;SEP&gt;"Chapter 5 provides information about the Legal-BERTimbau model and its contributions to semantic search systems."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;ECLI-INDEXER6&quot;">
  <data key="d0">"TOOL"</data>
  <data key="d1">"ecli-indexer6 is a tool used by IRIS members to collect legal documents from dgsi.pt and index them into Elasticsearch."&lt;SEP&gt;"ecli-indexer6 is a tool used to collect legal documents for indexing into Elasticsearch."&lt;SEP&gt;"ecli-indexer6 is a tool used in Project IRIS for collecting legal documents from dgsi.pt, a public database, and indexing them into Elasticsearch to create the corpus for semantic search system development."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;DGSI.PT&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"dgsi.pt is a public database from which legal documents are extracted using ecli-indexer6 for indexing into Elasticsearch."&lt;SEP&gt;"dgsi.pt is a public database from which legal documents are retrieved using ecli-indexer6."&lt;SEP&gt;"dgsi.pt is a public database from which legal documents are extracted using ecli-indexer6."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;RELATOR 1&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Relator 1 is a specific person mentioned in one of the indexed legal documents."&lt;SEP&gt;"Relator 1 is mentioned as one of the relators in indexed legal documents."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;SUPREMO TRIBUNAL DE JUSTICA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Supremo Tribunal de Justica is identified as the tribunal in one of the indexed legal documents."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;DGSI - INDEXER -STJ&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"dgsi - indexer -STJ is an organization involved in a legal document corpus and has generated splits for training, testing, and validation datasets."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;TRAINING DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The Training dataset is part of the dataset split created by dgsi - indexer -STJ for model development."&lt;SEP&gt;"Training dataset refers to a subset of documents used for training machine learning models."&lt;SEP&gt;"Training dataset refers to the data used for both pre-training and fine-tuning, which can be large or small depending on the task."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;VALIDATION DATASET&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Validation dataset serves as another event to ensure model robustness and generalization during development."&lt;SEP&gt;"Validation dataset, similar to testing dataset, helps in fine-tuning and validating the model's performance during development."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;ACORDAO&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Acordao is a type of legal document that contains rulings and decisions, reflecting judicial proceedings."&lt;SEP&gt;"An Acordao is a document generated by dgsi - indexer -STJ, reflecting a legal judgement or decision."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;UNANIMIDADE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"UNANIMIDADE refers to the unanimous voting outcome of the Acordao."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;REVISTA&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Revista refers to the review process for legal documents, which can be challenged or confirmed based on new evidence or arguments."&lt;SEP&gt;"The Revista is part of the process involving an Acordao, potentially related to reviewing or revising legal documents."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;BI-ENCODER&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A Bi-Encoder is implemented for creating embeddings independently, allowing for efficient search through cosine similarity."&lt;SEP&gt;"Bi-Encoder is a model used to create embeddings independently for semantic search."&lt;SEP&gt;"Bi-Encoders generate sentence embeddings for given sentences independently and compare them using cosine similarity. Sentence-BERT (SBERT) is an example of a Bi-Encoder that embeds sentences A and B separately, producing embeddings u and v respectively."&lt;SEP&gt;"Bi-Encoder is a solution used to create embeddings independently for semantic search."&lt;SEP&gt;"Bi-Encoder is a type of neural network architecture that creates embeddings for sentences independently, useful for semantic search tasks."&lt;SEP&gt;"Bi-Encoder is a solution implemented for creating embeddings independently to enhance semantic search performance."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c&lt;SEP&gt;chunk-a8661700ca5b769e561d0c13fe452ae7&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;CROSS-ENCODER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A Cross-Encoder was considered but not utilized in this scenario due to the feasibility issues it would present."&lt;SEP&gt;"Cross-Encoder is mentioned as an alternative method not utilized in this scenario, implying it would make the search less feasible compared to Bi-Encoder."&lt;SEP&gt;"Cross-Encoders receive both sentences simultaneously and output a value between 0 and 1 representing the similarity of those sentences. They do not return sentence embeddings but compare the similarity directly."&lt;SEP&gt;"The Cross-Encoder is a model that calculates the score margin between positive and negative passages for training purposes."</data>
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f&lt;SEP&gt;chunk-486e9fdbc67025b64b42032778600c9c&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;HYBRID SEARCH SYSTEMS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Hybrid Search Systems combine both semantic and lexical approaches, developed as part of this work."&lt;SEP&gt;"Hybrid search systems combining both semantic and lexical approaches were developed as part of the solution architecture."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;DOCUMENT PRE-PROCESSING&quot;">
  <data key="d0">"PROCESS"</data>
  <data key="d1">"Document Pre-processing involves splitting entire documents into smaller units to ensure Legal-BERTimbau can effectively process the data."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;SUBSECTION 4.2.1&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Subsection 4.2.1 is referenced for analyzing documents in more detail, possibly indicating specific issues or concerns within the pre-processing phase."&lt;SEP&gt;"Subsection 4.2.1 refers to a specific part of the document where detailed analysis was required."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;GENERATIVE LANGUAGE MODEL (GLM)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Generative Language Model (GLM) specifically GPT3.5, is used for generating user-friendly responses based on retrieved results."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;PURELY SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Purely Semantic Search System is a specific architecture discussed in detail, focusing on semantic search without lexical elements."&lt;SEP&gt;"The Purely Semantic Search System is a component of the described system that focuses on semantic capabilities using Elasticsearch and cosine similarity functions."&lt;SEP&gt;"The Purely Semantic Search System relies on semantic capabilities and uses Elasticsearch with cosine similarity to search through embeddings."&lt;SEP&gt;"The Purely Semantic Search System is a specific type of search system within the Semantic Search System framework that focuses solely on semantic analysis."&lt;SEP&gt;"The Purely Semantic Search System relies solely on semantic capabilities, using Elasticsearch and cosine similarity for search results."&lt;SEP&gt;"The Purely Semantic Search System uses only semantic capabilities of embedding models for search, utilizing Elasticsearch and cosine similarity."</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87</data>
</node>
<node id="&quot;HUGGINGFACE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Hugging Face is a company known for developing state-of-the-art models like Legal-BERTimbau."&lt;SEP&gt;"HuggingFace is a platform where the STS dataset is publicly available."&lt;SEP&gt;"HuggingFace is the platform where the custom STS dataset stjiris/IRIS sts is publicly available."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87</data>
</node>
<node id="&quot;BRWAC&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"BrWaC (Brazilian Web Corpus) is mentioned as the dataset used to train BERTimbau, indicating its importance in the model’s training process."&lt;SEP&gt;"BrWaC is a large Portuguese corpus that was used to train BERTimbau."&lt;SEP&gt;"BrWaC (Brazilian Web Corpus) is the specific dataset on which BERTimbau was initially trained, providing a large Portuguese corpus for language model training."&lt;SEP&gt;"BrWaC refers to the Brazilian Web Corpus used as a pre-training dataset for BERTimbau."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;LEGAL LANGUAGE MODEL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Legal Language Model is an organization or system related to the development of language models for legal domain adaptation."&lt;SEP&gt;"The Legal Language Model is a language model specifically designed for legal text, which may be used for various tasks such as domain adaptation and natural language inference."&lt;SEP&gt;"The Legal Language Model refers to the overall language model designed for legal texts and documents, including Legal-BERTimbau."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;EMBEDDING CREATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The embedding's creation refers to the process of generating embeddings, which involves adapting a language model to a specific domain."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;NEURAL NETWORKS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Neural Networks are mentioned as a key technology used in the development of language models like BERTimbau."&lt;SEP&gt;"Neural Networks are used in creating and training models like BERTimbau for language representation tasks."&lt;SEP&gt;"Neural Networks are part of the IRIS project's technology stack, enhancing semantic information retrieval capabilities."&lt;SEP&gt;"Neural Networks refer to computational models inspired by biological neural networks, used in machine learning to facilitate the transfer of 'learned internal states' from large pre-trained language models."&lt;SEP&gt;"Neural Networks are mentioned as facilitating the usage of concepts like transfer learning in language representation tasks."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;LARGE PRE-TRAINED LANGUAGE MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Large pre-trained language models, such as BERT, are used to facilitate transfer learning and provide learned internal states that can be fine-tuned."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;TRANSFER LEARNING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Transfer Learning is a technique that allows a pre-trained language model like BERTimbau to be fine-tuned for specific tasks without overfitting when the training dataset is limited."&lt;SEP&gt;"Transfer Learning is described as a technique that allows pre-trained large language models to be fine-tuned for specific tasks, which is relevant to Legal-BERTimbau's creation."&lt;SEP&gt;"Transfer learning is a technique where a model trained on one task is fine-tuned for another specific task, leveraging pre-trained language models like BERTimbau."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;DOMAIN ADAPTATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Domain Adaptation (DA) is a technique used to address the issue of performance degradation when training data from one domain is used for testing on another. It involves re-training models using unsupervised learning tasks to adapt them for different domains."&lt;SEP&gt;"Domain Adaptation is a process used to adapt machine learning models to specific domains or contexts."&lt;SEP&gt;"Domain Adaptation is a technique used to fine-tune pre-trained language models like BERTimbau for specific domains."&lt;SEP&gt;"Domain Adaptation is a type of fine-tuning for language models aimed at understanding new domains based on pre-training data."&lt;SEP&gt;"Domain Adaptation refers to adjusting models or algorithms to perform well on data from a new domain different from the training data."&lt;SEP&gt;"Domain Adaptation is a technique aimed at improving a model's performance when applied to data from a different domain than the one it was trained on."&lt;SEP&gt;"Domain Adaptation is a technique used to improve model generalization when there's a lack of labeled data in the target domain."&lt;SEP&gt;"Domain Adaptation refers to the process of adapting a machine learning model to work effectively in a new context or with different types of data, particularly relevant for legal text processing."&lt;SEP&gt;"Domain Adaptation refers to the process of adapting models to new domains or languages."&lt;SEP&gt;"Domain adaptation is a technique aimed at improving the effectiveness of models when applied to new data, particularly in the absence of labeled data for the target domain."&lt;SEP&gt;"Domain Adaptation refers to adjusting models to perform better in specific legal domains."&lt;SEP&gt;"Domain adaptation is a technique used to improve model generalization when there is a lack of labeled data in the target domain."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681&lt;SEP&gt;chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;5.1 DOMAIN ADAPTATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"5.1 Domain Adaptation is a section describing the process of adapting language models to new domains."</data>
  <data key="d2">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is another variant of BERTimbau trained using Transformer-based Sequential Denoising Auto-Encoder (TSDAE) on legal documents, designed to handle sentence embedding tasks."&lt;SEP&gt;"This is the name of a specific model variant created for STS evaluation using BERT."&lt;SEP&gt;"This is the name of a variant created for STS evaluation."&lt;SEP&gt;"stjiris/bert-large-portuguese-cased-legal-tsdae is a variant of BERT adapted through TSDAE technique, used for Portuguese legal texts and showed lower average loss compared to MLM adaptation in the evaluation."&lt;SEP&gt;"This is the variant of a language model used for STS evaluation."&lt;SEP&gt;"stjiris/bert-large-portuguese-cased-legal-tsdae is a model variant subjected to TSDAE technique for domain adaptation."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;RUFIMELO/LEGAL-BERTIMBAU-LARGE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is a BERT-based language model used in the context of Legal domain."&lt;SEP&gt;"This is another variant of the Legal-BERTimbau model used as the base for creating SBERT variants."</data>
  <data key="d2">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The organization or project that created these datasets is not explicitly mentioned but refers to the effort of developing relatedness scores for sentences in multiple languages including Portuguese."&lt;SEP&gt;"This refers to a dataset specifically translated from English for the Portuguese legal domain."&lt;SEP&gt;"This refers to a set of specific STS (Sentence Text Similarity) datasets including assin, assin2, and stsb multilingual Portuguese sub-dataset, which are used for developing models."&lt;SEP&gt;"This refers to a specific dataset used for evaluating semantic textual similarity in multiple languages, including Portuguese."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;HUGGINGFACE STJIRIS/IRIS STS DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The HuggingFace platform hosts the IRIS STS dataset, providing access to it for further training of models."&lt;SEP&gt;"The location where the dataset is publicly available, referring to a platform or repository on HuggingFace known as 'stjiris/IRIS sts'."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;OPENAI’S GPT3 TEXT-DAVINCI-003 MODEL API&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The GPT3 model API was used to generate sentence pairs for the custom STS dataset."&lt;SEP&gt;"This refers to an external organization's tool (API) used in the dataset creation process for generating sentence pairs and relatedness scores."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;BERTIMBAU’S PAPER&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERTimbau's paper is referenced as a source for fine-tuning models with specific datasets."&lt;SEP&gt;"The event or publication of BERTimbau’s paper that describes a similar fine-tuning stage, providing context for model training strategies."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-STS-V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This denotes a specific variant of the trained model, indicating it was trained on the custom STS dataset including three original datasets."&lt;SEP&gt;"This is a variant of the Bert model fine-tuned on three original datasets including STS, designed for Portuguese legal domain tasks."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-STS-V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is another variant of the Bert model fine-tuned on all four datasets, including the IRIS dataset, for Portuguese legal domain tasks."&lt;SEP&gt;"This is another variant of the trained model, signifying it was also trained on the custom STS dataset including /IRIS sts."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;NLI ANNOTATIONS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"NLI (Natural Language Inference) annotations refer to labels indicating entailment, contradiction, or neutral relationship between sentence pairs."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;RELATEDNESS SCORES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Relatedness scores are numerical values assigned to sentence pairs based on their semantic similarity, ranging from 0 to 5."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;LEARNING RATE OF 10^-5&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A specific learning rate value (10^-5) was used during the model training process."&lt;SEP&gt;"A specific learning rate value (10^-5) was used in the fine-tuning process to adjust how quickly or slowly the model learns from the training data."&lt;SEP&gt;"The learning rate of 10^-5 was defined for the MLM task to prevent overfitting on a small dataset with numerous parameters."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;ADAM OPTIMIZATION ALGORITHM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A specific optimization algorithm used in the fine-tuning process."&lt;SEP&gt;"The Adam optimization algorithm is a method used for training deep learning models, including BERT variants, by adjusting the learning rate over time."&lt;SEP&gt;"The Adam optimization algorithm is a method used to update parameters in machine learning models, including BERT variants."&lt;SEP&gt;"The Adam optimization algorithm was used during the training process with a learning rate of \(10^{-5}\)."&lt;SEP&gt;"The Adam optimization algorithm was used to train the models with a batch size of 8 over five epochs."&lt;SEP&gt;"The Adam optimization algorithm is used during training with a learning rate of 10^-5 for five epochs."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-ce4847f54b29367988561206721bdbb7&lt;SEP&gt;chunk-de764363085fa944c487f5f0db894d4d&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;SBERT VARIANTS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SBERT (Sentence-BERT) is a variant of the BERT model that generates sentence embeddings, and several SBERT variants were developed based on different datasets."&lt;SEP&gt;"SBERT variants are described as performing better than state-of-the-art multilingual models on certain Portuguese datasets but not outperforming them on STS benchmark multi MT dataset."&lt;SEP&gt;"SBERT variants are described as performing better than state-of-the-art multilingual models on certain Portuguese datasets but not outperforming them on the STS benchmark multi MT dataset."&lt;SEP&gt;"SBERT variants are described as performing better than state-of-the-art multilingual models on the STS task for assin and assin2 datasets, indicating their effectiveness in handling Portuguese language tasks."&lt;SEP&gt;"SBERT variants refers to a series of models that performed better on the STS task for Portuguese datasets as compared to state-of-the-art multilingual models."&lt;SEP&gt;"SBERT variants refers to specific models developed by the authors for evaluating performance on STS tasks."&lt;SEP&gt;"SBERT Variants are specific versions of Sentence-BERT models that have been adapted for our domain, performing better than some multilingual models but not all on the STS task."&lt;SEP&gt;"SBERT refers to Sentence-BERT models that are trained on various datasets like STS to improve their performance in natural language understanding tasks."&lt;SEP&gt;"SBERT variants refer to specific models that performed better than state-of-the-art multilingual models on STS tasks for certain datasets. They are used in evaluating performance across different language datasets."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db&lt;SEP&gt;chunk-de764363085fa944c487f5f0db894d4d&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V0&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is a model name with specific configurations for training, indicating it's part of the experimentation process."&lt;SEP&gt;"This is the name of a model trained with NLI and STS data for legal documents in Portuguese."&lt;SEP&gt;"This is a model name, not directly an organization but part of the study process."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Another model variant, part of the fine-tuning study process."&lt;SEP&gt;"Another variant of the previous model, trained further with additional epochs or adjustments."&lt;SEP&gt;"This is a model name with specific configurations for training, indicating it's part of the experimentation process."&lt;SEP&gt;"This is another model name that underwent similar training processes and fine-tuning methods as its counterparts."&lt;SEP&gt;"This is another model trained with NLI and STS data for legal documents in Portuguese."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A more advanced version of a previously trained model."&lt;SEP&gt;"A variant of the previous model, trained further with additional epochs or adjustments."&lt;SEP&gt;"Similar to the previous models, this one also involves specific training configurations for NLI and STS tasks."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A variant of the previous model, trained further with additional epochs or adjustments."&lt;SEP&gt;"This model has a similar configuration as its predecessors but with potential updates or improvements based on the learning process."&lt;SEP&gt;"Another advanced variant, part of the fine-tuning study process."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is a model name, indicating a T5-based model used in QA and Squad V1.1 tasks for Portuguese language."&lt;SEP&gt;"This model was used for query generation and is fine-tuned for the Portuguese language, indicating its role in generating relevant questions based on legal document summaries."&lt;SEP&gt;"pierreguillou/t5-base-qa-squad-v1.1-portuguese is a model associated with the development of language understanding tasks."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is a model name, referring to a specific BERT variant developed for the Portuguese legal language with Multilingual Knowledge Distillation and Natural Language Inference/STSB tasks."&lt;SEP&gt;"This is a specific model developed as part of the research, indicating an organization or group that has created this model."&lt;SEP&gt;"stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1 is another model variant developed using the MKD technique."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;NEIL REIMERS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Neil Reimers developed the Multilingual Knowledge Distillation (MKD) technique, which is central to this work."&lt;SEP&gt;"Neil Reimers is credited as the developer of MKD (Multilingual Knowledge Distillation) technique."&lt;SEP&gt;"Neil Reimers is credited for developing the Multilingual Knowledge Distillation (MKD) technique."&lt;SEP&gt;"The developer of the Multilingual Knowledge Distillation (MKD</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;TED 2020 – PARALLEL SENTENCES CORPUS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"TED 2020 – Parallel Sentences Corpus is a dataset used in this work, containing TED and TED-X transcripts translated into multiple languages."&lt;SEP&gt;"TED 2020 – Parallel Sentences Corpus is the dataset used for training, which contains translated transcripts from various languages including Portuguese."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;LEGAL-BERTIMBAU-LARGE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Legal-BERTimbau-large is a BERT model used as a base for creating SBERT variants and fine-tuning with specific datasets."&lt;SEP&gt;"Legal-BERTimbau-large is the student model used in the MKD technique, designed to learn Portuguese language."&lt;SEP&gt;"Legal-BERTimbau-large refers to a large BERT model adapted specifically for legal language tasks."&lt;SEP&gt;"Legal-BERTimbau-large is a specific model utilized in this work for legal text processing and training."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;SENTENCE-TRANSFORMERS/STSB-ROBERTA-LARGE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is the teacher model mentioned, indicating an organization or group that has created this model."&lt;SEP&gt;"sentence-transformers/stsb-roberta-large is the teacher model chosen to train the student models on Portuguese language knowledge."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;METADATA KNOWLEDGE DISTILLATION (METAKD)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Metadata Knowledge Distillation is a new technique developed to improve information retrieval through dense vectors."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V0&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Another specific model developed in the research, showing an organization's contribution."&lt;SEP&gt;"This is another specific model developed as part of the research, indicating an organization or group that has created this model."&lt;SEP&gt;"stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v0 is a model variant developed using the MKD technique."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V0&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Another specific model developed in the research, showing an organization's contribution."&lt;SEP&gt;"stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v0 is a model variant developed using the MKD technique."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is a specific model developed as part of the research, indicating an organization or group that has created this model."&lt;SEP&gt;"stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v1 is another model variant developed using the MKD technique."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;COVID-19&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"COVID-19 refers to the subject the documents are related to, which involves identifying documents and processing sentences for a specific study or task."&lt;SEP&gt;"The event of COVID-19 is the subject of the documents, related to their content and analysis."&lt;SEP&gt;"The event related to the subject of documents, which is COVID-19, influencing the document processing and embedding generation."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;DOCUMENTS'&lt; RELATIONSHIP&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The organization or entity involved in generating and processing these documents, likely an academic institution or research group."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;FIGURE 5.6&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 5.6 is a visual representation showing the process of embedding adjustment through tags and centroids."&lt;</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;EMBEDDING ADJUSTMENT PROCESS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The process of adjusting embeddings based on tags and centroids."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;STS TASK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"STS (Semantic Textual Similarity) task evaluates how well models can understand and compare text similarity, crucial for evaluating language model performance."&lt;SEP&gt;"STS task is a process to evaluate semantic textual similarity, determining how semantically similar two sentences are."&lt;SEP&gt;"The STS (Semantic Textual Similarity) task involves comparing the similarity of sentence pairs."&lt;SEP&gt;"The STS task evaluates the performance of models on sentence similarity tasks. It includes datasets like assin and assin2."&lt;SEP&gt;"STS stands for Sentence Text Similarity, a specific task in natural language processing focused on evaluating the similarity between pairs of sentences."&lt;SEP&gt;"The STS (Semantic Textual Similarity) task is an evaluation metric used to measure how well a model can understand and compare text similarity."&lt;SEP&gt;"The STS task is a specific evaluation for determining the semantic similarity between sentences, which is crucial in validating models' performance."&lt;SEP&gt;"STS (Semantic Textual Similarity) task is a key evaluation metric used to assess the performance of language models on tasks related to text similarity."&lt;SEP&gt;"STS Task refers to the sentence similarity task evaluated using SBERT variants and multilingual models."&lt;SEP&gt;"STS task refers to the evaluation of semantic textual similarity between two sentences, a crucial part of evaluating language model performance."&lt;SEP&gt;"The STS task involves evaluating the semantic textual similarity between sentence pairs, often used for training and testing models."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-de764363085fa944c487f5f0db894d4d&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;CHAPTER 6&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Chapter 6 analyzes the performance of our search system and developed language models."&lt;SEP&gt;"Chapter 6 in this work focuses on the evaluation of search system architectures and developed language models tailored to the Portuguese legal context."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;COURT PROFESSIONALS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Court professionals are individuals such as judges who rely on the search system to retrieve relevant documents for decision-making."&lt;SEP&gt;"Court professionals are the target users of the improved information retrieval process aimed to help with decision-making."&lt;SEP&gt;"Court professionals are the users of the search system who need to retrieve relevant documents for decision-making in court cases."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;JUDGES&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Judges are a specific subset of court professionals who have to go through immense and often dispersed information to formulate decisions."&lt;SEP&gt;"Judges are key users of the search system, tasked with making decisions based on retrieved information from court proceedings."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;INFORMATION RETRIEVAL PROCESS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Information Retrieval process is the task of retrieving relevant documents for use in court decision-making by judges."&lt;SEP&gt;"The information retrieval process refers to the system designed to assist court professionals, particularly judges, in finding relevant documents."&lt;SEP&gt;"The information retrieval process is a series of steps involved in locating and extracting relevant documents from large collections of data."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;TIME-CONSUMING AND COMPLEX PROCESS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This describes the current state of the information retrieval process for judges, highlighting its challenges."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;RELEVANT DOCUMENTS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Relevant documents refer to the legal materials that are crucial for court professionals in making informed decisions."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;STS TASK.&quot; &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The STS task is a specific evaluation process used to assess the performance of models in determining semantic similarity between sentences."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;DOMAIN ADAPTATION&quot; &quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Domain Adaptation refers to techniques aimed at improving model generalization when there's limited labeled data in the target domain."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;BERTIMBAU LARGE&quot; &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERTimbau large is a variant of BERTimbau, used for comparison and evaluation purposes in this context."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot; &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"stjiris/bert-large-portuguese-cased-legal-mlm is a model variant that was fine-tuned using MLM (Masked Language Model) technique."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot; &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"stjiris/bert-large-portuguese-cased-legal-tsdae is a model variant that was fine-tuned using TSDAE (Temporal Shifted Domain Adaptation Encoder) technique."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;MLM TASK&quot; &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The MLM task involves predicting masked tokens in the input text, used here to evaluate models’ handling of Portuguese legal documents."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;MASKING&quot; &quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Masking refers to the process of replacing words with [MASK] tokens in the MLM task for training models."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;FINE-TUNING&quot; &quot;">
  <data key="d0">"PROCESS"</data>
  <data key="d1">"Fine-tuning involves adjusting model parameters based on specific tasks or datasets, used here to improve performance on STS and MLM tasks."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;PEARSON CORRELATION&quot; &quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Pearson correlation is a statistical measure of the linear relationship between two variables, used here to evaluate model similarity scores in the STS task."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;STATE-OF-THE-ART MULTILINGUAL MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"State-of-the-art multilingual models are described as being outperformed by SBERT variants on the STS task for Portuguese datasets, but not on stsb multi mt dataset."&lt;SEP&gt;"State-of-the-art multilingual models are mentioned to have performed at least equally as well, if not better, than the SBERT variants on the STS task for the stsb multi mt dataset."&lt;SEP&gt;"State-of-the-art multilingual models refer to the current best-performing models in the field of natural language processing used as a benchmark."&lt;SEP&gt;"State-of-the-art multilingual models, including those mentioned like paraphrase-multilingual-mpnet-base-v2, have competitive performance but were outperformed by the SBERT variants on specific Portuguese datasets."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;ASSIN1 AND ASSIN2 DATASETS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"These are Portuguese datasets where the SBERT variants outperformed state-of-the-art multilingual models."&lt;SEP&gt;"These are specific STS benchmark datasets used to evaluate the performance of natural language processing models. They assess the quality of paraphrases in Brazilian Portuguese."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;STSB MULTI MT DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"STSB Multi MT is a dataset composed of multiple multilingual translations from the original STSbenchmark, used to train multilingual models."&lt;SEP&gt;"STSB multi MT is a multilingual translation dataset derived from the original STSbenchmark, focusing on evaluating models across multiple languages and their translations."&lt;SEP&gt;"STSB multi mt is a dataset composed of multilingual translations from the original STSbenchmark, used for training multilingual models."&lt;SEP&gt;"This is a dataset used for evaluating performance on the STS task, composed of different multilingual translations from the original STSbenchmark dataset. It's mentioned that SBERT variants did not outperform state-of-the-art multilingual models on this dataset."&lt;SEP&gt;"stsb multi mt is a dataset composed of different multilingual translations from the original STSbenchmark dataset, and multilingual models did engage with multiple translations from the"&lt;SEP&gt;"stsb multi mt is a dataset composed of different multilingual translations from the original STSbenchmark dataset, and multilingual models did not outperform SBERT variants on this dataset."&lt;SEP&gt;"STSB multi mt is a multilingual version of the STS benchmark dataset, used to test models on multiple language translations."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;ASSIN1&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"These are Portuguese datasets where the SBERT variants outperformed state-of-the-art multilingual models."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;ASSIN2&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"ASSIN2 is a specific dataset used for evaluating the performance of SBERT variants, indicating its importance in the evaluation process."&lt;SEP&gt;"These are Portuguese datasets where the SBERT variants outperformed state-of-the-art multilingual models."&lt;SEP&gt;"assin and assin2 are sub-datasets derived from the STSbenchmark dataset."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;MULTILINGUAL MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Multilingual Models refer to state-of-the-art language models trained on multiple languages, used as a benchmark in the STS evaluation."&lt;SEP&gt;"Multilingual models are a class of language processing tools that can handle multiple languages, compared against Legal-BERTimbau."&lt;SEP&gt;"Multilingual models are general-purpose models designed to handle multiple languages and their specificities in various tasks including sentence similarity evaluation."&lt;SEP&gt;"Multilingual models are mentioned to have outperformed SBERT variants on the stsb multi mt dataset, showing a comparison of performance across different multilingual translation datasets."&lt;SEP&gt;"Multilingual models are state-of-the-art models that can handle multiple languages and are commonly compared against other models in cross-lingual tasks."</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;STSB MULTI MT&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"STSB multi MT is a dataset composed of multiple translations from the original STSbenchmark dataset, used for training multilingual models to handle various languages during the evaluation process."&lt;SEP&gt;"STSB multi MT is another dataset used for evaluating multilingual models, distinct from ASSIN and ASSIN2 datasets."&lt;SEP&gt;"stsb multi mt is a multilingual version of the STS benchmark dataset composed of multiple translations."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;LEXRANK SUMMARIZATION TECHNIQUE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"LexRank is a summarization technique that selects sentences based on their centrality within the text, useful for generating summaries but not ideal for full semantic exploration in queries."&lt;SEP&gt;"LexRank is a technique used for text summarization, which helps in identifying the most important sentences in a document summary."&lt;SEP&gt;"LexRank summarization technique was initially used to retrieve the most important sentence from document summaries but did not fully utilize the semantic search capabilities due to the nature of the approach."&lt;SEP&gt;"LexRank is a summarization technique utilized initially to extract important sentences from document summaries, but did not fully leverage semantic search capabilities."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;GPT3 MODEL PROVIDED BY OPEN AI&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GPT-3 is a powerful language model provided by OpenAI that can rewrite sentences while maintaining their meaning."&lt;SEP&gt;"The GPT3 model generated rewritten sentences with the same meaning, often using exact keywords in different orders, indicating a specific approach to query generation."&lt;SEP&gt;"The GPT3 model was utilized to rewrite sentences while maintaining meaning, though it often used exact keywords in different orders without providing the desired query examples."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;STS BENCHMARK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"STS benchmark refers to a standard dataset used as the original source for STSB multi MT, highlighting its foundational role in evaluations."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;EVALUATION ARCHITECTURE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Evaluation Architecture describes the structure and components of evaluation systems used to assess models and their effectiveness in various metrics."&lt;SEP&gt;"The Evaluation Architecture refers to the framework or design used for evaluating the performance of search systems and their metrics."&lt;SEP&gt;"The evaluation architecture describes the setup and steps involved in assessing the search system's performance, providing a structured approach to testing."&lt;SEP&gt;"Evaluation Architecture refers to the setup for evaluating systems discussed beginning on page 70."&lt;SEP&gt;"The Evaluation Architecture describes the overall process for evaluating the search system's performance using generated queries and sentence embeddings."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;QUERY GENERATION TECHNIQUE 1 (LEXRANK)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"LexRank summarization technique was used for generating queries but did not fully explore semantic capabilities due to its nature."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;SYNONYMS AND SIMILAR EXPRESSIONS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The use of synonyms and similar expressions to replace top 20 keywords from the summary helped preserve meaning while reducing exact keyword reuse, showcasing an approach to query generation and evaluation."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;MULTILINGUAL MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A Multilingual model would perform worse than Legal-BERTimbau in the Semantic Search System."&lt;SEP&gt;"A multilingual model used in a Semantic Search System that performs worse than Legal-BERTimbau."&lt;SEP&gt;"A type of semantic search system using multilingual models that perform worse than Legal-BERTimbau for certain metrics."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;LEXICAL-FIRST APPROACH&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"A Lexical-First approach in Semantic Search Systems which performs closer to BM25 and sometimes surpasses it."&lt;SEP&gt;"A method proposed by the team, which closely matches BM25's performance and sometimes outperforms it."&lt;SEP&gt;"A proposed approach that performs closer to BM25 and sometimes surpasses it, indicating better performance with similar documents."&lt;SEP&gt;"The Lexical-First approach performs closer to BM25 and can occasionally surpass it, indicating its effectiveness."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;LEXICAL+SEMANTIC APPROACH&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"A combination of Lexical and Semantic approaches that outperforms the purely semantic system but does not match or exceed BM25."&lt;SEP&gt;"Another method proposed by the team that can occasionally surpass BM25 in certain metrics."&lt;SEP&gt;"This approach shows competitive performance compared to BM25 and outperforms the general Semantic Search System significantly."&lt;SEP&gt;"The Lexical+Semantic approach maintains a tight performance between BM25 but can outperform it in some cases."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;V0 MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Models fine-tuned on pre-existing and manually annotated datasets, showing better performance in certain metrics compared to V1 models."&lt;SEP&gt;"Models fine-tuned only on pre-existing and manually annotated datasets (V0) generally perform better than V1 models in both the Search and Discovery metrics."::</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;V1 MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Models fine-tuned on a custom STS dataset (V1) perform marginally worse than V0 models in both the Search and Discovery metrics."&lt;SEP&gt;"Models fine-tuned on a custom STS dataset (V1), performing marginally worse than V0 models in some metrics."&lt;SEP&gt;"Models fine-tuned on the custom STS dataset (V1) perform marginally worse than V0 models in both the Search and Discovery metrics."::</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;LEXICAL + SEMANTIC APPROACH&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"A combination of Lexical and Semantic approaches that outperforms the purely semantic system but does not match or exceed BM25."&lt;SEP&gt;"Another method proposed by the team that can occasionally surpass BM25 in certain metrics."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;FIGURE 6.2&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 6.2 is a visual representation where Search System Evaluation – Search metric - Models V0 is presented."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;FIGURE 6.3&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 6.3 is a visual representation where Search System Evaluation – Search metric - Models V1 is presented."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;FIGURE 6.4&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 6.4 is a visual representation where Search System Evaluation - Discovery metric - Models V0 is presented."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;FIGURE 6.5&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 6.5 is a visual representation where Search System Evaluation - Discovery metric - Models V1 is presented."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;PURELY SEMANTIC&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Purely Semantic refers to a method that relies solely on semantic capabilities without combining with lexical techniques, demonstrating varying performance across different metrics."&lt;SEP&gt;"Purely Semantic refers to a search system architecture that relies solely on semantic analysis without considering lexical information."&lt;SEP&gt;"Purely Semantic refers to a technique that relies solely on semantic analysis without combining with lexical methods."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"Purely Semantic refers to another search system architecture that uses only semantic capabilities, evaluated in both Tables 6.3 and 6.4."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;LEXICAL-FIRST&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Lexical-First is a search system architecture focusing on lexical capabilities first, also evaluated in both Tables 6.3 and 6.4."&lt;SEP&gt;"Lexical-First is a search system architecture that prioritizes lexical analysis before integrating semantic elements, indicating specific performance patterns in the evaluation tables."&lt;SEP&gt;"Lexical-First is an approach that prioritizes lexical features before incorporating semantic information in the search process."&lt;SEP&gt;"Lexical-First is an approach that prioritizes lexical analysis before applying semantic techniques."&lt;&lt;relationship_description&gt;</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;LEXICAL + SEMANTIC&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Lexical + Semantic Search System combines both lexical and semantic capabilities, showing competitive or superior performance metrics compared to BM25 in different scenarios."&lt;SEP&gt;"Lexical + Semantic is a combined system that integrates both lexical and semantic techniques to improve search results."&lt;SEP&gt;"Lexical + Semantic combines both lexical and semantic capabilities, providing a hybrid search system."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"Lexical + Semantic refers to the combined approach of using both lexical and semantic capabilities, performing well in Table 6.4 but similar to BM25 in Table 6.3."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;SEARCH METRIC&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Search metric is a performance evaluation method used to assess the Semantic Search System's ability to find the original document from which a query was generated."&lt;SEP&gt;"The Search metric evaluates the performance of different models in terms of Top 1 to Top 10 rankings."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"The Search metric evaluates the performance of search systems based on specific criteria such as Top 1 to Top 10 results, providing quantitative insights into system effectiveness."&lt;SEP&gt;"The Search metric evaluates the system's ability to find which document a certain query refers to, based on the original document tags."</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;OUR WORK&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Our work refers to the research conducted by a team or organization that explored different search system architectures."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;ALEX (MENTIONED IN CONCLUSION) &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Alex is a researcher involved in the project, contributing insights and findings."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;ELASTIC-SEARCH&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Elastic-Search is a platform used to host and manage a semantic search system, providing the infrastructure for semantic queries."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"Elastic-Search is a popular open-source full-text search and analytics engine used for hosting semantic search systems."&lt;SEP&gt;"Elastic-Search refers to the technology used for hosting the semantic search system, indicating its role in system development."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;LEXICAL-FIRST HYBRID SEARCH SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical-First Hybrid Search System is one of the variants developed as part of the thesis work, designed to prioritize lexical search over semantic search."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;LEXICAL+SEMANTIC HYBRID SEARCH SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical+Semantic Hybrid Search System is another variant developed as part of the thesis work, balancing both lexical and semantic search methods."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;BM25 TECHNIQUE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BM25 is a method used in information retrieval to rank search results, compared against the developed Semantic Search System."&lt;SEP&gt;"BM25 is a traditional information retrieval method that was benchmarked against the developed Semantic Search System to evaluate its performance."&lt;SEP&gt;"BM25 technique is a method used in information retrieval to rank search results based on relevance."&lt;SEP&gt;"BM25 technique is used as a baseline comparison for evaluating the performance of the Semantic Search Systems and Lexical-First/Hybrid systems."&lt;SEP&gt;"BM25 technique is a method used to rank search results based on relevance and was compared with Legal-BERTimbau during the evaluation of semantic search systems."&lt;SEP&gt;"BM25 is a traditional ranking algorithm used in Information Retrieval, often compared with the performance of new models to gauge their effectiveness."&lt;SEP&gt;"BM25 technique is an algorithm used to evaluate the relevance of documents based on keywords, employed here as part of the Hybrid Search System."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;ROUGE-1 SCORE, ROUGE-2 SCORE&quot;">
  <data key="d0">"MEASURE"</data>
  <data key="d1">"ROUGE-1 and ROUGE-2 scores are metrics used to evaluate the quality of generated summaries, with ROUGE-1 being more focused on unigrams and ROUGE-2 on bigrams."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;SUPPLEMENTARY INFORMATION SYSTEMS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Supplementary Information Systems include tools or methods that complement the primary Semantic Search System, enhancing its overall performance."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;LEGAL-BERTIMBAU MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Legal-BERTimbau is a model used for research with specific scores on ROUGE-1 and ROUGE-2 metrics."&lt;SEP&gt;"Legal-BERTimbau models are advanced NLP models used for semantic search and research."&lt;SEP&gt;"Legal-BERTimbau models are research-based language models used to achieve specific scores and techniques."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;ROUGE-1 SCORE OF 47.92 AND A ROUGE-2 SCORE OF 22.50&quot;">
  <data key="d0">"METRIC"</data>
  <data key="d1">"The metrics indicate the performance of Legal-BERTimbau in comparison to similar work, showing improved results."&lt;SEP&gt;"The metrics indicate the performance of Legal-BERTimbau models in comparison to similar work, showing improved results."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"EPIA Conference on Artificial Intelligence is an established European conference focusing on AI research and development."&lt;SEP&gt;"The EPIA Conference on Artificial Intelligence is an accepted venue for the submission and presentation of scientific papers in AI research."&lt;SEP&gt;"This is a well-established European conference in AI to which a paper was submitted and accepted."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;ALBERTINA PT -PT&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Albertina PT -PT refers to a model specifically trained for the Portuguese jurisprudence, using European Portuguese language."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;THESIS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The thesis is a research document that covers the main aspects of the legal-BBERTimbau models and their impact."&lt;SEP&gt;"The thesis is a research document that covers the main aspects of the legal-BERTimbau models and their impact."&lt;SEP&gt;"The thesis is a research work that aims to develop a reliable search system for court decisions."&lt;</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;PAPER&quot;">
  <data key="d0">"PUBLICATION"</data>
  <data key="d1">"A scientific paper was drafted covering the main aspects of this research, including the hybrid system and training techniques."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;METADATA KNOWLEDGE DISTILLATION TECHNIQUE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Metadata Knowledge Distillation technique is highlighted as a novel method used in training Legal-BBERTimbau, bringing good results."&lt;SEP&gt;"The Metadata Knowledge Distillation technique is highlighted as a novel method used in training Legal-BERTimbau, bringing good results."&lt;SEP&gt;"This novel technique was used in training Legal-BERTimbau and showed good results."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;HYBRID SEARCH SYSTEM&quot;">
  <data key="d0">"SYSTEM"</data>
  <data key="d1">"A combination of search techniques that can match or outperform BM25 capabilities."&lt;SEP&gt;"The Hybrid Search System combines pure semantic search and knowledge graph embeddings for better query expansion."&lt;SEP&gt;"The Hybrid Search System combines lexical search techniques with the reach of large language models like Legal-BERTimbau."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;KNOWLEDGE GRAPH EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Incorporating knowledge graph embeddings or entity recognition can lead to a more sophisticated and accurate Search System that aids legal practitioners in research and decision-making."&lt;SEP&gt;"Knowledge Graph Embeddings are a potential improvement to the Hybrid Search System, helping in understanding the context of queries."&lt;SEP&gt;"Knowledge graph embeddings are a method for representing structured knowledge which can enhance the Search System’s accuracy and sophistication."&lt;SEP&gt;"Knowledge Graph Embeddings are used to incorporate knowledge into the system, potentially leading to more sophisticated and accurate search results."&lt;SEP&gt;"These are structured representations used for better context understanding in query expansion systems."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;ACTIVE LEARNING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Active learning is a machine learning technique enabling continuous improvement through user feedback."&lt;SEP&gt;"Active learning is a machine learning technique that enables the system to continuously learn from user’s feedback, improving its performance over time."&lt;SEP&gt;"Active learning is a machine learning technique that enables the system to continuously learn from user’s feedback, helping in document retrieval."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;GLM (GENERAL LANGUAGE MODEL)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"GLM is a general language model that includes advanced algorithms for text generation and understanding tasks."&lt;SEP&gt;"GLM refers to a class of language models, with GPT-3 as an example, used in text generation and understanding tasks."&lt;SEP&gt;"GLM refers to a general language model such as GPT-3 used in various tasks like generating responses based on user queries."&lt;SEP&gt;"GLM refers to a general language model like GPT-3 that is used for generating responses based on user queries."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;GPT-3&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"GPT-3 is a specific instance of GLM that has been used for various natural language processing tasks."&lt;SEP&gt;"GPT-3 is an advanced machine learning model, part of the GLM family, designed for generating natural language responses to complex queries."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;SEARCH SYSTEM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A search system was developed to assist judges in court decision processes, a contribution from this work."&lt;SEP&gt;"The Search System is a tool or application designed to retrieve relevant documents based on user queries, enhanced by techniques such as entity recognition and knowledge graph embeddings."&lt;SEP&gt;"The Search System is a tool that could benefit from entity recognition and knowledge graph embeddings to improve document retrieval accuracy."&lt;SEP&gt;"The Search System refers to a system that uses techniques like entity recognition and active learning to provide relevant document retrieval and improved user interaction."&lt;SEP&gt;"The search system refers to the technology or methodology used for information retrieval in legal contexts, specifically tailored for Portuguese legal documents."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;ENTITY RECOGNITION&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Entity Recognition is a method for identifying named entities such as people, organizations, and locations within text, which can help expand queries with relevant entities."&lt;SEP&gt;"Entity Recognition is a technique for identifying named entities such as people, organizations, and locations in given text, which can help expand queries by including relevant entities."&lt;SEP&gt;"Entity recognition is a technique that identifies named entities such as people, organizations, and locations in text."&lt;SEP&gt;"Entity recognition identifies named entities such as people, organizations, and locations in a given text. It helps expand queries by including relevant entities."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Semantic Textual Similarity is a shared task related to evaluating how similar two pieces of text are semantically."&lt;SEP&gt;"Semantic textual similarity is a task that involves determining the degree to which pairs of sentences have similar meaning."&lt;SEP&gt;"Semantic textual similarity measures the degree of semantic equivalence between pieces of text."&lt;SEP&gt;"Semantic Textual Similarity refers to the task of determining the similarity between two pieces of text based on their semantic meaning rather than surface-level differences."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce&lt;SEP&gt;chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;SEMEVAL-2015 TASK 2: SEMANTIC TEXTUAL SIMILARITY, ENGLISH, SPANISH AND PILOT ON INTERPRETABILITY&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2015 task 2 focused on semantic textual similarity in multiple languages with an emphasis on interpretability."&lt;SEP&gt;"This event refers to a specific task in the SemEval 2015 workshop focusing on semantic textual similarity across different languages and interpretability."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;SEMEVAL-2014 TASK 10: MULTILINGUAL SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2014 Task 10 was a task within a semantic evaluation workshop where participants explored multilingual semantic textual similarity."&lt;SEP&gt;"SemEval-2014 task 10 involved tasks related to multilingual semantic textual similarity, which helped in evaluating the performance of systems across multiple languages."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;SEMEVAL-2016 TASK 1: SEMANTIC TEXTUAL SIMILARITY, MONOLINGUAL AND CROSS-LINGUAL EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2016 Task 1 was a task within a semantic evaluation workshop where participants explored monolingual and cross-lingual evaluations of semantic textual similarity."&lt;SEP&gt;"This event included tasks for both monolingual and cross-lingual semantic textual similarity, aiding in the evaluation of models across different languages."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;*SEM 2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"*SEM 2013 Shared Task was a shared task within the Second Joint Conference on Lexical and Computational Semantics where participants explored semantic textual similarity."&lt;SEP&gt;"The *SEM 2013 shared task focused on semantic textual similarity, providing a platform for researchers to evaluate their systems."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;SEMEVAL-2016&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2016 is an international workshop held in San Diego, California, focusing on semantic evaluation."&lt;SEP&gt;"SemEval-2016 is an international workshop held in San Diego, California, focusing on semantic evaluation."&lt;SEP&gt;"SemEval-2016 was an international workshop focused on semantic evaluation held in San Diego, California, in 2016."&lt;SEP&gt;"SemEval-2016 is an international workshop on semantic evaluation held in San Diego, California, in 2016."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SAN DIEGO, CALIFORNIA&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"San Diego, California hosted the 10th International Workshop on Semantic Evaluation (SemEval-2016)."&lt;SEP&gt;"San Diego, California is the location where SemEval-2016 took place."&lt;SEP&gt;"San Diego, California is the location where SemEval-2016 took place."&lt;SEP&gt;"San Diego, California is the location where SemEval-2016 was held."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;ATLANTA, GEORGIA, USA&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Atlanta, Georgia, USA hosted the *SEM 2013 conference where Semantic Textual Similarity was part of the shared tasks."&lt;SEP&gt;"Atlanta, Georgia, USA is the location where the Second Joint Conference on Lexical and Computational Semantics (SemEval-2013) took place."&lt;SEP&gt;"Atlanta, Georgia, USA is the location where the 2013 SEM conference took place."&lt;SEP&gt;"Atlanta, Georgia, USA is the location where SemEval-2013 was held."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SEMEVAL-2012 TASK 6&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2012 task 6 focused on a pilot for semantic textual similarity."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;BRWAC: A WACKY CORPUS FOR BRAZILIAN PORTUGUESE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"brWaC is a corpus created for Brazilian Portuguese, highlighting its unique nature."&lt;SEP&gt;"brWaC is a corpus created for research in Brazilian Portuguese language processing."&lt;SEP&gt;"brWaC is a corpus created for the study of Brazilian Portuguese language, used in computational processing tasks."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SEMEVAL-2017 TASK 1&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2017 task 1 evaluated semantic textual similarity across multiple languages and focused on multilingual and crosslingual evaluation."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;A LARGE ANNOTATED CORPUS FOR LEARNING NATURAL LANGUAGE INFERENCE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The dataset is a significant annotated corpus used in the study of natural language inference tasks."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SEMEVAL-2017&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2017 was an international workshop focusing on semantic evaluation, including the task 1 for multilingual and crosslingual focused evaluation."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;VANCOUVER, CANADA&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Vancouver, Canada hosted the SemEval-2017 event."&lt;SEP&gt;"Vancouver, Canada is the location where the 11th International Workshop on Semantic Evaluation (SemEval-2017) took place."&lt;SEP&gt;"Vancouver, Canada is the location where SemEval-2017 took place."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;*SEM 2013 SHARED TASK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The *SEM 2013 shared task involved semantic textual similarity."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;COPENHAGEN, DENMARK&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Copenhagen, Denmark hosted the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;CONNEAU, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"A Conneau is a researcher involved in tasks related to semantic textual similarity."&lt;SEP&gt;"Aurma Conneau is an author involved in various SemEval tasks and BERT research."&lt;SEP&gt;"Aurko Conneau is an author involved in tasks and workshops related to semantic textual similarity and natural language processing."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;KIEL, D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Daniel Kiefer is an author involved in BERT research."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SCHWENK, H.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"H Schwenk has worked on supervised learning of universal sentence representations and contributed to BERT pre-training."&lt;SEP&gt;"Holger Schwenk is an author involved in BERT research."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;BARRAU, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Léo Barrau is an author involved in BERT research."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;BORDES, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"A Bordes has been involved in the creation and training of deep bidirectional transformers such as BERT."&lt;SEP&gt;"Antoine Bordes is an author involved in BERT research."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;DAI, Z.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zhilin Dai is an author involved in natural language processing and IR with contextual neural modeling."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;CALLAN, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Callan is an author of a research paper on deeper text understanding for information retrieval with contextual neural language modeling."&lt;SEP&gt;"Callan is an author of a text on deeper text understanding for IR with contextual neural modeling in 2019."&lt;SEP&gt;"James Callan is an author involved in natural language processing and IR with contextual neural modeling."&lt;SEP&gt;"Callan is an author of a text understanding paper for information retrieval using a neural language model."&lt;SEP&gt;"Callan, J. is an author of a text on deeper text understanding for IR with contextual neural modeling."&lt;SEP&gt;"Callan is the author of a text on deeper text understanding for IR with contextual neural language modeling."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8&lt;SEP&gt;chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;DEVLIN, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Devlin co-authored BERT: Pre-training of deep bidirectional transformers for language understanding."&lt;SEP&gt;"Devlin is the co-author of BERT, a pre-trained deep bidirectional transformer for language understanding."&lt;SEP&gt;"Jacob Devlin is an author behind the BERT model."&lt;SEP&gt;"Jacob Devlin authored BERT, a pre-trained bidirectional transformer model for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8&lt;SEP&gt;chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;CHANG, M.-W.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Chang co-authored BERT: Pre-training of deep bidirectional transformers for language understanding."&lt;SEP&gt;"Chang is a co-author of BERT: Pre-training of deep bidirectional transformers for language understanding."&lt;SEP&gt;"Chang is one of the authors behind BERT along with Devlin."&lt;SEP&gt;"Minh-Thang Luu Chang is an author involved in the BERT project."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8&lt;SEP&gt;chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;LEE, K.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kwang-Ho Lee is an author involved in the BERT project."&lt;SEP&gt;"Lee is part of the team that developed BERT alongside Devlin and Chang."&lt;SEP&gt;"Lee co-authored BERT: Pre-training of deep bidirectional transformers for language understanding."&lt;SEP&gt;"Lee is a co-author of BERT: Pre-training of deep bidirectional transformers for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8&lt;SEP&gt;chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;TOUTANOVA, K.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kristina Toutanova is an author involved in the BERT project."&lt;SEP&gt;"Toutanova co-authored BERT with Devlin, Chang, and Lee."&lt;SEP&gt;"Toutanova is a co-author of BERT: Pre-training of deep bidirectional transformers for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8&lt;SEP&gt;chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;CORDEIRO, N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N Cordeiro is a researcher who applied NLP to Portuguese consumer law in her master’s thesis."&lt;SEP&gt;"Nuno Cordeiro is a researcher who applied NLP to Portuguese consumer law."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;TECNICO, INSTITUTO SUPERIOR&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Instituto Superior Tecnico (IST) of the University of Lisbon is an educational and research institution involved in NLP projects."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;LISBON, PORTUGAL&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Lisbon, Portugal hosted some of the events related to SemEval tasks and BERT research."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;DEVLIN, J., CHANG, M.-W., LEE, K., TOUTANOVA, K.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These individuals are authors of the BERT paper, which introduced pre-training of deep bidirectional transformers for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ERKCAN, G., RAEDEV, D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Erkcan and Raedev developed LexRank, a graph-based lexical centrality as salience in text summarization in 2004."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;FONSECA, E., SANTOS, L., CRISCUOLO, M., ALUISIO, S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These authors introduced the ASSIN project for evaluating semantic similarity and textual inference in 2016."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GURURARANG, S., MARASOVIC, A., SWAYAMDIPATA, S., LO, K., BELTAGY, I., DOWNEY, D., SMITH, N. A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"This group published a paper on Don't stop pretraining: Adapt language models to domains and tasks in 2020."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;HE, P., LIU, X., GAO, J., CHEN, W.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These authors presented DeBERTA, Decoding-enhanced BERT with disentangled attention at ICLR 2021."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;HENDERSON, P., KRAS, M. S., ZHENG, L., GUHA, N., MANNING, C. D., JURAFSKY, D., HO, D. E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"This group introduced the Pile of Law project for learning responsible data filtering from the law and a 256GB open-source legal dataset in 2022."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;INTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY AND EXPLORING ENGINEERING (IJITEE), VAISSNA, V., DEEPALAKSHMI, P.&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"IJITEE is an organization that published a paper on AI-based analysis in the legal domain."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KIM, M., RABELO, J., GOEBEL, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These authors worked on BM25 and transformer-based legal information extraction and entailment in 2021."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KIM, S.-W., GIL, J.-M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"This pair developed research paper classification systems based on TF-IDF and LDA schemes in 2019."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KINGMA, D. P., BA, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These authors introduced the Adam optimization method for stochastic optimization in 2015."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MARELLI, M., MENINI, S., BARONIS, M., BENTIVOGLI, L., BERNARDI, R., ZAMPARELLI, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"This group introduced the SICK dataset for natural language understanding tasks in 2015."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SICK CURE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The term 'Sick cure' refers to the introduction of the SICK dataset, which is an event marking a significant contribution to NLP research."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ICLR 2015&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"ICLR 2015 is an International Conference held in San Diego, CA, USA from May 7-9, 2015."&lt;SEP&gt;"ICLR 2015 is an event held in San Diego, CA, USA from May 7-9, 2015."&lt;SEP&gt;"ICLR 2015 is an event held in San Diego, CA, USA, focusing on language and computational linguistics research."&lt;SEP&gt;"ICLR 2015 is an event held in San Diego, CA, USA from May 7-9, 2015, focusing on machine learning research."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;SAN DIEGO, CA, USA&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"San Diego, CA, USA hosted the ICLR 2015 conference where discussions and proceedings took place."&lt;SEP&gt;"San Diego, CA, USA hosted the ICLR 2015 conference, a significant event for researchers in natural language processing."&lt;SEP&gt;"San Diego, CA, USA is the location where ICLR 2015 was held."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;Y. BENGIO AND Y. LECUN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Y. Bengio and Y. LeCun are co-editors of ICLR 2015 proceedings."&lt;SEP&gt;"Y. Bengio and Y. LeCun are the editors of ICLR 2015 proceedings."&lt;SEP&gt;"Y. Bengio and Y. LeCun are key figures in the field of machine learning and neural networks, contributing significantly to both theoretical research and practical applications."&lt;SEP&gt;"Y. Bengio and Y. LeCun are key figures in machine learning research, editing important conference proceedings."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;MACHINE TRANSLATED MULTILINGUAL STS BENCHMARK DATASET&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Machine translated multilingual STS benchmark dataset is a dataset created by P. May for evaluating semantic text similarity."&lt;SEP&gt;"The organization HuggingFace created the Machine translated multilingual STS benchmark dataset."&lt;SEP&gt;"This is a specific technology or dataset used for evaluating machine translation and text similarity tasks."&lt;SEP&gt;"This dataset is a machine-translated multilingual STS benchmark used for evaluating semantic textual similarity."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;1ST INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS, ICLR 2013&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"ICLR 2013 is an international conference held in Scottsdale, Arizona, USA from May 2-4, 2013."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;T. MIKOLOV AND CO-AUTHORS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"T. Mikolov and other authors published a paper on efficient estimation of word representations in vector space at ICLR 2013."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;GLOVE: GLOBAL VECTORS FOR WORD REPRESENTATION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GloVe is a specific algorithm used for generating word embeddings, contributing to the field of natural language processing."&lt;SEP&gt;"GloVe is a technology published by J. Pennington, R. Socher, and C. Manning that provides global vectors for word representation."&lt;SEP&gt;"This paper introduced GloVe, a method for learning word vector representations from data."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;J. PENNINGTON, R. SOCHER, AND C. MANNING&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. Pennington, R. Socher, and C. Manning are the authors of the GloVe technology."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;M. ARELLI ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"M. Arelli, M. Menini, M. Baron, L. Bentivogli, R. Bernardi, and R. Zamparelli are researchers involved in the SICK project."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;SICK&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SICK is a dataset for evaluating compositional distributional semantic models."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;P. MAY&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"P. May is a researcher associated with the machine translated multilingual STS benchmark dataset."&lt;SEP&gt;"P. May is the creator of the Machine translated multilingual STS benchmark dataset."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;H. NGUYEN ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"H. Nguyen and co-authors from N Guyen et al.'s group published a paper on deep learning for legal processing in COLIEE 2020."&lt;SEP&gt;"H. Nguyen, H. Vuong, P. M. Nguyen, T. B. Dang, Q. M. Bu, V. T. Sinh, C. M. Nguyen, V. D. Tran, K. Satoh, and M. L. Nguyen are authors of the JNLP team paper."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;JNLP TEAM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The JNLP team is a group that focuses on deep learning for legal processing in COLIEE 2020."&lt;SEP&gt;"The JNLP team is a research group that published papers on deep learning for legal processing in COLIEE 2020."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;P. ROSENBERG, X. SONG, J. GAO, S. TIWARY, R. MAJUMDER, AND L. DENG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These individuals are authors of the MS MARCO dataset."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;MS MARCO&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"MS MARCO is a dataset used for training the T5 model."&lt;SEP&gt;"MS MARCO is a human-generated machine reading comprehension dataset."&lt;SEP&gt;"MS MARCO is the dataset used for fine-tuning the T5 model in GenQ. It helps improve the model’s performance on specific NLP tasks."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;S. K. NIGAM ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"S. K. Nigam, N. Goel, and A. Bhattacharya are authors of the nigam@coliee-22 paper."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;NIGAM@COLIEE-22&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"S. K. Nigam and co-authors from nigam@coliee-22's group published a paper on legal case retrieval and entailment using cascading models in COLIEE 2022."&lt;SEP&gt;"The nigam@coliee-22 paper focuses on legal case retrieval and entailment using cascading models."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;C. RAFFEL ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"C. Raffel and co-authors published a paper on exploring limits of transfer learning using a unified text-to-text transformer."&lt;SEP&gt;"C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu are authors of the unified text-to-text transformer paper."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;EXPLORING THE LIMITS OF TRANSFER LEARNING WITH A UNIFIED TEXT-TO-TEXT TRANSFORMER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This paper discusses transfer learning using unified text-to-text transformers."&lt;SEP&gt;"This technology was published by C. Raffel et al."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;ANDLIU, P. J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"ANDLIU, P. J. is an author of a paper on exploring the limits of transfer learning with a unified text-to-text transformer."&lt;SEP&gt;"ANDLIU, P. J. is an author of a research paper on transfer learning with a unified text-to-text transformer."&lt;SEP&gt;"ANDLIU, P. J. is an author of the paper 'Exploring the limits of transfer learning with a unified text-to-text transformer' published in Journal of Machine Learning Research."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;REAL, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"REAL, L. co-authored 'The assin 2 shared task: a quick overview' with Fonseca and Oliveira."&lt;SEP&gt;"REAL, L. is involved in the ASIN 2 shared task, contributing to computational processing of Portuguese language."&lt;SEP&gt;"REAL, L. is involved in the work 'The assin 2 shared task: a quick overview', which discusses the ASSIN-2 shared task at an international conference."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;FONSECA, E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"FONSECA, E. is a co-author on the ASIN 2 shared task and contributes to computational processing of Portuguese language."&lt;SEP&gt;"Fonseca is the author of ASSIN, a system for semantic similarity and textual inference evaluation."&lt;SEP&gt;"FONSECA, E., along with H. G. OLIVEIRA, contributed to 'The assin 2 shared task: a quick overview', which provides an overview of the ASSIN-2 shared task at an international conference."&lt;SEP&gt;"Fonseca is involved in the ASSIN project focusing on semantic similarity and textual inference."&lt;SEP&gt;"Fonseca was involved in the ASSIN project focusing on semantic similarity and textual inference."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8&lt;SEP&gt;chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;OLIVEIRA, H. G.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"OLIVEIRA, H. G. is involved in the ASIN 2 shared task, contributing to computational processing of Portuguese language."&lt;SEP&gt;"OLIVEIRA, H. G., along with E. FONSECA, contributed to 'The assin 2 shared task: a quick overview', which discusses the ASSIN-2 shared task at an international conference."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;REIMERS, N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"REIMERS, N. continues to work on sentence embeddings and knowledge distillation techniques with GUREVYCH, I."&lt;SEP&gt;"REIMERS, N. has contributed significantly to sentence embeddings using BERT and knowledge distillation techniques."&lt;SEP&gt;"REIMERS, N. has co-authored several papers including 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks' and 'Making monolingual sentence embeddings multilingual using knowledge distillation'."&lt;SEP&gt;"REIMERS, N. is an author of 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks', 'Making monolingual sentence embeddings multilingual using knowledge distillation', and 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;GUREVYCH, I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"GUREVYCH, I. has co-authored several papers with N. REIMERS, including 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks' and 'Making monolingual sentence embeddings multilingual using knowledge distillation'."&lt;SEP&gt;"GUREVYCH, I. is involved in both BERT-based language models and zero-shot evaluation benchmarks like BEIR."&lt;SEP&gt;"GUREVYCH, I. works closely with REIMERS, N. on developing multilingual sentence embeddings through various methods."&lt;SEP&gt;"GUREVYCH, I. is an author of 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks', 'Making monolingual sentence embeddings multilingual using knowledge distillation', and 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;ROBERTSON, S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"ROBERTSON, S. and Zaragoza, H. provide insights into the Probabilistic Relevance Framework in information retrieval."&lt;SEP&gt;"ROBERTSON, S., along with H. ZARAGOZA, co-authored the paper 'The Probabilistic Relevance Framework: BM25 and Beyond' published in Found. Trends Inf. Retr."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;ZARAGOZA, H.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"ZARAGOZA, H. co-authored with Robertson on the Probabilistic Relevance Framework, contributing to information retrieval methods."&lt;SEP&gt;"ZARAGOZA, H., along with S. ROBERTSON, co-authored the paper 'The Probabilistic Relevance Framework: BM25 and Beyond' published in Found. Trends Inf. Retr."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;RODRIGUES, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"RODRIGUES, J. contributed to the work 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*' published in 2023."&lt;SEP&gt;"RODRIGUES, J. is involved in advancing neural encoding of Portuguese language using Transformer Albertina PT."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;GOMES, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"GOMES, L. collaborates with Rodrigues and others on linguistic advancements for Portuguese."&lt;SEP&gt;"GOMES, L., along with J. RODRIGUES, J. SILVA, A. BRANCO, R. SANTOS, H. L. CARDOSO, and T. OS´ORIO, contributed to 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*' published in 2023."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;SILVA, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"SILVA, J. is part of the team advancing neural encoding for Portuguese language."&lt;SEP&gt;"SILVA, J., along with J. RODRIGUES, L. GOMES, A. BRANCO, R. SANTOS, H. L. CARDOSO, and T. OS´ORIO, contributed to 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*' published in 2023."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;BRANCO, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"BRANCO, A. contributes to linguistic projects focused on Portuguese and Transformers."&lt;SEP&gt;"BRANCO, A., along with J. RODRIGUES, L. GOMES, J. SILVA, R. SANTOS, H. L. CARDOSO, and T. OS´ORIO, contributed to 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*' published in 2023."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;SANTOS, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"SANTOS, R. collaborates with other researchers in developing Portuguese language models."&lt;SEP&gt;"SANTOS, R., along with J. RODRIGUES, L. GOMES, J. SILVA, A. BRANCO, H. L. CARDOSO, and T. OS´ORIO, contributed to 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*' published in 2023."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;CARDOSO, H. L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"CARDOSO, H. L. is involved in linguistic research related to Portuguese language."&lt;SEP&gt;"CARDOSO, H. L., along with J. RODRIGUES, L. GOMES, J. SILVA, A. BRANCO, R. SANTOS, and T. OS´ORIO, contributed to 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*' published in 2023."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;OS´ORIO, T.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"OS´ORIO, T. contributes to linguistic projects focused on developing neural encodings for Portuguese."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;RONG, X.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"RONG, X. explains the learning process of Word2Vec in a research paper."&lt;SEP&gt;"RONG, X., is the author of 'Word2Vec Parameter Learning Explained' and published it in CoRR."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;SOUZA, F.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"SOUZA, F. has contributed to multiple papers including 'BERTimbau: Pretrained BERT Models for Brazilian Portuguese' and 'Portuguese named entity recognition using BERT -CRF'."&lt;SEP&gt;"SOUZA, F. is involved in creating BERTimbau models for Brazilian Portuguese and named entity recognition tasks."&lt;SEP&gt;"SOUZA, F. is an author of 'BERTimbau: Pretrained BERT Models for Brazilian Portuguese' and 'Portuguese named entity recognition using BERT-CRF.'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;NOGUEIRA, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N OGUEIRA , R. is an author of 'BERTimbau: Pretrained BERT Models for Brazilian Portuguese' and 'Portuguese named entity recognition using BERT-CRF.'"&lt;SEP&gt;"NOGUEIRA, R. collaborates with Souza and others on developing BERT-based language models."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;DE ALENCAR LOTUFO, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"DE ALENCAR LOTUFO, R. works closely with Souza and NOGUEIRA on Portuguese NER using BERT-CRF methods."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;THAKUR, N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"THAKUR, N. is an author of 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models'"&lt;SEP&gt;"THAKUR, N. is involved in the BEIR benchmark for evaluating information retrieval models in a zero-shot setting."&lt;SEP&gt;"THAKUR, N., along with N. REIMERS, A. R ¨UCKL ´E, A. SRIVASTAVA, and I. GUREVYCH, contributed to 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models' which was published in the 35th Conference on Neural Information Processing Systems Datasets and Benchmarks Track."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;RUCKLE, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"RUCKLE, A. collaborates with Thakur and others in the BEIR benchmark project."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;SRIVASTAVA, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"SRIVASTAVA, A. contributes to the BEIR benchmark along with Thakur, RUCKLE, and GUREVYCH."&lt;SEP&gt;"SRIVASTAVA, A. is an author of 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;VASWANI, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"VASWANI, A. co-authored the influential paper 'Attention is All You Need' on transformer architectures."&lt;SEP&gt;"VASWANI, A., along with N. SHAZEER, N. PARMAR, J. U SZKOREIT, L. JONES, A. N. G OMEZ, L.U. K AISER, and I. POLOSUKHIN, co-authored 'Attention is all you need' which was published in Advances in Neural Information Processing Systems."&lt;SEP&gt;"VASWANI, A. is an author of 'Attention is all you need'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;HAZEER, N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"HAZEER, N. collaborated with Vaswani and others in developing transformer models."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;PARMAR, N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"PARMAR , N. is a co-author of 'Attention is all you need' with Vaswani and Shazeer."&lt;SEP&gt;"PARMAR, N. co-authored the influential paper 'Attention is All You Need' on transformer architectures."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;U SZKOREIT, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"U SZKOREIT, J. collaborated with Vaswani and others in developing transformer models."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;JONES, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J ONES , L. is a co-author of 'Attention is all you need' with Vaswani, Hazeeer, Parma, and Uzuki."&lt;SEP&gt;"JONES, L. contributed to the influential paper 'Attention is All You Need' on transformer architectures."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;GOMEZ, A. N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"G OMEZ , A. N. is a co-author of 'Attention is all you need' with Vaswani, Hazeeer, Parma, Jones, and Uzuki."&lt;SEP&gt;"GOMEZ, A. N. collaborated with Vaswani and others in developing transformer models."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;KAISER, L.U.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"K AISER , L.U. is a co-author of 'Attention is all you need' with Vaswani, Hazeeer, Parma, Jones, Gomez, and Uzuki."&lt;SEP&gt;"KAISER, L.U. contributed to the influential paper 'Attention is All You Need' on transformer architectures."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;POLOSUKHIN, I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"POLOSUKHIN , I. is a co-author of 'Attention is all you need' with Vaswani, Hazeeer, Parma, Jones, Gomez, and Kaiser."&lt;SEP&gt;"POLOSUKHIN, I. collaborated with Vaswani and others in developing transformer models."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;VIRTANEN, P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"V ITANEN , P. is the author of multiple libraries or tools related to scientific computing in Python"&lt;SEP&gt;"VIRTANEN, P. is involved in Python library development related to scientific computing."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;GOMMERS, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"G OMMERS , R. is a co-author with Virtanen on multiple projects related to scientific computing in Python"&lt;SEP&gt;"GOMMERS, R. collaborates with Virtanen on developing numerical and array programming tools for Python."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;OLIPHANT, T. E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"O LIPHANT , T. E. is the creator of NumPy, a fundamental package for numerical computations in Python."&lt;SEP&gt;"OLIPHANT, T. E. is involved in the development of numerical and array programming tools for Python."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;HABERLAND, M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"H ABERLAND , M. is an author with Virtanen on multiple projects related to scientific computing in Python"&lt;SEP&gt;"HABERLAND, M. collaborates with Virtanen on developing scientific computing libraries like NumPy."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;REDDY, T.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"R EDDY , T. is the creator of SciPy, a library for scientific and technical computing that complements NumPy."&lt;SEP&gt;"REDDY, T. contributes to the development of numerical and array programming tools for Python."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;COURNAPEAU, D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"C OURNAPEAU , D. is the creator of Scikit-learn, a simple and efficient tool for data mining and data analysis in Python"&lt;SEP&gt;"COURNAPEAU, D. collaborates with Virtanen on developing scientific computing libraries like NumPy."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;Y.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Y. is an author of research papers on transfer learning with a unified text-to-text transformer."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;L I&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"L I is an author of research papers on transfer learning with a unified text-to-text transformer."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;W.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"W. is an author of research papers on transfer learning with a unified text-to-text transformer."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;N., K AISER &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N. Kaiser is an author of a research paper."&lt;SEP&gt;"N., K AISER is a contributor to the Attention is all you need paper."&lt;SEP&gt;"N., K AISER is an author of the 'Attention is all you need' paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;L.U. ANDPOLOSUKHIN , I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"L.U. ANDPOLOSUKHIN and I. are co-authors of the 'Attention is all you need' paper."&lt;SEP&gt;"L.U. ANDPOLOSUKHIN, I. are contributors to the Attention is all you need paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;V IRTANEN , P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"P. V IRTANEN is a contributor to SciPy 1.0."&lt;SEP&gt;"P. V IRTANEN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"P. V IRTANEN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"P. Virtanen is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;G OMMERS , R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"R. G OMMERS is a contributor to SciPy 1.0."&lt;SEP&gt;"R. G OMMERS is an author of the SciPy 1.0 paper."&lt;SEP&gt;"R. G OMMERS is an author of the SciPy 1.0 paper."&lt;SEP&gt;"R. Gommers is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;O LIPHANT , T. E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"T. E. O LIPHANT is a contributor to SciPy 1.0."&lt;SEP&gt;"T. E. O LIPHANT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"T. E. O LIPHANT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"T. E. Oliphant is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;H ABERLAND , M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"M. H ABERLAND is a contributor to SciPy 1.0."&lt;SEP&gt;"M. H ABERLAND is an author of the SciPy 1.0 paper."&lt;SEP&gt;"M. H ABERLAND is an author of the SciPy 1.0 paper."&lt;SEP&gt;"M. Haberland is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;R EDDY , T.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"T. R EDDY is a contributor to SciPy 1.0."&lt;SEP&gt;"T. R EDDY is an author of the SciPy 1.0 paper."&lt;SEP&gt;"T. R EDDY is an author of the SciPy 1.0 paper."&lt;SEP&gt;"T. Reddy is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;C OURNAPEAU , D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"D. C OURNAPEAU is a contributor to SciPy 1.0."&lt;SEP&gt;"D. C OURNAPEAU is an author of the SciPy 1.0 paper."&lt;SEP&gt;"D. C OURNAPEAU is an author of the SciPy 1.0 paper."&lt;SEP&gt;"D. Couraianne is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;BUROVSKI , E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"E. B UROVSKI is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. BUROVSKI is a contributor to SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;P ETERSON , P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"P. P ETERSON is a contributor to SciPy 1.0."&lt;SEP&gt;"P. P ETERSON is an author of the SciPy 1.0 paper."&lt;SEP&gt;"P. P ETERSON is an author of the SciPy 1.0 paper."&lt;SEP&gt;"P. Peterson is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;W ECKESSER , W.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"W. W ECKESSER is a contributor to SciPy 1.0."&lt;SEP&gt;"W. W ECKESSER is an author of the SciPy 1.0 paper."&lt;SEP&gt;"W. W ECKESSER is an author of the SciPy 1.0 paper."&lt;SEP&gt;"W. Weckesser is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;B RIGHT , J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. B RIGHT is a contributor to SciPy 1.0."&lt;SEP&gt;"J. BRIGHT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"J. Bright is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;VAN DER WALT, S. J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"S. J. VAN DER WALT is a contributor to SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;B RETT , M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"M. B RETT is a contributor to SciPy 1.0."&lt;SEP&gt;"M. B RETT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"M. B RETT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"M. Brett is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;WILSON , J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. W ILSON is an author of the SciPy 1.0 paper."&lt;SEP&gt;"J. WILSON is a contributor to SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;M ILLMAN , K. J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"K. J. M ILLMAN is a contributor to SciPy 1.0."&lt;SEP&gt;"K. J. M ILLMAN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"K. J. M ILLMAN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"K. J. Millman is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;M AYOROV , N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N. M AYOROV is a contributor to SciPy 1.0."&lt;SEP&gt;"N. M AYOROV is an author of the SciPy 1.0 paper."&lt;SEP&gt;"N. M AYOROV is an author of the SciPy 1.0 paper."&lt;SEP&gt;"N. Mayorov is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;N ELSON , A. R. J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"A. R. J. N ELSON is a contributor to SciPy 1.0."&lt;SEP&gt;"A. R. J. N ELSON is an author of the SciPy 1.0 paper."&lt;SEP&gt;"A. R. J. N ELSON is an author of the SciPy 1.0 paper."&lt;SEP&gt;"A. R. J. Nelson is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;J ONES , E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"E. J ONES is a contributor to SciPy 1.0."&lt;SEP&gt;"E. J ONES is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. J ONES is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. Jones is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;K ERN, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"R. K ERN is a contributor to SciPy 1.0."&lt;SEP&gt;"R. K ERN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"R. K ERN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"R. Kern is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;L ARSON , E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"E. L ARSON is a contributor to SciPy 1.0."&lt;SEP&gt;"E. L ARSON is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. L ARSON is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. Larson is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;C AREY , C. J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"C. J. C AREY is a contributor to SciPy 1.0."&lt;SEP&gt;"C. J. C AREY is an author of the SciPy 1.0 paper."&lt;SEP&gt;"C. J. C AREY is an author of the SciPy 1.0 paper."&lt;SEP&gt;"C. J. Carey is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;P OLAT,˙I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"I. P OLAT is a contributor to SciPy 1.0."&lt;SEP&gt;"I. P OLAT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"I. P OLAT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"I. Polat is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;F ENG, Y.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Y. F ENG is a contributor to SciPy 1.0."&lt;SEP&gt;"Y. F ENG is an author of the SciPy 1.0 paper."&lt;SEP&gt;"Y. F ENG is an author of the SciPy 1.0 paper."&lt;SEP&gt;"Y. Feng is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;M OORE , E. W.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"E. W. M OORE is a contributor to SciPy 1.0."&lt;SEP&gt;"E. W. M OORE is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. W. M OORE is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. W. Moore is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;V ANDER PLAS, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. V ANDER PLAS is a contributor to SciPy 1.0."&lt;SEP&gt;"J. V ANDER PLAS is an author of the SciPy 1.0 paper."&lt;SEP&gt;"J. V ANDER PLAS is an author of the SciPy 1.0 paper."&lt;SEP&gt;"J. Vander Plas is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;L AXALDE , D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"D. L AXALDE is a contributor to SciPy 1.0."&lt;SEP&gt;"D. L AXALDE is an author of the SciPy 1.0 paper."&lt;SEP&gt;"D. L AXALDE is an author of the SciPy 1.0 paper."&lt;SEP&gt;"D. Laxalde is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;P ERKTOLD, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. P ERKTOLD is a contributor to SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;C IMRMAN , R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"R. C IMRMAN is a contributor to SciPy 1.0."&lt;SEP&gt;"R. C IMRMAN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"R. C IMRMAN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"R. Cimrman is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;H ENRIKSEN, I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"I. H ENRIKSEN is a contributor to SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;Q UINTERO , E. A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"E. A. Q UINTERO is a contributor to SciPy 1.0."&lt;SEP&gt;"E. A. Q UINTERO is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. A. Q UINTERO is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. A. Quintero is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;H ARRIS, C. R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"C. R. H ARRIS is a contributor to SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;A RCHIBALD, A. M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"A. M. A RCHIBALD is a contributor to SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;RIBEIRO , A. H.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"A. H. RIBEIRO is a contributor to SciPy 1.0."&lt;SEP&gt;"A. H. RIBEIRO is an author of the SciPy 1.0 paper."&lt;SEP&gt;"A. H. RIBEIRO is an author of the SciPy 1.0 paper."&lt;SEP&gt;"A. H. Ribeiro is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;P EDREGOSA, F.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"F. P EDREGOSA is a contributor to SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;V ANMULBREGT , P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"P. V ANMULBREGT is a contributor to SciPy 1.0."&lt;SEP&gt;"P. V ANMULBREGT is an author of the SciPy 1.0 paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SCIPY1.0 Contributors refers to a group of individuals who contributed to the development and maintenance of SciPy 1.0."&lt;SEP&gt;"The SCIPY1.0 Contributors are an organization of contributors who worked on the development and improvement of the SciPy library."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;CURRAN ASSOCIATES, INC.&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Curran Associates, Inc. published 'Attention is all you need.'"&lt;SEP&gt;"Curran Associates, Inc. published the Attention is all you need paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;W ANG, K.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"K. W ANG is a contributor to various research papers."&lt;SEP&gt;"K. W ANG is also a co-author of TSDAE and GPL papers."&lt;SEP&gt;"K. W ANG is an author of multiple papers including 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' and 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval'."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;R EIMERS , N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N. R EIMERS is a contributor to multiple research papers."&lt;SEP&gt;"N. R EIMERS is an author of multiple papers including 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' and 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval'."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;GUREVYCH , I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"GUREVYCH, I. is an author of 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks' and 'Making monolingual sentence embeddings multilingual using knowledge distillation.'"&lt;SEP&gt;"I. GUREVYCH is a contributor to multiple research papers."&lt;SEP&gt;"I. GUREVYCH is an author of multiple papers including 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' and 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval'."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2&lt;SEP&gt;chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;THAKUR , N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N. THAKUR is involved in the development of GPL for unsupervised domain adaptation."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;68-71&quot;">
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
  <data key="d1">"The events Language Model Evaluation and Search System Evaluation are documented in pages 68 to 71 of the document."::</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;81-83&quot;">
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
  <data key="d1">"The event Conclusion is summarized on pages 81 to 83 of the document, covering contributions and future work."::</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TSDAE ARCHITECTURE&quot;">
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-86921910d0aba929bef9ae955431a4fa</data>
  <data key="d1">"Both TSDAE Architecture and T5 diagram are technologies discussed in the text, possibly related through their use or description of a Transformer-based model."&lt;SEP&gt;"The TSDAE Architecture refers to a specific transformer-based sequential denoising auto-encoder model used for training purposes."&lt;SEP&gt;"TSDAE Architecture is a part of the series of training tasks and system architectures described in the text, marking a step in the development process."&lt;SEP&gt;"TSDAE Architecture refers to a specific architecture used for denoising auto-encoders based on transformers. It is discussed in detail starting from page 31."&lt;SEP&gt;"Transformer-based Sequential Denoising Auto-Encoder (TSDAE) Architecture is described, showing an advanced model for sequential data processing."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;FINE-TUNING SBERT&quot;">
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
  <data key="d1">"Both Fine-Tuning SBERT and Metadata Knowledge Distillation Ideology are related to model training processes, suggesting a link in the context of fine-tuning and knowledge distillation techniques."&lt;SEP&gt;"These techniques are both involved in model fine-tuning and knowledge distillation processes, indicating a relationship in the document."&lt;&lt;SEP&gt;"Fine-Tuning SBERT refers to the process of fine-tuning Sentence-BERT (SBERT) models for specific tasks by adjusting parameters on a small dataset."&lt;SEP&gt;"Fine-Tuning SBERT is described as a specific fine-tuning task within the series of training tasks, indicating ongoing development efforts."&lt;SEP&gt;"Fine-Tuning SBERT involves fine-tuning the Sentence-BERT model, detailed starting from page 35."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;METADATA KNOWLEDGE DISTILLATION IDEOLOGY&quot;">
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
  <data key="d1">"Both Fine-Tuning SBERT and Metadata Knowledge Distillation Ideology are related to model training processes, suggesting a link in the context of fine-tuning and knowledge distillation techniques."&lt;SEP&gt;"These techniques are both involved in model fine-tuning and knowledge distillation processes, indicating a relationship in the document."&lt;&lt;SEP&gt;"Metadata Knowledge Distillation Ideology refers to the underlying principles guiding the process of distilling knowledge from metadata for use in model training or evaluation."&lt;SEP&gt;"Metadata Knowledge Distillation Ideology outlines the principles behind metadata knowledge distillation, a concept used to improve model performance through distilled knowledge."&lt;SEP&gt;"Metadata Knowledge Distillation Ideology discusses the philosophical approach behind using metadata in knowledge distillation, starting from page 63."</data>
  <data key="d0">"CONCEPT"</data>
</node>
<node id="&quot;SEARCH SYSTEM EVALUATION – SEARCH METRIC - MODELS V0&quot;">
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421&lt;SEP&gt;chunk-86921910d0aba929bef9ae955431a4fa</data>
  <data key="d1">"Both evaluations are related to search metrics for different model versions, indicating a relationship in the context of comparing model performance."&lt;&lt;SEP&gt;"The Search System Evaluation – Search metric - Models V0 is a specific evaluation method focusing on searching tasks with Model Version 0, providing insights into its performance."&lt;SEP&gt;"The first set of search system evaluations, focusing on the search metric using Models V0, is described starting from page 73."&lt;SEP&gt;"This is a figure showing evaluation results for models fine-tuned on pre-existing datasets, indicating their performance in the Search metric."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;SEARCH SYSTEM EVALUATION – SEARCH METRIC - MODELS V1&quot;">
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
  <data key="d1">"Both evaluations are related to search metrics for different model versions, indicating a relationship in the context of comparing model performance."&lt;&lt;SEP&gt;"The Search System Evaluation – Search metric - Models V1 evaluates the search task performance of Model Version 1, comparing it against previous models."&lt;SEP&gt;"The second set of search system evaluations, also focusing on the search metric but using Models V1, is detailed beginning on page 74."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;3.4 TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE&quot;">
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
  <data key="d1">"Both sections refer to related techniques in the document, possibly discussing similar models or architectures."&lt;&lt;SEP&gt;"The TSDAE Architecture refers to a specific transformer-based sequential denoising auto-encoder model used for training purposes."&lt;SEP&gt;"TSDAE Architecture is a specific architecture used in training tasks, indicating its role in model development."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;3.5 T5 DIAGRAM&quot;">
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
  <data key="d1">"A visual representation of the T5 model architecture, which is likely another type of transformer model used in various tasks."&lt;SEP&gt;"Both sections refer to related techniques in the document, possibly discussing similar models or architectures."&lt;&lt;SEP&gt;"T5 diagram provides a visual representation of the T5 model, highlighting its structure and components."</data>
  <data key="d0">"VISUALIZATION"</data>
</node>
<node id="&quot;STS&quot;">
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
  <data key="d1">"Both STS and SNLI are part of the broader field of NLP but focus on different aspects; STS measures textual similarity, while SNLI is about natural language inference."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ICE&quot;">
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
  <data key="d1">"The word 'ice' co-occurs more frequently with the word 'solid' than it does with 'gas', indicating a stronger semantic connection."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STEAM&quot;">
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
  <data key="d1">"The word 'steam' co-occurs less frequently with the word 'gas' compared to ice's relation with solid, showing contrasting co-occurrence patterns."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ICE STEAM&quot;">
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
  <data key="d1">"Both words 'ice' and 'steam' are related to water but not strongly with fashion, indicating thematic connections in broader context."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ACTIVATION FUNCTIONS&quot;">
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
  <data key="d1">"RNN uses various activation functions to introduce non-linearity and enable processing of sequential data."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;LSTM&quot;">
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
  <data key="d1">"Both Backpropagation Through Time and LSTMs are techniques aimed at improving the performance of neural networks in sequence tasks."&lt;SEP&gt;"Long Short-Term Memory networks, used as a baseline to compare against Transformers, are more complex and require longer training times."&lt;SEP&gt;"LSTM refers to Long Short-Term Memory networks, which are used in handling long-term dependencies but require long training times and do not allow for parallel computation."&lt;SEP&gt;"Long Short-Term Memory (LSTM) is a variant of RNN designed to handle long-term dependencies in sequences."&lt;SEP&gt;"LSTM refers to Long Short-Term Memory networks, a variant of RNN designed to handle long-term dependencies."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;DECODER BLOCK&quot;">
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
  <data key="d1">"Masked Multi-Head Attention is used specifically in the decoder block to ensure predictions are based on past information only by masking future positions."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FINE-TUNING&quot;">
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1&lt;SEP&gt;chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
  <data key="d1">"Adam Optimizer is used during the fine-tuning phase of BERT to train the model." "&lt;SEP&gt;"Adam Optimizer is utilized in BERT's fine-tuning process."&lt;SEP&gt;"Adam Optimizer is utilized in BERT's fine-tuning process." "&lt;SEP&gt;"Fine-tuning is a phase where an additional layer is added and the network is retrained for a few epochs."&lt;SEP&gt;"Fine-tuning is a phase in training a neural network where an additional layer is added after the final BERT layer and the entire network is trained for a few epochs with the Adam Optimizer."&lt;SEP&gt;"Fine-tuning is another phase in the training process of BERT, focusing on specific applications with additional layers."&lt;SEP&gt;"Fine-tuning is a phase in the training of deep learning models where an additional layer is added after the final BERT layer and the entire network is trained with the Adam Optimizer for a few epochs."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;PASSAGES&quot;">
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
  <data key="d1">"The Query Generation step generates queries based on multiple passages."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;QUERY GENERATION STEP&quot;">
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
  <data key="d1">"The Query Generation step generates queries based on multiple passages."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;NLI&quot;">
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
  <data key="d1">"Both NLI and STS are tasks that involve analyzing the relationship between sentences, with NLI focusing on entailment and contradiction while STS measures similarity."::</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STSB EVALUATION&quot;">
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
  <data key="d1">"SBERT -STSb-base performs well on the STS benchmark test set, indicating its effectiveness in tasks like sentence similarity and paraphrase identification."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;OVGU&quot;">
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
  <data key="d1">"OvGU is another team that achieved top scores in the COLIEE 2021 competition, using a two-stage TF-IDF vectorization combined with Sentence-BERT embeddings."&lt;SEP&gt;"OvGU is another team that achieved top scores using a two-stage TF-IDF vectorization combined with Sentence-BERT embeddings for Task 3."&lt;SEP&gt;"OvGU used Sentence-BERT embeddings in their approach for the third task of COLIEE 2021, focusing on statute law retrieval."&lt;SEP&gt;"OvGU is the name of another team or organization that participated in COLIEE 2021 and used a two-stage TF-IDF vectorization combined with Sentence-BERT embeddings for Task 3."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;JNLP &quot;">
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
  <data key="d1">"Both JNLP and nigam used pre-trained models and developed unique approaches for tasks in COLIEE 2021, indicating a competitive relationship."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;COLIEE 2021 - TASK 3 RESULTS&quot; &quot;">
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
  <data key="d1">"The COLIEE 2021 - Task 3 results show specific metrics for Team OvGU and nigam, indicating performance evaluation in statute law retrieval."&lt;SEP&gt;"The COLIEE 2021 - Task 3 results show specific metrics for Team OvGU and nigam, indicating performance in statute law retrieval."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;COLIEE 2021 - TASK 1 RESULTS&quot; &quot;">
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
  <data key="d1">"The COLIEE 2021 - Task 1 results provide specific metrics for teams JNLP and nigam, showing performance in case law retrieval."&lt;SEP&gt;"The COLIEE 2021 - Task 1 results provide specific metrics for teams JNLP and nigam, showing their performance in case law retrieval."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;LESSE&quot;">
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-4178cfa608054c267be41d058b830af4</data>
  <data key="d1">"INESC-ID is involved in the development of LeSSE along with Nuno Cordeiro and Imprensa Nacional-Casa da Moeda."&lt;SEP&gt;"LeSSE is a system created by Nuno Cordeiro that merges common document retrieval techniques with semantic search abilities."&lt;SEP&gt;"LeSSE refers to a system created by Nuno Cordeiro that merges common document retrieval techniques with semantic search abilities."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;TASK 1 RESULTS&quot;">
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
  <data key="d1">"Table 3.4 shows the results of Task 1."&lt;SEP&gt;"Task 1 results are shown in Table 3.4, indicating a completed phase of research."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;TASK 3 RESULTS&quot;">
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
  <data key="d1">"Table 3.5 shows the results of Task 3."&lt;SEP&gt;"Task 3 results are shown in Table 3.5, indicating another completed phase of research."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;ALBERTINA PT-PT&quot;">
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
  <data key="d1">"Albertina PT-PT is a BERT model for European Portuguese, developed by FCUL as part of its research work."&lt;SEP&gt;"Albertina PT-PT is a model focused on Portuguese jurisprudence, potentially offering better comprehension of European Portuguese legislation."&lt;SEP&gt;"FCUL developed the PT-PT version of Albertina, contributing to the research in natural language and speech through the NLX group."&lt;SEP&gt;"Albertina PT-PT is a model specifically for Portuguese jurisprudence, trained on European Portuguese language."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;FCUL&quot;">
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
  <data key="d1">"FCUL developed the PT-PT version of Albertina, contributing to the research in natural language and speech through the NLX group."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ALBERTINA PT-BR&quot;">
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
  <data key="d1">"Albertina PT-BR is a BERT model for Brazilian Portuguese, developed by FEUP in partnership with FCUL. It outperforms BERTimbau on certain STS tasks but falls short in others."&lt;SEP&gt;"The BrWaC corpus is used in the pre-training of the Albertina model for Brazilian Portuguese (PT-BR), providing relevant data for training."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;LEGAL-BERTIMBAU MODEL&quot;">
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
  <data key="d1">"Legal-BERTimbau is a specific version of BERTimbau adapted for legal document analysis and processing, which was used to create embeddings for evaluation purposes."&lt;SEP&gt;"The Legal-BERTimbau model is a specialized semantic search tool for legal applications, contributing to the effectiveness of the system."&lt;SEP&gt;"The Legal-BERTimbau model works within the Elasticsearch environment to contribute to the semantic search system."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;CHAPTER 1 AND CHAPTER 5&quot;">
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
  <data key="d1">"Chapter 1 introduces Project IRIS and its constraints, while Chapter 5 provides detailed information about the Legal-BERTimbau model within the project."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;METAKD TECHNIQUE&quot;">
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
  <data key="d1">"The stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1 model incorporates the MetaKD technique, enhancing its performance on specific tasks."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
  <data key="d1">"The process of embedding adjustment is based on the tags and centroids derived from these documents."&lt;SEP&gt;"This is a version of a BERT model generated through Metadata Knowledge Distillation, optimized for legal texts and STS tasks."&lt;SEP&gt;"This is a model name representing an organization's project or research output."&lt;SEP&gt;"This is a model name, likely representing a version of a BERT model adjusted through Metadata Knowledge Distillation for legal tasks."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
  <data key="d1">"The generated models are directly linked to the processing of these documents and their embeddings."&lt;SEP&gt;"The process of embedding adjustment is based on the tags and centroids derived from these documents."&lt;SEP&gt;"This is another variant of a BERT model using TSDAE for training, optimized for legal texts and STS tasks."&lt;SEP&gt;"This is another model name, related to the same organizational project or research."&lt;SEP&gt;"This is another model name, representing a version of BERT adjusted through Metadata Knowledge Distillation for legal tasks."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
  <data key="d1">"A further version of the BERT model with TSDAE training, potentially improved from v0."&lt;SEP&gt;"The generated models are directly linked to the processing of these documents and their embeddings."&lt;SEP&gt;"The process of embedding adjustment is based on the tags and centroids derived from these documents."&lt;SEP&gt;"This is a variation of the previous model, continuing the same research line."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;MODEL TRAINING&quot;">
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
  <data key="d1">"The embedding adjustment process is part of the broader model training framework."&lt;SEP&gt;"The process involves training models using embeddings and centroid calculations."</data>
  <data key="d0">"PROCESS"</data>
</node>
<node id="&quot;BERTIMBAU LARGE&quot;">
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-41c849f637d2cf0401100a6cc855d2d2</data>
  <data key="d1">"BERTimbau large is a variant of BERT designed for Portuguese legal texts, which was used as a reference model in the evaluation."&lt;SEP&gt;"BERTimbau large is a version of the BERTimbau model used to generate sentence embeddings for different sentences."&lt;SEP&gt;"BERTimbau large refers to a version of the BERT model used for generating sentence embeddings."&lt;SEP&gt;"Both models were fine-tuned but on different techniques (MLM vs. MLM variant), comparing their performance in STS task."&lt;/|&gt;"model comparison, technique variation"&lt;SEP&gt;"BERTimbau large is another model variant used as a comparison baseline in the evaluation."&lt;SEP&gt;"BERTimbau large refers to a specific version of the BERTimbau model used for generating sentence embeddings."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;">
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
  <data key="d1">"Both models were fine-tuned but on different techniques (MLM vs. MLM variant), comparing their performance in STS task."&lt;/|&gt;"model comparison, technique variation"&lt;SEP&gt;"This is a specific model variant obtained from applying Masked Language Modeling (MLM&lt;SEP&gt;"This is a specific model variant obtained from applying Masked Language Modeling (MLM) fine-tuning on BERTimbau for the legal domain."&lt;SEP&gt;"stjiris/bert-large-portuguese-cased-legal-mlm is a variant of BERT adapted through MLM technique, used for Portuguese legal texts and showed better performance in the evaluation."&lt;SEP&gt;"This is a variant of BERTimbau trained using Masked Language Modeling (MLM) on legal documents, designed to handle technical language and jargon."&lt;SEP&gt;"stjiris/bert-large-portuguese-cased-legal-mlm is a model variant subjected to MLM technique for domain adaptation."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2&quot;">
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-41c849f637d2cf0401100a6cc855d2d2</data>
  <data key="d1">"Paraphrase-multilingual-mpnet-base-v2 is a multilingual model for generating sentence embeddings and paraphrases."&lt;SEP&gt;"Paraphrase-multilingual-mpnet-base-v2 serves as a baseline for the STS task evaluation against state-of-the-art multilingual models."&lt;/|&gt;"baseline comparison, model validation"&lt;SEP&gt;"paraphrase-multilingual-mpnet-base-v2 is another multilingual model that was used as a baseline for evaluating the performance of Legal-BERTimbau in semantic searches."&lt;SEP&gt;"Paraphrase-multilingual-mpnet-base-v2 is another multilingual model used to generate sentence embeddings, compared against Legal-BERTimbau."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;ALL-MPNET-BASE-V2&quot;">
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2&lt;SEP&gt;chunk-67e58f1a8dccff23d808e5fa663753db</data>
  <data key="d1">"All-mpnet-base-v2 is another baseline used in comparing the performance of different versions of Legal-BERTimbau against state-of-the-art multilingual models."&lt;/|&gt;"baseline comparison, model validation"&lt;SEP&gt;"Another model mentioned, used for comparison with SBERT variants."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;CUSTOM STS DATASET&quot;">
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
  <data key="d1">"Custom STS dataset refers to a specific dataset used for fine-tuning the Legal-BERTimbau model."&lt;SEP&gt;"The Custom STS dataset is used for fine-tuning models to improve their performance in the Search metric evaluation process."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;SEMANTICALLY-BASED SYSTEM&quot;">
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
  <data key="d1">"The Lexical+Semantic approach outperforms the purely semantic system but does not match or exceed BM25 in terms of performance."&lt;</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ORGANIZATION&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Project IRIS is an organization where the research and development of the semantic search system were conducted."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;PORTUGUESE JURISPRUDENCE&quot;">
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
  <data key="d1">"Albertina PT -PT is specifically trained on Portuguese jurisprudence, indicating its relevance in the legal domain."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;LEGAL DOMAIN&quot;">
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"The Legal Domain refers to the specific field where the search system can be applied, particularly useful given the complexity of document relevance determination in this area."&lt;SEP&gt;"The legal domain involves Supremo Tribunal de Justiça as it pertains to legal proceedings and decisions."&lt;SEP&gt;"The legal domain refers to contexts where determining document relevance can be challenging and expert feedback important."&lt;SEP&gt;"The legal domain refers to the vast area of law and legal practices where the search systems are applied."&lt;&lt;relationship_description&gt;</data>
  <data key="d0">"LOCATION"</data>
</node>
<node id="&quot;LEGAL-BBERTIMBAU MODELS&quot;">
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
  <data key="d1">"Legal-BBERTimbau models were submitted to and accepted by EPIA Conference on Artificial Intelligence, showcasing their significance in the field of AI."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;MULTILINGUAL AND CROSSLINGUAL EVALUATION&quot;">
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
  <data key="d1">"The SemEval-2017 task 1 was focused specifically on semantic textual similarity multilingual and crosslingual evaluation."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;">
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
  <data key="d1">"BERT is described as pre-trained on large amounts of text data to improve language understanding."&lt;SEP&gt;"Jacob Devlin authored BERT, a pre-trained model for language understanding."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;HENDERSON, P.&quot;">
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
  <data key="d1">"Both authors have contributed significantly to the field of deep learning and neural modeling."&lt;SEP&gt;"Henderson is involved in the Pile of Law project alongside Kras, Zheng, Guha, Manning, Jurafsky, and Ho."&lt;SEP&gt;"Henderson is involved in learning responsible data filtering from the law and a 256GB open-source legal dataset."&lt;SEP&gt;"Henderson is involved in the work of learning responsible data filtering from law and a 256GB open-source legal dataset."</data>
  <data key="d0">"PERSON"</data>
</node>
<node id="&quot;ICLR 2013&quot;">
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
  <data key="d1">"ICLR 2013 is an event held in Scottsdale, Arizona, USA from May 2-4, 2013."&lt;SEP&gt;"T. Mikolov and co-authors presented their work at ICLR 2013."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;REDBEDDY, T.&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"REIMERS, N. and REDDY, T. collaborate on BERT-based language models and knowledge distillation techniques."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;HAZEER, R.&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"VIRTANEN, P. and HAZEER, R. collaborate on developing numerical and array programming tools for Python libraries like NumPy."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ATTENTION IS ALL YOU NEED PAPER&quot;">
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
  <data key="d1">"N., K AISER contributed to the development of the 'Attention is all you need' paper."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SCIPY 1.0&quot;">
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
  <data key="d1">"The SCIPY1.0 Contributors are involved in developing and maintaining SciPy 1.0."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;GPL&quot;">
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
  <data key="d1">"K. W ANG is a co-author of the GPL paper."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Master of Science Degree in Computer Science and Engineering is awarded by the institution where Rui Filipe Coimbra Pereira de Melo completed his thesis."&lt;SEP&gt;"This degree indicates Rui's academic qualification, emphasizing his specialization in computer science."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;ABSTRACT&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Abstract is a summary of the research work, outlining key methodologies and findings."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;ARTIFICIAL INTELLIGENCE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Artificial Intelligence is a key concept in this thesis, focusing on developing advanced search systems."&lt;SEP&gt;"Artificial Intelligence refers to the use of advanced technologies in developing the Semantic Search System."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;NATURAL LANGUAGE PROCESSING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Natural Language Processing involves the interaction between computers and human (natural) languages."&lt;SEP&gt;"Natural Language Processing is a key component of the Semantic Search System, enabling better understanding and processing of legal text."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;JURISPRUDENCE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Jurisprudence is the body of law or legal principles within which the Semantic Search System is developed to assist in decision-making processes."&lt;SEP&gt;"Jurisprudence refers to legal knowledge and principles used in the development of the Semantic Search System for judicial decision-making."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;FINE-TUNING ON DOWNSTREAM TASKS&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Fine-tuning involves adapting a pre-trained model for specific tasks, often using less labeled data than required for initial training."&lt;SEP&gt;"Fine-tuning on Downstream Tasks involves adjusting pre-trained models for specific tasks after initial training."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;CONSTRAINTS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Constraints refer to the limitations or requirements that must be considered when developing or implementing NLP systems."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;THE CORPUS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The Corpus refers to a collection of data processed by the semantic search system, used for training and testing purposes."&lt;SEP&gt;"The Corpus refers to a collection of documents or data that serves as input for processing in the Semantic Search System and related tasks."&lt;SEP&gt;"The Corpus refers to a collection of data used in processing and understanding by various search and language models."</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;DATA PROCESSING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Data Processing is a step involved in preparing data for semantic search systems, ensuring it aligns with system requirements."&lt;SEP&gt;"Data Processing is an event or process involved in preparing the corpus or other data for use in the Semantic Search System and related tasks."&lt;SEP&gt;"Data Processing is an event or process step within the creation and maintenance of the Semantic Search System's corpus."</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;LEXICAL-FIRST SEARCH SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical-First Search System describes another type of search system that prioritizes lexical elements over purely semantic ones."&lt;SEP&gt;"Lexical-First Search System is one of the search architectures described in the document, focusing on lexical processing first."&lt;SEP&gt;"Lexical-First is an approach that prioritizes lexical analysis before applying semantic techniques."&lt;&lt;relationship_description&gt;&lt;SEP&gt;"The Lexical-First Search System combines lexical search techniques with the reach of large language models. It uses BM25 ranking before cosine similarity evaluation."&lt;SEP&gt;"The Lexical-First Search System is another specific type of search system that prioritizes lexical analysis before applying semantic processing."&lt;SEP&gt;"The Lexical-First Search System combines lexical search techniques with semantic embedding methods using cosine similarity and BM25 ranking."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87</data>
</node>
<node id="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Lexical + Semantic Search System combines lexical and semantic processing to enhance search capabilities."&lt;SEP&gt;"The Lexical + Semantic Search System combines both lexical and semantic approaches in its search methodology."&lt;SEP&gt;"The Lexical + Semantic Search System integrates scores from both BM25 and semantic information retrieval methods, using Legal-BERTimbau for verification."&lt;SEP&gt;"The Lexical + Semantic Search System refers to a system that combines lexical and semantic methods for search and retrieval tasks."&lt;SEP&gt;"Lexical + Semantic Search System is a combined approach of lexical and semantic capabilities, performing well in Table 6.4."&lt;SEP&gt;"Lexical + Semantic Search System is a combined approach of lexical and semantic capabilities, performing well in Table 6.4.")&lt;SEP&gt;"Lexical + Semantic Search System is a hybrid approach combining both lexical and semantic search functionalities."&lt;SEP&gt;"This system integrates both lexical and semantic information retrieval, utilizing scores from BM25 and cosine similarity metrics to rank results."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87</data>
</node>
<node id="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A Transformer-based Sequential Denoising Auto-Encoder is a neural network architecture that uses transformers for sequential denoising tasks, often used in language modeling."&lt;SEP&gt;"Transformer-based Sequential Denoising Auto-Encoder is a specific type of technology used in Domain Adaptation."&lt;SEP&gt;"Transformer-based Sequential Denoising Auto-Encoder is a specific model used in the process of Domain Adaptation for Legal Language Model."</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;SEMANTIC TEXTUAL SIMILARITY CUSTOM DATASET&quot;">
  <data key="d0">"DATA SET"</data>
  <data key="d1">"Semantic Textual Similarity Custom Dataset is a specialized dataset used for training models to understand textual similarities in legal contexts."</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;NATURAL LANGUAGE INFERENCE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Natural Language Inference is a process used to infer meanings from natural language text, often involving understanding the relationships between sentences or documents."&lt;SEP&gt;"Natural Language Inference is a task that involves determining whether a given hypothesis logically follows from a given premise, often used in evaluating language understanding capabilities."</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;CONTRIBUTIONS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Contributions highlight key findings or advancements from the research, which will be published in future work."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;FUTURE WORK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Future Work outlines areas for further research and development based on current contributions."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;WORD EMBEDDINGS IN A 3D VECTOR SPACE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"This is an example of how words are represented in a three-dimensional space, providing a geometric interpretation of word meanings."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE WORD CONTEXT&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The CBOW model for Word2Vec uses the context of a single word to predict another related word, highlighting local dependencies."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;MULTIPLE WORDS CONTEXT IN CBOW MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"In this variant of the CBOW model, the prediction is based on multiple surrounding words, emphasizing broader contexts."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;VECTOR SPACE WITH A QUERY EMBEDDING AND MULTIPLE SENTENCE EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This illustrates how queries and sentences can be embedded into vector spaces for comparison or analysis."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;EXAMPLE OF WORD EMBEDDINGS IN A 3D VECTOR SPACE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"An example illustrating how words are represented in a three-dimensional space for geometric interpretation."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;WORD2VEC WITH CBOW MODEL BASED ON MULTIPLE WORDS CONTEXT&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The CBOW model variant that uses multiple surrounding words to predict a target word, focusing on broader contexts."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;RECURRENT NETWORK FULLY CONNECTED STRUCTURE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A fully connected recurrent neural network structure used for sequential data processing."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;RNN STRUCTURE ILLUSTRATION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"An illustration of the RNN architecture, which is a fundamental building block in sequence learning tasks."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;LONG SHORT-TERM MEMORY (LSTM) ARCHITECTURE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Long Short-Term Memory (LSTM) architecture, an improvement over traditional RNNs for better handling long-term dependencies."&lt;SEP&gt;"The LSTM architecture is introduced as an improvement over traditional RNNs for handling long-term dependencies."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;TRANSFORMER MODEL EXPLANATION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A novel approach using self-attention mechanisms to process sequential data, emphasizing parallelization and efficiency."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;SCALED DOT-PRODUCT ATTENTION MECHANISM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A key attention mechanism in the Transformer model that allows efficient alignment of input features across different positions."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;MULTI-HEAD ATTENTION MECHANISM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"An advanced form of attention that allows the model to focus on different parts of the input, enhancing its capabilities."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;VECTOR SPACE WITH QUERY AND SENTENCE EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Illustration of vector space representations for queries and sentences, useful in similarity comparisons."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;BI-ENCODER METHOD&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A method used in information retrieval to compare sentence pairs by encoding them separately and then comparing their embeddings."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;CROSS-ENCODER METHOD&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"An advanced version of the Bi-Encoder that directly compares pairs of sentences, providing more nuanced results."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;LEXRANK ALGORITHM&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"A graph-based ranking algorithm similar to PageRank used for text summarization by extracting important sentences based on their relevance."&lt;SEP&gt;"LexRank is a method for unsupervised extractive text summarization using eigenvector centrality scores and cosine similarity."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;DATA DISTRIBUTIONS IN MODELS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Refers to the statistical distributions of data used in various models or experiments, important for understanding model performance under different conditions."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;MASKED LANGUAGE MODELING TECHNIQUE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A technique where parts of a text are masked out and the model is trained to predict these missing words, enhancing general language understanding."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;T5 MODEL DIAGRAM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A diagram illustrating the structure of the T5 model, a sequence-to-sequence learning framework with applications in various NLP tasks."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;GENQ SYSTEM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"This term likely refers to a specific generation or question-answering system, though it is not fully defined in this context."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION PROCESS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Multilingual Knowledge Distillation is a process for transferring knowledge across languages, described beginning on page 37."&lt;SEP&gt;"Multilingual Knowledge Distillation is a process used to transfer knowledge across languages in multilingual models, an important aspect of model training and evaluation."&lt;SEP&gt;"The Multilingual Knowledge Distillation Process is a method used to transfer knowledge from multilingual models to monolingual ones, often to improve the performance of smaller or less resource-intensive models."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;SYSTEM ARCHITECTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"System Architecture refers to the overall design and layout of the system described in section 4.1, indicating the structure and components involved."&lt;SEP&gt;"System Architecture refers to the overall structure and design of the system being discussed, which includes various components and their interactions."&lt;SEP&gt;"System Architecture refers to the overall design of systems discussed in detail starting from page 49."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;LEXICAL-FIRST SEARCH SYSTEM RETRIEVAL METHOD&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical-First Search System Retrieval Method is a specific retrieval method discussed for its role in search systems, highlighting a particular approach to information retrieval."&lt;SEP&gt;"The Lexical-First Search System Retrieval Method is a specific retrieval method that prioritizes lexical search before considering other factors like semantic similarity."&lt;SEP&gt;"The Lexical-First Search System Retrieval Method is a search retrieval method described beginning on page 50."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM RETRIEVAL METHOD&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical + Semantic Search System Retrieval Method describes another retrieval method used alongside lexical-first methods, emphasizing the integration of semantic understanding in search processes."&lt;SEP&gt;"The Lexical + Semantic Search System Retrieval Method combines both lexical and semantic approaches to improve the accuracy of information retrieval processes."&lt;SEP&gt;"The Lexical + Semantic Search System Retrieval Method combines lexical and semantic search, detailed starting from page 51."&lt;SEP&gt;"This event refers to a method used for retrieving results by combining both lexical and semantic approaches, as illustrated in Figure 4.3."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;TRAINING TASKS OVERVIEW&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Training Tasks Overview provides an overview of the training tasks discussed beginning on page 56."&lt;SEP&gt;"Training Tasks Overview provides an overview of the various training tasks involved in the model development process, highlighting their roles and objectives."&lt;SEP&gt;"Training Tasks Overview provides an overview of various training tasks and their roles within the development process of a system or model."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;TSDAE TRAINING LOSS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TSDAE Training Loss refers to the loss function associated with TSDAE training, discussed beginning on page 58."&lt;SEP&gt;"The TSDAE Training Loss is a loss function associated with the TSDAE architecture, which measures the discrepancy between the original and reconstructed sequences during training."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V0&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Search System Evaluation - Discovery metric - Models V0 assesses the discovery capability of the system with Model Version 0 using specific metrics."&lt;SEP&gt;"The first set of discovery metric evaluations using Models V0 is described starting from page 75."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V1&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Search System Evaluation - Discovery metric - Models V1 evaluates the discovery performance of the system with Model Version 1, comparing it against earlier versions."&lt;SEP&gt;"The second set of discovery metric evaluations, this time using Models V1, is detailed beginning on page 76."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;30&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Page number indicating a starting point for a section."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;3.6 GENQ&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GenQ is a generative pseudo labeling process used in training tasks, indicating its importance in model development."&lt;SEP&gt;"GenQ is an unspecified technology or method possibly related to generation quality testing or evaluation."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;3.7 GENERATIVE PSEUDO LABELING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Generative Pseudo Labeling is a technique used for training models where pseudo labels are generated using another model, often a pre-trained one."&lt;SEP&gt;"Generative Pseudo Labeling is a technique used to generate labels for data, enhancing the training process of models."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;3.8 FINE-TUNING SBERT&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Fine-Tuning SBERT is a specific fine-tuning task described in the text, indicating ongoing development efforts."&lt;SEP&gt;"Fine-Tuning SBERT refers to the process of fine-tuning Sentence-BERT (SBERT) models for specific tasks by adjusting parameters on a small dataset."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;3.9 MULTILINGUAL KNOWLEDGE DISTILLATION PROCESS&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Multilingual Knowledge Distillation Process is a technique used to transfer knowledge across languages, an important aspect of model training and evaluation."&lt;SEP&gt;"The Multilingual Knowledge Distillation Process is a method used to transfer knowledge from multilingual models to monolingual ones, often to improve the performance of smaller or less resource-intensive models."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;4.1 SYSTEM ARCHITECTURE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"System Architecture describes the overall design and layout of the system, indicating its role in the development process."&lt;SEP&gt;"System Architecture refers to the overall structure and design of the system being discussed, which includes various components and their interactions."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;4.2 LEXICAL-FIRST SEARCH SYSTEM RETRIEVAL METHOD&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Lexical-First Search System Retrieval Method is a specific retrieval method used in search systems, highlighting a particular approach to information retrieval."&lt;SEP&gt;"The Lexical-First Search System Retrieval Method is a specific retrieval method that prioritizes lexical search before considering other factors like semantic similarity."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;4.3 LEXICAL + SEMANTIC SEARCH SYSTEM RETRIEVAL METHOD&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical + Semantic Search System Retrieval Method describes another retrieval method used alongside lexical-first methods, emphasizing the integration of semantic understanding in search processes."&lt;SEP&gt;"The Lexical + Semantic Search System Retrieval Method combines both lexical and semantic approaches to improve the accuracy of information retrieval processes."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;5.1 TRAINING TASKS OVERVIEW&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Training Tasks Overview provides an overview of the various training tasks involved in the model development process, highlighting their roles and objectives."&lt;SEP&gt;"Training Tasks Overview provides an overview of various training tasks and their roles within the development process of a system or model."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;5.2 MASKED LANGUAGE MODELING (MLM) TRAINING LOSS&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Masked Language Modeling Training Loss is mentioned as part of the training process, indicating its importance in measuring performance during training."&lt;SEP&gt;"The Masked Language Modeling (MLM) Training Loss is a specific loss function used during the MLM training phase to measure how well the model predicts masked tokens in the input sequence."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;5.3 TSDAE TRAINING LOSS&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"TSDAE Training Loss is a specific loss function used in the training of TSDAE architecture, highlighting its role in model development."&lt;SEP&gt;"The TSDAE Training Loss is a loss function associated with the TSDAE architecture, which measures the discrepancy between the original and reconstructed sequences during training."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;5.4 METADATA KNOWLEDGE DISTILLATION IDEOLOGY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Metadata Knowledge Distillation Ideology outlines the principles behind metadata knowledge distillation, an important concept used to improve model performance through distilled knowledge."&lt;SEP&gt;"Metadata Knowledge Distillation Ideology refers to the underlying principles guiding the process of distilling knowledge from metadata for use in model training or evaluation."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;5.5 METADATA KNOWLEDGE DISTILLATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Metadata Knowledge Distillation is a process described in the text, indicating its role in transferring knowledge across models and languages."&lt;SEP&gt;"Metadata Knowledge Distillation is a process where knowledge from large pre-trained models is transferred to smaller, fine-tuned models using metadata as a guide."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;6.1 EVALUATION ARCHITECTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Evaluation Architecture describes the structure and components of evaluation systems used to assess models and their effectiveness in various metrics."&lt;SEP&gt;"The Evaluation Architecture refers to the framework or design used for evaluating the performance of search systems and their metrics."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;6.2 SEARCH SYSTEM EVALUATION – SEARCH METRIC - MODELS V0&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Search System Evaluation – Search metric - Models V0 is a specific evaluation method for search systems, indicating its role in assessing model performance."&lt;SEP&gt;"The Search System Evaluation – Search metric - Models V0 is a specific evaluation method focusing on searching tasks with Model Version 0, providing insights into its performance."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;6.3 SEARCH SYSTEM EVALUATION – SEARCH METRIC - MODELS V1&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Search System Evaluation – Search metric - Models V1 is another version of the search system evaluation method, highlighting ongoing improvements and developments."&lt;SEP&gt;"The Search System Evaluation – Search metric - Models V1 evaluates the search task performance of Model Version 1, comparing it against previous models."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;6.4 SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V0&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Search System Evaluation - Discovery metric - Models V0 is a specific discovery metric used in evaluating search systems, indicating its role in assessing model performance."&lt;SEP&gt;"The Search System Evaluation - Discovery metric - Models V0 assesses the discovery capability of the system with Model Version 0 using specific metrics."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;6.5 SEARCH SYSTEM EVALUATION - DISCOVERY METRIC - MODELS V1&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Search System Evaluation - Discovery metric - Models V1 is another version of the discovery metric used in evaluating search systems, highlighting ongoing improvements and developments."&lt;SEP&gt;"The Search System Evaluation - Discovery metric - Models V1 evaluates the discovery performance of the system with Model Version 1, comparing it against earlier versions."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;SPEARMAN CORRELATION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Spearman correlation is a statistical measure used in evaluating model performance, indicating its relevance in the context of evaluation metrics."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;BEST MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Best Model refers to the top-performing model in specific evaluations, indicating its success and relevance."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;LEXICAL TECHNIQUES&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical techniques are traditional search methods that involve searching for specific word matches from a given query, but they may not retrieve all relevant information.")&lt;SEP&gt;"Lexical techniques are used to search for specific word matches in a given query and are limited in their effectiveness."&lt;SEP&gt;"Traditional lexical techniques search for specific word matches from a given query but may not retrieve all relevant information."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;SEMANTIC TECHNIQUES&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Semantic techniques are employed in search systems to understand query semantics and find relevant documents by meaning similarity."&lt;SEP&gt;"Semantic techniques help in understanding the semantic meaning of queries and finding document passages closer in meaning to the query, providing a broader range of relevant documents."&lt;SEP&gt;"Semantic techniques work by understanding the semantic meaning of a query and finding document passages closer in meaning to the query."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;CONSISTENCY IN APPLYING THE LAW&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Maintaining consistency in applying the law is crucial to avoid imbalances and weakenings in the justice system."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;INESC-ID LISBOA IRIS PROJECT&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"INESC-ID Lisboa IRIS Project is focused on developing summarization approaches for court decisions and creating useful representations."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;INFORMATION RETRIEVAL (IR) SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"An information retrieval system is essential for managing and accessing legal resources efficiently."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;NL NATURAL LANGUAGE PROCESSING (NLP)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"NLP is a field of study focused on processing and understanding natural language, which is essential for many text-related tasks."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;NN NEURAL NETWORK (NN)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"NN refers to a type of machine learning model used in various NLP applications such as classification, prediction, and more."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;NSP NEXT SENTENCE PREDICTION (NSP)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"NSP is an event or task within NLP that involves predicting the next sentence given the context."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;QA QUESTION AND ANSWER (QA)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"QA refers to the process of generating answers based on questions, a common application in NLP."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;RNN RECURRENT NEURAL NETWORK (RNN)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"RNN is a type of neural network designed to handle sequential data, useful in tasks like language modeling and machine translation."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;SBERT SENTENCE-BERT (SBERT)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SBERT is a model used for sentence-level semantic similarity, often employed in tasks requiring deep understanding of text meaning."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;SNLI STANFORD NATURAL LANGUAGE INFERENCE (SNLI)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SNLI is an event or dataset commonly used in NLP research to test the ability to infer relationships between sentences."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;T5 TEXT-TO-TEXT TRANSFER TRANSFORMER (T5)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"T5 is a transformer-based model designed for text-to-text tasks, including summarization and translation."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;TF-IDF TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TF-IDF is a statistical measure used to evaluate the importance of words in documents, often used in information retrieval and text analysis."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;TF TERM FREQUENCY (TF)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Term frequency measures how frequently a term appears in a document, useful for various text processing tasks."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;TSDAE TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TSDAE is an autoencoder architecture designed to handle sequential data and denoise it, often used in NLP tasks."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;UKP UBIQUITOUS KNOWLEDGE PROCESSING (UKP)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"UKP refers to a research group or organization focused on developing knowledge processing technologies."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;OKAPI BM25 (BM25)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Okapi BM25 is a widely used algorithm for Information Retrieval and incorporates Term Frequency-Inverse Document Frequency (TF-IDF) for ranking documents based on query relevance."&lt;SEP&gt;"Okapi BM25 is a widely used algorithm in information retrieval, incorporating TF-IDF and known for its performance in benchmarks."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;INFORMATION RETRIEVAL (IR)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Information Retrieval (IR) systems help in efficiently accessing and managing vast volumes of information, particularly relevant legal resources.")&lt;SEP&gt;"Information Retrieval refers to the process of searching for relevant documents based on queries, encompassing both classical and modern approaches."&lt;SEP&gt;"Information Retrieval refers to the process of searching for relevant information from large collections of data or documents based on user queries."&lt;SEP&gt;"Information Retrieval is an indispensable tool for accessing required legal resources efficiently."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;2.1 LEXICAL APPROACHES FOR INFORMATION RETRIEVAL&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This section describes traditional methods of information retrieval, specifically focusing on lexical approaches."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;2.1.1 TERM FREQUENCY ALGORITHM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"This section explains the term frequency (TF) component of TF-IDF, which measures how often a term appears in a document."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;2.1.2 BEST MATCHING ALGORITHM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"This section describes the Best Matching (BM25) algorithm, which is a ranking function used in information retrieval."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;2.1.3 DISTANCE METRICS FOR LEXICAL APPROACHES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"This section discusses traditional methods of searching similar documents using distance metrics such as Jaccard Similarity."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;LEXICAL SEARCHES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Lexical searches refer to methods that focus on literal word matches and can miss important passages or documents if certain words are absent."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;SYNONYMS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Synonyms are words with similar meanings that can be used interchangeably in different contexts. They pose challenges for lexical searches since they might not match exact terms."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;LEXICAL APPROACHES&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Lexical approaches involve searching for specific word matches rather than understanding the contextual meaning, often limited in retrieving important information due to missing exact words or synonyms."&lt;SEP&gt;"Lexical approaches refer to information retrieval methods based on literal word matches and can fail when important passages or synonyms are involved."&lt;SEP&gt;"Lexical approaches involve searching for specific words or exact matches in documents, often leading to retrieval issues when synonyms are used."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;EMBEDDING SPACES&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Embedding spaces refer to multidimensional spaces where words and other data points are represented. These spaces allow for arithmetic operations on word vectors."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;UNSUPERVISED TRAINING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Unsupervised training is a method used in creating Word2Vec and GloVe models without labeled data."&lt;SEP&gt;"Unsupervised training is a type of machine learning where the model learns from unlabelled data, commonly used in creating word embeddings like Word2Vec and GloVe."&lt;SEP&gt;"Unsupervised training refers to the process where BERT learns to predict words without labeled data, which can introduce bias when only one direction of context is considered."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1&lt;SEP&gt;chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;ONE-HOT ENCODING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"One-hot encoding is used to represent input words in the CBOW and Skip-Gram models as vectors."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;HIDDEN LAYER&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The hidden layer in neural network models like Word2Vec contains neurons that process information between the input and output layers."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;OUTPUT LAYER&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The output layer in neural network models represents the final predicted word or context after processing through the hidden layer."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;CORPUS OF TEXT&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The corpus of text is a collection of documents or sentences used as input for training machine learning models like GloVe."&lt;SEP&gt;"The corpus of text is input to the GloVe model, used for generating word embeddings."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;HIGH-DIMENSIONAL SPACE&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"High-dimensional space refers to the multidimensional vector space where each word is represented by its vector embedding generated through GloVe's model."&lt;SEP&gt;"High-dimensional space refers to the vector space where each word in the corpus is represented based on its co-occurrence probabilities."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;HIDDEN VECTOR&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Hidden vector in RNN refers to the information carried over from one iteration to another, crucial for maintaining context across sequences."&lt;SEP&gt;"Hidden vector in an RNN stores information from previous iterations and is crucial for maintaining context over time."&lt;SEP&gt;"The Hidden Vector is produced by the Output Gate and used to determine what information will be transmitted to the next cell in a sequence."&lt;SEP&gt;"The hidden vector in an RNN holds information from previous iterations and is crucial for processing sequential data."&lt;SEP&gt;"The hidden vector is derived from the output of the Output Gate combined with a tanh layer, passed to the next cell."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f&lt;SEP&gt;chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;ACTIVATION FUNCTION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Activation functions like Sigmoid, Tanh, or ReLU are used to introduce non-linearity into the neural network model. They are essential components of both GloVe and RNN architectures."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;TANH&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Tanh (Hyperbolic Tangent) is a function used in neural networks for its non-linearity and output range between -1 and 1."&lt;SEP&gt;"tanh is an activation function used in neural networks, often with RNNs, to introduce non-linearity."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f&lt;SEP&gt;chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;SIGMOID FUNCTION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Sigmoid functions are used in gates for binary decisions, ranging from 0 to 1."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;TANH FUNCTION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Tanh functions are part of the Input and Output Gates, providing a range between -1 and 1."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;CONTEXTUALIZED ENCODING SEQUENCE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"This sequence is produced by the encoder and is used in the Transformer model for mapping inputs into a meaningful context."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;SELF-ATTENTION MECHANISM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A new mechanism introduced by Google that facilitates long-range dependencies, avoids gradient vanishing/exploding, and enables parallel computation in Transformers."&lt;SEP&gt;"Self-Attention is a mechanism in Transformers that allows the model to focus on different positions of the input sequence, enabling efficient processing without recurrence."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;CT-1&quot;">
  <data key="d0">"VARIABLE"</data>
  <data key="d1">"Ct-1 represents the previous cell state before the current step in the LSTM process." &lt;|"previous state, current step"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;FT&quot;">
  <data key="d0">"SYMBOL"</data>
  <data key="d1">"ft is the output of the Forget Gate and controls how much information from the previous cell state (Ct-1) is kept or discarded." &lt;|"gate output, control mechanism"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;IT&quot;">
  <data key="d0">"SYMBOL"</data>
  <data key="d1">"it is the output of the Input Gate and determines which values will be updated in the Cell State." &lt;|"gate output, update determination"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;˜CT&quot;">
  <data key="d0">"VARIABLE"</data>
  <data key="d1">"˜Ct represents the new candidate vector for updating the cell state before it undergoes a tanh function transformation." &lt;|"candidate vector, update proposal"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;WF, WI, WC, WO, BF, BI, BC&quot;">
  <data key="d0">"SYMBOL"</data>
  <data key="d1">"These are weight matrices and bias vectors used in various gate computations (Forget Gate, Input Gate, Cell State Update)" &lt;|"gate computation, matrix usage"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;HT-1&quot;">
  <data key="d0">"VARIABLE"</data>
  <data key="d1">"ht-1 represents the previous hidden state before the current step in the LSTM process." &lt;|"previous state, current step"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;XT&quot;">
  <data key="d0">"SYMBOL"</data>
  <data key="d1">"xt is the input at time step t used by various gates for their computations." &lt;|"input vector, gate computation"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;CT&quot;">
  <data key="d0">"VARIABLE"</data>
  <data key="d1">"Ct represents the current cell state after updates from the Input and Forget Gates." &lt;|"current state, update result"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;Σ&quot;">
  <data key="d0">"SYMBOL"</data>
  <data key="d1">"σ is used as a shorthand for sigmoid functions in various gate computations." &lt;|"sigmoid function, computation symbol"</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;SENTENCE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A sentence in this context refers to a unit of language that conveys complete thoughts and can be embedded for semantic analysis."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Embeddings are vectorial representations of words, paragraphs, or documents in a vector space used in Semantic Search."&lt;SEP&gt;"Embeddings are vectorial representations used in natural language processing, transforming words, sentences, or documents into numerical vectors in a high-dimensional space."&lt;SEP&gt;"Embeddings refer to numerical representations used in machine learning models for processing text data."&lt;SEP&gt;"Embeddings represent the vector representations of sentences or documents, used in machine learning models for various tasks such as similarity calculations."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-427843b4c7ba44f1dcc8f571081e36ae&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;SYMMETRIC SEMANTIC SEARCH&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Symmetric Semantic Search involves matching a query with text, expecting similar meaning retrievals from the system based on user inputs such as 'Capital crimes correspond to the worst types of crimes one can commit'."&lt;SEP&gt;"Symmetric Semantic Search is a type of semantic search where queries and sentences are embedded into the same vector space."&lt;SEP&gt;"Symmetric semantic search is a type of semantic search where it matches a query input with text, expecting similar meanings as retrieval results."&lt;SEP&gt;"Symmetric semantic search is a type of semantic search where the query and sentence embeddings are compared directly to find the closest matches."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;ASYMMETRIC SEMANTIC SEARCH&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Asymmetric Semantic Search is another type of semantic search where queries and sentences might be embedded into different vector spaces."&lt;SEP&gt;"Asymmetric Semantic Search provides answers in response to short queries by returning more substantial paragraphs or sentences, for example, 'What are the 20 consequences of robbing?'"&lt;SEP&gt;"Asymmetric semantic search is another type of semantic search providing answers to questions, usually returning more detailed paragraphs or sentences in response to short queries."&lt;SEP&gt;"Asymmetric semantic search is another type of semantic search where the comparison between queries and sentences takes into account their asymmetry in the search context."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;EXTRACTIVE TECHNIQUES&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Extractive techniques aim to retrieve the most important sentences from a document without considering their semantic meanings, focusing on selecting relevant sentences based on certain criteria."&lt;SEP&gt;"Extractive techniques retrieve important sentences from a document without considering their meaning, focusing on key phrases and information."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;ABSTRACTIVE TECHNIQUES&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Abstractive techniques use complex models to understand the semantics of documents, aiming to generate more coherent summaries."&lt;SEP&gt;"Abstractive techniques use more complex models to understand and extract the semantics of text documents for creating summaries that can be less literal but more coherent and informative."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;CAPITAL CRIMES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Capital crimes refer to severe offenses that typically result in the death penalty or are considered the worst types of crimes one can commit."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;QUERY EMBEDDING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A query embedding is a vectorial representation of a user's search input, used in semantic search for matching with text embeddings."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;SENTENCE EMBEDDING&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A sentence embedding is a vectorial representation of a sentence, used in semantic search to find similar sentences or match queries with text."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;VECTOR SPACE&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Vector Space is a mathematical concept used to represent words or sentences as vectors for semantic search."&lt;SEP&gt;"Vector space refers to the space where embeddings are represented and compared, enabling the matching of queries and sentences based on their similarity scores."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;CONNECTIVITY MATRIX&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A connectivity matrix is an adjacency matrix derived from intra-sentence cosine similarity, which represents how sentences are connected within a document."&lt;SEP&gt;"Connectivity matrix is used as an adjacency matrix for sentences, where cosine similarity or Jaccard similarity serves as edge weights."&lt;SEP&gt;"Connectivity matrix refers to a matrix representing the adjacency between elements in a graph, here based on intra-sentence cosine similarity for sentences."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;PAGERANK METHOD&quot;">
  <data key="d0">"ALGORITHM"</data>
  <data key="d1">"PageRank is an algorithm used to rank the importance of nodes in a graph, which is adapted here to calculate LexRank scores for sentences."&lt;SEP&gt;"The PageRank method calculates the relevance of nodes in a graph by iterating to find the stationary distribution of a Markov chain."&lt;SEP&gt;"The PageRank method is an algorithm used to rank nodes in a graph by considering their relevance and connectivity, utilized here to generate LexRank scores for phrases within sentences."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;LEXRANK SCORE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"LexRank score is a measure derived from the PageRank algorithm that assesses the importance of each phrase based on its similarity with other phrases in the document."&lt;SEP&gt;"LexRank score is calculated using PageRank and represents the importance or relevance of each sentence within the document."&lt;SEP&gt;"LexRank score is calculated using the PageRank algorithm, representing the importance of sentences based on their similarity scores."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS (BERT)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERT is a bidirectional encoder model that uses the Transformers architecture to understand text contextually."&lt;SEP&gt;"BERT is a bidirectional encoder model that uses the Transformers architecture toOf course! However, I need more specific information about which entities were missed and their context to add them accurately. Could you please provide details on what kind of entities (e.g., names, places, organizations</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;BOOKSCORPUS (800M WORDS) AND ENGLISH WIKIPEDIA (2500M WORDS)&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"These are datasets used for pre-training BERT, consisting of a large amount of text data from books and Wikipedia articles."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;PRE-TRAINING PHASE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The pre-training phase is a part of the BERT model's development where specific techniques are used to improve language understanding and prediction accuracy."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;TRANSFORMER ARCHITECTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Transformer architecture is the framework used by BERT to process and understand text sequences in a bidirectional manner."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;3.1.1 DOMAIN ADAPTATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This section discusses domain adaptation techniques for BERT models."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSAE)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TSAE is a technique mentioned in the text that can be used for domain adaptation."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;SUBSECTION 3.1.1&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Subsection 3.1.1 covers multiple DA techniques, specifically Masked Language Modeling and Transformer-based Sequential Denoising Auto-Encoder."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;UBIQUITOUS KNOWLEDGE PROCESSING LAB&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Ubiquitous Knowledge Processing Lab is a research team that developed GenQ, an unsupervised domain adaptation method for dense retrieval models."&lt;SEP&gt;"The Ubiquitous Knowledge Processing Lab team is responsible for developing GenQ, an unsupervised domain adaptation method for dense retrieval models."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;MLM (MASKED LANGUAGE MODEL)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"MLM is a previous approach used for language modeling before the introduction of TSDAE."&lt;SEP&gt;"MLM is a training technique used for generating embeddings."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;APRIL 14TH, 2021&quot;">
  <data key="d0">"DATE"</data>
  <data key="d1">"The date when TSDAE was firstly published by Nils Reimers."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;GENQ APPROACH&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The GenQ approach is an algorithm used in the Query Generation step to generate queries for multiple passages."</data>
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</node>
<node id="&quot;MULTIPLE NEGATIVES RANKING (MNR) LOSS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Multiple Negatives Ranking (MNR) loss is used in the fine-tuning process of models like SBERT with generated query pairs."</data>
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</node>
<node id="&quot;GENERATIVE PSEUDO LABELING (GPL)&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Generative Pseudo Labeling is an improved state-of-the-art technique to perform domain adaptation, proposed by a group of researchers from UKP lab."</data>
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</node>
<node id="&quot;SENTENCE PAIR COMPARISON&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Sentence Pair Comparison is an activity where SBERT and BERT are tested for speed and efficiency in finding similar sentences."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STS BENCHMARK (STSB)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"STS Benchmark is a popular dataset used to evaluate supervised semantic textual similarity systems."&lt;SEP&gt;"STS benchmark (STSb) is a dataset used for evaluating supervised Semantic Textual Similarity systems, which SBERT was trained on after pre-training on NLI datasets."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STANFORD NATURAL LANGUAGE INFERENCE (SNLI)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SNLI is one of the datasets used in NLI tasks, containing pairs of sentences with labels indicating their relationship."&lt;SEP&gt;"The SNLI is a dataset used for NLP tasks like natural language inference, containing pairs of sentences labeled as entailment, contradiction, or neutral."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;MULTI-GENRE NLI&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Multi-Genre NLI is another dataset used in NLI tasks, providing a diverse set of sentence pairs for training models."&lt;SEP&gt;"This refers to the Multi-Genre Natural Language Inference (NLI) dataset which includes diverse text genres."&lt;SEP&gt;"Multi-Genre NLI is another dataset used in the research, contributing diverse sentence pairs for training models."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STSB&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"STSb is a dataset used for evaluating sentence embeddings, focusing on natural language understanding tasks."&lt;SEP&gt;"STSb is a dataset used in evaluating sentence embeddings, focusing on natural language understanding tasks."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;ROBERTA&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"RoBERTa is a pre-trained model similar to BERT, used for various NLP tasks and evaluations."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;ERTIMBAU&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"ERTimbau is an organization involved in the development and training of BERTimbau models."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;ZHUYUN DAI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zhuyun Dai is a researcher who explored the use of BERT for ad-hoc document retrieval in IR, contributing insights into text understanding through contextual models."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;JAMIE CALLAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jamie Callan is a co-author with Zhuyun Dai on the study of using BERT for information retrieval, focusing on its effectiveness and the importance of context in natural language processing."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;ROBUST04&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Robust04 is one of the datasets tested in the performance evaluation of information retrieval techniques, focusing on well-written text."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;CLUEWEB09-B&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"ClueWeb09-B is another dataset used for testing the performance of information retrieval techniques, often alongside Robust04."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;JNLP&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"JNLP is a team that participated in the COLIEE 2021 competition and focused on using self-labeled approaches for task 1."&lt;SEP&gt;"JNLP refers to the team or organization that participated in COLIEE 2021 and focused on dealing with large articles using text chunking."&lt;SEP&gt;"JNLP represents a team participating in the COLIEE competition, focusing on combining lexical and semantic techniques."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;NIGAM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"nigam is a team name associated with an approach that combined transformer-based and traditional IR techniques for legal information retrieval tasks."&lt;SEP&gt;"nigam refers to the third team member who proposed an approach combining transformer-based and traditional IR techniques."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;MI-YONG KIM ET AL.&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Mi-Yong Kim et al. discuss deep learning techniques for legal information retrieval in COLIEE 2021."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;COLIEE 2021&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The COLIEE 2021 edition focused on four challenges: case law retrieval, case law entailment, statute law retrieval, and statute law entailment."&lt;SEP&gt;"The Competition on Legal Information Extraction and Entailment (COLIEE) is an event where teams compete to develop techniques for legal information retrieval."&lt;SEP&gt;"The COLIEE 2021 event focused on legal information extraction and entailment tasks, including case law retrieval and statute law retrieval."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;TASK 1&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Task 1 involves case law retrieval and entailment, focusing on large articles using text chunking and a self-labeled approach."&lt;SEP&gt;"Task 1 is an event in the COLIEE 2021 competition focusing on case law retrieval and entailment."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;TASK 3&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Task 3 focuses on statute law retrieval, using techniques like TF-IDF vectorization combined with Sentence-BERT embeddings."&lt;SEP&gt;"Task 3 is another event in the COLIEE 2021 competition focused on statute law retrieval."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;BERTIMBAU BASE (BERT -BASE)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERTimbau Base is a BERT model trained on Brazilian Portuguese Web and used in the Legal Semantic Search Engine system."&lt;SEP&gt;"BERTimbau Base is a BERT model used in the Legal Semantic Search Engine for Brazilian Portuguese text processing."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;JO ˜AO RODRIGUES ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jo ˜ao Rodrigues et al. are the authors of the Albertina model, a state-of-the-art BERT model for European Portuguese and Brazilian Portuguese encoder models."&lt;SEP&gt;"Jo ˜ao Rodrigues et al. are the authors who shared the Albertina PT model at the end of their research work in May 2023."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;ALBERTINA [35]&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Albertina is a new state-of-the-art BERT model developed by Jo ˜ao Rodrigues et al., representing the latest advancements in BERT models for Portuguese languages."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;COSINE SIMILARITY METRIC&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Cosine similarity is a measure used in the search index creation process to assess the relevance of query segments."&lt;SEP&gt;"Cosine similarity metric is a method used to evaluate the performance of different models in generating sentence embeddings."&lt;SEP&gt;"The cosine similarity metric is used for scoring segments and queries during the search process."</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;POPULATION BASED TRAINING (PBT)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Population Based Training (PBT) was used for hyperparameter optimization, combining Grid search and Hand Tuning."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;RAY TUNE LIBRARY&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Ray Tune library was used for hyperparameter search during the training process, indicating its importance in the workflow."&lt;SEP&gt;"The Ray Tune library was integrated into the Trainer class function for hyperparameter search during model training."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;DEBERTA [16]&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"DeBERTa is an architecture used as a starting point for developing the Albertina PT model. It is a state-of-the-art encoder model."&lt;SEP&gt;"DeBERTa is the architecture used as a starting point for developing the Albertina model."&lt;SEP&gt;"DeBERTa is referenced as the starting point for Albertina's architecture."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;ASSIM2 DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The assim2 dataset is used to evaluate the STS task, and Albertina PT BR outperforms BERTimbau on this particular dataset."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;FCUL - NLX – NATURAL LANGUAGE AND SPEECH GROUP&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"NLX – Natural Language and Speech Group is a group within the Faculdade de Ciências da Universidade de Lisboa (FCUL) involved in developing the Albertina PT model."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;FEUP - LABORATÓRIO DE INTELIGÊNCIA ARTIFICIAL E CIÊNCIA DE COMPUTADORES&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Laboratório de Inteligência Artificial e Ciência de Computadores is a lab within Faculdade de Engenharia da Universidade do Porto (FEUP) involved in developing the Albertina PT model."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;IRIS MEMBERS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"IRIS members are the team responsible for collecting data to populate the Legal-BERTimbau model and ElasticSearch indices."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;HTML CONTENT&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"HTML content indicates the format and structure of the legal document corpus, highlighting its digital nature."&lt;SEP&gt;"HTML content refers to the format of documents extracted and indexed into Elasticsearch."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;31690 DOCUMENTS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"31690 documents is the total count of legal documents indexed by the system."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;SENTENCETRANSFORMERS PYTHON LIBRARY&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SentenceTransformers Python Library is a library used for hosting the variants of BERTimbau created for various tasks."&lt;SEP&gt;"The SentenceTransformers Python library is used to create and manage embeddings with the Legal-BERTimbau model."&lt;SEP&gt;"This library is used to create and manage embeddings with the Legal-BERTimbau model." "&lt;SEP&gt;"The SentenceTransformers Python library was used to create sentence embeddings using Legal-BERTimbau model."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;PRE-PROCESSING&quot;">
  <data key="d0">"PROCESS"</data>
  <data key="d1">"The pre-processing involves splitting entire documents into smaller units to make Legal-BERTimbau more effective." "</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;DOCUMENTS IN THE ORIGINAL DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Documents refer to the original dataset used for training and processing." "</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;LEGAL-BERTIMBAU MODEL HOSTED ON THE HUGGING FACE HUB8&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Legal-BERTimbau model is hosted on the Hugging Face Hub and managed through the SentenceTransformers Python library." "</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;HUGGING FACE HUB&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Hugging Face Hub is a platform where models like Legal-BERTimbau are hosted."&lt;SEP&gt;"The Hugging Face Hub hosts the Legal-BERTimbau model for usage in sentence embedding generation." "</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;INDICES ON ELASTICSEARCH&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Indices are created in Elasticsearch where embeddings are stored for efficient retrieval." "</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;SBERT MODEL: LEGAL-BERTIMBAU&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This refers to the specific SBERT (Sentence-BERT) model that has been fine-tuned for legal document similarity tasks."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;TRAINING TASKS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The training of various versions of Legal-BERTimbau involved multiple tasks and adaptations, which are detailed in Chapter 5."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;OVERFITTING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Overfitting is a term mentioned as a potential issue when training models from scratch with small datasets, which highlights the need for transfer learning approaches like Legal-BERTimbau."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;STSB MULTI MT PORTUGUESE SUB-DATASET&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This dataset was used to fine-tune the models for the STS task, containing 8628 pairs of sentences and their similarity labels."&lt;SEP&gt;"This resource provides data with 8,628 pairs of sentences and labels for STS evaluation in the Portuguese language."</data>
  <data key="d2">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;ASSIN DATASET&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Assin is one of the datasets used for evaluating sentence similarity tasks."&lt;SEP&gt;"The assin dataset is a Brazilian Portuguese dataset with 10,000 sentence pairs, where 5,000 were used for training."&lt;SEP&gt;"This is an event or data resource containing 10,000 pairs of sentences for training STS tasks, with half used in the training process."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;ASSIN2 DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"ASSIN2 Dataset is a specific dataset used for evaluating sentence similarity tasks in Portuguese."&lt;SEP&gt;"Assin2 is another dataset used for evaluating sentence similarity tasks, similar to Assin but with more data points or variations."&lt;SEP&gt;"This is another Brazilian Portuguese dataset containing 9,448 sentence pairs, from which 6,500 were used for training."&lt;SEP&gt;"assin2 is another event/resource containing 9,448 pairs of sentences, from which 6,500 were used for training."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;STJIRIS/IRIS STS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This organization provided a custom STS dataset for further training models in the Portuguese legal domain."&lt;SEP&gt;"stjiris/IRIS sts is another dataset used for the STS task, indicating a collaborative effort in model development."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;SENTENCETRANSFORMER LIBRARY&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SentenceTransformer is a library used to create SBERT models from BERT variants."&lt;SEP&gt;"The SentenceTransformer library was used to define the SBERT version of Legal-BERTimbau-large."</data>
  <data key="d2">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;LEARNING RATE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A specific parameter, \(10^{-5}\), was used for the learning rate during model training."&lt;SEP&gt;"Learning rate refers to the step size at each iteration while moving toward a minimum of a loss function during training."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;BATCH SIZE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Batch size is the number of samples processed before the model weights are updated during training."&lt;SEP&gt;"The batch size of 8 was utilized during the training process with five epochs."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae&lt;SEP&gt;chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;EPOCHS&quot;">
  <data key="d0">"PARAMETER"</data>
  <data key="d1">"The model was trained over a period consisting of five epochs."</data>
  <data key="d2">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;MLM AND TSDAE DOMAIN ADAPTATION&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"These techniques were performed on BERTimbau before training the large version of the model."</data>
  <data key="d2">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;ASSIN AND ASSIN2 DATASETS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"ASSIN and ASSIN2 are datasets specifically designed for evaluating sentence similarity tasks in Portuguese."&lt;SEP&gt;"ASSIN and ASSIN2 are specific Portuguese-language datasets used for evaluating sentence similarity tasks."&lt;SEP&gt;"assin and assin2 datasets are used for the STS task, providing information about relatedness between sentences."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;CENTROID CALCULATION&quot;">
  <data key="d0">"PROCESS"</data>
  <data key="d1">"The process involves calculating the centroid of embeddings and adjusting them slightly towards it."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;10-6 LEARNING RATE&quot;">
  <data key="d0">"SETTING"</data>
  <data key="d1">"A learning rate of 10^-6 was used in the training process, a hyperparameter setting for optimization."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;3 SENTENCES BATCH SIZE&quot;">
  <data key="d0">"SETTING"</data>
  <data key="d1">"A batch size of 3 sentences per epoch was selected as part of the model's training parameters."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;1000 DOCUMENT SAMPLE&quot;">
  <data key="d0">"DATA SAMPLE"</data>
  <data key="d1">"The embeddings were adjusted based on a random sample of 1000 documents from the training set."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;GRID SEARCH OPTIMIZATION ALGORITHM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A grid search optimization algorithm is a method used for hyperparameter tuning in machine learning models, systematically trying different combinations of parameters."&lt;SEP&gt;"A grid search optimization algorithm was used to define hyperparameters and optimize the model's performance."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;STS TASK EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Evaluation of the model's performance is done using the STS task methodology."&lt;SEP&gt;"The model's performance was evaluated using the STS task, a metric for sentence similarity tasks."&lt;SEP&gt;"STS stands for Semantic Textual Similarity task, which evaluates how well models can measure the similarity between texts."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Another version of the BERT model with similar features to v0 but potentially updated or refined."&lt;SEP&gt;"This is a variation of the previous model, continuing the same research line."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;DOCUMENTS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The documents are related to the subject of COVID-19 and involve various techniques for embedding processing."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;TSDAE (TEXT SEGMENTATION AND DATA AUGMENTATION)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TSDAE refers to a specific training technique used in some of the models."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;GPL (GENERALIZATION AND TRANSFER LEARNING)&quot;">
  <data key="d0">"SETTING"</data>
  <data key="d1">"GPL settings are used to improve model generalizability and transfer learning capabilities."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;TSDAE-MKD-NLI-STS-V1&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"tsdae-mkd-nli-sts-v1 is a variant of Legal-BERTimbau, part of a set of language models tailored for Portuguese legal context."&lt;SEP&gt;"tsdae-mkd-nli-sts-v1 is one of the variants of a language model used in the project."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;MLM-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"mlm-gpl-nli-sts-MetaKD-v0 is another variant of Legal-BERTimbau, designed for the Portuguese legal domain."&lt;SEP&gt;"mlm-gpl-nli-sts-MetaKD-v0 is another variant of a language model used in the project."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;MLM-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"mlm-gpl-nli-sts-MetaKD-v1 is yet another variant of Legal-BERTimbau, focusing on the Portuguese legal context."&lt;SEP&gt;"mlm-gpl-nli-sts-MetaKD-v1 is yet another variant of a language model used in the project."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;TSDAE-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This is the best-performing model selected for analysis, showing promising results with improved metrics."::&lt;SEP&gt;"tsdae-gpl-nli-sts-MetaKD-v0 is a specific variant of Legal-BERTimbau, developed for the Portuguese legal scenario."&lt;SEP&gt;"tsdae-gpl-nli-sts-MetaKD-v0 is one of the variants of a language model used in the project."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;TSDAE-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"tsdae-gpl-nli-sts-MetaKD-v1 is another variant of a language model used in the project."&lt;SEP&gt;"tsdae-gpl-nli-sts-MetaKD-v1 is another version of the Legal-BERTimbau models, designed for Portuguese legal context."</data>
  <data key="d2">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</node>
<node id="&quot;LARGE LANGUAGE MODEL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Large Language Model refers to the context of models like BERT that are trained on extensive datasets and have large numbers of parameters."&lt;SEP&gt;"The large language model is a key component of the search system architecture used to determine semantic similarity between sentences."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95&lt;SEP&gt;chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;ORIGINAL STSBENCHMARK DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The original STSbenchmark dataset is mentioned as the source of different multilingual translations in the stsb multi mt dataset."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;ASSIM BENCHMARK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Assin and Assin2 are benchmark tasks used to evaluate model performance on Portuguese datasets."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;STS EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"STS Evaluation refers to a quantitative assessment of the performance of SBERT variants compared to state-of-the-art multilingual models on Portuguese datasets."&lt;SEP&gt;"STS evaluation is the process of assessing model performance on various datasets."&lt;SEP&gt;"STS evaluation is the semantic textual similarity task that needs to be performed after adapting the model variants."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;PORTUGUESE DATASETS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Portuguese datasets are specific data sets used in the STS evaluation for language models."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;ELASTICSEARCH INDEX&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"An ElasticSearch index where embeddings of other summaries were stored during the Negative Mining stage."&lt;SEP&gt;"ElasticSearch index is a service or technology that stores and retrieves document data, used in this context for storing sentence embeddings."&lt;SEP&gt;"ElasticSearch index is a technology used to store and retrieve document embeddings generated by Legal-BERTimbau."&lt;SEP&gt;"ElasticSearch index is the storage location where sentence embeddings are indexed and stored."&lt;SEP&gt;"ElasticSearch index stores sentence embeddings for evaluation purposes in search system performance testing."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;SENTENCE-TRANSFORMERS/ALL-MPNET-BASE-V2&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Sentence-transformers/all-mpnet-base-v2 is a multilingual model used for generating sentence embeddings in this context."&lt;SEP&gt;"Sentence-transformers/all-mpnet-base-v2 is another multilingual model that was used as a baseline for evaluating the performance of Legal-BERTimbau in semantic searches."&lt;SEP&gt;"Sentence-transformers/all-mpnet-base-v2 is another multilingual model used for sentence embeddings."</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</node>
<node id="&quot;STS DATASET&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"STS (SemEval Task 4) dataset refers to the custom dataset used for fine-tuning models."&lt;SEP&gt;"STS (Semantics Textual Similarity) dataset is a custom dataset used to fine-tune the Legal-BERTimbau model without and with STS data for performance evaluation."</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</node>
<node id="&quot;TOP RESULTS SIZES&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Top results sizes refer to different result sets evaluated in terms of their relevance and accuracy, such as Top 1, Top 2, etc., during the performance assessment of semantic search systems."&lt;SEP&gt;"Top results sizes refer to different result sets evaluated in the performance of the Semantic Search System, such as Top 1, Top 2, etc."</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</node>
<node id="&quot;CUSTOM STS DATASET (V1 MODELS)&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"Models fine-tuned on a custom Semantic Textual Similarity (STS) dataset show slightly lower performance compared to those using pre-existing and manually annotated datasets (V0 models)."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;PRE-EXISTING AND MANUALLY ANNOTATED DATASETS (V0 MODELS)&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"Models fine-tuned only on existing and manually annotated data perform better than V1 models in certain metrics."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;ORIGINAL SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The original search system used as a baseline, which performs worse than BM25 in terms of identifying query sources."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;MODELS V0 AND V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Two sets of models, V0 and V1, where V1 uses a custom STS dataset and performs slightly worse than V0."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;TOP 1, TOP 2, TOP 3, TOP 5, TOP 10&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Top N refers to the top N results in terms of relevance for a given query, used as performance metrics in search system evaluations."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;DOCTORATE STUDENTS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Doctorate students are involved in the project as collaborators or mentors, contributing to the development of advanced search systems."|</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;SOFT SKILLS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Soft skills refer to interpersonal attributes such as communication, teamwork, and leadership that are crucial in a collaborative project environment."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;HARD SKILLS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Hard skills are technical abilities such as programming and data processing that were developed during the project."|</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;SEARCH SYSTEM EVALUATION – SEARCH METRIC&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This event involves evaluating the performance metrics for search systems."|</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;DATA PRE-PROCESSING&quot;">
  <data key="d0">"PROCESS"</data>
  <data key="d1">"Data Pre-Processing refers to the initial steps in handling raw text data before performing further tasks."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;LEXRANK TECHNIQUE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"LexRank is a method used for extractive summarization and was part of the research on Legal-BERTimbau models."&lt;SEP&gt;"The LexRank technique is an extractive summarization method implemented to optimize the task of summarizing legal documents."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;ROUGE-1 SCORE 47.92, ROUGE-2 SCORE 22.50&quot;">
  <data key="d0">"METRIC"</data>
  <data key="d1">"These are metrics indicating the performance of Legal-BERTimbau model in text summarization tasks."&lt;SEP&gt;"These are the performance metrics of Legal-BERTimbau models in comparison to similar works."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;PILE OF LAW&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Pile of Law is a dataset published on July 1, 2022, by Peter Henderson et al., which includes a large corpus of legal and administrative data from multiple U.S. entities and addresses ethical challenges in data filtering."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;PETER HENDERSON&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Peter Henderson is an author of the Pile of Law paper, contributing to discussions on responsible data filtering in the legal domain."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;EMBEDDING GENERATION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Generating embeddings involves converting text into numerical vectors to facilitate various NLP tasks."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;USER’S QUERY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A user's query is the input provided by a user for which the system needs to generate a response or retrieve relevant documents."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;SEARCH SYSTEM’S RESPONSE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The Search System's response is the output provided by the system to a user’s query or task."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;USER’S FEEDBACK&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"User's feedback is input provided by users to the system regarding the relevance or accuracy of generated responses."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;DOCUMENT RETRIEVAL&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The process of retrieving relevant documents from a database based on a query or task."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;KNOWLEDGE GRAPH&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A knowledge graph is a structured representation of information that can be used to improve the accuracy and context of search results."&lt;SEP&gt;"A structured representation used for better context understanding in the query expansion system."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;USER’S INTERACTION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The process of a user interacting with the Search System through queries and receiving responses or documents."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The 2013 SemEval shared task focused on semantic textual similarity."&lt;SEP&gt;"The 2013 shared task focused on semantic textual similarity and was part of the Joint Conference on Lexical and Computational Semantics (SEM) in Atlanta, Georgia, USA."&lt;SEP&gt;"The 2013 shared task focused on semantic textual similarity as part of SemEval."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SEMEVAL-2012 TASK 6: A PILOT ON SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2012 Task 6 was a pilot task aimed at semantic textual similarity and part of the Joint Conference on Lexical and Computational Semantics (SEM) in USA."&lt;SEP&gt;"The 2012 SemEval Task 6 was a pilot study focused on semantic textual similarity."&lt;SEP&gt;"The 2012 task focused on a pilot study for semantic textual similarity as part of SemEval."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SEMEVAL-2017 TASK 1: SEMANTIC TEXTUAL SIMILARITY MULTILINGUAL AND CROSSLINGUAL FOCUSED EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2017 Task 1 focused on semantic textual similarity in multiple languages and was part of the International Workshop on Semantic Evaluation (SemEval-2017) in Vancouver, Canada."&lt;SEP&gt;"The 2017 SemEval Task 1 evaluated semantic textual similarity tasks across multiple languages."&lt;SEP&gt;"The 2017 task focused on a multilingual and crosslingual evaluation of semantic textual similarity as part of SemEval."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;KIELA, D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"D Kiela is another researcher contributing to the development of deep bidirectional transformers for language understanding."&lt;SEP&gt;"Daniele Kiela contributed to tasks and workshops focused on deep learning models for text understanding."&lt;SEP&gt;"Daniele Kiela is another author contributing to tasks and workshops focused on deep learning models for text understanding."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;BARRAULT, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"L Barrault is a co-author involved in the development of BERT for language understanding."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;DAI, Z.&quot; &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Z Dai is a researcher focusing on deeper text understanding for information retrieval using contextual neural language modeling."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;CALLAN, J.&quot; &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J Callan has contributed to the development of deep bidirectional transformers and natural language inference data for IR."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;BOWMAN, S. R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"S.R. Bowman is a researcher involved in tasks related to natural language processing."&lt;SEP&gt;"Sam Bowman is an author involved in creating a large annotated corpus for natural language inference."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;ANGELI, G.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"G Angeli has contributed to large annotated corpora for learning natural language inference."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;POTTS, C.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"C Potts is a researcher contributing to the development of deep bidirectional transformers and natural language understanding models."&lt;SEP&gt;"Chris Potts is an author involved in creating a large annotated corpus for natural language inference."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;MANNING, C. D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"C.D. Manning has been involved in supervised learning of universal sentence representations and natural language inference data for IR."&lt;SEP&gt;"Manning contributes to the Pile of Law project in collaboration with Henderson, Kras, and Guha."&lt;SEP&gt;"Christopher Manning contributed to the creation of a large annotated corpus for natural language inference."&lt;SEP&gt;"Manning collaborates with Guha on learning responsible data filtering from the law and a 256GB open-source legal dataset."&lt;SEP&gt;"Manning is involved in the work of learning responsible data filtering from law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8&lt;SEP&gt;chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;DIAZ-GAZPIO, I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"I Diaz-Gazpio is a researcher involved in the 2017 SemEval task related to semantic textual similarity evaluation."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SPECIA, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"L Specia has contributed to multilingual and crosslingual focused evaluations of semantic textual similarity tasks."&lt;SEP&gt;"Laura Specia is an author involved in tasks and workshops related to semantic textual similarity and natural language processing."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot; TOUTANOVA, K.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Toutanova co-authored BERT with Devlin, Chang, and Lee."&lt;SEP&gt;"Toutanova co-authored BERT: Pre-training of deep bidirectional transformers for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ERKAN, G.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Erkan is the author of LexRank, a graph-based lexical centrality measure for text summarization."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;RAEV, D. R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Raev co-authored LexRank with Erkan."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SANTOS, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Santos collaborated with Fonseca on the ASSIN project."&lt;SEP&gt;"Santos collaborates with Fonseca on the ASSIN project."&lt;SEP&gt;"Santos contributed to the development of ASSIN with Fonseca."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;CRISCUOLO, M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Criscuolo co-authored ASSIN with Fonseca and Santos."&lt;SEP&gt;"Criscuolo is involved in the ASSIN project focusing on semantic similarity and textual inference."&lt;SEP&gt;"Criscuolo was involved in the ASSIN project focusing on semantic similarity and textual inference."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ALUISIO, S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Aluisio collaborated with Fonseca on the ASSIN project."&lt;SEP&gt;"Aluisio collaborates with Fonseca on the ASSIN project."&lt;SEP&gt;"Aluisio is a co-author of the ASSIN system along with Fonseca and Santos."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GURURARANG, S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Gururarang developed Don't Stop Pretraining in collaboration with other authors."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MARASOVIC, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Marasovic is a co-author of the Don't Stop Pretraining paper along with Gururarang and others."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SWAYAMDIPTA, S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Swayamdipta contributed to the development of Don't Stop Pretraining with Gururarang and Marasovic."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;LO, K.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lo is a co-author on papers like Don't Stop Pretraining."&lt;SEP&gt;"Lo is involved in the work of adapting language models to domains and tasks through pretraining."&lt;SEP&gt;"Lo was involved in the work of adapting language models to domains and tasks through pretraining."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BELTAGY, I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Beltagy collaborated with Gururangan on adapting language models to domains and tasks through pretraining."&lt;SEP&gt;"Beltagy collaborates with Gururangan on adapting language models to domains and tasks through pretraining."&lt;SEP&gt;"Beltagy works with Lo, Downey, and Smith on projects involving language models."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;DOWNEY, D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Downey collaborated with Gururangan on adapting language models to domains and tasks through pretraining."&lt;SEP&gt;"Downey collaborates with Beltagy, Lo, and Smith in research related to language models."&lt;SEP&gt;"Downey collaborates with Gururangan on adapting language models to domains and tasks through pretraining."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SMITH, N. A. A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Smith is involved in the development of Don't Stop Pretraining with Beltagy and others."&lt;SEP&gt;"Smith is involved in the work of adapting language models to domains and tasks through pretraining."&lt;SEP&gt;"Smith was involved in the work of adapting language models to domains and tasks through pretraining."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;HE, P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"He authored DeBERTA along with Liu, Gao, and Chen."&lt;SEP&gt;"He is a developer of DeBERTa: Decoding-enhanced BERT with disentangled attention."&lt;SEP&gt;"He is involved in developing DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;LIU, X.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Liu co-authored DeBERTA with He, Gao, and Chen."&lt;SEP&gt;"Liu collaborated with He on developing DeBERTa: Decoding-enhanced BERT with disentangled attention."&lt;SEP&gt;"Liu collaborates with He on developing DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GAO, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Gao is a co-author of the DeBERTA paper along with He and Liu."&lt;SEP&gt;"Gao is a developer of DeBERTa: Decoding-enhanced BERT with disentangled attention."&lt;SEP&gt;"Gao is involved in the work of developing DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;CHEN, W.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Chen collaborated with He on developing DeBERTa: Decoding-enhanced BERT with disentangled attention."&lt;SEP&gt;"Chen collaborates with He on developing DeBERTa: Decoding-enhanced BERT with disentangled attention."&lt;SEP&gt;"Chen contributed to the creation of DeBERTA with He, Gao, and Liu."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KRAS, M. S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kras collaborated with Henderson on learning responsible data filtering from the law and a 256GB open-source legal dataset."&lt;SEP&gt;"Kras collaborates with Henderson on learning responsible data filtering from law and a 256GB open-source legal dataset."&lt;SEP&gt;"Kras works with Henderson on the Pile of Law project focused on learning from legal data."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ZHENG, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zheng collaborates with Henderson and Kras in the Pile of Law project."&lt;SEP&gt;"Zheng is involved in learning responsible data filtering from the law and a 256GB open-source legal dataset."&lt;SEP&gt;"Zheng is involved in the work of learning responsible data filtering from law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GUHA, N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Guha is involved in learning responsible data filtering from the law and a 256GB open-source legal dataset."&lt;SEP&gt;"Guha is involved in the work of learning responsible data filtering from law and a 256GB open-source legal dataset."&lt;SEP&gt;"Guha is part of the team that works on the Pile of Law project with Henderson and Kras."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;JURAFSKY, D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jurafsky is involved in learning responsible data filtering from the law and a 256GB open-source legal dataset."&lt;SEP&gt;"Jurafsky is involved in the development of the Pile of Law project with Henderson, Kras, and Guha."&lt;SEP&gt;"Jurafsky is involved in the work of learning responsible data filtering from law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;HO, D. E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ho collaborates on the Pile of Law project along with Henderson, Kras, and Guha."&lt;SEP&gt;"Ho collaborates with Henderson on learning responsible data filtering from the law and a 256GB open-source legal dataset."&lt;SEP&gt;"Ho is involved in the work of learning responsible data filtering from law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;IJITEE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"IJITEE is an international journal for innovative technology and exploring engineering."&lt;SEP&gt;"IJITEE stands for International Journal of Innovative Technology and Exploring Engineering, which published an article on artificial intelligence-based analysis in the legal domain."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;VAISSNAVE, V.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"VaissnaVe co-authored a paper on AI analysis in the legal domain with Deepalakshmi P."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;DEEPALAKSHMI, P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Deepalakshmi is a co-author of a paper on AI analysis in the legal domain with VaissnaVe V."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KIM, M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kim is an author of a paper on BM25 and transformer-based legal information extraction and entailment."&lt;SEP&gt;"Kim is involved in researching BM25 and transformer-based legal information extraction and entailment."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;RABELO, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Rabelo co-authored the research with Kim on legal information extraction and entailment using transformers."&lt;SEP&gt;"Rabelo collaborates with Kim on researching BM25 and transformer-based legal information extraction and entailment."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GOEBEL, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Goebel contributed to a paper on BM25 and transformer-based legal information extraction and entailment with Kim and Rabelo."&lt;SEP&gt;"Goebel is a co-author of the work on BM25 and transformer-based legal information extraction and entailment."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KINGMA, D. P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kingma developed Adam: A method for stochastic optimization."&lt;SEP&gt;"Kingma is involved in the development of Adam: A method for stochastic optimization."&lt;SEP&gt;"Kingma is the author of Adam: A Method for Stochastic Optimization along with Ba J."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BA, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ba co-authored the Adam stochastic optimization method with Kingma."&lt;SEP&gt;"Ba collaborated with Kingma on the development of Adam: A method for stochastic optimization."&lt;SEP&gt;"Ba collaborates with Kingma on the development of Adam: A method for stochastic optimization."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;T. MIKOLOV AND OTHERS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"T. Mikolov along with K. Chen, G. Corrado, and J. Dean are authors who presented at the 1st International Conference on Learning Representations (ICLR 2013)."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;N GUYEN ET AL.&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"H N Guyen and co-authors from JNLP team published a paper on deep learning for legal processing in COLIEE 2020."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;PENGCHENG CENTER FOR ARTIFICIAL INTELLIGENCE RESEARCH&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Pengcheng Center for Artificial Intelligence Research is the organization where C. Raffel and co-authors are affiliated."&lt;SEP&gt;"The Pengcheng Center for Artificial Intelligence Research is implied by the work done by C. Raffel and co-authors."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;REAL, E. FONSECA, AND H. G. OLIVEIRA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"L Real, E. Fonseca, and H. G. Oliveira are the authors of The assin 2 shared task quick overview paper."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;COLIEE 2020&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"COLIEE 2020 is a competition where research teams like the JNLP team present their work."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;REAL ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Real, P., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. are the authors of MS MARCO dataset."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;JOURNAL OF MACHINE LEARNING RESEARCH&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Journal of Machine Learning Research is a peer-reviewed scientific journal for the publication of research articles, reviews and technical notes in machine learning."&lt;SEP&gt;"Journal of Machine Learning Research is an academic journal that publishes papers on machine learning."&lt;SEP&gt;"Journal of Machine Learning Research publishes research papers in the field of machine learning and related areas."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;INTER-INTERNATIONAL CONFERENCE ON COMPUTATIONAL PROCESSING OF THE PORTUGUESE LANGUAGE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The International Conference on Computational Processing of the Portuguese Language (ICCPPL) is an event where researchers present their work on computational linguistics and natural language processing."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;P.J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"P.J. is an author associated with the research on transfer learning and unified text-to-text transformer."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;L.U., AND POLOSUKHIN , I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"L.U. and I. POLOSUKHIN are co-authors of the 'Attention is all you need' paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;B UROVSKI , E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"E. B UROVSKI is an author of the SciPy 1.0 paper."&lt;SEP&gt;"E. Burovski is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;V AN DER WALT, S. J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"S. J. V AN DER W ALT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"S. J. V AN DER WALT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"S. J. V AN DER WALT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"S. J. van der Walt is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;W ILSON , J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. W ILSON is an author of the SciPy 1.0 paper."&lt;SEP&gt;"J. Wilson is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;P ERKTOLD , J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. P ERKTOLD is an author of the SciPy 1.0 paper."&lt;SEP&gt;"J. Perktold is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;H ENRIKSEN , I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"I. H ENRIKSEN is an author of the SciPy 1.0 paper."&lt;SEP&gt;"I. Henriksen is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;H ARRIS , C. R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"C. R. H ARRIS is an author of the SciPy 1.0 paper."&lt;SEP&gt;"C. R. Harris is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;A RCHIBALD , A. M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"A. M. A RCHIBALD is an author of the SciPy 1.0 paper."&lt;SEP&gt;"A. M. Archibald is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;P EDREGOSA , F.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"F. P EDREGOSA is an author of the SciPy 1.0 paper."&lt;SEP&gt;"F. Pedregosa is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;V ANMULBREGT, P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"P. V ANMULBREGT is an author of the SciPy 1.0 paper."&lt;SEP&gt;"P. Van Mulbregt is an author of SciPy 1.0."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;K. WANG &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"K. WANG is an author of multiple papers including 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' and 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval'."&lt;SEP&gt;"K. Wang is an author involved in multiple research papers."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;N. R EIMERS &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N. R EIMERS is an author of multiple papers including 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' and 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval'."&lt;SEP&gt;"N. Reimers is an author involved in multiple research papers."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;I. GUREVYCH &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"I. GUREVYCH is an author of multiple papers including 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' and 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval'."&lt;SEP&gt;"I. Gurevych is an author involved in multiple research papers."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;SENTENCE-BERT&quot;">
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
  <data key="d1">"SBERT is part of the Sentence-BERT family, highlighting their shared lineage or relationship."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;DATASET&quot;">
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
  <data key="d1">"The IRIS project involves developing datasets from legal documents."&lt;</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;LANGUAGE MODEL&quot;">
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
  <data key="d1">"Language models are integrated into the search system to enhance its functionality."&lt;</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;OUTPUT OF OUTPUT GATE&quot;">
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
  <data key="d1">"The hidden vector is derived from the combination of the Output Gate's sigmoid output and the tanh layer, representing the processed data ready for transmission to the next cell." &lt;|"data processing, transmission"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;DIMENSIONALITY OUTPUT&quot;">
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
  <data key="d1">"The Feed-Forward Neural Network produces outputs with 512 dimensions, crucial for processing and encoding sequence information in the Transformer model." &lt;|"output dimensionality, processing"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STATE-OF-THE-ART MODELS&quot;">
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
  <data key="d1">"BERT is one of several state-of-the-art models discussed in this chapter, showcasing its significance in NLP advancements."&lt;SEP&gt;"BERT represents an advancement in the field of Natural Language Processing (NLP) as part of state-of-the-art models."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;INPUT EMBEDDINGS&quot;">
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
  <data key="d1">"The Token Embedding Layer forms part of the input embeddings along with Segmentation and Position Embedding Layers, contributing to the overall representation of words in BERT."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;NEGATIVE MINING STEP&quot;">
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
  <data key="d1">"The Query Generation step generates positive and negative passages which are then mined in the Negative Mining step."|&lt;&gt;"query generation, passage mining"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;PSEUDO LABELING STEP&quot;">
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
  <data key="d1">"Negative passages retrieved by the Negative Mining step are used as inputs for the Pseudo Labeling step to train a Cross-Encoder model."|&lt;&gt;"passage retrieval, model training"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;MNR LOSS&quot;">
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
  <data key="d1">"SBERT is fine-tuned using MNR loss with generated query pairs."|&lt;&gt;"fine-tuning, loss function"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SENTENCE EMBEDDING GENERATION&quot;">
  <data key="d2">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
  <data key="d1">"BERT model is used in generating sentence embeddings through mean pooling of all tokens present in a sentence."|&lt;&gt;"embedding generation, token processing"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STSBENCHMARK (STSB)&quot;">
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
  <data key="d1">"STSBenchmark (STSb) was used for supervised learning, training SBERT on labeled data."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SNLI AND MULTI-GENRE NLI&quot;">
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
  <data key="d1">"The researchers used SNLI and Multi-Genre NLI datasets for unsupervised learning tasks."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;BERT -FIRSTP&quot;">
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
  <data key="d1">"The dataset Robust04 was used in testing the performance with scores related to the score of the first passage (BERT -FirstP), evaluating BERT's understanding in well-written text contexts."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;BERT -MAXP&quot;">
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
  <data key="d1">"The dataset ClueWeb09-B was used in testing the performance with scores related to the best passage (BERT -MaxP), assessing BERT's ability to understand the highest-scoring passages."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;BERT -SUMP&quot;">
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
  <data key="d1">"The dataset Robust04 was also used in testing the performance with scores related to the sum of all passage scores (BERT -SumP), evaluating BERT's comprehensive understanding across multiple passages."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;OVGU TEAM&quot;">
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
  <data key="d1">"OvGU used two-stage TF-IDF vectorization combined with Sentence-BERT embeddings for the statute law retrieval task of COLIEE 2021."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;NIGAM TEAM&quot;">
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
  <data key="d1">"The nigam team proposed an approach combining transformer-based and traditional IR techniques for Task 1 using SBERT and Sent2Vec combined with BM25."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;INESC-ID, IMPRENSA NACIONAL-CASA DA MOEDA&quot;">
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
  <data key="d1">"INESC-ID and Imprensa Nacional-Casa da Moeda partnered to develop LeSSE as a system for legal document retrieval and semantic search."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1&quot;">
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d&lt;SEP&gt;chunk-ce4847f54b29367988561206721bdbb7</data>
  <data key="d1">"Another advanced variant, part of the fine-tuning study process."&lt;SEP&gt;"This model is derived from Legal-BERTimbau-large as the student for Portuguese learning."&lt;SEP&gt;"stjiris is a component of the developed model, possibly part of an organization or project."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;METAKD (METADATA KNOWLEDGE DISTILLATION)&quot;">
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
  <data key="d1">"MetaKD is a new technique developed for improving information retrieval through dense vectors by encoding metadata tags."&lt;SEP&gt;"MetaKD is applied to further train the model for better performance in specific tasks."</data>
  <data key="d0">"TECHNOLOGY"</data>
</node>
<node id="&quot;TED 2020 – PARALLEL SENTENCES CORPUS&quot;[33]">
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
  <data key="d1">"This dataset is used as part of the training for the model, contributing to its knowledge base."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;LEARNING RATE, BATCH SIZE&quot;">
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
  <data key="d1">"The learning rate and batch size settings are crucial in adjusting embeddings during training."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;DOCUMENT SAMPLE&quot;">
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
  <data key="d1">"The document sample is used to calculate centroids for embedding adjustment."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;QUERY GENERATION&quot;">
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40&lt;SEP&gt;chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
  <data key="d1">"The ElasticSearch index stores sentence embeddings generated from 1000 legal documents to evaluate query generation techniques in the search system."&lt;SEP&gt;"This refers to a specific step in the GPL process where queries are generated from document summaries."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;TOP 1&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"BM25 shows different performance levels at various Top N results, indicating its baseline ranking function."|&lt;&gt;"baseline, performance comparison"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TOP 3&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Lexical-First outperforms Purely Semantic but underperforms BM25 at Top 3 results."|&lt;&gt;"performance balance, intermediate comparison"&lt;SEP&gt;"Lexical-First outperforms Purely Semantic but underperforms BM25 in Top 3 results, indicating a balanced approach between lexical and semantic features."|&lt;&gt;"performance balance, intermediate comparison"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TOP 10&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Lexical + Semantic shows the best performance across all Top N results, especially at Top 10, demonstrating its comprehensive approach."|&lt;&gt;"comprehensive approach, superior performance"&lt;SEP&gt;"Lexical + Semantic shows the best performance across all Top N results, especially at Top 10."|&lt;&gt;"comprehensive approach, superior performance"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;WORK PRESENTED&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"The work is a segment of the larger Project IRIS, emphasizing its contribution to the development and exploration of advanced search systems."|&lt;&gt;"project scope, research focus"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SEARCH SYSTEM EVALUATION – DISCOVERY METRIC&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Purely Semantic's performance is evaluated for its discovery capabilities in the Discovery metric."|&lt;&gt;"discovery capability evaluation, baseline comparison"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FORMULATING A DECISION&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Judges use the search system to formulate decisions in the legal domain."|&lt;&gt;"decision-making process, user application"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FORMULATING A DECISION BASED ON INCOMPLETE INFORMATION LIMITATION AND RISKS&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Incomplete information poses limitations and risks when formulating judicial decisions."|&lt;&gt;"information quality impact, risk assessment"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;INCOMPLETE INFORMATION&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Incomplete information poses limitations and risks when formulating judicial decisions."|&lt;&gt;"information quality impact, risk assessment"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;JUDICIAL WORK EASE AND ENRICHMENT&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Relevant and insightful information helps ease and enrich the work of judges in the legal domain."|&lt;&gt;"benefit realization, application effectiveness"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;RELEVANT AND INSIGHTFUL INFORMATION&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Relevant and insightful information helps ease and enrich the work of judges in the legal domain."|&lt;&gt;"benefit realization, application effectiveness"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;IRRELEVANT INFORMATION&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Irrelevant information contributes to the limitations and risks when formulating decisions based on incomplete data."|&lt;&gt;"relevance impact, risk assessment"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STJ PROTOTYPE&quot;">
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
  <data key="d1">"Project IRIS members developed the STJ Prototype for judicial use."|&lt;&gt;"team contribution, system implementation"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;DATASET ANNOTATION&quot;">
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
  <data key="d1">"Dataset Annotation is a specific task mentioned under Future Work in section 7.2, related to improving data quality through annotation processes."&lt;SEP&gt;"Development and improvement of datasets are crucial for the advancement of models like Legal-BERTimbau, affecting their performance and applicability in legal domains."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;SUPERVISED LEARNING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;">
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
  <data key="d1">"D Kiela's work contributed to the development and application of BERT in natural language processing."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;INTERNATIONAL CONFERENCE ON COMPUTATIONAL PROCESSING OF THE PORTUGUESE LANGUAGE&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2&lt;SEP&gt;chunk-9b1e1352f338d948b4876b635d24d01c</data>
  <data key="d1">"The International Conference on Computational Processing of the Portuguese Language is a conference where multiple researchers present their work."&lt;SEP&gt;"Their work was presented at this conference."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;COLIEE 2022&quot;">
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
  <data key="d1">"The nigam@coliee-22 group participated in COLIEE 2022 with their research."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ACL&quot;">
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-dc84a2863207e7e3c024c8d2254064a2&lt;SEP&gt;chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
  <data key="d1">"ACL is an organization involved in semantic evaluation tasks, as referenced in the bibliography."&lt;SEP&gt;"The ACL is the organization that hosts events like ICCPPL where N. Reimers and I. Gurevych present their work."&lt;SEP&gt;"ACL is the Association for Computational Linguistics, which hosts and publishes many of these conferences and papers."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"Association for Computational Linguistics is a professional organization that supports the field of computational linguistics."&lt;SEP&gt;"The ACL is the organization that hosts events like ICCPPL where N. Reimers and I. Gurevych present their work."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;ACM, JMLR&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"N. Robertson's work on the probabilistic relevance framework is published in the Journal of Machine Learning Research, indicating a relationship with ACM and JMLR."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;F. SOUZA, R. NOGUEIRA, R. LOTUFO&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"These researchers have contributed to named entity recognition using BERT-CRF techniques in Brazilian Portuguese, which aligns with the focus of N. Reimers and I. Gurevych's work."&lt;SEP&gt;"These researchers have contributed to named entity recognition using BERT-CRF techniques in Brazilian Portuguese, which aligns with the focus of Reimers and Gurevych's work."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;N. REIMERS, I. GUREVYCH&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"These researchers have contributed to named entity recognition using BERT-CRF techniques in Brazilian Portuguese, which aligns with the focus of N. Reimers and I. Gurevych's work."&lt;SEP&gt;"These researchers have contributed to named entity recognition using BERT-CRF techniques in Brazilian Portuguese, which aligns with the focus of Reimers and Gurevych's work."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;A. VASWANI, N. SHAZEER, N. PARMAR, J. USZKOREIT, L. JONES, A.N. GOMEZ, L.U. KAISER, I. POLOSUKHIN&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"Both researchers have contributed to multiple research areas in natural language processing, showing a relationship through shared interests and potential collaboration."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;L.I.W.&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"Both authors have contributed to the same research topic on transfer learning and unified text-to-text transformer, indicating a potential collaborative relationship."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;J. RODRIGUES, L. GOMES, J. SILVA, A. BRANCO, R. SANTOS, H.L. CARDOSO, T. OSÓRIO&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"The Portuguese researchers have contributed to named entity recognition using BERT-CRF techniques in Brazilian Portuguese, which aligns with the focus of N. Reimers and I. Gurevych's work on language processing models."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;P.J., L.I.W.&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"X. Rong has written about Word2Vec parameter learning, which could be relevant to the research by P.J. and L.I.W. on transfer learning."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;X. RONG&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"X. Rong has written about Word2Vec parameter learning, which could be relevant to the research by P.J. and L.I.W. on transfer learning."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;N. THAKUR, A. RÜCKLÉ, A. SRIVASTAVA, I. GUREVYCH&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"Both groups of researchers have contributed to multiple research areas in natural language processing, showing a relationship through shared interests and potential collaboration."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SUPERVISORS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The supervisors are the guiding authority for the thesis and provide necessary support, advice, and feedback to the author."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;PROF. MARIA LUÍS TORRES RIBEIRO MARQUES DA SILVA COHEUR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Maria Luís Torres Ribeiro Marques da Silva Coheur is the chairperson of the examination committee who oversees the final review process."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;PROF. JOSÉ LUIS BRINQUETE BORBINHA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"José Luis Brinquete Borbinha is a member of the examination committee who participates in evaluating and reviewing the thesis."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;FAMILY&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Rui's family provided unwavering support throughout his academic journey and thesis development."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;TERM FREQUENCY ALGORITHM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A term frequency algorithm is mentioned as part of lexical approaches for information retrieval."&lt;SEP&gt;"Term Frequency Algorithm is discussed as part of lexical approaches in information retrieval techniques."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;BEST MATCHING ALGORITHM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Best Matching Algorithm is discussed as part of lexical approaches in information retrieval techniques."&lt;SEP&gt;"The best matching algorithm is described as another approach in the context of lexical information retrieval techniques."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;DISTANCE METRICS FOR LEXICAL APPROACHES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Distance metrics are used to measure the similarity between terms or documents, often used with lexical approaches in information retrieval."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;WORD AND SENTENCE EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Word and Sentence Embeddings is discussed as part of neural networks for semantic information retrieval methods."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;RECURRENT NEURAL NETWORK&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Recurrent Neural Network is a type of neural network that processes sequences, often used in NLP tasks."&lt;SEP&gt;"Recurrent Neural Networks are mentioned in the context of neural networks for semantic information retrieval methods."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;LONG SHORT-TERM MEMORY&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Long Short-Term Memory (LSTM) is a specific architecture of Recurrent Neural Networks designed to handle long-term dependencies."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;ARCHITECTURE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Architecture refers to the overall structure or design of the Semantic Search System, including different search systems and their components."</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;AUTOMATIC QUERY GENERATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Automatic Query Generation is an event or method discussed under Language Model Evaluation in section 6.1.2, dealing with query generation processes."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;ALBERTINA&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Albertina is a new BERT model developed by João Rodrigues et al. that represents the state-of-the-art for both European and Brazilian Portuguese encoders."&lt;SEP&gt;"Albertina is a new state-of-the-art BERT model developed by João Rodrigues et al., specifically for European and Brazilian Portuguese."&lt;SEP&gt;"Albertina is mentioned as part of the future work, possibly referring to an organization or project involved in dataset annotation."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;ARCHITECTURE IMPROVEMENTS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Architecture Improvements are proposed as part of the future work, indicating potential changes or enhancements to the system architecture."&lt;SEP&gt;"Architecture Improvements is a section in the document that discusses advancements in architectural design, likely within a technical or academic context."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;BIBLIOGRAPHY&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Bibliography is a section where references are listed, often compiled by an organization or institution."&lt;SEP&gt;"Bibliography is mentioned at the end of the document, indicating a list of references used in the research."&lt;SEP&gt;"The Bibliography section marks the end of the document, indicating a reference or citation list." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;LIST OF FIGURES&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"List of Figures is an event or section that provides a visual guide to figures and tables referenced throughout the document."</data>
  <data key="d2">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</node>
<node id="&quot;AI ARTIFICIAL INTELLIGENCE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"AI refers to artificial intelligence, a broad field of computer science focusing on creating intelligent machines that can perform tasks requiring human-like intelligence."::</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;SUPREME COURT OF JUSTICE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Supreme Court of Justice is another term for Supremo Tribunal de Justiça (STJ), playing a crucial role in making well-informed decisions that impact specific cases and future ones."&lt;SEP&gt;"The Supreme Court of Justice, also known as Supremo Tribunal de Justic ¸a, plays a crucial role in making well-informed, lawful, and ethical decisions."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;COURT DECISIONS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Court decisions are legal documents that form the basis of summarization efforts within the IRIS project."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;DATASETS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Datasets are a key asset produced by the thesis work, derived from Portuguese legal documents."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;LANGUAGE MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Language models developed for Semantic Textual Similarity (STS) and summarization as part of the thesis contributions."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;LEXICON APPROACHES FOR INFORMATION RETRIEVAL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical approaches are used in the IRIS project to improve information retrieval."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF) &quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"TF-IDF is a ranking function that evaluates the relevance of terms in documents within a set, considering term frequency and document frequency."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;LEXICAL APPROACHES FOR INFORMATION RETRIEVAL&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Lexical approaches are traditional methods of information retrieval that search for exact query words in documents."&lt;SEP&gt;"This event refers to a traditional method of searching for document content related to a query using techniques that search for exact query words."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;DOCUMENT&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Documents are the primary units of content being searched for in Information Retrieval processes."</data>
  <data key="d2">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</node>
<node id="&quot;GLOBAL VECTORS FOR WORD REPRESENTATION (GLOVE)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GloVe is a method of word embedding developed at Stanford, used to create models through unsupervised training."&lt;SEP&gt;"GloVe is an implementation of word embeddings created by researchers at Stanford University in 2014."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;CBOW MODEL ARCHITECTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The CBOW model uses context words to predict a target word, with the hidden layer and output layer defined by neurons and vectors respectively."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;SKIP-GRAM MODEL ARCHITECTURE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Skip-Gram model predicts context words given a target word using neural networks and one-hot encoded vectors."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;GLOVE'S LOSS FUNCTION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"GloVe's loss function is formally defined using a weighting function, dot product of input vectors, and bias terms to optimize word embeddings based on co-occurrence probabilities."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;WORD ICE AND STEAM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"These are terms representing specific words used to illustrate co-occurrence probabilities in the GloVe model."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;SOLID, GAS, WATER, FASHION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"These concepts represent contexts in which 'ice' and 'steam' co-occur more or less frequently, showing their relationship with other terms."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;PROBABILITIES FOR TARGETS WORD ICE AND STEAM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"These are specific words used to demonstrate co-occurrence probabilities within GloVe’s method, illustrating how they relate to other concepts like solid, gas, water, and fashion."&lt;SEP&gt;"This event involves the calculation of probabilities for the words 'ice' and 'steam', demonstrating their relationships to other concepts."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;FEED-FORWARD NEURAL NETWORKS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Feed-forward neural networks are part of the RNN architecture, processing sequential or time-series data without cycles."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;RNN CELL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"RNN cells are the basic processing units of Recurrent Neural Networks, iterating over themselves to process sequential data."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;LOGISTIC FUNCTION (SIGMOID), HYPERBOLIC TANGENT (TANH), OR RECTIFIED LINEAR UNIT (RELU)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"These activation functions are used in RNNs to introduce non-linearity and influence the output vector's form."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;BACKPROPAGATION THROUGH TIME (BPTT)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"BPTT is the gradient-based technique used in training RNNs for sequential data processing."&lt;SEP&gt;"Backpropagation Through Time (BPTT) is a gradient-based technique used for training RNNs."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;VANISHING OR EXPLODING GRADIENTS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Vanishing or Exploding Gradients is an issue faced by standard RNNs when trying to learn long-term dependencies."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;RNNS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"RNNs stands for Recurrent Neural Networks, which can be unrolled arbitrarily deep but may suffer from long training times and the gradient vanishing/exploding problem."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;ENCODER&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Encoder component in the Transformer model receives an input sequence and maps it into a contextualized encoding sequence, using multiple layers with attention and feed-forward neural networks."</data>
  <data key="d2">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</node>
<node id="&quot;MASKED MULTI-HEAD ATTENTION COMPONENT&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The Masked Multi-Head Attention Component applies a mask in the Decoder Block to ensure predictions are made based only on past information, preventing the model from attending to future positions in the input sequence."&lt;SEP&gt;"The Masked Multi-Head Attention Component is a specialized layer in the Transformer Model that applies a mask to ensure predictions are based on past information."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;QUERY, KEY, VALUE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Query, Key, and Value are the terms used in the Multi-Head Attention mechanism where queries, keys, and values are vector representations of words."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;WEIGHTED SUM OF THE VALUES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Weighted Sum of Values is a method used to calculate the output by combining the values based on their weights assigned by the attention mechanism."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;ATTENTION FUNCTION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Attention Function is a mechanism that assigns different weights to each word's vector, capturing the importance between words in a sentence."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;QUERIES AND KEYS OF DIMENSION DK, VALUES OF DIMENSION DV&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Queries and Keys have dimensions dk, while Values have dimensions dv; these are used in the attention function."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;PROJECTION MATRICES WQ I, WK I, WV I&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Projection Matrices WQ_i, WK_i, and WV_i are used to project linearly keys, values, and queries for different heads of multi-head attention."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;HEAD I&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Head i refers to each parallel instance of the Multi-Head Attention mechanism."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;PARALLEL ATTENTION LAYERS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Parallel Attention Layers are multiple instances of the attention mechanism running in parallel, each with different projection matrices."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;MASKED ATTENTION COMPONENT&quot;">
  <data key="d0">"ROLE"</data>
  <data key="d1">"The Masked Attention Component is a specialized layer that applies a mask to ensure predictions are based only on past information."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;INPUT SEQUENCE CURRENT POSITION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Input Sequence Current Position refers to the current word in the input sentence, and the mask ensures that only positions before this one can be attended to."</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;BI-ENCODERS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Bi-Encoders generate sentence embeddings independently and compare them using cosine similarity. They use models like Sentence-BERT to produce embeddings."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;CROSS-ENCODERS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Cross-Encoders compare the similarity between two sentences simultaneously, outputting a value between 0 and 1 without generating individual sentence embeddings."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;INTRA-SENTENCE COSINE SIMILARITY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Intra-sentence Cosine Similarity is used as the basis for connectivity matrix and edge weights in constructing the sentence similarity graph of LexRank."&lt;SEP&gt;"Intra-sentence cosine similarity is a measure used to calculate the edge weights in the graph representation of sentences."</data>
  <data key="d2">chunk-5d200aadc0a3370985a8b9824fa2c738&lt;SEP&gt;chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;JACCARD SIMILARITY&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Jaccard Similarity can be used alternatively to cosine similarity in constructing the sentence graph within LexRank."&lt;SEP&gt;"Jaccard similarity is another measure used alongside cosine similarity to determine the edge weights in the graph representation of sentences."&lt;SEP&gt;"Jaccard similarity measures the similarity between finite sample sets, which can be applied here for calculating edge weights in a graph representation of sentences."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056&lt;SEP&gt;chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</node>
<node id="&quot;GRAPH REPRESENTATION OF SENTENCES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A graph where sentences are represented as nodes and edges denote their similarity scores using cosine or Jaccard similarity."&lt;SEP&gt;"This refers to the process where each sentence is represented as a node in a graph and edges between nodes denote their similarity scores."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;SENTENCE SIMILARITY GRAPH&quot;">
  <data key="d0">"GRAPH"</data>
  <data key="d1">"A sentence similarity graph uses cosine similarity or Jaccard similarity to connect sentences as nodes, with edge weights representing their similarity scores."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;DAMPING FACTOR&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The damping factor is a parameter used in the PageRank algorithm to ensure that the scores converge during calculations."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;ADJACENCY MATRIX&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"An adjacency matrix derived from intra-sentence cosine similarity or Jaccard similarity, representing how sentences are connected within a document."&lt;SEP&gt;"An adjacency matrix that represents the connectivity between sentences based on cosine similarity in a sentence's graph representation."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;PRE-TRAINING AND FINE-TUNING&quot;">
  <data key="d0">"PROCESS"</data>
  <data key="d1">"These phases refer to the training process of BERT where it is first trained on large corpora (pre-training) and then fine-tuned for specific tasks."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;PRE-TRAINING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Pre-training is a phase where BERT learns to understand language through tasks like Masked Language Modeling and Next Sentence Prediction."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;L&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"L represents the number of layers (Transformer blocks) in BERT models."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;H&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"H denotes the hidden size in BERT models."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;A&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A refers to the number of self-attention heads in BERT models."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;TOTAL PARAMETERS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Total Parameters indicate the total number of trainable parameters in a BERT model, which varies based on its size (BERT BASE vs. BERT LARGE)."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;TASK DATA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Task Data consists of any observable task distribution information used for a specific task, usually non-randomly sampled from a wider distribution within a larger target domain."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;DATA FROM DIFFERENT DOMAINS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Data from Different Domains refers to datasets used for training or testing a model that come from contexts other than those it was originally trained in, potentially leading to performance degradation due to domain shift issues."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;DOMAIN ADAPTATION TECHNIQUES&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Domain Adaptation Techniques are methods employed to mitigate the effects of domain shift and improve a model's performance on new datasets from different domains."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;MLM TASK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The MLM task involves masking words in input sentences and training the model to predict them, originally introduced by BERT."&lt;SEP&gt;"The MLM task involves predicting masked tokens, which was crucial for evaluating model performance using negative log-likelihood loss."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95&lt;SEP&gt;chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;FIGURE 3.2: DATA DISTRIBUTIONS&quot;">
  <data key="d0">"FIGURE"</data>
  <data key="d1">"Figure 3.2 depicts data distributions, based on [15]."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;FIGURE 3.3: MASKED LANGUAGE MODELING&quot;">
  <data key="d0">"FIGURE"</data>
  <data key="d1">"Figure 3.3 illustrates the masked language modeling task, based on https://www.sbert.net/examples/unsupervised_learning/MLM."</data>
  <data key="d2">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</node>
<node id="&quot;NLP BENCHMARKS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"NLP benchmarks refer to various Natural Language Processing tasks that are used to evaluate and compare different models."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;SNLI (STANFORD NATURAL LANGUAGE INFERENCE) DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The SNLI dataset, a part of the NLI challenge, contains pairs of sentences labeled for entailment, contradiction, and neutrality."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;AGIRRE ET AL. DATASETS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Datasets created by Agirre et al. for the STS tasks 2012-2016, used in evaluating model performance on semantic similarity."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STS BENCHMARK DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The STS benchmark dataset, also used to evaluate supervised STS systems, contains labeled sentence pairs with gold labels from 0 to 5 for similarity."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STSB (STS BENCHMARK)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The STS benchmark is a widely used dataset for evaluating semantic textual similarity models, containing labeled sentence pairs with gold labels from 0 to 5."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STS12, STS13, STS14, STS15, STS16&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"These are specific versions of the STS task spanning from 2012 to 2016."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;GLOVE EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GloVe (Global Vectors for Word Representation) is a technique used in word and sentence embedding, but mentioned here in comparison to BERT embeddings."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;AVG. GLOVE EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Average GloVe embeddings are a method of representing text using vector spaces, with performance measures given for various models."&lt;SEP&gt;"GloVe embeddings are an average of vector representations generated by the GloVe model, which is a static word embedding technique."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;BERT CLS-VECTOR&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERT CLS-vector is a specific embedding technique from BERT where the [CLS] token's output vector is used to represent the whole sentence."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SBERT -NLI-BASE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SBERT - NLI-base refers to a sentence embedding model fine-tuned on Natural Language Inference (NLI) tasks and the STS benchmark dataset."&lt;SEP&gt;"SBERT -NLI-base uses a base version of the pre-trained BERT model fine-tuned on NLI tasks, showing performance measures in evaluations."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SBERT -NLI-LARGE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SBERT - NLI-large is an enhanced version of SBERT, using a larger BERT model for more complex sentence embeddings."&lt;SEP&gt;"SBERT -NLI-large employs a larger version of the pre-trained BERT model fine-tuned on NLI tasks, with specific performance measures provided."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SROBERTA-NLI-BASE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SRoBERTa-NLI-base is another variant using RoBERTa for NLI tasks, with detailed performance measures given in evaluations."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SROBERTA-NLI-LARGE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SRoBERTa-NLI-large utilizes a larger version of the RoBERTa model fine-tuned on NLI tasks, showing specific performance metrics in evaluations."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;PARALLEL SENTENCES ((S1, T1), ...,(SN, TN))&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Parallel sentences are pairs of sentences where one sentence is the translation of the other, used in the MKD process."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;TEACHER MODEL M&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"The teacher model is a pre-trained monolingual model that produces vectors for both source and target sentences during the MKD process."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;STUDENT MODEL ˆM&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"The student model, in the MKD process, aims to learn from the teacher model by producing embeddings close to those of the teacher model."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;PARAPHRASE-MULTILINGUAL-MPNET-BASE (768 DIMENSIONS)&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"Paraphrase-multilingual-mpnet-base is a multilingual SBERT version that provides relatively accurate embeddings for sentences in multiple languages."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;NEURALMIND/BERT-BASE-PORTUGUESE-CASED BERT -BASE 12 110M&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"neuralmind/bert-base-portuguese-cased is a pre-trained BERT model for the Portuguese language, with specific architecture details."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;NEURALMIND/BERT-LARGE-PORTUGUESE-CASED BERT -LARGE 24 335M&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"neuralmind/bert-large-portuguese-cased is a pre-trained BERT model for the Portuguese language, with specific architecture details."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;BRAZILIAN WEB AS CORPUS (BRWAC)&quot;">
  <data key="d0">"CORPUS"</data>
  <data key="d1">"The Brazilian Web as Corpus (BrWaC) is a large Portuguese corpus used for pretraining BERTimbau models."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;LEAD RESEARCHER FROM THE TEAM THAT PUBLISHED SBERT&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"The lead researcher from the team that published SBERT introduced Multilingual Knowledge Distillation (MKD) in 2020."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;STS BENCHMARK TEST SET EVALUATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The SBERT evaluation on the STS benchmark test set is a process to assess the performance of the model based on semantic text similarity."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;TEACHER MODEL M AND STUDENT MODEL ˆM&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"Teacher and student models are used in the MKD process to train multilingual embedding models."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;MEAN-SQUARED LOSS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Mean-squared loss is used to minimize the difference between embeddings produced by the teacher and student models during the MKD process."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;HTML BODY AND TITLES/FOOTNOTES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The HTML body, ignoring titles and footnotes, was used to create the processed corpus for pretraining BERTimbau models."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;1E−4 LEARNING RATE&quot;">
  <data key="d0">"VALUE"</data>
  <data key="d1">"A learning rate of 1e-4 was used during the training of BERTimbau models with BrWaC corpus."</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;ZHUYUN DAI AND JAMIE CALLAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zhuyun Dai and Jamie Callan are researchers who explored the use of contextual neural language models like BERT for information retrieval tasks."&lt;SEP&gt;"Zhuyun Dai and Jamie Callan are researchers who studied the use of BERT for information retrieval in natural language queries."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;ROBUST04 AND CLUEWEB09-B&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"These are two datasets used to test the performance of information retrieval techniques, including BERT."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;MI-Y OUNG KIM ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mi-Y oung Kim et al. refers to a group of researchers who discussed the use of deep learning techniques for legal information retrieval in COLIEE 2021."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;TEAM F1-SCORE PRECISION RECALL MAP&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The team performance metrics such as F1-score, Precision, Recall, and MAP are used to evaluate the effectiveness of various approaches in COLIEE 2021."</data>
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
</node>
<node id="&quot;PORTUGUESE CONSUMER LAW&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Portuguese Consumer Law refers to a set of legal documents and regulations that need to be made more accessible to Portuguese citizens."&lt;SEP&gt;"Portuguese Consumer Law refers to the legal framework that Nuno Cordeiro aimed to make more accessible and understandable through his system LeSSE."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;DIÁRIO DA REPÚBLICA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Diário da República is mentioned as an annotated question source for fine-tuning the language model."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;POPULATION BASED TRAINING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Population Based Training (PBT) is a hyperparameter optimization technique used in the context of training the model."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;TRAINER CLASS FUNCTION HYPERPARAMETER SEARCH&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The trainer class function hyperparameter search is a part of the Ray Tune library and indicates the method used for optimizing hyperparameters."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;80% TRAINING SET, 10% VALIDATION SET, 10% TEST SET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"These are specific proportions mentioned for dividing the dataset during the fine-tuning process."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;2 NVIDIA GEFORCE RTX 3090 GPUS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This specifies the hardware used for training the model, indicating its computational power and significance in the workflow."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;EUROPEAN PORTUGUESE (PT-PT)&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"European Portuguese (PT-PT) refers to the specific dialect of Portuguese that the Albertina model was developed to handle."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;BRAZILIAN PORTUGUESE (PT-BR)&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Brazilian Portuguese (PT-BR) is another version of the Albertina model, trained on a different data set."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;MAY 2023&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"May 2023 is the time when Jo˜ão Rodrigues et al., shared their new state-of-the-art model, Albertina."&lt;SEP&gt;"May 2023 marks the time when Jo ˜ão Rodrigues et al. shared their new model."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;STS TASK OVER THE ASSIN2 DATASET&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The STS task over the assin2 dataset is a performance evaluation metric for Albertina PT-BR compared to BERTimbau."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;JO˜ÃO RODRIGUES ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jo˜ão Rodrigues et al. are the researchers who shared their brand-new state-of-the-art model, Albertina."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;FACULDADE DE CIˆNCIAS DA UNIVERSIDADE DE LISBOA (FCUL)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"FCUL is a partner in the development of the Albertina model and more specifically the NLX–Natural Language and Speech Group."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;FACULDADEDE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"FEUP is another partner in the development of the Albertina model, particularly through their Laborat´orio de Inteligˆencia Artificial e Ciˆencia de Computadores."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;RE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Re is a party in a legal dispute, typically representing an entity seeking compensation or resolution of a conflict."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;AUTOR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Autor is another party involved in the legal case, usually opposing Re and claiming for differences in payment."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;DGSI - INDEXER -STJ &quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"dgsi - indexer -STJ is the source of the legal document corpus, indicating its origin and management."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;JURISPRUDENCIA &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Jurisprudencia refers to a body of law established by judicial decisions, which in this context is unknown but related to the legal rulings."</data>
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</node>
<node id="&quot;DE ARMAS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"De Armas refers to a specific person whose name is mentioned in the document."</data>
  <data key="d2">chunk-338cf6d446307fa476c0b025098bcd87</data>
</node>
<node id="&quot;PASSAGEM RELEVANTE PARA A QUEST&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The passagem relevante para a quest is an event or piece of text relevant to the query, providing legal information."</data>
  <data key="d2">chunk-338cf6d446307fa476c0b025098bcd87</data>
</node>
<node id="&quot;QUEM, DE NOITE E ACOMPANHADO, ENTRA NUMA CASA MUSEU, DEPOIS DE ARROMBAR A PORTA E DE L ´A RETIRA V ´ARIAS ARMAS PEC ¸AS DE MUSEU E DELAS SE APROPRIA, CONTRA VONTADE DO DONO, PRATICA OS CRIMES DE INTRODUC ¸ ˜AO EM LUGAR VEDADO AO P ´UBLICO E FURTO QUALIFICADO.&quot; (DOCUMENTO ID: 9EWRY OMBF LERWH5 W2G)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This is a specific event described in the document involving illegal actions within a museum, including breaking into the premises and stealing exhibits without the owner's consent."</data>
  <data key="d2">chunk-338cf6d446307fa476c0b025098bcd87</data>
</node>
<node id="&quot;SBERT MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"SBERT (Sentence BERT) is a deep learning framework for sentence-level tasks that builds on top of BERT and allows finding similar passages by creating dense vector representations."&lt;SEP&gt;"SBERT Model refers to the Legal-BERTimbau implementation used for finding similar passages through semantic textual similarity."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;PORTUGUESE LEGAL DOMAIN&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The Portuguese Legal Domain is the specific area of law in which Legal-BERTimbau is used and adapted."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;NVIDIA GEFORCE RTX 3090 24 GB GPU&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The NVIDIA GeForce RTX 3090 24 GB GPU is used for training the BERTimbau model."</data>
  <data key="d2">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;SENTENCETRANSFORMERS PYTHON LIBRARY, TENSORFLOW, PYTORCH, OR JAX&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"These are tools that can be easily used with the developed variants of the BERTimbau model."</data>
  <data key="d2">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;HUGGINGFACE PLATFORM, USING THE HUGGINGFACE’S TRANSFORMERS LIBRARY&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The HuggingFace platform provides hosting for the developed variants and uses its Transformers library to manage them."</data>
  <data key="d2">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;TSDAE TECHNIQUE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"TSDAE (Temporal Structure Disentangled Adversarial Enhancement) is a domain adaptation technique used to improve model generalization on new data."&lt;SEP&gt;"The TSDAE technique is an unsupervised sentence embedding approach used in the adaptation process."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-V0&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This is the name of a model variant created for STS evaluation, combining BERT and Legal-BERTimbau-large."</data>
  <data key="d2">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;SBERT VERSION OF LEGAL-BERTIMBAU-LARGE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This variant is created by adding an independent linear layer and fine-tuning with specific datasets, generating 1024 dimension embeddings."</data>
  <data key="d2">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;LEARNING RATE (1E-5)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The learning rate used during training is \(10^{-5}\), which affects the convergence speed and accuracy of the model."</data>
  <data key="d2">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</node>
<node id="&quot;HUGGINGFACE: STJIRIS/IRIS STS DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"HuggingFace is a platform where the STS (Sentence Text Similarity) dataset from stjiris/IRIS is publicly available. The IRIS dataset includes specific STS tasks for evaluation."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;GPT3 TEXT-DAVINCI-003 MODEL API&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GPT3 text-davinci-003 is a powerful natural language generation tool used in creating sentence pairs for STS tasks."&lt;SEP&gt;"The GPT3 text-davinci-003 model API was used to generate sentence pairs and assign relatedness scores, particularly those with higher relatedness values (4 to 5</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;STS FINE-TUNING STAGE DESCRIBED IN BERTIMBAU’S PAPER&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This refers to the process of fine-tuning models on STS datasets as described in BERTimbau’s research paper, particularly using a learning rate of \(10^{-5}\) and Adam optimization algorithm."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;SNLI DATASET&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The SNLI dataset is another Natural Language Inference (NLI</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V0&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Another model variant trained using GPL, NLI, and STS data."&lt;SEP&gt;"Similar to the previous variant, this one was also fine-tuned for legal documents in Portuguese."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;NEGATIVE MINING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This is another stage in the GPL process where similar passages that do not answer the generated queries are retrieved."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;MKD (MULTILINGUAL KNOWLEDGE DISTILLATION)&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"MKD is a technique developed by Neil Reimers for knowledge distillation in multilingual models."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;SUBSECTION 3.1.2.B&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Refers to the section where MKD technique was introduced."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;ENGLISH&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Refers to a language used in the models and techniques mentioned."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;STSBENCHMARK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"STS benchmark is a standard dataset used for evaluating sentence textual similarity tasks."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;ASSIN&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"assin and assin2 are sub-datasets derived from the STSbenchmark dataset."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;STSB MULTI MTIS DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"STSB multi mt is specifically mentioned as a multilingual version of the STS benchmark dataset used to test models on multiple translations."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;PORTUGUESE LANGUAGE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Portuguese Language refers to the language being evaluated in various tasks and datasets, emphasizing its domain-specific nature."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;GPT3 MODEL PROVIDED BY OPENAI&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"GPT-3 is a large language model from Anthropic (previously known as OpenAI) used for rewriting sentences while maintaining meaning."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;FIGURE 6.2.1&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 6.2.1 represents the evaluation architecture described later in the text, providing a visual representation of the process steps."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;LEGAL SEARCH SYSTEM&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A legal search system based on semantic techniques, designed to retrieve relevant information from vast volumes of data efficiently."&lt;SEP&gt;"The Legal Search System is the system developed for performing semantic searches on legal documents, using BERTimbau large embeddings stored in an ElasticSearch index."</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65&lt;SEP&gt;chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;TOP 10, AND TOP 20 RESULTS&quot;">
  <data key="d0">"METRIC"</data>
  <data key="d1">"These metrics evaluate the performance of different models based on the top N results sizes (from 1 to 20), showing varying levels of performance across models."::</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;NLI TRAINING&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Models that received NLI training show a 4.3% improvement in the Search metric and a 5.4% improvement in the Discovery measure."::</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;GPL TRAINING APPROACH&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Models subjected to GPL training show a 3.2% improvement in the Search metric and a 1.7% improvement in the Discovery metric."::</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;JUDGE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Judges are mentioned as users of the search system, performing complex and time-consuming tasks related to legal decision-making."&lt;&lt;relationship_description&gt;</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;PRE-PROCESSING TEXT DATA&quot;">
  <data key="d0">"PROCESS"</data>
  <data key="d1">"The process of pre-processing text data is crucial for handling raw text before further tasks are performed."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;ORGANIZATIONS MENTIONED IN THE TEXT: [ACL, GLM, GPT-3]&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"These organizations are associated with research or technology related to machine learning and natural language processing."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;ACTIVE LEARNING TECHNIQUE&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Active Learning is a technique where the system learns from user feedback to improve its performance over time, especially useful in legal domains."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;SEMEVAL-2015 TASK 2: SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SemEval-2015 Task 2 was a task within a semantic evaluation workshop where participants explored semantic textual similarity, English, Spanish, and pilot on interpretability."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;BRWAC CORPUS FOR BRAZILIAN PORTUGUESE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The brWaC corpus is a Wacky Corpus created for Brazilian Portuguese, used as a resource for language processing tasks."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;BOOS, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Rafael Boos is an author involved in creating a corpus for Brazilian Portuguese."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;PRESENTES, K.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Katharina Presentes is part of the team that created brWaC Corpus."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;VILLAVICENCIO, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Andrea Villavicencio contributed to creating brWaC Corpus."&lt;SEP&gt;"Andrea Villavicencio is involved in creating a corpus for Brazilian Portuguese."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;PADRÓ, M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Manuel Padró contributed to the creation of brWaC Corpus."&lt;SEP&gt;"Manuel Padró is involved in the creation of brWaC Corpus."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;ANGELIS, G.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Giuseppe Angelis contributed to the creation of a large annotated corpus for natural language inference."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;CER, D.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Daniel Cer is involved in various tasks and workshops related to semantic textual similarity."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;DIAB, M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mona Diab is an author involved in creating a large annotated corpus for natural language inference."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;GONZALEZ -AGIRRE, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Aitor Gonzalez-Agirre contributed to the creation of a large annotated corpus for natural language inference."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;GUO, W.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Wen Guo is an author involved in tasks and workshops related to semantic textual similarity."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;LOPEZ-GAZPIO, I.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Iñigo Lopez-Gazpio contributed to SemEval-2017 Task 1 on semantic textual similarity multilingual and crosslingual focused evaluation."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;DEBERTA&quot;(">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"DeBERTa refers to a decoding-enhanced BERT model with disentangled attention, highlighting advancements in language models."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BERT&quot;(">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERT is a pre-trained deep bidirectional transformer for language understanding, significant in the field of natural language processing."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SIGIR CONFERENCE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The SIGIR Conference is an organization associated with ACM, hosting research and development in information retrieval."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;DEVLIN, J.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Devlin, J. is an author of BERT: Pre-training of deep bidirectional transformers for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;CHANG, M.-W.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Chang, M.-W. is an author of BERT: Pre-training of deep bidirectional transformers for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;LEE, K.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lee, K. is an author of BERT: Pre-training of deep bidirectional transformers for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot; TOUTANOVA, K.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Toutanova, K. is an author of BERT: Pre-training of deep bidirectional transformers for language understanding."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ERKKAN, G.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Erkkan, G. is a co-author of Lexrank: Graph-based lexical centrality as salience in text summarization."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;RAEV, D. R.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Raev, D. R. is a co-author of Lexrank: Graph-based lexical centrality as salience in text summarization."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;FONSECA, E.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Fonseca, E. is an author of ASSIN: Avaliacao de similaridade semantica e inferencia textual."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SANTOS, L.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Santos, L. is an author of ASSIN: Avaliacao de similaridade semantica e inferencia textual."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;CRISCUOLO, M.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Criscuolo, M. is an author of ASSIN: Avaliacao de similaridade semantica e inferencia textual."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ALUISIO, S.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Aluisio, S. is an author of ASSIN: Avaliacao de similaridade semantica e inferencia textual."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GURURANGAN, S.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Gururangan, S. is an author of Don’t stop pretraining: Adapt language models to domains and tasks."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MARASOVIC, A.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Marasovic, A. is an author of Don’t stop pretraining: Adapt language models to domains and tasks."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SWAYAMBIPTA, S.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Swayambipta, S. is an author of Don’t stop pretraining: Adapt language models to domains and tasks."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;LO, K.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lo, K. is an author of Don’t stop pretraining: Adapt language models to domains and tasks."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BELTAGY, I.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Beltagy, I. is an author of Don’t stop pretraining: Adapt language models to domains and tasks."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;DOWNEY, D.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Downey, D. is an author of Don’t stop pretraining: Adapt language models to domains and tasks."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SMITH, N. A. A.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Smith, N. A. A. is an author of Don’t stop pretraining: Adapt language models to domains and tasks."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;HE, P.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"He, P. is an author of DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;LIU, X.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Liu, X. is an author of DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GAO, J.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Gao, J. is an author of DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;CHEN, W.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Chen, W. is an author of DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;HENDERSON, P.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Henderson, P. is an author of Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KRASS, M. S.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Krass, M. S. is an author of Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ZHENG, L.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zheng, L. is an author of Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GUHA, N.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Guha, N. is an author of Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MANNING, C. D.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Manning, C. D. is an author of Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;JURAFSKY, D.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jurafsky, D. is an author of Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;HO, D. E.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ho, D. E. is an author of Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;IJITEE&quot;(">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"IJITEE is a journal or conference associated with international innovative technology and exploring engineering."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;VAISSNAIVE, V.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Vaissnaive, V. is an author of An Artificial Intelligence based Analysis in Legal domain."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;DEEPALAKSHMI, P.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Deepalakshmi, P. is an author of An Artificial Intelligence based Analysis in Legal domain."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KIM, M.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kim, M. is an author of BM25 and transformer-based legal information extraction and entailment."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;RABELO, J.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Rabelo, J. is an author of BM25 and transformer-based legal information extraction and entailment."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GOEBEL, R.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Goebel, R. is an author of BM25 and transformer-based legal information extraction and entailment."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KIM, S.-W.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kim, S.-W. is an author of Research paper classification systems based on TF-IDF and LDA schemes."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GIL, J.-M.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Gil, J.-M. is an author of Research paper classification systems based on TF-IDF and LDA schemes."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;KINGMA, D. P.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kingma, D. P. is an author of Adam: A method for stochastic optimization."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BA, J.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ba, J. is an author of Adam: A method for stochastic optimization."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MARELLI, M.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Marelli, M. is an author of a SICK cure."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MENINI, S.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Menini, S. is an co-author of a SICK cure."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BARONI, M.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Baroni, M. is an co-author of a SICK cure."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BENTIVOGLI, L.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Bentivogli, L. is an co-author of a SICK cure."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BERNARDI, R.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Bernardi, R. is an co-author of a SICK cure."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ZAMPARELLI, R.&quot;(">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zamparelli, R. is an co-author of a SICK cure."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;LREC 2014&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"LREC 2014 is an important event for researchers in language resources and evaluation, where M ARELLI et al. presented their work."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;REYKJAVIK, ICELAND&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Reykjavik, Iceland hosted the LREC 2014 conference, a significant event for language resource development and evaluation."&lt;SEP&gt;"Reykjavik, Iceland hosted the LREC'14 conference."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;MS MARCO: A HUMAN GENERATED MACHINE READING COMPREHENSION DATASET&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"MS MARCO is a specific dataset used for evaluating machine reading comprehension models, contributing to the field of natural language understanding."&lt;SEP&gt;"This dataset is designed for machine reading comprehension tasks and may be discussed in relation to advancements in natural language processing."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;OSÓRIO, T.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"OSÓRIO, T., along with J. RODRIGUES, L. GOMES, J. SILVA, A. BRANCO, R. SANTOS, and H. L. CARDOSO, contributed to 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*' published in 2023."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;N OGUEIRA, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N OGUEIRA, R., along with F. SOUZA, contributed to the work 'BERTimbau: Pretrained BERT Models for Brazilian Portuguese' published in Intelligent Systems."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;LOTUFO, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"LOTUFO, R. is an author of 'Portuguese named entity recognition using BERT-CRF.'"&lt;SEP&gt;"LOTUFO, R., along with F. SOUZA and R. F. N OGUEIRA, contributed to 'Portuguese named entity recognition using BERT -CRF' which was published in CoRR."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;V IRTANEN, P.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"V IRTANEN, P., along with R. G OMMERS, T. E. O LIPHANT, M. H ABERLAND, T. REDDY, D. C OURNAPEAU, and others, contributed to the work 'Making monolingual sentence embeddings multilingual using knowledge distillation' which was published in the 2020 Conference on Empirical Methods in Natural Language Processing."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;THIRTY-FIFTH CONFERENCE ON NEURAL INFORMATION PROCESSING SYSTEMS DATASETS AND BENCHMARKS TRACK (ROUND 2)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track is an event where researchers evaluate information retrieval models."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;N., K AISER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N., K AISER is an author of the 'Attention is all you need' paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;BRIGHT , J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. BRIGHT is an author of the SciPy 1.0 paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SciPy1.0 Contributors refers to a group of people contributing to the SciPy project."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;K . W ANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"K . W ANG is an author of the 'TSDAE' paper and 'GPL' paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;N . R EIMERS &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N. R EIMERS is an author of both the 'TSDAE' and 'GPL' papers."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;I . GUREVYCH &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"I. GUREVYCH is an author of both the 'TSDAE' and 'GPL' papers."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;L. U. ANDPOLOSUKHIN &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"L. U. ANDPOLOSUKHIN is a co-author of the 'Attention is all you need' paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;I. POLAT &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"I. POLAT is an author of the SciPy 1.0 paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;EXAMINATION COMMITTEE&quot;">
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
  <data key="d1">"The supervisors are part of the examination committee that oversees Rui's thesis, ensuring its quality and validity."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;EXAMINATION COMMITTEE CHAIRPERSON&quot;">
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
  <data key="d1">"Both supervisors are involved in the examination process and are also part of the committee chair's team."&lt;SEP&gt;"The examination committee chair, Prof. Maria Luís Torres Ribeiro Marques da Silva Coheur, is part of the supervisory team."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TECHNOLOGY&quot;">
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
  <data key="d1">"GloVe is part of the broader field of NLP technologies."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TECHNOLOGIES&quot;">
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
  <data key="d1">"RNNs are foundational in understanding sequence data, which includes a lot of tasks that GloVe and other NLP techniques aim to solve."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;NLP TECHNIQUES&quot;">
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
  <data key="d1">"BERTimbau is an adaptation of BERT specifically tailored to the Portuguese language and its legal information retrieval needs."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;EXPLORING EMBEDDINGS MODELS&quot;">
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
  <data key="d1">"Both contributions were part of the thesis work, with embeddings models informing the development of the search system."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STATE-OF-THE-ART MODELS AND TECHNIQUES&quot;">
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
  <data key="d1">"BERT is a state-of-the-art model that falls within this category of advanced NLP techniques."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;EVENT&quot;">
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
  <data key="d1">"The event of fine-tuning the T5 model on MS MARCO for two epochs is directly linked with using it to generate queries. This process improves the model’s performance on specific tasks."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FINE-TUNING ON MS MARCO FOR TWO EPOCHS&quot;">
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
  <data key="d1">"The event of fine-tuning the T5 model on MS MARCO for two epochs is directly linked with using it to generate queries. This process improves the model’s performance on specific tasks."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;BERT EMBEDDINGS&quot;">
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
  <data key="d1">"BERT and GloVe are both techniques used for word and sentence embedding, with BERT outperforming GloVe in certain tasks."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;BERTIMBAU TRAINING PROCESS&quot;">
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
  <data key="d1">"The 1e-4 learning rate is a key parameter in the training process of BERTimbau models."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;PRETRAINING STAGE&quot;">
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
  <data key="d1">"The pretraining stage was identical to BERT, and BERTimbau utilized this method for its training." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FINE-TUNING STAGE&quot;">
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
  <data key="d1">"During the fine-tuning stage, Legal-BERTimbau was developed to ensure it could adequately understand legal records." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;MLM AND NSP METHODS&quot;">
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
  <data key="d1">"BERTimbau was trained using MLM and NSP methods during its pretraining stage." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STS-B DATASET&quot;">
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
  <data key="d1">"Jo˜ão Rodrigues et al. evaluated Albertina PT on the STS-B dataset, indicating a performance comparison."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ACORDAO &quot;">
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
  <data key="d1">"Acordao is a specific instance that contributes to the established jurisprudence."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;PERSON&quot;">
  <data key="d2">chunk-338cf6d446307fa476c0b025098bcd87</data>
  <data key="d1">"De Armas refers to a specific person whose name is mentioned in the document." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;BATCH SIZE OF 8 FOR FIVE EPOCHS&quot;">
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
  <data key="d1">"The batch size of 8 and 5 epochs was part of the configuration used in the STS fine-tuning process as per BERTimbau's method.".</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SBERT VARIANTS OUTPERFORM OTHERS THAT DO NOT USE NLI DATA&quot;">
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
  <data key="d1">"The performance improvement observed in SBERT variants was due to the inclusion of NLI data during the fine-tuning stage.".</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;NLIIN&quot;">
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
  <data key="d1">"The 'nliin' suffix denotes the model was trained with NLI data, distinguishing it from other models."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;T5-BASE-QA-SQUAD-V1.1-PORTUGUESE&quot;">
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
  <data key="d1">"The T5 model was used to generate queries, which influenced the GPL technique."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V0&quot;">
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
  <data key="d1">"Model name for a variant trained with GPL technique and NLI/STS data."&lt;SEP&gt;"These models were both trained using the GPL technique and NLI data, making them similar in structure."</data>
  <data key="d0">"ORGANIZATION"</data>
</node>
<node id="&quot;TRAINING TASKS SUCH AS THIS&quot;">
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
  <data key="d1">"The BERT variant was trained using a specific set of tasks and datasets to improve its performance."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;EVALUATION OF MODELS&quot;">
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
  <data key="d1">"Domain Adaptation technique is crucial for the evaluation of models, especially in addressing the problem of model generalization." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;LEXICAL-FIRST AND LEXICAL+SEMANTIC SEARCH SYSTEMS&quot;">
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
  <data key="d1">"The developed Semantic Search System includes two specific versions: Lexical-First and Lexical+Semantic. These systems are based on preliminary evaluation showing improved performance over BM25 technique."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;MARELLI ET AL.&quot;">
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
  <data key="d1">"MARELLI et al.'s work was presented at LREC'14, which is another important event for researchers in language and computational linguistics, indicating their active participation in the research community."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;MAY 7-9, 2015&quot;">
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
  <data key="d1">"This date marks the duration of ICLR 2015, highlighting its timing and duration as a significant event for researchers in natural language processing."&lt;SEP&gt;"This is a specific date range when ICLR 2015 took place."</data>
  <data key="d0">"EVENT"</data>
</node>
<node id="&quot;M ARELLI ET AL.&quot;">
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
  <data key="d1">"P. May is part of the research team that presented work at LREC'14, indicating his involvement and contribution to the field of computational linguistics."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ANDPOLOSUKHIN , L.U.&quot;">
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
  <data key="d1">"Both are co-authors on the 'Attention is all you need' paper."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SUPERVISORS: PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOÃO DIAS&quot;">
  <data key="d0">"ROLE"</data>
  <data key="d1">"Professors Pedro Alexandre Simões dos Santos and João Dias are the supervisors who guided Rui Filipe Coimbra Pereira de Melo throughout his thesis work."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;EXAMINATION COMMITTEE: PROF. MARIA LUÍSA TORRES RIBEIRO MARQUES DA SILVA COHEUR, PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOSÉ LUÍS BRINQUETE BORBINHA&quot;">
  <data key="d0">"ROLE"</data>
  <data key="d1">"The examination committee includes Professors Maria Luísa Torres Ribeiro Marques da Silva Coheur, Pedro Alexandre Simões dos Santos, and José Luís Brinquete Borbinha."</data>
  <data key="d2">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</node>
<node id="&quot;MODELOS BERT ESPECIALMENTE TREINADOS (VARIANTES LEGAL-BERTIMBAU)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Specialized BERT models, particularly Legal-BERTimbau variants, were used to enhance the semantic search system."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;SISTEMAS DE PESQUISA HÍBRIDA QUE INCORPORAM TANTO TÉCNICAS LEXICAIS COMO SEMÂNTICAS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Hybrid search systems combining both lexical and semantic techniques were employed in this work."</data>
  <data key="d2">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</node>
<node id="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;/&quot;LONG SHORT-TERM MEMORY (LSTM)&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Both RNN and LSTM are types of neural networks used in sequence learning tasks within NLP."</data>
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</node>
<node id="&quot;SEMANTIC SEARCH SYSTEM 43&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Semantic Search System 43 is a specific section or module within the broader Semantic Search System."&lt;</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;4.2 THE CORPUS 46&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"4.2 The Corpus 46 refers to a specific section discussing the corpus used in data processing steps."&lt;</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;4.3 ARCHITECTURE 48&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"4.3 Architecture 48 refers to a specific section detailing the architecture of the Semantic Search System."&lt;</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;LEXICAL-FIRST SEARCH SYSTEM 50&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical-First Search System 50 refers to a specific section detailing this particular approach within the Semantic Search System."&lt;</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM 51&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Lexical + Semantic Search System 51 refers to a specific section detailing this hybrid system that combines both lexical and semantic search functionalities."&lt;</data>
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</node>
<node id="&quot;VECTOR SPACE WITH QUERY EMBEDDING AND SENTENCE EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A vector space involving a query and multiple sentences is described, showing its relevance to information retrieval tasks."</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.1&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.1 is an example of Word Embeddings in a 3D vector space, highlighting its visual representation." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.2&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.2 shows the Word2Vec with CBOW model based on one-word context, indicating a diagrammatic explanation of the model." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.3&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.3 illustrates the Word2Vec with CBOW model based on multiple words context, further detailing the model's functionality." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.4&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.4 details the Word2Vec with Skip-Gram model, providing a visual explanation of another approach to word embeddings." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.5&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.5 is a reference to the visual representation of a Recurrent Network Fully Connected architecture."&lt;SEP&gt;"Figure 2.5 shows the Recurrent Network Fully Connected structure, indicating its role in sequential data processing." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;FIGURE 2.6&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.6 outlines the RNN Structure, highlighting its key components and architecture." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.7&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.7 describes the LSTM Structure, focusing on managing long-term dependencies in sequential data." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.8&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.8 introduces the Transformer Model with a diagram, indicating its importance for efficient sequence processing." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.9&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.9 explains Scaled Dot-Product Attention and Multi-Head Attention mechanisms, showing their application in the Transformer model." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.10&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.10 demonstrates a vector space with query embedding and sentence embeddings, indicating its use in information retrieval tasks." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.11&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 2.11 introduces the Bi-Encoder and Cross-Encoder concepts, highlighting their role in comparing pairs of texts or queries." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 2.12&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"A visual representation of a weighted cosine similarity graph from LexRank."&lt;SEP&gt;"Figure 2.12 describes LexRank, a method for text summarization based on eigenvector centrality." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca&lt;SEP&gt;chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;FIGURE 3.1&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 3.1 presents BERT input representation, indicating its significance in handling natural language inputs." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 3.2&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 3.2 shows data distributions, likely representing a visualization or analysis of dataset characteristics." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 3.3&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 3.3 details Masked Language Modeling, indicating its role in training models to predict words from context." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 3.4&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 3.4 describes the TSDAE Architecture, showing a Transformer-based Sequential Denoising Auto-Encoder." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 3.5&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 3.5 illustrates the T5 diagram, highlighting its role in text-to-text transformations and summarization tasks." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;FIGURE 3.6&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 3.6 introduces GenQ, likely a method or model for generating questions from context." "</data>
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
</node>
<node id="&quot;30-34&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Pages 30-34 refer to various models and their descriptions."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;56-76&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Pages 56-76 cover the training tasks, evaluation methods, and details of specific models."</data>
  <data key="d2">chunk-86921910d0aba929bef9ae955431a4fa</data>
</node>
<node id="&quot;ACRONYMS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Acronyms list various technical terms and their abbreviations commonly used in the context of search system evaluations and natural language processing tasks."</data>
  <data key="d2">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</node>
<node id="&quot;SUPREMO TRIBUNAL DE JUSTIC ¸A (STJ)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"STJ serves as Portugal’s highest judiciary court, also known as the Supreme Court of Justice, playing a crucial role in making well-informed, lawful, and ethical decisions."</data>
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</node>
<node id="&quot;PORTUGUESE LEGAL DOCUMENTS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Portuguese legal documents are the source materials used to develop multiple datasets as part of the IRIS project."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;CHAPTER 2&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Chapter 2 provides essential elements for understanding subsequent content, focusing on techniques and developed solutions."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;CHAPTER 3&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Chapter 3 showcases state-of-the-art models and relevant scientific papers used in the final solution."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;CHAPTER 4&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Chapter 4 details the architecture and requirements of the semantic search system."&lt;SEP&gt;"Chapter 4 enumerates the implementation requirements and explains the search system architecture."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;CHAPTER 7&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Chapter 7 provides an overall retrospective pondering of the developed work and the achieved milestones, giving a quick insight into future work."</data>
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
</node>
<node id="&quot;DISTANCE METRICS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Distance metrics such as Jaccard Similarity measure are traditional ways of evaluating the similarity between two sets of information."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;SIMILARITY SEARCHES&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Similarity searches, like those using the Jaccard Similarity measure, aim to find documents with shared content or information."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;IMPORTANT PASSAGES OR DOCUMENTS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Important passages or documents may be missed in lexical searches if they lack exact word matches."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;SYNONYM USAGE ISSUE&quot;">
  <data key="d0">"ISSUE"</data>
  <data key="d1">"Synonyms can lead to the failure of retrieval in lexical approaches, as a document with a synonym might not match an exact query term."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;CONTEXTUAL MEANING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Contextual meaning in semantic search refers to understanding the deeper significance of words beyond their literal definitions."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;USER’S INTENTION&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"User's intention is a key aspect of semantic search, where the focus is on understanding what the user really means rather than just matching words."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;SENTENCE MEANING&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Sentence meaning in semantic search refers to comprehending the overall context and significance of sentences beyond individual word matches."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;EMBEDDING SPACE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Embedding space is a multidimensional space where words are represented by vectors based on their context and relationships to other words."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;MULTIDIMENSIONAL WORD REPRESENTATIONS&quot;">
  <data key="d0">"TECHNIQUE"</data>
  <data key="d1">"Multidimensional word representations allow for the comparison and verification of relatedness between words based on their vector distances in embedding spaces."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;GOOGLE TEAM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Google Team led by Tomas Mikolov developed and proposed Word2Vec in 2013."</data>
  <data key="d2">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</node>
<node id="&quot;CBOW&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"CBOW (Continuous Bag of Words) is a method within Word2Vec that uses the surrounding words as input to predict the corresponding word."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;MULTIPLE WORDS CONTEXT CBOW&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This refers to using multiple context words in the CBOW model, expanding its input layer for more complex predictions."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;CO-OCCURRENCE MATRIX MIJ&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Mij represents a pair of words occurring together and is central to GloVe's method of deriving semantic relationships."</data>
  <data key="d2">chunk-073485a6071a38818c47ff7188ec860b</data>
</node>
<node id="&quot;TARGETS WORD ICE AND STEAM&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"These are specific words used to demonstrate co-occurrence probabilities within GloVe's method, illustrating how they relate to other concepts like solid, gas, water, and fashion."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;TABLE 2.2.3&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Table 2.2.3 is a reference to the specific table in the document that shows co-occurrence probabilities for different concepts."</data>
  <data key="d2">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</node>
<node id="&quot;WAHANDWAX&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"These are not entities but parameters in the equations for an RNN model. They refer to weight matrices for hidden vectors and inputs respectively."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;VANISHING OR EXPLODING GRADIENT PHENOMENA&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"These terms describe issues encountered during the training of RNNs where gradients either become too small (vanish) or too large (explode)."</data>
  <data key="d2">chunk-3595353bc78e782128ef8148dfaf1357</data>
</node>
<node id="&quot;SCALED DOT-PRODUCT ATTENTION&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The Scaled Dot-Product Attention is a specific type of attention mechanism used in the Transformer Model to compute the attention scores between queries and keys."&lt;SEP&gt;"The Scaled Dot-Product Attention is a specific type of attention mechanism used in the Transformer Model to compute the attention scores between queries and keys."?&gt;&gt;</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;DK, DV DIMENSIONS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"dk and dv are dimensions used in the attention function where dk represents the key dimension and dv represents the value dimension."&lt;SEP&gt;"dk and dv are dimensions used in the attention function where dk represents the key dimension and dv represents the value dimension."?&gt;&gt;</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;H PARALLEL ATTENTION LAYERS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"h is a hyperparameter representing the number of parallel attention heads in the Multi-Head Attention Layer, each with its own projection matrices."&lt;SEP&gt;"h is a hyperparameter representing the number of parallel attention heads in the Multi-Head Attention Layer, each with its own projection matrices."?&gt;&gt;</data>
  <data key="d2">chunk-e27d93fef9843514dd8dbc524160d663</data>
</node>
<node id="&quot;DAMPING FACTOR (D)'">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A damping factor used in PageRank and LexRank algorithms to ensure convergence of scores, representing the probability of a random jump from one node to another."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;SUMMARIZING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The process of selecting sentences with high LexRank scores for summarization."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;[13]'">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Citation reference for the figure in the text, possibly indicating an external source or document used as a basis for the graph."</data>
  <data key="d2">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</node>
<node id="&quot;TOKEN EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Token Embeddings are values assigned to each word based on vocabulary IDs in BERT's input representation process."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;SEGMENTATION EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Segmentation Embeddings distinguish whether a word belongs to sentence A or B, enabling the distinction between sentences in BERT’s input."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;POSITION EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Position Embeddings indicate the position of each word within a sentence, which is crucial for understanding the context and order of words."</data>
  <data key="d2">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</node>
<node id="&quot;TRANSFORMER&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A general term for a family of architectures including both the original and modified versions discussed (TSDAE and T5)."</data>
  <data key="d2">chunk-dece8100a2db817f460c5edaaa852208</data>
</node>
<node id="&quot;SBERT -NLI&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SBERT -NLI is a variant of SBERT that was pre-trained on NLI datasets before fine-tuning on STSb, indicating an approach to improve sentence representation through multi-step training."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;STNLI (STANFORD NATURAL LANGUAGE INFERENCE)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"STNLI is a dataset used for natural language inference tasks, providing premises and hypotheses to determine logical relationships."</data>
  <data key="d2">chunk-b6270162d82d1fef624d494a11c5caca</data>
</node>
<node id="&quot;AVG. BERT EMBEDDINGS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"BERT embeddings refer to averaged sentence encodings from the BERT model, which provides context-aware sentence representations."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;SBERT -NLI-STSB-LARGE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SBERT -NLI-STSb-large is a larger SBERT model fine-tuned on the NLI and STS benchmark datasets."</data>
  <data key="d2">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</node>
<node id="&quot;PRE-TRAINED MODELS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Pre-trained models refer to existing machine learning models that have been trained on large datasets, often in English."::&lt;br/&gt;"pre-training process, monolingual training"&lt;::8</data>
  <data key="d2">chunk-493aab369fd44529a838be55e938c506</data>
</node>
<node id="&quot;ROBUST04 AND CLUEWEB09-B &quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Robust04 and ClueWeb09-B are datasets used by Zhuyun Dai and Jamie Callan to test the performance of BERT models."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;HTML BODY&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"HTML body refers to the part of web pages that BERTimbau utilized for pretraining, excluding titles and footnotes."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;TOKENS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Tokens are units of text used in preprocessing for the pretraining stage of BERTimbau."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;[CLS] AND [SEP] TOKENS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"[CLS] and [SEP] tokens are special tokens used in the pretraining example generation process to separate sequences."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;50% ADJACENT SEQUENCE SELECTION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"50% of the time, an adjacent sequence is chosen for pretraining examples to form contiguous text."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;15% TOKEN REPLACEMENT&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"15% of the tokens in each example pair are replaced by special [MASK] tokens or random tokens from the vocabulary."</data>
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
</node>
<node id="&quot;2022 THESIS WORK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The 2022 Thesis Work of Nuno Cordeiro refers to the creation of Legal Semantic Search Engine (LeSSE)."</data>
  <data key="d2">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</node>
<node id="&quot;NLX–NATURAL LANGUAGE AND SPEECH GROUP&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This group at FEUP was involved in developing Albertina."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;LABORATÓRIO DE INTELIGÊNCIA ARTIFICIAL E CIÊNCIA DE COMPUTADORES&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This lab at FCUL also participated in the development of Albertina."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;PT -PT VERSION&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"This refers to European Portuguese."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;PT -BR VERSION&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"This refers to Brazilian Portuguese."</data>
  <data key="d2">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</node>
<node id="&quot;INDEX&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"An index is used in Elasticsearch to store documents in dedicated data structures, allowing for efficient retrieval of information."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;NODE CLUSTER&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Node clusters are single instances of Elasticsearch that form part of its architecture and contribute to the scalability and resilience of the system."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;SHARD&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Shards are subsets of index documents used in Elasticsearch to maintain performance and handle failures through replication."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;JURISPRUDENCIA.1.0&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"jurisprudencia.1.0 is an index used to store and retrieve indexed legal documents in Elasticsearch."</data>
  <data key="d2">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</node>
<node id="&quot;GPT3.5&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GPT3.5 is the Generative Language Model (GLM) used to provide user-friendly responses based on retrieved results."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;FURTO DE ARMAS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Furto de Armas is an example query used to demonstrate the system's output."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;PASSAGE RELEVANTE PARA A QUEST ˜AO&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Passage Relevante para a Questão refers to a specific sentence or passage retrieved as relevant for the given query."</data>
  <data key="d2">chunk-486e9fdbc67025b64b42032778600c9c</data>
</node>
<node id="&quot;SYSTEM ID: 9EWRY OMBF LERWH5 W2G&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The document ID refers to a specific system output, indicating it is part of an organization's database or system."</data>
  <data key="d2">chunk-338cf6d446307fa476c0b025098bcd87</data>
</node>
<node id="&quot;COSINE SIMILARITY CALCULATION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Cosine Similarity Calculation is the process of measuring the similarity between dense vector and query embedding using cosine similarity without normalization."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;LEGAL-BERTIMBAU TRAINING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The event of training Legal-BERTimbau involves using various techniques and additional tasks combined with each other, leading to multiple model versions as shown in Figure 5."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;FIGURE 4.3&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Figure 4.3 illustrates the Lexical + Semantic Search System Retrieval Method, showing the process visually."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;LARGE CORPUS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A large corpus is used to train BERTimbau, providing it with a substantial amount of data for fine-tuning and adaptation."</data>
  <data key="d2">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</node>
<node id="&quot;BATCH SIZE OF 2&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"A batch size of 2 was used in the fine-tuning stages, affecting the training process and loss calculation."</data>
  <data key="d2">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;EVALUATION SPLIT&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Evaluation split is a part of the dataset used to evaluate model performance during training stages."</data>
  <data key="d2">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</node>
<node id="&quot;BERTIMBAU'S PAPER&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"BERTimbau's paper describes the fine-tuning process and performance improvements on custom datasets, similar to the one described here."</data>
  <data key="d2">chunk-de764363085fa944c487f5f0db894d4d</data>
</node>
<node id="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A more advanced version of the previous model variant."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;LARGE MODELS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Refers to a group of large language models used for training."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;8 BATCH SIZE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"The batch size used during the training of the large models."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;5 EPOCHS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Indicates a defined number of iterations for the model training process."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;10−5 LEARNING RATE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Learning rate value for Adam optimization algorithm used during training."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE12 MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A pre-trained T5 model used for generating queries from document summaries."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;ORIGINAL BERTIMBAU LARGE FINE-TUNED FOR STS&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A specific BERT model used in the Negative Mining stage for STS tasks."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;TRIPLETS (POSITIVE PASSAGE, NEGATIVE PASSAGE AND MARGIN SCORE)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The triplets created during the final step of the Pseudo Labeling process."</data>
  <data key="d2">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</node>
<node id="&quot;ST斯英语音回归任务(STS REGRESSION TASK)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The STS regression task is the fine-tuning process applied to model variants after MKD technique training."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;DESCRITORES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Descritores are manually annotated brief tags used to identify main document subjects, aiding in information retrieval."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;DOCUMENT CLUSTERS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Clusters of documents form based on their metadata tags and sentence embeddings, indicating related content."</data>
  <data key="d2">chunk-ce4847f54b29367988561206721bdbb7</data>
</node>
<node id="&quot;DOCUMENT CLUSTER&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Document Cluster refers to a group of documents that are related and share common themes or topics."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;CENTROID&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"The centroid is a central point that represents the average embedding of a set of embeddings within a cluster."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;EPOCH&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"An epoch is a complete pass through the entire dataset in machine learning, used to update model parameters based on the data."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;MODEL ADJUSTMENT&quot;">
  <data key="d0">"PROCESS"</data>
  <data key="d1">"The process of adjusting embeddings based on centroids and tags to bring them closer together."</data>
  <data key="d2">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</node>
<node id="&quot;NEGATIVE LOG-LIKELIHOOD LOSS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Negative log-likelihood loss is a loss function used to evaluate model performance in MLM tasks."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;TABLE 6.1: MLM AVERAGE LOSS FOR LEGAL DOCUMENTS IN THE TEST SET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"This table provides average loss values for models evaluated on Portuguese legal document data using MLM tasks."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;TESTING DATASET FROM THE SPLITS GENERATED IN SUBSECTION 4.2&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"The testing dataset is a part of the split datasets used to evaluate model performance after domain adaptation techniques were applied."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;MASKED LANGUAGE MODEL (MLM) TASK&quot;">
  <data key="d0">"TASK"</data>
  <data key="d1">"The MLM task involves predicting words that are masked in sentences, a common technique for evaluating language models."</data>
  <data key="d2">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</node>
<node id="&quot;MLM-STS-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This is a specific model mentioned in the list of models evaluated."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;MLM-NLI-STS-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Another model from the list, used for comparison with SBERT variants."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;MLM-GPL-NLI-STS-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A more specific model mentioned in the evaluation process."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;MLM-MKD-NLI-STS-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Another detailed model used for comparison and evaluation."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;TSDAE-STS-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Yet another model from the list, evaluated against SBERT variants."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;TSDAE-NLI-STS-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A specific variant of tsdae used for STS evaluation."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;TSDAE-GPL-NLI-STS-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Another model from the list, possibly a more complex variant."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;TSDAE-MKD-NLI-STS-V0&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"A detailed model used in the evaluation process."</data>
  <data key="d2">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</node>
<node id="&quot;GPT3 MODEL&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"GPT3 Model is an advanced language model provided by OpenAI used to rewrite sentences while maintaining meaning, though it often replicated exact keywords."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;TF-IDF KEYWORDS&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"TF-IDF Keywords are top 20 words identified as important in document summaries, used to replace exact keywords with synonyms or similar expressions."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;ORIGINAL STS BENCHMARK DATASET&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"The Original STS Benchmark Dataset is the source dataset from which STSB Multi MT derives, used for training multilingual models."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;DOCUMENT SUMMARIES&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Document Summaries are brief versions of legal documents used to generate sentences for query examples in the evaluation process."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;QUERY GENERATION TECHNIQUES&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"Query Generation Techniques include LexRank, GPT3, and T5 models used to create queries from document summaries."</data>
  <data key="d2">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</node>
<node id="&quot;SYNONYMS WEBSITE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Synonyms website is a platform that provides synonyms for Brazilian Portuguese words."</data>
  <data key="d2">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</node>
<node id="&quot;TOP 10, TOP 20&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"These are specific top result sizes used in evaluating the performance of Search Systems."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;SEARCH SYSTEM EVALUATION – DISCOVERY METRIC - MODELS V0&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"This is another figure showing similar evaluations but for the Discovery metric, focusing on model performance."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;TOP 1, TOP 2, TOP 3, TOP 5, TOP 10, AND TOP 20&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"These are different top result sizes used in evaluating Search System performance across various metrics."</data>
  <data key="d2">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</node>
<node id="&quot;TABLE 6.3&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 6.3 is an evaluation metric for a search system, focusing on the Search metric with different models and their top-ranking performances."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;TABLE 6.4&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 6.4 evaluates a discovery metric of a search system, highlighting improvements over BM25 in the Discovery metric."</data>
  <data key="d2">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</node>
<node id="&quot;MULTIPLE PROJECT IRIS MEMBERS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Multiple members of Project IRIS contribute to developing the Semantic Search System, working on various tasks related to it."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;SUPERVISED LEARNING TECHNIQUES&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"Supervised learning techniques are used in creating and fine-tuning language models for the Semantic Search System."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;ROUGE-1 SCORE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"ROUGE-1 score is a metric used to evaluate the quality of generated summaries, with a higher value indicating better performance."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;ROUGE-2 SCORE&quot;">
  <data key="d0">"CONCEPT"</data>
  <data key="d1">"ROUGE-2 score is another metric used for evaluating summary generation, focusing on n-grams beyond bigrams."</data>
  <data key="d2">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</node>
<node id="&quot;PORTUGUESE LEGAL DOMAIN DATASETS&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"These are manually annotated data sets used in research and development of language models like Legal-BERTimbau."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;PILE OF LAW: LEARNING RESPONSIBLE DATA FILTERING FROM THE LAW&quot;">
  <data key="d0">"PUBLICATION"</data>
  <data key="d1">"This is a paper published on July 1, 2022, discussing legal datasets and ethical challenges related to them."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;THESIS WORK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The thesis is mentioned as part of this research, indicating ongoing academic activities."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;QUERY EXPANSION SYSTEM&quot;">
  <data key="d0">"SYSTEM"</data>
  <data key="d1">"The query expansion system is described as a component of the hybrid search system, aiming to improve document retrieval."</data>
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</node>
<node id="&quot;GLM (GENERAL LANGUAGE MODEL) PROMPTS&quot;">
  <data key="d0">"APPROACH"</data>
  <data key="d1">"Improvements in the prompts given to a GLM can provide better model responses regarding user's query, guiding seamless interaction with the Search System."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;GIRRE, E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"E. GIRRE is an author of several Semeval tasks."</data>
  <data key="d2">chunk-d23192c04e35b99777b833e26dafed9f</data>
</node>
<node id="&quot;USA, 2012&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"USA in 2012 is the location and year where SemEval-2012 was held."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;CHAM, 2014&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Cham in 2014 is the location and year where brWaC was presented."&lt;SEP&gt;"Cham in 2014 is the location where brWaC was presented."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;VANCOUVER, CANADA, 2017&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Vancouver, Canada in 2017 is the location where SemEval-2017 was held."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;SUPERVISED LEARNING OF UNIVERSAL SENTENCE REPRESENTATIONS FROM NATURAL LANGUAGE INFERENCE DATA&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This supervised learning task involved using NLI data for training universal sentence representations."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;COPENHAGEN, DENMARK, 2017&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Copenhagen, Denmark in 2017 is the location where this supervised learning task was presented."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;INSTITUTO SUPERIOR TECNICO, UNIVERSIDADE DE LISBOA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Instituto Superior Tecnico, Universidade de Lisboa is the institution where the master's thesis was completed."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;Z. DAI AND J. CALLAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Z. Dai and J. Callan are authors of a paper on deeper text understanding for IR with contextual neural language modeling."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;J. DEVLIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. Devlin is an author associated with the BERT model."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;M.-W. CHANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"M.-W. Chang is a co-author of the BERT model paper."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;K. LEE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"K. Lee is a co-author of the BERT model paper."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;K. TOUTANOVA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"K. Toutanova is a co-author of the BERT model paper."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;LINGUISTIC SOCIETY OF AMERICA (LSA)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The LSA could be inferred as a relevant organization since the BERT paper references it for its empirical methods in natural language processing."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;INSTITUTE SUPERIOR TECNICO, UNIVERSIDADE DE LISBOA (IST)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"IST is the institution where Z. Dai completed his master's thesis on NLP applied to Portuguese consumer law."</data>
  <data key="d2">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</node>
<node id="&quot;ERGUN, G.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ergun authored Lexrank: Graph-based lexical centrality as salience in text summarization."&lt;SEP&gt;"Ergun is the author of Lexrank: Graph-based lexical centrality as salience in text summarization."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;RADEV, D. R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Radev is a co-author of Lexrank: Graph-based lexical centrality as salience in text summarization."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;GURURANGAN, S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Gururangan is involved in adapting language models to domains and tasks through pretraining."&lt;SEP&gt;"Gururangan was involved in adapting language models to domains and tasks through pretraining."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MARASOVIĆ, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Marasović collaborated with Gururangan on adapting language models to domains and tasks through pretraining."&lt;SEP&gt;"Marasović collaborates with Gururangan on adapting language models to domains and tasks through pretraining."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;SWAYAMB DIPSTA, S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Swayamb dipsta collaborated with Gururangan on adapting language models to domains and tasks through pretraining."&lt;SEP&gt;"Swayamb dipsta collaborates with Gururangan on adapting language models to domains and tasks through pretraining."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;VAISHNAV, V.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Vaishnav is involved with Deepalakshmi in publishing an article on artificial intelligence-based analysis in the legal domain."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;DEEPALAKSHMI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Deepalakshmi collaborates with Vaishnav on the publication of an article on artificial intelligence-based analysis in the legal domain."</data>
  <data key="d2">chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MARELLI, M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"MARELLI, M. is a researcher involved in evaluating compositional distributional semantic models."&lt;SEP&gt;"Marelli is involved in a project called SICK cure, though details are not provided here."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c&lt;SEP&gt;chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;MENINI, S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"MENINI, S. is a co-author involved in evaluating compositional distributional semantic models."&lt;SEP&gt;"Menini collaborates with Marelli on the SICK cure project."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c&lt;SEP&gt;chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BARONI, M.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"BARONI, M. is a researcher involved in evaluating compositional distributional semantic models."&lt;SEP&gt;"Baroni collaborates with Marelli and others on the SICK cure project."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c&lt;SEP&gt;chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BENTIVOGLI, L.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"BENTIVOGLI, L. is a researcher involved in evaluating compositional distributional semantic models."&lt;SEP&gt;"Bentivogli collaborates with Marelli and others on the SICK cure project."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c&lt;SEP&gt;chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;BERNARDI, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"BERNARDI, R. is a researcher involved in evaluating compositional distributional semantic models."&lt;SEP&gt;"Bernardi collaborates with Marelli and others on the SICK cure project."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c&lt;SEP&gt;chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;ZAMPARELLI, R.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"ZAMPARELLI, R. is a researcher involved in evaluating compositional distributional semantic models."&lt;SEP&gt;"Zamparelli collaborates with Marelli and others on the SICK cure project."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c&lt;SEP&gt;chunk-2016e530048d82125b70492619ae1cd8</data>
</node>
<node id="&quot;EFFICIENT ESTIMATION OF WORD REPRESENTATIONS IN VECTOR SPACE&quot;">
  <data key="d0">"TECHNOLOGY"</data>
  <data key="d1">"This paper describes a method for efficiently estimating word representations using vector space models."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;JNLP TEAM: DEEP LEARNING FOR LEGAL PROCESSING IN COLIEE 2020&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The JNLP team is a research group focusing on deep learning applications in legal processing."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;NIGAM@COLIEE-22: LEGAL CASE RETRIEVAL AND ENTAILMENT USING CASCADING OF LEXICAL AND SEMANTIC-BASED MODELS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"This work focuses on legal case retrieval and entailment using a combination of lexical and semantic-based models."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;THE ASSIN 2 SHARED TASK: A QUICK OVERVIEW&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This event is a shared task focused on assessing similarity between pairs of sentences."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;T. MIKOLOV&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"T. Mikolov is a researcher who contributed to the efficient estimation of word representations in vector space."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;K. CHEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"K. Chen is a researcher who contributed to the efficient estimation of word representations in vector space."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;G. CORRADO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"G. Corrado is a researcher who contributed to the efficient estimation of word representations in vector space."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;J. DEAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. Dean is a researcher who contributed to the efficient estimation of word representations in vector space."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;H. NGUYEN, P. M. NGUYEN, T. B. DANG, Q. M. BU , V. T. SINH, C. M. NGUYEN, V. D. TRAN, K. SATOH, AND M. L. NGUYEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These individuals are part of the JNLP team involved in deep learning for legal processing in COLIEE 2020."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;L. REAL, E. FONSECA, H. OLIVEIRA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"These individuals are part of the team that contributed to The ASSIN 2 shared task."</data>
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</node>
<node id="&quot;FONSECA , E.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"FONSECA, E. is an author of the paper 'The assin 2 shared task: a quick overview' co-authored with Real and Oliveira."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;OLIVEIRA , H. G.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"OLIVEIRA, H. G. is an author of the paper 'The assin 2 shared task: a quick overview' co-authored with Real and Fonseca."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;REIMERS , N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"REIMERS, N. is an author of 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks' and 'Making monolingual sentence embeddings multilingual using knowledge distillation.'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;ROBERTSON , S.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"ROBERTSON, S. is an author of 'The Probabilistic Relevance Framework: BM25 and Beyond'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;ZARAGOZA , H.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"ZARAGOZA, H. is a co-author of 'The Probabilistic Relevance Framework: BM25 and Beyond' with Robertson."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;RODRIGUES , J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"RODRIGUES, J. is an author of 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;RÜCKLÉ, A.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"R ¨UCKL ´E, A. is an author of 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models'"</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;HAZEEER, N.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"SHAZEER , N. is a co-author of 'Attention is all you need' with Vaswani."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;UZUKI, J.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"U SZKOREIT , J. is a co-author of 'Attention is all you need' with Vaswani, Hazeeer, and Parmar."</data>
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</node>
<node id="&quot;L.U. AND POLOSUKHIN &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"L.U. and Polosukhin are authors of the 'Attention is all you need' paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;SCIPY1.0 C ONTRIBUTORS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"SciPy Contributors refers to the collective group contributing to the development and maintenance of the SciPy library."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;NATURE METHODS&quot;">
  <data key="d0">"LOCATION"</data>
  <data key="d1">"Nature Methods is the journal where the 'SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python' paper was published."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;N. T HAKUR &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"N. Thakur is an author of 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval' paper."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;I. W ANG &quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"I. Wang is an author involved in multiple research papers."</data>
  <data key="d2">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</node>
<node id="&quot;GLOVE&quot;/&quot;RECURRENT NEURAL NETWORK (RNN)/LONG SHORT-TERM MEMORY (LSTM)&quot;">
  <data key="d2">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
  <data key="d1">"BERT is a state-of-the-art model that builds upon and improves the capabilities of traditional NLP technologies like RNN/LSTM for better understanding and generating natural language. GloVe can also be used to enhance BERT's word embedding capabilities."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;DATA PROCESSING 47&quot;">
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
  <data key="d1">"The Corpus and Data Processing are directly related, as the former serves as input for the latter process."&lt;</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;PURELY SEMANTIC SEARCH SYSTEM 50&quot;">
  <data key="d2">chunk-1721070d8e5848c74ff9604584ac59f8</data>
  <data key="d1">"The Corpus is used as input for the Purely Semantic Search System, which processes it to generate search results."&lt;</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FIGURE 2.2 AND FIGURE 2.3&quot;">
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
  <data key="d1">"The CBOW model is detailed in Figures 2.2 and 2.3, showing its variations based on context size." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FIGURE 2.5 AND FIGURE 2.6&quot;">
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
  <data key="d1">"The Recurrent Network Fully Connected structure is detailed in Figures 2.5 and 2.6, showing its components." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FIGURE 2.8 AND FIGURE 9.10&quot;">
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
  <data key="d1">"The Transformer Model is introduced in both Figures 2.8 and 9.10, showing its versatility in different contexts." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FIGURE 3.4 AND FIGURE 3.5&quot;">
  <data key="d2">chunk-4438e56d1dbc938d09784326b42337ca</data>
  <data key="d1">"The TSDAE architecture is illustrated in Figures 3.4 and 3.5, showing its structure for sequential data processing." "</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SUPREMO TRIBUNAL DE JUSTIC ¸A&quot;">
  <data key="d2">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
  <data key="d1">"NLI is not directly related to STJ, but both involve legal processes."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SEMANTIC STRATEGIES&quot;">
  <data key="d2">chunk-ae090822f3d4769354cc463665e2df89</data>
  <data key="d1">"The IRIS project aims to develop summarization approaches using semantic strategies for STJ."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;SBERT MODIFICATION&quot;">
  <data key="d2">chunk-813c546217d2864adf1fc0789841ad36</data>
  <data key="d1">"The SBERT modification is suggested to be applied to small portions of text in order to achieve better performance, implying that Bertimbau can benefit from this approach for specific tasks."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;COLIEE 2021 &quot;">
  <data key="d2">chunk-4178cfa608054c267be41d058b830af4</data>
  <data key="d1">"JNLP participated in the COLIEE 2021 event and focused on using self-labeled approaches for task 1."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;TEXTO&quot;">
  <data key="d2">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
  <data key="d1">"The Texto contains detailed descriptions and rulings from an Acordao, making it relevant for legal searches."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ETHICAL CHALLENGES&quot;">
  <data key="d2">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
  <data key="d1">"This publication discusses and addresses ethical issues faced during dataset creation and usage."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ASICK&quot;">
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
  <data key="d1">"MARELLI, M. and co-authors are working on the ASICK project for evaluating compositional distributional semantic models."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;LREC'14&quot;">
  <data key="d2">chunk-9b1e1352f338d948b4876b635d24d01c</data>
  <data key="d1">"The LREC'14 conference was held in Reykjavik, Iceland."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;PARMA, N.&quot;">
  <data key="d2">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
  <data key="d1">"Both Vaswani and Parma are authors on 'Attention is all you need' indicating collaboration in research."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;">
  <data key="d3">68.0</data>
  <data key="d4">"Rui Filipe Coimbra Pereira de Melo developed a semantic search system for this judicial institution, indicating an effort to enhance its operational efficiency."&lt;SEP&gt;"Rui Filipe Coimbra Pereira de Melo is working on developing a semantic search system for the judicial organization, indicating his involvement and dedication to improving its operations."&lt;SEP&gt;"Rui is working on a semantic search system specifically to assist the Supreme Court of Justice in its decision-making process."&lt;SEP&gt;"The Supremo Tribunal de Justiça is the subject of the research, which aims to assist in its decision-making process."&lt;SEP&gt;"The Semantic Search System was developed by Rui Filipe Coimbra Pereira de Melo for use in the Supremo Tribunal de Justiç a, assisting its decision-making process." "&lt;SEP&gt;"The Semantic Search System was developed by Rui Filipe Coimbra Pereira de Melo for use in the Supremo Tribunal de Justiça, assisting its decision-making process."</data>
  <data key="d5">"research collaboration, support and development"&lt;SEP&gt;"research development, legal support"&lt;SEP&gt;"research focus, support and development"&lt;SEP&gt;"research, improvement"&lt;SEP&gt;"system development, legal information retrieval"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86&lt;SEP&gt;chunk-2b8710310791165f23bcc4f86fda6d82</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Semantic Search System is intended to be implemented within the Supremo Tribunal de Justiça to enhance information retrieval processes."</data>
  <data key="d5">"technology implementation, enhancement"</data>
  <data key="d6">chunk-2b8710310791165f23bcc4f86fda6d82</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;SUPREMO TRIBUNAL DE JUSTIÇA SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Supremo Tribunal de Justiça uses the semantic search system to improve information retrieval within its operations."</data>
  <data key="d5">"information retrieval, technology adoption"</data>
  <data key="d6">chunk-2b8710310791165f23bcc4f86fda6d82</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Sistema de Busca Semântica assists in the decision-making process at the Supremo Tribunal de Justiça."</data>
  <data key="d5">"support, assistance"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;PORTUGAL&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Supremo Tribunal de Justiça operates in Portugal, where the technology is developed to support its operations."</data>
  <data key="d5">"location, jurisdiction"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"Research related to this legal institution was submitted and accepted at the conference."&lt;SEP&gt;"The conference is mentioned as a venue where research related to legal systems and jurisprudence might be presented or discussed."</data>
  <data key="d5">"academic contribution, recognition"&lt;SEP&gt;"research domain, influence"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;LEGAL DOMAIN&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The legal domain involves Supremo Tribunal de Justiça as it pertains to legal proceedings and decisions."</data>
  <data key="d5">"domain relevance, jurisdiction"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;JURISPRUDENCE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Jurisprudence guides the development and application of the Semantic Search System in the Supremo Tribunal de Justiç a for decision-making processes." "&lt;SEP&gt;"Jurisprudence guides the development and application of the Semantic Search System in the Supremo Tribunal de Justiça for decision-making processes."</data>
  <data key="d5">"legal guidance, application context"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;RELATOR 1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Relator 1 is associated with a case handled by the Supreme Court, indicating their role in legal proceedings."</data>
  <data key="d5">"legal context, authority"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;LEGAL-BERTIMBAU&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Supremo Tribunal de Justiça is a location where potential applications of Legal-BERTimbau might be relevant for legal searches and data retrieval systems."</data>
  <data key="d5">"legal domain, application relevance"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;PILE OF LAW&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"The legal institution might use datasets like Pile of Law for better understanding and processing of legal texts and documents."</data>
  <data key="d5">"legal application, data usage"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA&quot;" target="&quot;PORTUGUESE LEGAL DOMAIN DATASETS&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The legal institution could benefit from the use of these annotated datasets in research."</data>
  <data key="d5">"legal application, research support"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;SUPREMO TRIBUNAL DE JUSTIÇ A&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Rui Filipe Coimbra Pereira de Melo developed a semantic search system for this judicial institution, indicating an effort to enhance its operational efficiency."</data>
  <data key="d5">"system development, legal information retrieval"</data>
  <data key="d6">chunk-2b8710310791165f23bcc4f86fda6d82</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS&quot;">
  <data key="d3">19.0</data>
  <data key="d4">"Pedro Alexandre Simões dos Santos is Rui's supervisor who has provided guidance throughout his thesis work."&lt;SEP&gt;"Pedro Alexandre Simões dos Santos supervised Rui Filipe Coimbra Pereira de Melo throughout his thesis work and provided guidance in the development of the Semantic Search System."</data>
  <data key="d5">"supervision, mentorship"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;PROF. JOÃO MIGUEL DE SOUSA DE ASSIS DIAS&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"João Miguel De Sousa de Assis Dias is another supervisor who has been involved in guiding Rui's thesis work."&lt;SEP&gt;"João Miguel de Sousa de Assis Dias also supervised Rui Filipe Coimbra Pereira de Melo during his thesis work, contributing to the development of the Semantic Search System."</data>
  <data key="d5">"supervision, mentorship"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;PROF. MARIA LUÍSA TORRES RIBEIRO MARQUES DA SILVA COHEUR&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Maria Luísa Torres Ribeiro Marques da Silva Coheur evaluated and chaired the examination committee for Rui Filipe Coimbra Pereira de Melo's thesis, overseeing his research on the Semantic Search System."&lt;SEP&gt;"Maria Luísa Torres Ribeiro Marques da Silva Coheur has supervised parts of the thesis and is part of the examination committee."</data>
  <data key="d5">"evaluation, supervision"&lt;SEP&gt;"supervision, evaluation"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;PROF. JOSÉ LUÍS BRINQUETE BORBINHA&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"José Luís Brinquete Borbinha has been a member of the examination committee and is involved in Rui's thesis work."&lt;SEP&gt;"José Luís Brinquete Borbinha was a member of the examination committee that evaluated Rui Filipe Coimbra Pereira de Melo's thesis and contributed to its development."</data>
  <data key="d5">"evaluation, contribution"&lt;SEP&gt;"supervision, evaluation"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;PROJECT IRIS MEMBERS&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Rui collaborated with Project IRIS members, gaining valuable experience and insights from their work."&lt;SEP&gt;"Rui has collaborated with Project IRIS members on developing the semantic search system for the Supreme Court of Justice."</data>
  <data key="d5">"collaboration, development"&lt;SEP&gt;&lt;source_entity&gt;</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;THESIS TO OBTAIN THE MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING&quot;">
  <data key="d3">11.0</data>
  <data key="d4">"Rui authored this thesis as part of his degree requirements."&lt;SEP&gt;"The thesis is directly related to Rui's academic pursuit for his Master's degree."</data>
  <data key="d5">"academic pursuit, research work"&lt;SEP&gt;&lt;source_entity&gt;</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;LARGE LANGUAGE MODELS&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Rui's research utilizes and develops models such as Large Language Models to enhance information retrieval."</data>
  <data key="d5">"research utilization, development"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;ABSTRACT&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Abstract summarizes Rui Filipe Coimbra Pereira de Melo's research and findings on the Semantic Search System."&lt;SEP&gt;"The Abstract summarizes Rui Filipe Coimbra Pereira de Melo's research and findings on the Semantic Search System." "</data>
  <data key="d5">"research summary, development outcome"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;SUPERVISORS&quot;">
  <data key="d3">3.0</data>
  <data key="d4">"Both supervisors provided critical advice and support, significantly influencing Rui's research."&lt;SEP&gt;"Rui is supervised by Pedro Alexandre Simões dos Santos and João Miguel de Sousa de Assis Dias, who guide him through the research process."&lt;SEP&gt;"The supervisors were the guiding authority for Rui throughout his thesis, offering continuous support and feedback."</data>
  <data key="d5">&lt;source_entity&gt;</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;FAMILY&quot;">
  <data key="d3">1.0</data>
  <data key="d4">"The family provided unwavering support, which was crucial for Rui's academic success."</data>
  <data key="d5">&lt;source_entity&gt;</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;SUPERVISORS: PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOÃO DIAS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Rui Filipe Coimbra Pereira de Melo is supervised by both Professors Pedro Alexandre Simões dos Santos and João Dias throughout his thesis work."</data>
  <data key="d5">"supervision, guidance"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;RUI FILIPE COIMBRA PEREIRA DE MELO&quot;" target="&quot;EXAMINATION COMMITTEE: PROF. MARIA LUÍSA TORRES RIBEIRO MARQUES DA SILVA COHEUR, PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOSÉ LUÍS BRINQUETE BORBINHA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The examination committee includes members who were also supervisors, ensuring a cohesive guidance and evaluation process for the thesis work."</data>
  <data key="d5">"guidance, evaluation"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;BI-ENCODER&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Bi-Encoder is central to the Semantic Search System, creating necessary embeddings independently."</data>
  <data key="d5">&lt;"embedding creation, core component"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;HYBRID SEARCH SYSTEMS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Semantic and Hybrid Search Systems are part of the work's development, with hybrid systems combining semantic and lexical approaches."</data>
  <data key="d5">&lt;"approach integration, component"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;ELASTICSEARCH&quot;">
  <data key="d3">34.0</data>
  <data key="d4">"ElasticSearch is an integral part of the Semantic Search System, providing functionality for data management."&lt;SEP&gt;"Elasticsearch is used as a dense vector database to store and index embeddings generated by Legal-BERTimbau, supporting efficient semantic search."&lt;SEP&gt;"The semantic search system uses Elasticsearch to store and retrieve indexed legal documents efficiently."&lt;SEP&gt;"ElasticSearch is part of the constraints or functionalities within the Semantic Search System described in detail."&lt;</data>
  <data key="d5">"architecture component, data management"&lt;SEP&gt;"architecture component, functionality constraint"&lt;SEP&gt;"database, retrieval engine"&lt;SEP&gt;&lt;"database, storage solution"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c&lt;SEP&gt;chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;BM25&quot;">
  <data key="d3">34.0</data>
  <data key="d4">"BM25 outperforms the Semantic Search System in terms of identifying query sources and search metric performance, indicating superior capabilities."&lt;SEP&gt;"BM25 outperforms the Semantic Search System in terms of identifying query sources and search metric performance, indicating superior capabilities."&lt;&lt;SEP&gt;"BM25 outperforms the Semantic Search Systems, especially in identifying query sources and maintaining performance metrics."</data>
  <data key="d5">"performance comparison, superior algorithm"&lt;SEP&gt;"superior performance, query source identification"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">34.0</data>
  <data key="d4">"Project IRIS is involved in developing the Semantic Search System as a part of its larger goals and objectives."&lt;SEP&gt;"Project IRIS members are involved in developing the Semantic Search System as part of their collaborative project."&lt;SEP&gt;"The Semantic Search System was developed within Project IRIS and benefits from collaboration with its members."&lt;SEP&gt;"The project, Project IRIS, involves the development of the Semantic Search System as part of its goals and tasks."</data>
  <data key="d5">"collaboration, development"&lt;SEP&gt;"development collaboration"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;LEGAL-BERTIMBAU&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Legal-BERTimbau models are used in the development of the Semantic Search System to improve its performance on Portuguese legal documents."&lt;SEP&gt;"Legal-BERTimbau variants were trained for the Semantic Search System and are integral to its functionality."</data>
  <data key="d5">"integration, adaptation"&lt;SEP&gt;"modeling, adaptation"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;STJ&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"The Semantic Search System is designed for STJ and aims to provide more accurate insights into jurisprudence for this legal entity."&lt;SEP&gt;"The developed Semantic Search System is intended to provide insights into jurisprudence for STJ, aiding judges' work and decision-making process."&lt;SEP&gt;"The Semantic Search System is designed to support the STJ and improve its decision-making process through more accurate document suggestions."</data>
  <data key="d5">"support mechanism"&lt;SEP&gt;"support, application"&lt;SEP&gt;"target, application"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;LEXICAL-FIRST HYBRID SEARCH SYSTEM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Lexical-First Hybrid Search System is an extension of the broader Semantic Search System and provides a variant approach."</data>
  <data key="d5">"variant development, enhancement"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;LEXICAL+SEMANTIC HYBRID SEARCH SYSTEM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Lexical+Semantic Hybrid Search System complements the Semantic Search System by balancing both lexical and semantic search methods."</data>
  <data key="d5">"complementary method, variant development"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;SUPPLEMENTARY INFORMATION SYSTEMS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Supplementary Information Systems work in conjunction with the Semantic Search System to enhance its overall functionality and accuracy."</data>
  <data key="d5">"enhancement, complementarity"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;ARTIFICIAL INTELLIGENCE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Artificial Intelligence is a key technology behind the development of the Semantic Search System for the Supremo Tribunal de Justiça."&lt;SEP&gt;"Artificial Intelligence is a key technology behind the development of the Semantic Search System for the Supremo Tribunal de Justiça." "</data>
  <data key="d5">"technology foundation, development basis"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;BERT&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"BERT is used in the Semantic Search System to enhance its capabilities over traditional methods like BM25." "</data>
  <data key="d5">"enhancement, advanced technology"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;NATURAL LANGUAGE PROCESSING&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Natural Language Processing is a component of the Semantic Search System, enabling better understanding and processing of legal text." "</data>
  <data key="d5">"component, language processing"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;STATE OF THE ART&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The State of the Art in NLP includes advancements such as Semantic Search Systems, which are critical for pushing the boundaries of information retrieval."</data>
  <data key="d5">"advancement within NLP field"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;CONSTRAINTS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"When developing a Semantic Search System, constraints play a crucial role in shaping the design and functionality to ensure practical and effective solutions."</data>
  <data key="d5">"design consideration"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;ALBERTINA PT&quot;">
  <data key="d3">11.0</data>
  <data key="d4">"Albertina PT is associated with or involved in the development or application of semantic search systems."&lt;SEP&gt;"Albertina PT might be involved with or related to Semantic Search System, as both are referenced within the document but not explicitly linked."&lt;</data>
  <data key="d5">"collaboration, system development"&lt;SEP&gt;"collaboration, unspecified relationship"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;THE CORPUS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Corpus is a key input for the Semantic Search System, serving as the basis for its operation."</data>
  <data key="d5">"data source, training material"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;LEXICAL-FIRST SEARCH SYSTEM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Lexical-First Search System is a component or variation within the broader Semantic Search System."</data>
  <data key="d5">"architecture component, system variant"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Lexical + Semantic Search System is a more advanced variant of the Semantic Search System, integrating both lexical and semantic processes."</data>
  <data key="d5">"advanced architecture, system evolution"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;STS DATASET&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"The STS dataset is used for fine-tuning and evaluating the Semantic Search System."&lt;SEP&gt;"The Semantic Search System was evaluated using both fine-tuned models without and with the custom STS dataset."</data>
  <data key="d5">"dataset usage, evaluation metric"&lt;SEP&gt;&lt;"fine-tuning, evaluation"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;EXPLORING EMBEDDINGS MODELS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both contributions were part of the thesis work, with embeddings models informing the development of the search system."</data>
  <data key="d5">"related research, technology foundation"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;V0 MODELS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"V0 models, fine-tuned on pre-existing and manually annotated datasets, perform slightly better than V1 models that use custom STS dataset in the Search metric."::</data>
  <data key="d5">"fine-tuning, dataset quality"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;LEXICAL-FIRST AND LEXICAL+SEMANTIC SEARCH SYSTEMS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The developed Semantic Search System includes two specific versions: Lexical-First and Lexical+Semantic. These systems are based on preliminary evaluation showing improved performance over BM25 technique."</data>
  <data key="d5">"development, system variants"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;PRE-PROCESSING TEXT DATA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The pre-processing step is essential for the Semantic Search System to function effectively. The system relies on well-preprocessed data."/&gt;</data>
  <data key="d5">"data dependency, preprocessing importance"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH SYSTEM&quot;" target="&quot;BM25 TECHNIQUE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BM25 is used as a benchmark technique to evaluate the performance of the developed Semantic Search System. The Semantic Search System outperforms BM25 in some scenarios."/&gt;</data>
  <data key="d5">"performance comparison, evaluation criteria"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA SEMANTIC SEARCH SYSTEM&quot;" target="&quot;SUPREMO TRIBUNAL DE JUSTIÇ A&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Supremo Tribunal de Justiça uses the semantic search system to improve information retrieval within its operations."</data>
  <data key="d5">"information retrieval, technology adoption"</data>
  <data key="d6">chunk-2b8710310791165f23bcc4f86fda6d82</data>
</edge>
<edge source="&quot;PROJECT IRIS MEMBERS&quot;" target="&quot;STJ PROTOTYPE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Project IRIS members developed the STJ Prototype for judicial use."|&lt;&gt;"team contribution, system implementation"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;PROJECT IRIS MEMBERS&quot;" target="&quot;LEXICAL-FIRST&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Project IRIS members worked on evaluating and developing the Lexical-First architecture as part of their contributions to the Semantic Search System project."</data>
  <data key="d5">"collaboration, development process"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;THESIS TO OBTAIN THE MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING&quot;" target="&quot;SUPERVISORS: PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOÃO DIAS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The supervisors are directly involved in guiding Rui Filipe Coimbra Pereira de Melo's thesis work towards obtaining a Master of Science degree in Computer Science and Engineering."</data>
  <data key="d5">"degree achievement, supervision"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;THESIS TO OBTAIN THE MASTER OF SCIENCE DEGREE IN COMPUTER SCIENCE AND ENGINEERING&quot;" target="&quot;NATURAL LANGUAGE PROCESSING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Natural Language Processing techniques are integral to the research and development described in the thesis."</data>
  <data key="d5">&lt;technique, development"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;BERT&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT is a subset of Large Language Models with specialized training."</data>
  <data key="d5">&lt;"sub-type, specialization"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;BM25&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Both Large Language Models like BERT and BM25 are used in the semantic search system, with BERT providing advanced capabilities."</data>
  <data key="d5">&lt;"comparison, advancement"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;SEARCH SYSTEM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Large language models are integral components of the search system and are evaluated for performance on STS tasks."</data>
  <data key="d5">"component evaluation, model performance"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LARGE LANGUAGE MODELS&quot;" target="&quot;DOMAIN ADAPTATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Domain adaptation is used to tailor large language models to better understand the Portuguese legal context."</data>
  <data key="d5">"model adaptation, contextual understanding"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;LEXRANK&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"Both BERT and LexRank are advanced NLP technologies, with BERT focusing on pre-training and fine-tuning for various tasks, while LexRank focuses on text summarization. They both contribute significantly to the field of Natural Language Processing."&lt;SEP&gt;"While LexRank focuses on text summarization, BERT can potentially integrate with LexRank for enhanced NLP tasks, combining the strengths of both technologies." "</data>
  <data key="d5">"NLP advancements, summarization techniques"&lt;SEP&gt;"integration, capability enhancement"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;MASKED LANGUAGE MODELING (MLM)&quot;">
  <data key="d3">56.0</data>
  <data key="d4">"BERT uses Masked Language Modeling (MLM) as one of its pre-training techniques to reduce prediction bias."&lt;SEP&gt;"BERT uses Masked Language Modeling as a technique during its pre-training phase to reduce bias in word prediction."&lt;SEP&gt;"BERT uses Masked Language Modeling as a technique during its pre-training phase to reduce bias in word prediction." "&lt;SEP&gt;"BERT uses Masked Language Modeling during its pre-training process to reduce bias and improve word prediction accuracy."&lt;SEP&gt;"BERT uses Masked Language Modeling during its pre-training phase to reduce bias and improve word prediction accuracy."</data>
  <data key="d5">"bias reduction, context-based prediction"&lt;SEP&gt;"language understanding, reduction in bias"&lt;SEP&gt;"technique, pre-training"&lt;SEP&gt;"training technique, reduction of bias"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;NEXT SENTENCE PREDICTION (NSP)&quot;">
  <data key="d3">51.0</data>
  <data key="d4">"BERT employs Next Sentence Prediction as part of its pre-training process to understand sentence relationships."&lt;SEP&gt;"BERT employs Next Sentence Prediction as part of its pre-training process to understand sentence relationships." "&lt;SEP&gt;"BERT incorporates Next Sentence Prediction as a part of its training process to understand sentence relationships and context better."&lt;SEP&gt;"BERT incorporates Next Sentence Prediction as part of its pre-training process to better understand sentence relationships."&lt;SEP&gt;"BERT uses Next Sentence Prediction (NSP) as a technique in the pre-training phase for understanding sentence relationships."</data>
  <data key="d5">"context understanding, relationship prediction"&lt;SEP&gt;"sentence relationship understanding, training task"&lt;SEP&gt;"sentence understanding, relationship comprehension"&lt;SEP&gt;"understanding, relationship"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;TOKEN EMBEDDING LAYER&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The Token Embedding Layer is part of BERT's input representation system."&lt;SEP&gt;"The Token Embedding Layer is part of BERT's input representation system." "</data>
  <data key="d5">"input, embedding"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;SEGMENTATION EMBEDDING LAYER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Segmentation Embedding Layer is part of the input embeddings in BERT."&lt;SEP&gt;"The Segmentation Embedding Layer is part of the input embeddings in BERT." "</data>
  <data key="d5">"embedding, distinction"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;POSITION EMBEDDING LAYER&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The Position Embedding Layer is also a component of BERT's input representation system."&lt;SEP&gt;"The Position Embedding Layer is also a component of BERT's input representation system." "</data>
  <data key="d5">"input, position"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;L, H, A&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"L, H, and A represent architectural parameters in BERT's design."</data>
  <data key="d5">"architecture, parameters"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;TRANSFORMER BLOCKS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Transformer blocks are part of the BERT model's architecture." "</data>
  <data key="d5">"architecture, component"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;SBERT&quot;">
  <data key="d3">88.0</data>
  <data key="d4">"Both BERT and SBERT are deep learning models, but they have different focuses: BERT is more general while SBERT specializes in sentence-level semantics."&lt;SEP&gt;"SBERT is a specific modification of BERT used in semantic embeddings, whereas BERT is the general model from which SBERT is derived."&lt;SEP&gt;"SBERT is an improvement over BERT in terms of speed and efficiency for semantic similarity tasks."&lt;SEP&gt;"SBERT, a modification of BERT, was noted to be less effective on long texts compared to BERT for tasks such as description queries and title queries."&lt;SEP&gt;"SBERT outperforms BERT in processing sentence pairs, being significantly faster."&lt;SEP&gt;"SBERT is an improved version of BERT, faster and more efficient for semantic similarity tasks."&lt;SEP&gt;"SBERT builds upon BERT's token embeddings and architecture, demonstrating faster performance in sentence pair comparisons."&lt;SEP&gt;"SBERT is a modification of BERT used for embedding smaller text passages to improve efficiency on long texts."</data>
  <data key="d5">"architecture, performance improvement"&lt;SEP&gt;"model derivation, specialization"&lt;SEP&gt;"model improvement, speed enhancement"&lt;SEP&gt;"model specialization, relationship between models"&lt;SEP&gt;"model variation, effectiveness comparison"&lt;SEP&gt;"modification, efficiency improvement"&lt;SEP&gt;"performance comparison, speed advantage"&lt;SEP&gt;"performance enhancement, speed increase"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4&lt;SEP&gt;chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;BERTIMBAU VARIANTS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERTimbau variants are modifications of BERT models, indicating a direct relation through model lineage."</data>
  <data key="d5">"model derivatives, modification"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;STATE-OF-THE-ART MODELS&quot;">
  <data key="d3">19.0</data>
  <data key="d4">"BERT is one of several state-of-the-art models discussed in this chapter, showcasing its significance in NLP advancements."&lt;SEP&gt;"BERT represents an advancement in the field of Natural Language Processing (NLP) as part of state-of-the-art models."</data>
  <data key="d5">"innovation, advancement"&lt;SEP&gt;"model comparison, innovation"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;BOOKSCORPUS (800M WORDS) AND ENGLISH WIKIPEDIA (2500M WORDS)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"BERT utilizes these datasets for pre-training to build a comprehensive language model."</data>
  <data key="d5">"dataset usage, pre-training material"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;ADAM OPTIMIZER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"During the fine-tuning phase, BERT uses Adam Optimizer to train its parameters and improve performance."</data>
  <data key="d5">"fine-tuning process, optimization algorithm"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;TRANSFORMER ARCHITECTURE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Transformer architecture is used by BERT to process and understand text in a bidirectional manner, enabling its sophisticated language capabilities."</data>
  <data key="d5">"architecture, model foundation"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;STATE OF THE ART&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The State of the Art section discusses BERT as one of the key models in NLP."</data>
  <data key="d5">"advancements discussion, specific model"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;STATE-OF-THE-ART MODELS AND TECHNIQUES&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"BERT is a state-of-the-art model that falls within this category of advanced NLP techniques."</data>
  <data key="d5">"innovation, natural language processing"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;PRE-TRAINING AND FINE-TUNING&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"These phases are essential in the development and application of BERT as an advanced NLP model."</data>
  <data key="d5">"model training, innovation"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;BOOKSCORPUS (800M WORDS)&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"BooksCorpus is used by BERT during the pre-training phase to provide a substantial amount of text data for training."</data>
  <data key="d5">"data source, pre-training material"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;ENGLISH WIKIPEDIA (2500M WORDS)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"English Wikipedia serves as another dataset for BERT's pre-training phase, contributing additional diverse text data."</data>
  <data key="d5">"diverse data, pre-training material"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;DEVLIN, J.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Jacob Devlin is known for developing BERT, a model used in natural language processing."</data>
  <data key="d5">"model creator"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;GLOVE&quot;/&quot;RECURRENT NEURAL NETWORK (RNN)/LONG SHORT-TERM MEMORY (LSTM)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT is a state-of-the-art model that builds upon and improves the capabilities of traditional NLP technologies like RNN/LSTM for better understanding and generating natural language. GloVe can also be used to enhance BERT's word embedding capabilities."</data>
  <data key="d5">"model evolution, improvement"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;SBERT -NLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SBERT -NLI builds upon BERT's architecture but with pre-training on NLI datasets, indicating an improved approach to handling natural language inference tasks."</data>
  <data key="d5">"architecture, pre-training, multi-genre support"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;SISTEMA DE PESQUISA SEMÂNTICA&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The Sistema de Pesquisa Semântica outperforms BM25 by 335% in retrieving relevant passages during the first consultation."</data>
  <data key="d5">"performance improvement, benchmark comparison"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;LEGAL-BERTIMBAU VARIANTS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Legal-BERTimbau variants will be compared against BM25 to evaluate their performance in retrieving relevant information for judges."</data>
  <data key="d5">"performance comparison, retrieval evaluation"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;LEXICAL-FIRST APPROACH&quot;">
  <data key="d3">46.0</data>
  <data key="d4">"The Lexical-First approach performs closer to BM25 and sometimes surpasses it in certain metrics."&lt;SEP&gt;"The Lexical-First approach performs closer to BM25 and sometimes surpasses it, showing competitive performance."&lt;&lt;SEP&gt;"The Lexical-First approach performs closer to BM25 and sometimes surpasses it, suggesting similar performance or outperformance in identifying relevant documents."::</data>
  <data key="d5">"approach comparison, performance similarity"&lt;SEP&gt;"competitive performance, close match"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;SEARCH METRIC&quot;">
  <data key="d3">59.0</data>
  <data key="d4">"BM25 is evaluated using the Search metric, which measures its performance in ranking documents based on relevance."&lt;SEP&gt;"BM25 outperforms the original Semantic Search System in the Search metric, indicating better identification of query sources."::&lt;SEP&gt;"BM25 shows specific performance metrics in the Search metric, demonstrating its effectiveness under certain conditions."&lt;SEP&gt;"BM25 outperforms the original Semantic Search System in the Search metric, indicating better query source identification capabilities."</data>
  <data key="d5">"performance evaluation, comparative analysis"&lt;SEP&gt;"performance evaluation, ranking accuracy"&lt;SEP&gt;"performance, search algorithm"&lt;SEP&gt;"query source identification, search performance"&lt;SEP&gt;&lt;"performance evaluation, comparative analysis"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;LEGAL-BERTIMBAU&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Legal-BERTimbau outperforms BM25 in certain STS datasets, demonstrating superior performance in information retrieval tasks."&lt;SEP&gt;"Legal-BERTimbau variants outperform BM25 in certain scenarios but maintain similar query identification ability."</data>
  <data key="d5">"performance comparison"&lt;SEP&gt;"performance comparison, baseline technique"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Sistema de Busca Semântica combines BM25 with the potential of Legal-BERTimbau to enhance performance."</data>
  <data key="d5">"performance enhancement, hybrid approach"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;ELASTICSEARCH&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"ElasticSearch uses BM25 as the default search method, indicating its importance in document searching operations."</data>
  <data key="d5">"default search engine, efficiency"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;LEXICAL+SEMANTIC APPROACH&quot;">
  <data key="d3">25.0</data>
  <data key="d4">"The Lexical+Semantic approach can occasionally outperform BM25 in specific scenarios."&lt;SEP&gt;"The Lexical+Semantic approach occasionally outperforms BM25, indicating better performance for specific tasks."::</data>
  <data key="d5">"approach comparison, outperformance"&lt;SEP&gt;"occasional superiority, competitive performance"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;HYBRID SEARCH SYSTEM&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"A Hybrid Search System can match or outperform BM25 capabilities in certain metrics."</data>
  <data key="d5">"superior system, competitive performance"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;ORIGINAL SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The original semantic search system is outperformed by BM25 in terms of performance metrics."</data>
  <data key="d5">"performance comparison, inferior algorithm"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;LEXICAL + SEMANTIC APPROACH&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Lexical+Semantic approach can occasionally outperform BM25 in specific scenarios."</data>
  <data key="d5">"occasional superiority, competitive performance"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;TOP 1&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BM25 shows different performance levels at various Top N results, indicating its baseline ranking function."|&lt;&gt;"baseline, performance comparison"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;SEARCH SYSTEM EVALUATION – SEARCH METRIC&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BM25 is used to evaluate search systems in terms of their performance metrics."|&lt;&gt;"performance evaluation, baseline comparison"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;LEXICAL-FIRST SEARCH SYSTEM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"BM25 is used in the Lexical-First Search System for pre-filtering top results before evaluating them with cosine similarity." "&lt;SEP&gt;"The Lexical-First Search System ranks the top 20 results using BM25 before applying cosine similarity for further ranking."</data>
  <data key="d5">"prioritization, ranking"&lt;SEP&gt;"ranking method, search system architecture"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;SEARCH SYSTEM EVALUATION&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The search system will be evaluated by comparing it to traditional techniques like BM25 on different dimensions of performance."</data>
  <data key="d5">"traditional comparison, performance metric"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;V1 MODELS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"V1 models perform marginally worse than V0 models in both Search and Discovery metrics, but BM25 still outperforms them in the Search metric."::</data>
  <data key="d5">"model comparison, performance contrast"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;NLI TRAINING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Models that received NLI training show a 4.3% improvement in the Search metric and a 5.4% improvement in the Discovery measure."::</data>
  <data key="d5">"training method, performance improvement"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;GPL TRAINING APPROACH&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Models subjected to GPL training show a 3.2% improvement in the Search metric and a 1.7% improvement in the Discovery metric."::</data>
  <data key="d5">"training method, performance improvement"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;TSDAE-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The tsdae-gpl-nli-sts-MetaKD-v0 model shows promising results with improved metrics and outperforms BM25 in both Search and Discovery metrics."::</data>
  <data key="d5">"model comparison, performance evaluation"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;COSINE SIMILARITY METRIC&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both BM25 and cosine similarity are techniques used to score segments based on their semantic relevance in the search engine."</data>
  <data key="d5">"relevance scoring, information retrieval"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;COSINE SIMILARITY&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both methods are used in different search systems to rank the relevance of search results, although they serve distinct roles in their respective architectures."</data>
  <data key="d5">"relevance ranking, technology integration"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"In the Lexical + Semantic Search System, BM25 scores are used in conjunction with cosine similarity to rank and filter results, combining both lexical and semantic information retrieval methods."</data>
  <data key="d5">"hybrid system architecture, information retrieval"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;DISCOVERY METRIC&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"BM25 does not have the same capability to recommend similar documents as Semantic Search Systems using models fine-tuned on custom datasets, showing its limitations in this aspect."</data>
  <data key="d5">"document recommendation, limitation"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;MODELS V0&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Models fine-tuned on pre-existing datasets (V0 models) perform slightly lower than those fine-tuned on custom STS datasets (V1 models), highlighting the impact of training methods."</data>
  <data key="d5">"training method, performance difference"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;MODELS V1&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Models using custom STS dataset (V1 models) perform marginally worse than those fine-tuned on pre-existing datasets (V0 models), showing a slight decrease in performance."</data>
  <data key="d5">"training method, performance gap"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;BM25&quot;" target="&quot;TABLE 6.3&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"BM25 is evaluated in Table 6.3, showing top-ranking performance metrics for different ranking positions."</data>
  <data key="d5">"performance comparison, evaluation"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA PORTUGUÊS&quot;" target="&quot;SISTEMA DE PESQUISA SEMÂNTICA&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Sistema de Pesquisa Semântica is designed to assist the Supremo Tribunal de Justiça português in its decision-making process."</data>
  <data key="d5">"support, assistance"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA PORTUGUÊS&quot;" target="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;">
  <data key="d3">37.0</data>
  <data key="d4">"The Supreme Court of Justice of Portugal uses the Semantic Search System for its decision-making processes."&lt;SEP&gt;"The Supremo Tribunal de Justiça português is the end-user of the Sistema de Busca Semântica developed for their decision-making process."&lt;SEP&gt;"The Supremo Tribunal de Justiça português is the user of the Sistema de Busca Semântica developed for their decision-making process."&lt;SEP&gt;"The semantic search prototype was developed to assist in decision-making at the Supremo Tribunal de Justiça português."</data>
  <data key="d5">"application, legal context"&lt;SEP&gt;"decision support, judicial process"&lt;SEP&gt;"user, development context"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA PORTUGUÊS&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"While not directly related, the work's focus on improving decision-making support can indirectly benefit from new techniques like Metadata Knowledge Distillation."</data>
  <data key="d5">"indirect application, technology potential"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA PORTUGUÊS&quot;" target="&quot;INFORMATION RETRIEVAL&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The work is focused on developing a solution for the Supremo Tribunal de Justiça português's decision-making process which involves information retrieval."</data>
  <data key="d5">"decision support, information processing"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE PESQUISA SEMÂNTICA&quot;" target="&quot;LEGAL-BERTIMBAU&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Legal-BERTimbau is a component of the Sistema de Pesquisa Semântica used for legal text analysis."</data>
  <data key="d5">"integration, functionality"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE PESQUISA SEMÂNTICA&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Metadata Knowledge Distillation is a new technique introduced within the Sistema de Pesquisa Semântica for adapting large language models to Portuguese jurisprudence."</data>
  <data key="d5">"innovation, adaptation"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE PESQUISA SEMÂNTICA&quot;" target="&quot;INTELIGÊNCIA ARTIFICIAL&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Inteligência Artificial is integrated into the Sistema de Busca Semântica for advanced text analysis and understanding."</data>
  <data key="d5">"integration, functionality"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;JOÃO RODRIGUES ET AL.&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"Although not directly mentioned, João Rodrigues et al. are likely involved in the development of Legal-BERTimbau due to their involvement with Albertina."&lt;SEP&gt;"The creators of Albertina also contributed to Legal-BERTimbau, extending their expertise in BERT models."</data>
  <data key="d5">"expertise transfer, collaboration"&lt;SEP&gt;"research collaboration, model development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;ELASTICSEARCH&quot;">
  <data key="d3">29.0</data>
  <data key="d4">"Legal-BERTimbau generates sentence embeddings which are stored in indices on Elasticsearch."&lt;SEP&gt;"The implementation constraints for the Legal-BERTimbau model include the use of Elasticsearch, indicating integration with this technology."&lt;SEP&gt;"Legal-BERTimbau is used to generate sentence embeddings which are then stored in Elasticsearch for searching and retrieval." "</data>
  <data key="d5">"embedding generation, database integration"&lt;SEP&gt;"technology integration, constraint adherence"&lt;SEP&gt;&lt;"embedding generation, storage"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6&lt;SEP&gt;chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;DOCUMENT PRE-PROCESSING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Document Pre-processing is essential to ensure Legal-BERTimbau can effectively handle the data by splitting documents into smaller units."</data>
  <data key="d5">&lt;"data preparation, processing enhancement"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;SBERT&quot;">
  <data key="d3">22.0</data>
  <data key="d4">"SBERT is a variant specifically adapted for Portuguese, enhancing the Legal-BERTimbau models used in the Semantic Search System." "&lt;SEP&gt;"SBERT is a version of Legal-BERTimbau that is designed for meaningful sentence processing, complementing the model’s capabilities."</data>
  <data key="d5">"specialized adaptation, model enhancement"&lt;SEP&gt;&lt;"model variant, integration"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c&lt;SEP&gt;chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;BERTIMBAU&quot;">
  <data key="d3">100.0</data>
  <data key="d4">"Legal-BERTimbau is an extension or adaptation of BERTimbau designed for a specific domain, the legal field, to improve understanding of legal records."&lt;SEP&gt;"Legal-BERTimbau is an extension or adaptation of BERTimbau designed for a specific domain, the legal field, to improve understanding of legal records." "&lt;SEP&gt;"Legal-BERTimbau is built on top of BERTimbau, adapting it to the legal domain by fine-tuning on relevant data."&lt;SEP&gt;"Legal-BERTimbau was created using BERTimbau as its base model. This relationship indicates a direct derivation and adaptation."&lt;SEP&gt;"Legal-BERTimbau is derived and fine-tuned based on BERTimbau for specific tasks in the Portuguese legal domain."&lt;SEP&gt;"Legal-BERTimbau is based on Bertimbau, specifically fine-tuned for the legal domain to ensure better understanding of legal records."&lt;SEP&gt;"Legal-BERTimbau is derived from BERTimbau through transfer learning and domain adaptation, making use of its pre-trained parameters on a large Portuguese corpus."</data>
  <data key="d5">"derived model, fine-tuning"&lt;SEP&gt;"domain adaptation, model evolution"&lt;SEP&gt;"foundation, adaptation"&lt;SEP&gt;"transfer learning, adaptation"&lt;SEP&gt;&lt;"base_model_relationship"&gt;</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36&lt;SEP&gt;chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;BRWAC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Legal-BERTimbau is trained on the BrWaC corpus, showing a dependency on the data used for training."</data>
  <data key="d5">&lt;"training_data_relationship"&gt;</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;EMBEDDING CREATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The embedding's creation process involved adapting Legal-BERTimbau using techniques such as domain adaptation and transfer learning."</data>
  <data key="d5">&lt;"creation_process"&gt;</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;MULTILINGUAL MODEL&quot;">
  <data key="d3">28.0</data>
  <data key="d4">"Legal-BERTimbau is a multilingual model that performs worse than the original Semantic Search System in the Search metric."::&lt;SEP&gt;"The Multilingual model performs worse than Legal-BERTimbau, which is an enhanced version used in the Semantic Search System."&lt;</data>
  <data key="d5">"enhancement, relative performance"&lt;SEP&gt;"model comparison, performance"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;SENTENCETRANSFORMERS PYTHON LIBRARY&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"Legal-BERTimbau model was used with SentenceTransformers Python library to generate sentence embeddings."&lt;SEP&gt;"The Legal-BERTimbau model is hosted on the SentenceTransformers Python library to generate sentence embeddings." "</data>
  <data key="d5">"model generation, embedding creation"&lt;SEP&gt;"model usage, embedding generation"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;COSINE SIMILARITY&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Cosine Similarity is one of the methods used in Legal-BERTimbau's training process."</data>
  <data key="d5">"training method, evaluation metric"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;SBERT MODEL: LEGAL-BERTIMBAU&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The SBERT model is a specific implementation of Legal-BERTimbau designed for finding similar passages in legal documents."</data>
  <data key="d5">"implementation, specific task"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;TRANSFER LEARNING&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Transfer Learning was used to create Legal-BERTimbau by fine-tuning BERTimbau on a specific legal domain dataset."</data>
  <data key="d5">"method, implementation"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;SENTENCE-TRANSFORMERS/ALL-MPNET-BASE-V2&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Both models are used for generating sentence embeddings but Legal-BERTimbau is specifically mentioned as part of the development process."</data>
  <data key="d5">&lt;"model comparison, multilingual approach"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;ELASTICSEARCH INDEX&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Legal-BERTimbau's sentence embeddings are stored in an ElasticSearch index for document retrieval."</data>
  <data key="d5">&lt;"storage, embedding retrieval"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Project IRIS members contribute to the creation and fine-tuning of Legal-BERTimbau models as part of their collaborative work."&lt;SEP&gt;"The Legal-BERTimbau model is developed as part of the ongoing Project IRIS and utilizes pre-defined aspects such as the ElasticSearch engine for its implementation."</data>
  <data key="d5">"contributions, adaptation"&lt;SEP&gt;"model development, project integration"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;LEXRANK TECHNIQUE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The LexRank technique was used in implementing an extractive summarization system using Legal-BERTimbau models."</data>
  <data key="d5">"methodology, application"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;ROUGE-1 SCORE 47.92, ROUGE-2 SCORE 22.50&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The performance metrics demonstrate the effectiveness of Legal-BERTimbau in text summarization tasks."</data>
  <data key="d5">"model evaluation, performance metrics"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"A paper based on research involving Legal-BERTimbau was accepted and submitted to the EPIA Conference on Artificial Intelligence."</data>
  <data key="d5">"publication, acceptance"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;DATASET ANNOTATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Development and improvement of datasets are crucial for the advancement of models like Legal-BERTimbau, affecting their performance and applicability in legal domains."</data>
  <data key="d5">"data development, model improvement"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;PILE OF LAW&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both Legal-BERTimbau and Pile of Law are part of broader research efforts in the legal domain, focusing on improving model performance and dataset quality."</data>
  <data key="d5">"model and dataset, legal research"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Legal-BERTimbau is a component used in the Semantic Search System to improve performance."</data>
  <data key="d5">"component, enhancement"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;FINE-TUNING STAGE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"During the fine-tuning stage, Legal-BERTimbau was developed to ensure it could adequately understand legal records." "</data>
  <data key="d5">"adaptation, refinement"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;JO˜ÃO RODRIGUES ET AL.&quot;">
  <data key="d3">3.0</data>
  <data key="d4">"While not directly mentioned, João Rodrigues et al. are likely involved in the development of Legal-BERTimbau due to their involvement with Albertina."&lt;SEP&gt;"While not directly mentioned, Jo˜ão Rodrigues et al. are likely involved in the development of Legal-BERTimbau due to their involvement with Albertina."</data>
  <data key="d5">"research collaboration, model development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Legal-BERTimbau is utilized in the Lexical + Semantic Search System to verify retrieved results based on semantic information retrieval." "</data>
  <data key="d5">"verification, semantic validation"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;SBERT MODEL&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The SBERT model Legal-BERTimbau is a specialized version created using techniques from neural networks and transfer learning to handle sentence-level tasks in the Portuguese legal domain."</data>
  <data key="d5">"domain adaptation, model specialization"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;SBERT VARIANTS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both SBERT and Legal-BERTimbau are language models used in creating sentence embeddings for evaluation, indicating their role in performance testing." "</data>
  <data key="d5">"model comparison, evaluation"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;LEGAL SEARCH SYSTEM&quot;">
  <data key="d3">30.0</data>
  <data key="d4">"Legal Search System uses Legal-BERTimbau for generating sentence embeddings which are stored in an ElasticSearch index for semantic search operations."&lt;SEP&gt;"Legal Search System uses Legal-BERTimbau for generating sentence embeddings which are stored in an ElasticSearch index."</data>
  <data key="d5">"model usage, embedding generation"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;STJ&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Legal-BERTimbau variants are specifically adapted for Portuguese legal domain relevant to STJ judgments. They surpass state-of-the-art multilingual models on assin and assin2 datasets."</data>
  <data key="d5">"adaptation, performance improvement"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;ALBERTINA PT&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Legal-BERTimbau is a related model but not directly connected with Albertina in this context."</data>
  <data key="d5">"related model, no direct connection"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;LEGAL-BERTIMBAU TRAINING&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The event of Legal-BERTimbau Training involves creating and fine-tuning the model using various techniques and tasks to generate multiple versions."</data>
  <data key="d5">"model development, versioning"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU&quot;" target="&quot;SEARCH METRIC&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Legal-BERTimbau is used in evaluating the performance of the Semantic Search System, particularly in the Search metric."</data>
  <data key="d5">"model evaluation, performance testing"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;" target="&quot;PROCESS OF DECISION MAKING&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The Sistema de Busca Semântica plays a crucial role in the process of decision making at the Supreme Court."</data>
  <data key="d5">"support, process enhancement"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;" target="&quot;BERTIMBAU&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Sistema de Busca Semântica utilizes BERTimbau, a specifically trained variant of BERT."</data>
  <data key="d5">"technology integration, performance improvement"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;" target="&quot;ALEX&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Alex leads the team that develops and tests the Sistema de Busca Semântica for the Supremo Tribunal de Justiça português."</data>
  <data key="d5">"leadership, development"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;" target="&quot;SAM RIVERA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Sam Rivera contributes to the development and testing of the Sistema de Busca Semântica."</data>
  <data key="d5">"contribution, team member"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;" target="&quot;MODELOS BERT ESPECIALMENTE TREINADOS (VARIANTES LEGAL-BERTIMBAU)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Specialized BERT models were used to improve the performance of the semantic search prototype."</data>
  <data key="d5">"enhancement, model improvement"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;" target="&quot;SISTEMAS DE PESQUISA HÍBRIDA QUE INCORPORAM TANTO TÉCNICAS LEXICAIS COMO SEMÂNTICAS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The hybrid systems combined with both lexical and semantic techniques were integrated into the semantic search system."</data>
  <data key="d5">"integration, technology combination"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;SISTEMA DE BUSCA SEMÂNTICA&quot;" target="&quot;INFORMATION RETRIEVAL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The semantic search prototype aims to improve information retrieval methods."</data>
  <data key="d5">"information retrieval enhancement, system improvement"</data>
  <data key="d6">chunk-206731ccea8c9feb0ba9feef0468e18a</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;LEXRANK&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Both GloVe and LexRank are technologies in Natural Language Processing, but they serve different purposes. GloVe is for learning word embeddings, while LexRank is a summarization technique."&lt;SEP&gt;"Both GloVe and LexRank are technologies used in NLP, but they serve different purposes: GloVe for word embeddings, and LexRank for text summarization."</data>
  <data key="d5">"technological diversity, complementary approaches"&lt;SEP&gt;"word embeddings, summarization"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;TOMAS MIKOLOV&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Tomas Mikolov also contributed to the development of GloVe, another method for word embeddings.&lt;"|&gt;collaboration, contributor"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;STANFORD UNIVERSITY&quot;">
  <data key="d3">63.0</data>
  <data key="d4">"Stanford University also developed GloVe, focusing on deriving semantic relationships between words using a co-occurrence matrix."&lt;SEP&gt;"Stanford University researchers also developed GloVe as part of their work in natural language processing."&lt;SEP&gt;"Stanford University researchers developed GloVe, an algorithm for vectorizing words based on co-occurrence matrices."&lt;SEP&gt;"Stanford University is also the institution where GloVe was developed, indicating another significant contribution in natural language processing technology."</data>
  <data key="d5">"development, innovation"&lt;SEP&gt;"research development, unsupervised learning"&lt;SEP&gt;"research institution, development origin"&lt;SEP&gt;"research origin, development"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;CO-OCCURRENCE MATRIX&quot;">
  <data key="d3">51.0</data>
  <data key="d4">"GloVe derives semantic relationships between words using co-occurrence matrices that represent the probability of word pairs appearing together in context."&lt;SEP&gt;"GloVe uses co-occurrence matrices to define word relationships and generate word embeddings."&lt;SEP&gt;"The GloVe method relies on co-occurrence matrices for defining its loss function and learning word embeddings based on statistical analysis of text corpora."&lt;SEP&gt;"The method GloVe relies on the co-occurrence matrix to learn word embeddings based on statistical analysis of text corpora."</data>
  <data key="d5">"data source, analysis method"&lt;SEP&gt;"modeling, statistical analysis"&lt;SEP&gt;"statistical basis, learning process"&lt;SEP&gt;&lt;"statistical foundation, co-occurrence analysis"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;RNN&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"Both GloVe and RNN are machine learning techniques used in natural language processing. They are related through their application in understanding sequential data."&lt;SEP&gt;"GloVe and RNN both deal with sequential data but through different methodologies—GloVe using static word vectors while RNN processes sequences of inputs iteratively."</data>
  <data key="d5">"machine learning, text analysis"&lt;SEP&gt;&lt;"sequence processing, methodology comparison"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Both GloVe and RNN are technologies used in NLP, but they serve different purposes with GloVe focusing on word embeddings and RNNs handling sequence data."&lt;SEP&gt;"Both GloVe and RNNs are technologies used in NLP, but they serve different purposes with GloVe focusing on word embeddings and RNNs handling sequence data."</data>
  <data key="d5">"technological complementarity"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;EMBEDDING SPACES&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both GloVe and embedding spaces use similar principles of representing words in multidimensional spaces, though they differ in their specific methodologies."</data>
  <data key="d5">"vector representation, similarity measures"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;CORPUS OF TEXT&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The GloVe model processes a corpus of text to generate word embeddings based on co-occurrence statistics."</data>
  <data key="d5">"text processing, embedding generation"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;TECHNOLOGY&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"GloVe is part of the broader field of NLP technologies."</data>
  <data key="d5">"NLP technologies"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;WORD EMBEDDINGS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"GloVe is a different implementation of word embeddings but shares the same goal of representing words semantically through vector spaces."</data>
  <data key="d5">"implementation, semantic representation"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;CO-OCCURRENCE MATRIX MIJ&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Mij is a key concept in GloVe's methodology, forming the basis of its co-occurrence matrix approach."</data>
  <data key="d5">"basis, methodology"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;WORD ICE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Word 'ice' is a key concept in the GloVe's co-occurrence matrix, showing its frequency of co-occurrence with various words."</data>
  <data key="d5">"key concept, statistical analysis"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;GLOVE&quot;" target="&quot;WORD STEAM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Word 'steam' is another key concept in the GloVe's co-occurrence matrix, demonstrating its relationship to other concepts like gas and water."</data>
  <data key="d5">"key concept, statistical analysis"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;" target="&quot;LONG SHORT-TERM MEMORY (LSTM)&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"LSTM is a specific type of RNN, indicating their close relation in the field of sequential data processing."</data>
  <data key="d5">"specialization within NLP technology"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;" target="&quot;FIGURE 2.5&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Figure 2.5 visually represents the architecture of a Recurrent Neural Network, which is used for processing sequential data."</data>
  <data key="d5">"visual representation, architectural description"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;TRANSFORMERS&quot;" target="&quot;BERT (BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Transformers and BERT are both advanced models in NLP, with Transformers being a more recent development that includes BERT as one of its variants."</data>
  <data key="d5">"evolutionary relation in NLP technology"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;TRANSFORMERS&quot;" target="&quot;TECHNOLOGIES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Transformers are a more recent development that has revolutionized the field of NLP, building on the foundations laid by RNNs and LSTMs like BERT."</data>
  <data key="d5">"revolutionary model, foundation"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;TRANSFORMERS&quot;" target="&quot;LSTM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Transformers were developed as an improvement over LSTMs to address their limitations, particularly long-term dependency handling and training time issues."</data>
  <data key="d5">&lt;"network evolution, efficiency gains"</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;LEXRANK&quot;" target="&quot;VECTOR SPACE WITH QUERY EMBEDDING AND MULTIPLE SENTENCE EMBEDDINGS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both concepts involve vector spaces where documents or sentences are represented as points. LexRank uses a graph-based approach while vector representations use numerical vectors for similarity searches."</data>
  <data key="d5">"vector representation, information retrieval"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;LEXRANK&quot;" target="&quot;TEXT SUMMARIZATION&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"LexRank is a method specifically used for text summarization, highlighting the direct relationship between these two concepts within the NLP field."&lt;SEP&gt;"LexRank is one specific algorithm used for text summarization tasks in NLP."</data>
  <data key="d5">"specific application within summarization"&lt;SEP&gt;"summarization algorithms"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;LEXRANK&quot;" target="&quot;EXTRACTIVE TECHNIQUES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"LexRank is an example of an unsupervised extractive summarization technique focusing on sentence selection based on eigenvector centrality scores."</data>
  <data key="d5">"sentence scoring, graph-based approach"</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;LEXRANK&quot;" target="&quot;VECTOR SPACE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Eigenvector centrality in LexRank is used to score sentences within a vector space representation of text."</data>
  <data key="d5">"sentence scoring, similarity graph"</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;LEXRANK&quot;" target="&quot;EIGENVECTOR CENTRALITY&quot;">
  <data key="d3">1.0</data>
  <data key="d4">"Eigenvector Centrality is used in LexRank for scoring sentences based on their importance within the document's graph representation."</data>
  <data key="d5">&lt;"sentence scoring, centrality measure"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;LEXRANK&quot;" target="&quot;INTRA-SENTENCE COSINE SIMILARITY&quot;">
  <data key="d3">1.0</data>
  <data key="d4">"Intra-sentence Cosine Similarity is a method used in constructing the sentence similarity graph of LexRank."</data>
  <data key="d5">&lt;"similarity measure, edge weight calculation"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;LEXRANK&quot;" target="&quot;JACCARD SIMILARITY&quot;">
  <data key="d3">1.0</data>
  <data key="d4">"Jaccard Similarity can be an alternative to cosine similarity in constructing the sentence graph within LexRank."</data>
  <data key="d5">&lt;"similarity measure, edge weight calculation"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;LEXRANK&quot;" target="&quot;FIGURE 2.12&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"LexRank is described in Figure 2.12, highlighting its use in text summarization based on eigenvector centrality." "</data>
  <data key="d5">"lexrank, text summarization"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;BERT (BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS)&quot;" target="&quot;DOMAIN ADAPTATION&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"BERT models can be fine-tuned using domain adaptation techniques to perform well on specific tasks, showing a clear relationship in adapting general-purpose NLP models."</data>
  <data key="d5">"adaptability of advanced NLP models"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;BERT (BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS)&quot;" target="&quot;PAGERANK METHOD&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"Both BERT and PageRank are methods that analyze the importance or relevance of nodes within a graph structure, though they operate on different models."</data>
  <data key="d5">"graph analysis, relevance assessment"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH TYPE&quot;" target="&quot;POSITIONAL ENCODING&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"While Positional Encoding is not directly related to Semantic Search, it plays an important role in preparing the data for such searches."</data>
  <data key="d5">"indirect relationship, data preparation"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH TYPE&quot;" target="&quot;TRANSFORMER MODEL&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The concept of Semantic Search is relevant to the Transformer Model as it involves processing and understanding textual information, which aligns with its design."</data>
  <data key="d5">"related concepts, model relevance"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;TEXT SUMMARIZATION&quot;" target="&quot;EXTRACTIVE TECHNIQUES&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"Extractive techniques are a key aspect of Text Summarization that focus on retrieving important sentences from documents without considering their meaning. This is mentioned as part of the text summarization process."&lt;SEP&gt;"Text summarization primarily uses extractive techniques which focus on selecting relevant sentences from a document without altering their meanings."</data>
  <data key="d5">"sentence selection, relevance preservation"&lt;SEP&gt;&lt;"sentence selection, information extraction"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;TEXT SUMMARIZATION&quot;" target="&quot;ABSTRACTIVE TECHNIQUES&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Abstractive techniques are another method used in Text Summarization that use complex models to understand and generate coherent summaries. This is part of the broader text summarization field."&lt;SEP&gt;"While text summarization can use abstractive techniques for creating more coherent summaries, the main approach discussed here is extractive."</data>
  <data key="d5">"coherence, complexity"&lt;SEP&gt;&lt;"semantic understanding, summary generation"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;TEXT SUMMARIZATION&quot;" target="&quot;QUERY EMBEDDING&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"In the context of text summarization, query embeddings can be used to understand user intent and generate relevant summaries based on that understanding."</data>
  <data key="d5">"query analysis, summary generation"</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;STATE OF THE ART&quot;" target="&quot;BERTIMBAU&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"BERTimbau is part of the broader State of the Art in NLP technologies."&lt;SEP&gt;"State of the Art refers to the current state or latest developments in a field such as NLP, and BERTimbau is an extension or variant of BERT developed for specific tasks."</data>
  <data key="d5">"latest advancements, extensions"&lt;SEP&gt;"state-of-the-art advancement, inclusion"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;STATE OF THE ART&quot;" target="&quot;DEEPER TEXT UNDERSTANDING&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The concept of Deeper Text Understanding is relevant to the broader field of the State of the Art in NLP."</data>
  <data key="d5">"conceptual framework, integration"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;STATE OF THE ART&quot;" target="&quot;LEGAL INFORMATION RETRIEVAL&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Legal Information Retrieval is a specific application that contributes to the overall State of the Art in NLP technologies."</data>
  <data key="d5">"application focus, inclusion"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;STATE OF THE ART&quot;" target="&quot;NLP APPLIED TO PORTUGUESE CONSUMER LAW&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"NLP Applied To Portuguese Consumer Law is an example of practical applications within the broader State of the Art in NLP."</data>
  <data key="d5">"practical application, representation"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;DEEPER TEXT UNDERSTANDING&quot;">
  <data key="d3">19.0</data>
  <data key="d4">"BERTimbau aims to enhance or support Deeper Text Understanding capabilities in specific domains."&lt;SEP&gt;"Deeper Text Understanding refers to the advancements in understanding text at a deeper level using technologies like BERT and Transformers, and BERTimbau is an example of such advancement."</data>
  <data key="d5">"domain adaptation, enhancement"&lt;SEP&gt;"text understanding, NLP models"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;ALBERTINA PT-BR&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Albertina PT-BR outperforms BERTimbau on certain STS tasks but falls short in others, indicating a comparative performance difference."</data>
  <data key="d5">"performance comparison, improvement opportunity"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"BERTimbau is used to fine-tune and generate embeddings for negative passages, supporting the training of models like stjiris/bert-large-portuguese-cased-legal-tsdae-nli-sts-v0 in a multi-step process."</data>
  <data key="d5">"fine-tuning, embedding generation"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;ZHUYUN DAI&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Zhuyun Dai's work on contextual models like BERT influenced the development of BERTimbau by providing a basis for fine-tuning on specific domains."</data>
  <data key="d5">"influence, research foundation"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;BRWAC CORPUS&quot;">
  <data key="d3">75.0</data>
  <data key="d4">"The BrWaC Corpus was used to pre-train BERTimbau before it was adapted for the Portuguese Legal Domain."&lt;SEP&gt;"The BrWaC corpus is a source of data used for pretraining BERTimbau models."&lt;SEP&gt;"The BrWaC corpus is a source of data used for pretraining BERTimbau models." "&lt;SEP&gt;"The BrWaC corpus was used to train the BERTimbau model, contributing a diverse and extensive dataset for pretraining."&lt;SEP&gt;"The BrWaC corpus was utilized in the pretraining stage of Bertimbau, providing diverse text for training."</data>
  <data key="d5">"data source, pretraining"&lt;SEP&gt;"training data, base model"&lt;SEP&gt;"training data, diversity"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36&lt;SEP&gt;chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;ALBERTINA PT&quot;">
  <data key="d3">32.0</data>
  <data key="d4">"Albertina PT BR outperforms BERTimbau on certain STS tasks but falls short in others, indicating a competitive relationship between the two models."&lt;SEP&gt;"Albertina outperforms BERTimbau in certain tasks but is still considered an improved version."&lt;SEP&gt;"Albertina PT-BR outperforms or underperforms BERTimbau depending on the task it's compared to."</data>
  <data key="d5">"model comparison, performance improvement"&lt;SEP&gt;"performance comparison, model competition"&lt;SEP&gt;&lt;"competition, performance"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;BRWAC&quot;">
  <data key="d3">32.0</data>
  <data key="d4">"BERTimbau was pre-trained with BrWaC, a Portuguese corpus, indicating its source of initial training data."&lt;SEP&gt;"BrWaC provided the initial training corpus for BERTimbau before it was adapted to the legal domain through Legal-BERTimbau."</data>
  <data key="d5">"initial training, language model"&lt;SEP&gt;"training data, base model"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;NLP TECHNIQUES&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"BERTimbau is an adaptation of BERT specifically tailored to the Portuguese language and its legal information retrieval needs."</data>
  <data key="d5">"adaptation, specific application"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;ZHUYUN DAI AND JAMIE CALLAN&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"Zhuyun Dai and Jamie Callan's research influenced the development of BERTimbau by exploring contextual neural language models like BERT for information retrieval tasks."&lt;SEP&gt;"Zhuyun Dai and Jamie Callan's research influenced the development of BERTimbau by exploring contextual neural language models like BERT for information retrieval tasks." "&lt;SEP&gt;"Zhuyun Dai and Jamie Callan's research influenced the development of Bertimbau through their exploration of BERT for information retrieval tasks."</data>
  <data key="d5">"influence, research basis"&lt;SEP&gt;"research influence, application exploration"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;PRETRAINING STAGE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The pretraining stage was identical to BERT, and BERTimbau utilized this method for its training." "</data>
  <data key="d5">"methodology, process"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;MLM AND NSP METHODS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERTimbau was trained using MLM and NSP methods during its pretraining stage." "</data>
  <data key="d5">"technique, process"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;SBERT MODIFICATION&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The SBERT modification is suggested to be applied to small portions of text in order to achieve better performance, implying that Bertimbau can benefit from this approach for specific tasks."</data>
  <data key="d5">"performance enhancement, adaptation"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;HTML BODY&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The HTML body was utilized as part of the BrWaC corpus for pretraining BERTimbau."</data>
  <data key="d5">"pretraining data, text source"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"This variant was created by fine-tuning BERTimbau on legal documents using MLM, showing its derivation from the original model."</data>
  <data key="d5">"fine-tuning, adaptation"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"This variant was created by fine-tuning BERTimbau on legal documents using TSDAE, showing its derivation from the original model."</data>
  <data key="d5">"fine-tuning, adaptation"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;BERTIMBAU&quot;" target="&quot;HUGGINGFACE PLATFORM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The variants of BERTimbau are hosted on the HuggingFace Platform for distribution and use by developers."</data>
  <data key="d5">"hosting, distribution"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;LEGAL INFORMATION RETRIEVAL&quot;" target="&quot;NLP APPLIED TO PORTUGUESE CONSUMER LAW&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Legal Information Retrieval and NLP Applied To Portuguese Consumer Law both apply NLP techniques to specific legal domains, with Legal Information Retrieval being broader in scope."</data>
  <data key="d5">"legal domain applications, NLP techniques"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;LEGAL INFORMATION RETRIEVAL&quot;" target="&quot;ALBERTINA PT&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Albertina PT is involved in Legal Information Retrieval tasks, directly linking the organization to this specific application area within NLP."</data>
  <data key="d5">"application domain"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;NLP APPLIED TO PORTUGUESE CONSUMER LAW&quot;" target="&quot;INSTITUTO SUPERIOR TECNICO, UNIVERSIDADE DE LISBOA&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"This master's thesis on NLP for Portuguese consumer law was completed at Instituto Superior Tecnico, Universidade de Lisboa."</data>
  <data key="d5">"thesis location and institution"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;JOÃO RODRIGUES ET AL.&quot;">
  <data key="d3">44.0</data>
  <data key="d4">"Albertina PT was introduced by João Rodrigues et al. in May 2023 as a new state-of-the-art model."&lt;SEP&gt;"Albertina was developed and shared by João Rodrigues et al."&lt;SEP&gt;"The creators of the Albertina model are João Rodrigues and his team, who shared it as a new state-of-the-art model in 2023."</data>
  <data key="d5">"creators, development process"&lt;SEP&gt;"research collaboration"&lt;SEP&gt;&lt;"introduction, innovation"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;DEBERTA&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Albertina is built on the DeBERTa architecture, making use of its structure and features for enhanced performance."</data>
  <data key="d5">"architecture basis, improvement"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;JO ˜AO RODRIGUES ET AL.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Albertina PT was shared by Jo ˜ao Rodrigues et al. in May 2023 during the closing stages of their research work."</data>
  <data key="d5">&lt;"authorship, sharing"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;FACULDADE DE CIÊNCIAS DA UNIVERSIDADE DE LISBOA (FCUL)&quot;">
  <data key="d3">34.0</data>
  <data key="d4">"FCUL is a partner in the development of Albertina, specifically through the NLX–Natural Language and Speech Group."&lt;SEP&gt;"FCUL was involved in the development of Albertina PT as part of its partnership with FEUP."&lt;SEP&gt;"FCUL is one of the organizations involved in developing Albertina."</data>
  <data key="d5">"partnership, development"&lt;SEP&gt;"partnership, model development"&lt;SEP&gt;&lt;"partnership, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;FACULDADE DE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)&quot;">
  <data key="d3">34.0</data>
  <data key="d4">"FEUP is another organization that contributed to the development of Albertina."&lt;SEP&gt;"FEUP is another partner in the development of Albertina, particularly through their Laboratório de Inteligência Artificial e Ciência de Computadores."&lt;SEP&gt;"FEUP was involved in the development of Albertina PT as part of its partnership with FCUL."</data>
  <data key="d5">"partnership, development"&lt;SEP&gt;"partnership, model development"&lt;SEP&gt;&lt;"partnership, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;DEBERTA [16]&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"DeBERTa architecture served as the starting point for developing Albertina."&lt;SEP&gt;"DeBERTa is the starting point for Albertina's architecture."</data>
  <data key="d5">"architecture base, foundation"&lt;SEP&gt;"architecture foundation"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;JO˜ÃO RODRIGUES ET AL.&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"Albertina was developed and shared by Jo˜ão Rodrigues et al."&lt;SEP&gt;"Jo˜ão Rodrigues et al. are involved in developing Albertina, including its PT-PT version."</data>
  <data key="d5">"research collaboration"&lt;SEP&gt;"research collaboration, model development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">3.0</data>
  <data key="d4">"Albertina's development as part of Project IRIS, aligning with predefined aspects."</data>
  <data key="d5">"project component"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;FACULDADE DE CIˆNCIAS DA UNIVERSIDADE DE LISBOA (FCUL)&quot;">
  <data key="d3">2.0</data>
  <data key="d4">"FCUL is a partner in the development of Albertina, specifically through the NLX–Natural Language and Speech Group."</data>
  <data key="d5">"partnership, model development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;FACULDADEDE ENGENHARIA DA UNIVERSIDADE DO PORTO (FEUP)&quot;">
  <data key="d3">2.0</data>
  <data key="d4">"FEUP is another partner in the development of Albertina, particularly through their Laborat´orio de Inteligˆencia Artificial e Ciˆencia de Computadores."</data>
  <data key="d5">"partnership, model development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;JO ˜ÃO RODRIGUES ET AL.&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"Jo ˜ão Rodrigues et al. are the creators of Albertina [35]."</data>
  <data key="d5">"creators, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;NLX–NATURAL LANGUAGE AND SPEECH GROUP&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The NLX group at FEUP was involved in developing Albertina."</data>
  <data key="d5">"participation, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;LABORATÓRIO DE INTELIGÊNCIA ARTIFICIAL E CIÊNCIA DE COMPUTADORES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"This lab at FCUL participated in the development of Albertina."</data>
  <data key="d5">"participation, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;PT -PT VERSION&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The PT-PT version of Albertina refers to its European Portuguese implementation."</data>
  <data key="d5">"language version, regional implementation"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;PT -BR VERSION&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The PT-BR version of Albertina is the Brazilian Portuguese implementation of the model."</data>
  <data key="d5">"language version, regional implementation"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;BRWAC CORPUS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"BrWaC corpus was used in training the PT-BR version of Albertina."</data>
  <data key="d5">"corpus usage, pre-training process"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT&quot;" target="&quot;ELASTICSEARCH&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"Although not directly related, Jo ˜ão Rodrigues et al.'s work might integrate Elasticsearch into their system due to pre-defined constraints."</data>
  <data key="d5">"integration potential, technology choice"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;GENERATIVE PSEUDO LABELING&quot;" target="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION&quot;">
  <data key="d3">36.0</data>
  <data key="d4">"Both events are mentioned early in the document and seem related to methods for improving model performance."&lt;SEP&gt;"Both events are part of a broader methodology development process focused on improving language model performance through various techniques."&lt;SEP&gt;"Both events are part of a broader methodology development process focused on improving language model performance through various techniques."::&lt;SEP&gt;"Generative Pseudo Labeling can be integrated into Multilingual Knowledge Distillation to enhance the quality of generated content across multiple languages."</data>
  <data key="d5">"methodology improvement"&lt;SEP&gt;"technique integration, multilingual support"&lt;SEP&gt;"technique integration, system improvement"&lt;SEP&gt;&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;GENERATIVE PSEUDO LABELING&quot;" target="&quot;GENQ&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Both GenQ and Generative Pseudo Labeling are methods or components used for training models, suggesting a relationship in the context of generative tasks."&lt;SEP&gt;"Both concepts are part of the training process for generative tasks and could be related through their application or context in the text."&lt;</data>
  <data key="d5">"training process, method association"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;GENERATIVE PSEUDO LABELING&quot;" target="&quot;SYSTEM EVALUATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Generative Pseudo Labeling is an event that likely contributes to the broader category of system evaluation described in the document."</data>
  <data key="d5">"technique, evaluation"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"Both events involve knowledge transfer but focus on different aspects: multilingual vs. metadata-driven approaches."::&lt;SEP&gt;"These two events are both related to improving model performance across different domains, suggesting a parallel development approach."</data>
  <data key="d5">"performance improvement"&lt;SEP&gt;&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION&quot;" target="&quot;SYSTEM EVALUATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Multilingual Knowledge Distillation is another technique contributing to the larger category of System Evaluation within the research process."</data>
  <data key="d5">"technique, evaluation"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION&quot;" target="&quot;BIBLIOGRAPHY&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Both events are part of the research methodology, suggesting they might be referenced in the bibliography."</data>
  <data key="d5">"research methodology"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;METADATA KNOWLEDGE DISTILLATION&quot;" target="&quot;SYSTEM EVALUATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Metadata Knowledge Distillation is a third event that contributes to the broader category of system evaluation in the document."</data>
  <data key="d5">"technique, evaluation"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;METADATA KNOWLEDGE DISTILLATION&quot;" target="&quot;STS TASK EVALUATION&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"The technique of Metadata Knowledge Distillation directly influences the STS task evaluation metric."&lt;SEP&gt;"The technique of Metadata Knowledge Distillation is used to optimize the model for STS tasks, improving its performance."</data>
  <data key="d5">&lt;"optimization, evaluation"&gt;&lt;SEP&gt;&lt;"technique, metric"&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;METADATA KNOWLEDGE DISTILLATION&quot;" target="&quot;DOCUMENTS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The documents are the source of data used in Metadata Knowledge Distillation for model optimization."</data>
  <data key="d5">&lt;"data, technique"&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;METADATA KNOWLEDGE DISTILLATION&quot;" target="&quot;BIBLIOGRAPHY&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Similarly, Metadata Knowledge Distillation is a method that could be referenced in the bibliography for further details and citations."</data>
  <data key="d5">"citation"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;OVERVIEW&quot;" target="&quot;SYSTEM EVALUATION&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The event Overview precedes detailed System Evaluations, providing foundational context and setting the stage for more in-depth analysis."&lt;SEP&gt;"The event Overview precedes detailed System Evaluations, providing foundational context and setting the stage for more in-depth analysis."::</data>
  <data key="d5">"contextual foundation, preparatory phase"&lt;SEP&gt;&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;OVERVIEW&quot;" target="&quot;LANGUAGE MODEL EVALUATION&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Overview sets context for detailed evaluations like Language Model Evaluation, providing foundational information."</data>
  <data key="d5">&lt;"context, overview"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;SYSTEM EVALUATION&quot;" target="&quot;68-71&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The events Language Model Evaluation and Search System Evaluation are documented in pages 68 to 71 of the document."::</data>
  <data key="d5">&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;SYSTEM EVALUATION&quot;" target="&quot;SEARCH SYSTEM EVALUATION&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Search System Evaluation is a part of the broader System Evaluation process, which also includes Language Model Evaluation."&lt;SEP&gt;"System Evaluation encompasses various sub-events such as Search System Evaluation, indicating a broader category or process."&lt;SEP&gt;"System Evaluation includes Search System Evaluation as one of its components, providing a holistic view of the system's effectiveness."</data>
  <data key="d5">"comprehensive evaluation, component of system evaluation"&lt;SEP&gt;"evaluation, encompassing"&lt;SEP&gt;"holistic assessment, component inclusion"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;SYSTEM EVALUATION&quot;" target="&quot;SEARCH SYSTEM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"System Evaluation is focused on assessing the performance and effectiveness of the search system."</data>
  <data key="d5">"evaluation focus, performance assessment"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LANGUAGE MODEL EVALUATION&quot;" target="&quot;68&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The event Language Model Evaluation is discussed on page 68 of the document."::</data>
  <data key="d5">&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;LANGUAGE MODEL EVALUATION&quot;" target="&quot;LEGAL-BERTIMBAU VARIANTS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Legal-BERTimbau variants undergo evaluation to ensure they are suitable for the Portuguese legal context and perform well on STS tasks."</data>
  <data key="d5">"adaptation, task suitability"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LANGUAGE MODEL EVALUATION&quot;" target="&quot;COURT PROFESSIONALS&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The goal of Language Model Evaluation is to improve information retrieval for court professionals who need accurate and relevant documents."</data>
  <data key="d5">&lt;"improvement, relevance"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LANGUAGE MODEL EVALUATION&quot;" target="&quot;SEARCH SYSTEM EVALUATION&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both events evaluate different aspects of models, with Language Model Evaluation focusing on language models and Search System Evaluation on search systems."</data>
  <data key="d5">"model evaluation"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;70&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The event Search System Evaluation is discussed on page 70 of the document."::</data>
  <data key="d5">&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;DISCOVERY METRIC&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Discovery metric is an evaluated aspect in Search System Evaluation, focusing on discovery capabilities of models."&lt;SEP&gt;"The event of Search System Evaluation involves evaluating the Discovery metric for search systems."</data>
  <data key="d5">"evaluation focus, metrics"&lt;SEP&gt;"evaluation process, metrics"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;SBERT SPEARMAN CORRELATION&quot;">
  <data key="d3">13.0</data>
  <data key="d4">"SBERT Spearman correlation is a part of the broader Search System Evaluation, focusing on specific metrics."::&lt;SEP&gt;"The Search System Evaluation includes an evaluation using SBERT Spearman correlation, indicating its importance in assessing the search system's performance."</data>
  <data key="d5">"evaluation metric, search system assessment"&lt;SEP&gt;&lt;"evaluation process, correlation measure"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;SENTENCE-BERT (SBERT)&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Sentence-BERT (SBERT) models are used in Search System Evaluation for various tasks like STS benchmark evaluation."::</data>
  <data key="d5">&lt;"model usage, evaluation process"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;BERTIMBAU VARIANTS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERTimbau variants are evaluated as part of the broader Search System Evaluation to improve search system performance."::</data>
  <data key="d5">&lt;"evaluation process, model adaptation"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;COLIEE 2021 - TASK 1 RESULTS&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Results from COLIEE 2021 - Task 1 are part of the broader Search System Evaluation process."&lt;SEP&gt;"The COLIEE 2021 - Task 1 results provide outcomes relevant to the broader Search System Evaluation context."::&lt;SEP&gt;"The evaluation of search systems may be related to tasks in competitions like COLIEE, which could involve similar techniques or technologies used in evaluating search systems."</data>
  <data key="d5">"evaluation context, subcomponent"&lt;SEP&gt;"technique overlap, relevance"&lt;SEP&gt;&lt;"evaluation outcome, task performance"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;COLIEE 2021 - TASK 3 RESULTS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Results from COLIEE 2021 - Task 3 are also part of the broader Search System Evaluation process."&lt;SEP&gt;"The COLIEE 2021 - Task 3 results are part of the broader Search System Evaluation outcomes."::</data>
  <data key="d5">"evaluation context, subcomponent"&lt;SEP&gt;&lt;"evaluation outcome, task performance"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;LEGAL-BERTIMBAU VARIANTS&quot;">
  <data key="d3">13.0</data>
  <data key="d4">"Legal-BERTimbau variants are specifically evaluated within the context of Search System Evaluation for legal document processing tasks."::&lt;SEP&gt;"The Legal-BERTimbau variants are part of the search system and thus their performance is evaluated as a whole within the search system evaluation process."</data>
  <data key="d5">"system components, holistic assessment"&lt;SEP&gt;&lt;"evaluation process, model adaptation"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;MLM AVERAGE LOSS FOR LEGAL DOCUMENTS IN THE TEST SET&quot;">
  <data key="d3">13.0</data>
  <data key="d4">"MLM average loss is an evaluated metric within the Search System Evaluation process, indicating its relevance and use."&lt;SEP&gt;"The MLM (Masked Language Modeling) average loss on legal documents is part of evaluating search system performance."::</data>
  <data key="d5">&lt;"evaluation metrics, model performance"&lt;SEP&gt;&lt;"evaluation process, model performance measure"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;STS EVALUATION ON PORTUGUESE DATASETS&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The STS (Semantic Textual Similarity) evaluation on Portuguese datasets is a component of the broader Search System Evaluation."::</data>
  <data key="d5">&lt;"evaluation process, language specific performance"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;MODELS V0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Models V0 are part of the initial stages in Search System Evaluation, showing the progression from early to advanced models."</data>
  <data key="d5">"evaluation progress, versioning"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;MODELS V1&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Models V1 represent advancements over Models V0 within the context of Search System Evaluation, indicating continuous improvement and development."</data>
  <data key="d5">"progression, improvement"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The search system is evaluated using the STS task as a key metric to determine its effectiveness in retrieving relevant information."</data>
  <data key="d5">"evaluation methodology, performance comparison"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;CONCLUSION&quot;" target="&quot;81-83&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The event Conclusion is summarized on pages 81 to 83 of the document, covering contributions and future work."::</data>
  <data key="d5">&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;CONCLUSION&quot;" target="&quot;CONTRIBUTIONS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Conclusion highlights contributions made by the research, setting the stage for their publication."</data>
  <data key="d5">&lt;"summary, contributions"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;WORD EMBEDDINGS&quot;" target="&quot;RNN STRUCTURE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both Word Embeddings and RNN Structures are foundational in sequence modeling tasks. Word embeddings provide numerical vectors for words, while RNN structures process these sequences over time steps."</data>
  <data key="d5">"data representation, sequence processing"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;WORD EMBEDDINGS&quot;" target="&quot;SEMANTICS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Word embeddings and semantics are related in that word embeddings aim to capture the contextual meaning of words, aligning with the goal of understanding user intention in semantic search."</data>
  <data key="d5">"contextual meaning, intrinsic meaning"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;WORD EMBEDDINGS&quot;" target="&quot;COSINE SIMILARITY&quot;">
  <data key="d3">38.0</data>
  <data key="d4">"Cosine Similarity and Word Embeddings are both techniques for comparing the relatedness of words in multidimensional spaces, but they serve different purposes and have different strengths."&lt;SEP&gt;"Cosine Similarity measures the similarity between vector representations of words generated by embedding spaces, which are often used in semantic search and information retrieval systems."&lt;SEP&gt;"Word embeddings use cosine similarity to measure the semantic similarity between words based on their vector representations."</data>
  <data key="d5">"comparison technique, vector space analysis"&lt;SEP&gt;"similarity measure, vector space"&lt;SEP&gt;"vector representation, similarity measurement"&lt;SEP&gt;&lt;"comparison technique, vector space analysis"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;WORD EMBEDDINGS&quot;" target="&quot;WORD2VEC&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Word2Vec is a specific implementation of word embeddings that uses neural networks for unsupervised training to represent words in multidimensional space."</data>
  <data key="d5">"neural network, unsupervised learning"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;WORD EMBEDDINGS&quot;" target="&quot;CBOW MODEL&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The CBOW model is discussed alongside Word Embeddings, implying their interconnection in the context of natural language processing." "</data>
  <data key="d5">"embedding models, word relationships"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;WORD EMBEDDINGS&quot;" target="&quot;FIGURE 2.1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Word Embeddings are visually represented in Figure 2.1, indicating their connection to the diagram." "</data>
  <data key="d5">"visual representation, embedding models"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;WORD EMBEDDINGS&quot;" target="&quot;UNSUPERVISED TRAINING&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Unsupervised training is a technique used in creating Word Embeddings like Word2Vec and GloVe."</data>
  <data key="d5">&lt;"training method, model development"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;CBOW MODEL&quot;" target="&quot;SKIP-GRAM MODEL&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both CBOW and Skip-Gram models are part of the word2vec framework. They use different approaches to represent words in vector space but share common goals in natural language processing."</data>
  <data key="d5">"word representation, machine learning techniques"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;CBOW MODEL&quot;" target="&quot;WORD2VEC&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"The CBOW model is a method used in Word2Vec to predict target words from context using surrounding words as inputs and outputs."&lt;SEP&gt;"The CBOW model is one implementation method of Word2Vec, which uses context words to predict the target word."</data>
  <data key="d5">"implementation method, prediction mechanism"&lt;SEP&gt;"model component, functionality"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;CBOW MODEL&quot;" target="&quot;HIDDEN LAYER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The CBOW model uses a hidden layer to represent context using surrounding words as input."</data>
  <data key="d5">"context representation, neural network component"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;CBOW MODEL&quot;" target="&quot;OUTPUT LAYER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The output layer in the CBOW model represents the predicted word after processing through the hidden layer."</data>
  <data key="d5">"prediction mechanism, output processing"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;CBOW MODEL&quot;" target="&quot;FIGURE 2.2 AND FIGURE 2.3&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The CBOW model is detailed in Figures 2.2 and 2.3, showing its variations based on context size." "</data>
  <data key="d5">"context variation, cbow model"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;SKIP-GRAM MODEL&quot;" target="&quot;COMMON BAG OF WORDS (CBOW) MODEL BASED ON A ONE WORD CONTEXT&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Both are part of Word2Vec and address the task of predicting words given a context but from different perspectives, CBOW using surrounding words and Skip-Gram focusing on target-word contexts."</data>
  <data key="d5">"word prediction models, context-based learning"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;SKIP-GRAM MODEL&quot;" target="&quot;WORD2VEC&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"The Skip-Gram model is another implementation method of Word2Vec, focusing on generating context from a given word."&lt;SEP&gt;"The Skip-Gram model is another method within Word2Vec that predicts context based on a given word, similar to CBOW but with different input-output dynamics."</data>
  <data key="d5">"implementation method, generation mechanism"&lt;SEP&gt;"model component, functionality"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;SKIP-GRAM MODEL&quot;" target="&quot;HIDDEN LAYER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Similar to CBOW, the Skip-Gram model uses a hidden layer but focuses on generating context from a given word."</data>
  <data key="d5">"context generation, neural network component"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;SKIP-GRAM MODEL&quot;" target="&quot;OUTPUT LAYER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The output layer in the Skip-Gram model represents the predicted context words after processing through the hidden layer."</data>
  <data key="d5">"prediction mechanism, output processing"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;SKIP-GRAM MODEL&quot;" target="&quot;FIGURE 2.4&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Skip-Gram model is illustrated in Figure 2.4, indicating a different approach to word embeddings." "</data>
  <data key="d5">"skip-gram model, visualization"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;RECURRENT NETWORK FULLY CONNECTED&quot;" target="&quot;FIGURE 2.5 AND FIGURE 2.6&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Recurrent Network Fully Connected structure is detailed in Figures 2.5 and 2.6, showing its components." "</data>
  <data key="d5">"rnn structure, diagram"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;RNN STRUCTURE&quot;" target="&quot;LSTM STRUCTURE&quot;">
  <data key="d3">33.0</data>
  <data key="d4">"Both RNN and LSTM structures are used for sequence modeling tasks. LSTMs improve upon RNNs by addressing the vanishing gradient problem through cell states and gates."&lt;SEP&gt;"Both are used for sequential data processing, with LSTM being an advanced form of RNN that addresses the vanishing gradient problem better."&lt;SEP&gt;"LSTM is described as an improvement over RNNs for handling long-term dependencies, showing a clear relationship in sequential data processing." "</data>
  <data key="d5">"neural network structures, dependency management"&lt;SEP&gt;"sequence processing, model improvement"&lt;SEP&gt;&lt;"sequential processing, improved performance"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;LSTM STRUCTURE&quot;" target="&quot;RNN STRUCTURE ILLUSTRATION&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both are used for sequential data processing, with LSTM being an advanced form of RNN that addresses the vanishing gradient problem better."</data>
  <data key="d5">&lt;"sequential processing, improved performance"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;LSTM STRUCTURE&quot;" target="&quot;FIGURE 2.7&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"LSTM Structure is described in Figure 2.7, highlighting its role in managing long-term dependencies." "</data>
  <data key="d5">"lstm structure, dependency management"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;SCALED DOT-PRODUCT ATTENTION AND MULTI-HEAD ATTENTION&quot;">
  <data key="d3">37.0</data>
  <data key="d4">"The Transformer Model uses these attention mechanisms, indicating a strong relationship through their application in the model." "&lt;SEP&gt;"The Transformer architecture uses attention mechanisms like Scaled Dot-Product and Multi-Head to process input sequences without the need for recurrence. These components are crucial in capturing context effectively."&lt;SEP&gt;"The Transformer model uses these attention mechanisms to process sequential data efficiently, enabling parallelization of tasks and handling long-range dependencies effectively."</data>
  <data key="d5">"attention mechanism, efficiency"&lt;SEP&gt;"attention mechanism, model architecture"&lt;SEP&gt;"attention mechanisms, efficient processing"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;POSITIONAL ENCODING&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"Positional Encoding is a key part of the Transformer Model, providing context crucial for its operation."&lt;SEP&gt;"Positional Encoding is used as part of the Transformer Model, contributing to providing contextual information for words within sentences."&lt;SEP&gt;"Positional Encoding is integrated into the Transformer Model as part of the Input Embedding, providing positional context for words in a sentence."</data>
  <data key="d5">"architecture component, context embedding"&lt;SEP&gt;"context provision, architecture component"&lt;SEP&gt;"contextualization, architecture component"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;MULTI-HEAD ATTENTION LAYER&quot;">
  <data key="d3">36.0</data>
  <data key="d4">"The Multi-Head Attention Layer is a key component in the Transformer Model responsible for generating weighted vectors based on different key-value pairs for each word."&lt;SEP&gt;"The Transformer Model contains the Multi-Head Attention Layer, which is essential for understanding the context of words based on their position within the sentence."&lt;SEP&gt;"The Multi-Head Attention Layer is part of the Transformer Model and plays a significant role in understanding word importance."</data>
  <data key="d5">"architecture component, attention mechanism"&lt;SEP&gt;"architecture integration, context understanding"&lt;SEP&gt;"attention mechanism, encoder block"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;FEED-FORWARD NEURAL NETWORK&quot;">
  <data key="d3">31.0</data>
  <data key="d4">"Both the Feed-Forward Neural Network and the Multi-Head Attention Layer are components of the Transformer Model, working together to process input data."&lt;SEP&gt;"The Feed-Forward Neural Network is another crucial component of the Transformer architecture, working alongside the multi-head attention layer to process and transform information."&lt;SEP&gt;"The Feed-Forward Neural Network is another key component of the Transformer Model that processes information through layers."</data>
  <data key="d5">"architecture component, processing module"&lt;SEP&gt;"component interaction, model architecture"&lt;SEP&gt;"information processing, parallel components"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;SEMANTIC SEARCH&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"While not a direct component of the Transformer model, semantic search can leverage its embeddings for improved search quality."</data>
  <data key="d5">"query understanding, vector representations"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;EMBEDDINGS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Transformer Model can create embeddings, which are vector representations used for understanding the underlying meaning of words, sentences, or documents."</data>
  <data key="d5">"embedding creation, semantic understanding"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;MASKED MULTI-HEAD ATTENTION COMPONENT&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Masked Multi-Head Attention Component is part of the Decoder Block within the overall Transformer Model."</data>
  <data key="d5">"architecture component, decoder block"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;TSDAE ARCHITECTURE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both models are advanced methods for sequence processing, suggesting they share goals and applications in natural language tasks." "</data>
  <data key="d5">"sequence modeling, advanced techniques"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL&quot;" target="&quot;FIGURE 2.8 AND FIGURE 9.10&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Transformer Model is introduced in both Figures 2.8 and 9.10, showing its versatility in different contexts." "</data>
  <data key="d5">"transformer model, attention mechanisms"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;SCALED DOT-PRODUCT ATTENTION AND MULTI-HEAD ATTENTION&quot;" target="&quot;FIGURE 2.9&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"These attention mechanisms are detailed in Figure 2.9, indicating their importance in the Transformer model's functionality." "</data>
  <data key="d5">"attention mechanism, transformer"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;BI-ENCODER AND CROSS-ENCODER&quot;" target="&quot;BERT INPUT REPRESENTATION&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Both Bi-Encoders and Cross-Encoders are used in natural language processing to encode pairs of documents. BERT's representations also include tokenization techniques that align well with these frameworks for capturing sentence relationships."</data>
  <data key="d5">"document encoding, contextual understanding"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;BI-ENCODER AND CROSS-ENCODER&quot;" target="&quot;FIGURE 2.11&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"These encoders are introduced in Figure 2.11, indicating their role in comparing pairs of texts or queries." "</data>
  <data key="d5">"encoder models, comparison tasks"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;BERT INPUT REPRESENTATION&quot;" target="&quot;MASKED LANGUAGE MODELING TECHNIQUE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both are integral to BERT's effectiveness; Masked LM trains the model to predict masked words, while its input representation is crucial for these predictions."</data>
  <data key="d5">&lt;"input representations, masked language modeling"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;BERT INPUT REPRESENTATION&quot;" target="&quot;FIGURE 3.1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT's input representation is visualized in Figure 3.1, indicating its importance for handling natural language inputs." "</data>
  <data key="d5">"bert, input representation"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;DATA DISTRIBUTIONS&quot;" target="&quot;MASKED LANGUAGE MODELING&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both concepts are related in the context of training models. Data distributions inform how data is sampled and used during training, while Masked Language Modeling focuses on predicting words based on these distributions."</data>
  <data key="d5">"data processing, model training"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;DATA DISTRIBUTIONS&quot;" target="&quot;FINE-TUNING PHASE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Fine-tuning the BERT model on specific data distributions helps improve its performance on new tasks and domains."</data>
  <data key="d5">"optimization, distribution"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;DATA DISTRIBUTIONS&quot;" target="&quot;FIGURE 3.2&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Data distributions are shown in Figure 3.2, likely representing a dataset analysis or visualization." "</data>
  <data key="d5">"data distribution, dataset analysis"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING&quot;" target="&quot;LEGAL LANGUAGE MODEL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Masked Language Modeling is utilized within the Legal Language Model for specific tasks such as domain adaptation."</data>
  <data key="d5">"technique application, task specialization"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING&quot;" target="&quot;DOMAIN ADAPTATION TECHNIQUES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Masked Language Modeling (MLM) is one of several domain adaptation techniques used in Subsection 3.1.1 to help models perform better on new datasets from different domains."</data>
  <data key="d5">8</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING&quot;" target="&quot;FIGURE 3.3&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Masked Language Modeling is detailed in Figure 3.3, highlighting its role in training models to predict words from context." "</data>
  <data key="d5">"masked language modeling, context prediction"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE&quot;" target="&quot;GENQ&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Both TSDAE and GenQ frameworks deal with sequential data but for different purposes. TSDAE focuses on denoising while GenQ is about generating questions from contexts."</data>
  <data key="d5">"sequential modeling, generative tasks"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;T5 DIAGRAM&quot;" target="&quot;TSDAE ARCHITECTURE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both TSDAE Architecture and T5 diagram are technologies discussed in the text, possibly related through their use or description of a Transformer-based model."</data>
  <data key="d5">"related techniques, illustration"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;T5 DIAGRAM&quot;" target="&quot;MASKED LANGUAGE MODELING (MLM) TRAINING LOSS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"T5 diagram is likely related to MLM Training Loss as both involve transformer-based models, suggesting a relationship through model training and evaluation."</data>
  <data key="d5">&lt;"model training, evaluation"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;T5 DIAGRAM&quot;" target="&quot;FIGURE 3.5&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The T5 diagram is provided in Figure 3.5, indicating its role in text-to-text transformations and summarization tasks." "</data>
  <data key="d5">"t5 model, summarization"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V0&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The model is trained using NLI data generated by GenQ, indicating a dependency on this query generation step for its training process."</data>
  <data key="d5">"query generation, training process"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"This model is used to generate queries in the GenQ stage of GPL technique, highlighting its direct involvement in creating necessary inputs for subsequent steps."</data>
  <data key="d5">"query generation, input creation"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The model is trained using NLI data generated by GenQ, indicating a dependency on this query generation step for its training process."</data>
  <data key="d5">"query generation, training process"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The model is trained using NLI data generated by GenQ, indicating a dependency on this query generation step for its training process."</data>
  <data key="d5">"query generation, training process"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;MLM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"GenQ aims to improve upon previous methods like MLM for semantic search and question-answering scenarios."</data>
  <data key="d5">&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;NILS REIMERS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Ubiquitous Knowledge Processing Lab team, led by Nils Reimers, developed GenQ as part of their research work."</data>
  <data key="d5">&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION IDEOLOGY&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"GenQ and Metadata Knowledge Distillation are both techniques used in the development of models, indicating they might share some underlying principles or methods."</data>
  <data key="d5">&lt;"knowledge transfer, model development"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;UBIQUITOUS KNOWLEDGE PROCESSING LAB TEAM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The GenQ method was developed by the Ubiquitous Knowledge Processing Lab team, highlighting their role in creating unsupervised domain adaptation techniques."</data>
  <data key="d5">"method development, team affiliation"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;T5 MODEL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"GenQ uses the fine-tuned T5 model to generate queries from passages. The T5 model is crucial for generating meaningful and relevant questions."</data>
  <data key="d5">"query generation, model usage"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;FIGURE 3.6&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"GenQ is introduced in Figure 3.6, likely showing a method for generating questions from context." "</data>
  <data key="d5">"genq, question generation"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;UBIQUITOUS KNOWLEDGE PROCESSING LAB&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"GenQ was developed and published by the Ubiquitous Knowledge Processing Lab team in October 2021."</data>
  <data key="d5">"development, publication"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;TEXT-TO-TEXT TRANSFER TRANSFORMER (T5)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"GenQ relies on the T5 model for generating queries."</data>
  <data key="d5">"model dependency, query generation"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;GENQ&quot;" target="&quot;PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE12 MODEL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The GenQ technique was used to generate queries using a pre-trained T5 model."</data>
  <data key="d5">"query generation, model usage"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;ALEX&quot;" target="&quot;FIRST CONTACT&quot;">
  <data key="d3">38.0</data>
  <data key="d4">"Alex acknowledges that they might be making their first contact with an unknown intelligence."&lt;SEP&gt;"Alex leads the team that might be making the First Contact with the unknown intelligence."&lt;SEP&gt;"Alex leads the team that might be making the First Contact with the unknown intelligence."</data>
  <data key="d5">"leadership, exploration"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;ALEX&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"Alex is a member of Project IRIS, contributing to the development of the Semantic Search System."&lt;SEP&gt;"Alex is part of Project IRIS and contributes to developing the Semantic Search System."/&gt;</data>
  <data key="d5">"collaboration, development"&lt;SEP&gt;"team collaboration, project contribution"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;ALEX&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Alex and his team are involved in research and development activities related to machine learning and scientific computing, similar to the contributions of SciPy 1.0 contributors."</data>
  <data key="d5">"collaboration, contribution"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;ALEX&quot;" target="&quot;HUMANITY'S RESPONSE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Alex and his team are the key figures in Humanity's Response to the unknown intelligence."</data>
  <data key="d5">"collective action, cosmic significance"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;ALEX&quot;" target="&quot;LESSE&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"Alex may be involved with or interested in the advancements made by Nuno Cordeiro’s LeSSE system."</data>
  <data key="d5">&lt;"NLP advancement, interest in research"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;ALEX&quot;" target="&quot;PRE-PROCESSING TEXT DATA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Alex is involved in the process of pre-processing text data as part of Project IRIS members."/&gt;</data>
  <data key="d5">"team involvement, process understanding"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;ALEX&quot;" target="&quot;MULTIPLE PROJECT IRIS MEMBERS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Alex interacts with multiple Project IRIS members in the development of the Semantic Search System and related tasks."</data>
  <data key="d5">"team collaboration"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SAM RIVERA&quot;" target="&quot;INTELLIGENCE&quot;">
  <data key="d3">36.0</data>
  <data key="d4">"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence."</data>
  <data key="d5">"communication, learning process"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f&lt;SEP&gt;chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;SAM RIVERA&quot;" target="&quot;DATA PRE-PROCESSING&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Sam Rivera works on data pre-processing tasks as part of the research team."</data>
  <data key="d5">"task division, contribution"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;SAM RIVERA&quot;" target="&quot;ROUGE-2 SCORE&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Sam Rivera's contribution includes summarization methods that can be evaluated with metrics like ROUGE-2 score."</data>
  <data key="d5">"contribution evaluation"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;MODELS V1&quot;" target="&quot;MODELS V0&quot;">
  <data key="d3">13.0</data>
  <data key="d4">"Models V0 and Models V1 are related as they represent different versions of the same model, with V1 being more recent."&lt;SEP&gt;"Models V1 are newer versions of Models V0 used in Search System Evaluation and Discovery metric evaluations, indicating an evolution in technology."</data>
  <data key="d5">"evolution, improvement"&lt;SEP&gt;"version evolution, comparison"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;DISCOVERY METRIC&quot;" target="&quot;PURELY SEMANTIC&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Purely Semantic's performance is evaluated by the Discovery metric, highlighting its strengths in uncovering relevant information at higher ranks."</data>
  <data key="d5">&lt;"performance comparison, effectiveness assessment"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;DISCOVERY METRIC&quot;" target="&quot;LEXICAL + SEMANTIC&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Lexical + Semantic Search System outperforms BM25 in the Discovery metric, especially at higher ranks like Top 1 and Top 10."</data>
  <data key="d5">&lt;"superior performance, effectiveness comparison"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;DISCOVERY METRIC&quot;" target="&quot;SEARCH METRIC&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both metrics evaluate the system’s performance but from different perspectives - one on finding the original document and another on retrieving relevant additional documents."</data>
  <data key="d5">&lt;"performance evaluation, complementary metrics"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;DISCOVERY METRIC&quot;" target="&quot;V0 MODELS&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"V0 models generally perform better than V1 models in the Discovery metric but still outperform by Lexical+Semantic"::&lt;SEP&gt;"V0 models generally perform better than V1 models in the Discovery metric, but Lexical+Semantic still outperforms them significantly."::</data>
  <data key="d5">"model comparison, discovery performance"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;DISCOVERY METRIC&quot;" target="&quot;LEXICAL+SEMANTIC APPROACH&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Lexical+Semantic approach significantly outperforms BM25 and general Semantic Search Systems in the Discovery metric."::</data>
  <data key="d5">"approach comparison, performance superiority"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;DISCOVERY METRIC&quot;" target="&quot;BM25 TECHNIQUE&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"BM25 technique is compared against the Discovery metric, showing its relevance in evaluating the Semantic Search System's performance."</data>
  <data key="d5">"comparison, evaluation method"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;DISCOVERY METRIC&quot;" target="&quot;PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Paraphrase-multilingual-mpnet-base-v2 is another multilingual model used as a baseline for comparing with Legal-BERTimbau in the Discovery metric."</data>
  <data key="d5">"baseline model, performance comparison"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;DISCOVERY METRIC&quot;" target="&quot;COSINE SIMILARITY METRIC&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Cosine similarity metric is used alongside BM25 to evaluate the Discovery metric, showing its relevance in assessing the system’s ability to retrieve relevant documents."</data>
  <data key="d5">"evaluation method, performance testing"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;BERTIMBAU VARIANTS&quot;" target="&quot;LEGAL-BERTIMBAU VARIANTS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Legal-BERTimbau variants are a specific subset or variation of BERTimbau models used in legal contexts."</data>
  <data key="d5">"specialization, subset relationship"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;BERTIMBAU VARIANTS&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERTimbau variants performed better than state-of-the-art multilingual models on the STS task for assin and assin2 datasets."</data>
  <data key="d5">"domain adaptation, performance improvement"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;BERTIMBAU VARIANTS&quot;" target="&quot;T5 MODEL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both BERTimbau and T5 models were explored for generating queries, with T5 being used to generate more varied sentences while maintaining the original meaning."</data>
  <data key="d5">"query generation, semantic search"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;BERTIMBAU VARIANTS&quot;" target="&quot;ELASTICSEARCH&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"ElasticSearch stores sentence embeddings generated by BERTimbau variants for efficient querying and retrieval."&lt;SEP&gt;"ElasticSearch stores sentence embeddings generated by BERTimbau variants for efficient querying and retrieval."::</data>
  <data key="d5">"data storage, performance evaluation"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;BERTIMBAU VARIANTS&quot;" target="&quot;SBERT VARIANTS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SBERT Variants are part of the BERTimbau family and have been adapted for our domain, performing better on some STS tasks."</data>
  <data key="d5">"domain adaptation, performance comparison"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">42.0</data>
  <data key="d4">"The Legal-BERTimbau variants are evaluated for their performance on the STS task, which is important for understanding their effectiveness in the Portuguese legal domain."&lt;SEP&gt;"The Legal-BERTimbau variants are evaluated using the STS task as a metric for their performance on the Portuguese legal context."&lt;SEP&gt;"The Legal-BERTimbau variants are evaluated on the STS task to ensure they perform well in assessing text similarity within the Portuguese legal context."</data>
  <data key="d5">"model evaluation, task relevance"&lt;SEP&gt;"performance evaluation, adaptation"&lt;SEP&gt;"performance evaluation, semantic similarity"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;DOMAIN ADAPTATION&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Domain adaptation is crucial in ensuring that the Legal-BERTimbau variants can effectively handle the Portuguese legal domain."&lt;SEP&gt;"The Legal-BERTimbau variants undergo a domain adaptation stage to ensure they are well-suited for the Portuguese legal context, highlighting the importance of this step in model development."</data>
  <data key="d5">"adaptation effectiveness, critical technique"&lt;SEP&gt;"model adaptation, effectiveness improvement"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;TSDAE-MKD-NLI-STS-V1&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"tsdae-mkd-nli-sts-v1 is part of the Legal-BERTimbau family of language model variants."</data>
  <data key="d5">"family membership, variant"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;MLM-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"mlm-gpl-nli-sts-MetaKD-v0 is a member of the Legal-BERTimbau family of language model variants."</data>
  <data key="d5">"family membership, variant"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;MLM-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"mlm-gpl-nli-sts-MetaKD-v1 is part of the Legal-BERTimbau family of language model variants."</data>
  <data key="d5">"family membership, variant"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;TSDAE-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"tsdae-gpl-nli-sts-MetaKD-v0 is a member of the Legal-BERTimbau family of language model variants."</data>
  <data key="d5">"family membership, variant"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;TSDAE-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"tsdae-gpl-nli-sts-MetaKD-v1 is part of the Legal-BERTimbau family of language model variants."</data>
  <data key="d5">"family membership, variant"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"stjiris/bert-large-portuguese-cased-legal-mlm is a variant of Legal-BERTimbau designed for handling Portuguese legal text."</data>
  <data key="d5">"model variant, domain adaptation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"stjiris/bert-large-portuguese-cased-legal-tsdae is a variant of Legal-BERTimbau designed for handling Portuguese legal text using TSDAE technique."</data>
  <data key="d5">"model variant, domain adaptation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU VARIANTS&quot;" target="&quot;BERTIMBAU LARGE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERTimbau large serves as a baseline comparison to other model variants in the evaluation of Portuguese legal text handling capabilities."</data>
  <data key="d5">"baseline comparison, model performance"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;SUPREMO TRIBUNAL DE JUSTIÇA (STJ)&quot;" target="&quot;INESC-ID LISBOA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"INESC-ID Lisboa is developing an information retrieval system specifically for STJ, indicating collaboration and support from INESC-ID in improving judicial processes."</data>
  <data key="d5">"collaboration, judicial process improvement"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;UKP&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"INESC-ID Lisboa and UKP are related in the context of NLP research; INESC-ID is developing projects like IRIS for UKP."</data>
  <data key="d5">"research collaboration, project development"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;IRIS PROJECT&quot;">
  <data key="d3">39.0</data>
  <data key="d4">"INESC-ID Lisboa developed the IRIS project aimed at court decisions summarization."&lt;SEP&gt;"INESC-ID Lisboa developed the IRIS project."&lt;SEP&gt;"INESC-ID Lisboa is involved in the development of the IRIS project aimed at improving STJ's decision-making process."&lt;SEP&gt;"INESC-ID Lisboa is responsible for developing and contributing to the IRIS project aimed at summarization approaches for court decisions."&lt;SEP&gt;"INESC-ID Lisboa is responsible for developing the IRIS project."&lt;</data>
  <data key="d5">"development, project leadership"&lt;SEP&gt;"project contribution, technology support"&lt;SEP&gt;"project development"&lt;SEP&gt;"project development, collaboration"&lt;SEP&gt;"project development, organization responsibility"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;SEMANTIC SEARCH SYSTEM FOR SUPREMO TRIBUNAL DE JUSTIÇA&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"INESC-ID Lisboa is involved in developing the Semantic Search System to aid judges at STJ."</data>
  <data key="d5">"system development, contribution"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;LEGAL DOCUMENTS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"INESC-ID Lisboa uses legal documents as a primary source of data for developing their IRIS project."</data>
  <data key="d5">"data usage, research basis"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;STATE-OF-THE-ART TECHNIQUES&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"INESC-ID Lisboa aims to incorporate state-of-the-art techniques in their project for advanced summarization and search functionalities."</data>
  <data key="d5">"technological advancement, innovation"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;STJ&quot;">
  <data key="d3">38.0</data>
  <data key="d4">"INESC-ID Lisboa develops summarization approaches specifically for STJ, contributing to their court decision process."&lt;SEP&gt;"INESC-ID Lisboa develops summarization methods specifically tailored for STJ, enhancing its decision-making capabilities."&lt;SEP&gt;"INESC-ID Lisboa works closely with STJ on the IRIS project."&lt;SEP&gt;"STJ collaborates with INESC-ID Lisboa on the IRIS project to improve its decision-making process through better information retrieval and summarization."&lt;SEP&gt;"Project IRIS aims to support STJ through the development of a legal search system."</data>
  <data key="d5">"collaboration, judicial improvement"&lt;SEP&gt;"collaboration, legal support"&lt;SEP&gt;"collaboration, partnership"&lt;SEP&gt;"collaboration, project goals"&lt;SEP&gt;"technological support, judicial enhancement"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"INESC-ID Lisboa developed Project IRIS for STJ."</data>
  <data key="d5">"project development, collaboration"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;LEGAL SEARCH SYSTEM&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Project IRIS aims to develop summarization approaches and improve the legal search system for STJ."</data>
  <data key="d5">"technology innovation, project goals"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;HUGGINGFACE PLATFORM&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"INESC-ID Lisboa published developed models and datasets on HuggingFace, making them publicly accessible."</data>
  <data key="d5">"publication platform"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;INFORMATION RETRIEVAL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Information Retrieval process involves developing summarization approaches for court decisions as part of the IRIS project developed by INESC-ID Lisboa."</data>
  <data key="d5">"project scope"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;MACHINE LEARNING (ML)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"INESC-ID Lisboa likely uses Machine Learning techniques in their development of semantic search and language models."</data>
  <data key="d5">"technique usage"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;INESC-ID LISBOA&quot;" target="&quot;ARTIFICIAL INTELLIGENCE (AI)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Artificial Intelligence plays a crucial role in the development of semantic search and language models by INESC-ID Lisboa."</data>
  <data key="d5">"role significance"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;NLP&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"NLP encompasses the task of Natural Language Inference, which is a specific area within NLP."</data>
  <data key="d5">"NLP subset, task definition"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;OPENAI’S GPT3 TEXT-DAVINCI-003 MODEL API&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The GPT3 API is used for generating sentence pairs which may be part of NLI tasks."</data>
  <data key="d5">"model usage, task relevance"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;INFORMATION RETRIEVAL (IR)&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"NLI is related to IR as both are part of natural language processing and can be used to enhance search capabilities."</data>
  <data key="d5">"NLP techniques, information retrieval"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;STS FINE-TUNING STAGE DESCRIBED IN BERTIMBAU’S PAPER&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The NLI data was used alongside STS data for model training as per the method described in BERTimbau's research paper."</data>
  <data key="d5">"model training, dataset utilization"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;SBERT VARIANTS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The NLI data was used to generate SBERT variants that outperform others not subjected to NLI data during the fine-tuning stage.".</data>
  <data key="d5">"model training, dataset utilization"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;SBERT VARIANTS OUTPERFORM OTHERS THAT DO NOT USE NLI DATA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The performance improvement observed in SBERT variants was due to the inclusion of NLI data during the fine-tuning stage.".</data>
  <data key="d5">"performance, dataset utilization"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;ASSIN AND ASSIN2 DATASETS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The NLI data was also utilized alongside the ASSIN and ASSIN2 datasets during model training to improve overall performance.".</data>
  <data key="d5">"model training, dataset utilization"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;SNLI DATASET&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SNLI dataset was part of the NLI datasets used to train models, particularly those containing entailment labels for sentences.".</data>
  <data key="d5">"model training, dataset utilization"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;NATURAL LANGUAGE INFERENCE (NLI)&quot;" target="&quot;SUPREMO TRIBUNAL DE JUSTIC ¸A&quot;">
  <data key="d3">3.0</data>
  <data key="d4">"NLI is not directly related to STJ, but both involve legal processes."</data>
  <data key="d5">"legal tasks, inference techniques"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;NN&quot;" target="&quot;T5&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both NN and T5 are types of neural networks but used in different contexts; NN is for sequence data, while T5 is for text-to-text transfer tasks."</data>
  <data key="d5">"neural network types, context usage"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;RNN&quot;" target="&quot;TSDAE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both TSDAE and RNN are neural network models used in NLP but serve different purposes; TSDAE deals with denoising auto-encoders, while RNN handles sequence data."</data>
  <data key="d5">"neural network types, task specialization"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;RNN&quot;" target="&quot;ACTIVATION FUNCTIONS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"RNN uses various activation functions to introduce non-linearity and enable processing of sequential data."</data>
  <data key="d5">&lt;"non-linearity, sequence processing"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;RNN&quot;" target="&quot;BACKPROPAGATION THROUGH TIME&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Backpropagation Through Time is a training technique used for RNNs, addressing the issues they face during learning."</data>
  <data key="d5">"training, gradient handling"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;RNN&quot;" target="&quot;GRADIENT CLIPPING&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Gradient clipping is used to improve RNNs by preventing gradients from becoming too large during training, which can help with exploding gradient issues."</data>
  <data key="d5">"gradient management, performance improvement"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;RNN&quot;" target="&quot;LSTM&quot;">
  <data key="d3">42.0</data>
  <data key="d4">"LSTM is a variant of RNN designed to handle long-term dependencies, while RNN has simpler structures like single tanh layers."&lt;SEP&gt;"LSTM is a variant of RNN specifically designed to handle long-term dependencies."&lt;SEP&gt;"LSTM is an advanced form of RNN designed to handle long-term dependencies more effectively, thus extending the capabilities of standard RNNs."</data>
  <data key="d5">"advancement, dependency handling"&lt;SEP&gt;"long-term dependency handling"&lt;SEP&gt;"variant, complexity, handling long-term dependencies"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;RNN&quot;" target="&quot;FEED-FORWARD NEURAL NETWORK&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"An RNN is a variant of feed-forward neural networks designed specifically for handling sequential or time-series data."</data>
  <data key="d5">"sequence processing, neural network architecture"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;RNN&quot;" target="&quot;VANISHING OR EXPLODING GRADIENTS&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Vanishing or Exploding Gradients are issues faced by RNNs during the training process."</data>
  <data key="d5">"training challenge, gradient dynamics"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;REIMERS AND GUREVYCH&quot;">
  <data key="d3">59.0</data>
  <data key="d4">"Reimers and Gurevych demonstrated the benefits of SBERT in comparison to BERT through their research."&lt;SEP&gt;"Reimers and Gurevych demonstrated the efficiency of SBERT in comparison tasks through experiments."&lt;SEP&gt;"Reimers and Gurevych showed that SBERT outperforms BERT in sentence pair comparison tasks."&lt;SEP&gt;"Reimers and Gurevych showed the efficiency of SBERT compared to BERT through a specific task of comparing sentences."</data>
  <data key="d5">"research contribution, model evaluation"&lt;SEP&gt;"research contribution, performance benchmark"&lt;SEP&gt;"research contribution, performance evaluation"&lt;SEP&gt;"research, demonstration"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;SIAMESE ARCHITECTURE&quot;">
  <data key="d3">40.0</data>
  <data key="d4">"The Siamese architecture is a key component of how SBERT processes sentence pairs more efficiently than BERT."&lt;SEP&gt;"The Siamese architecture is a key component of how SBERT processes sentence pairs."&lt;SEP&gt;"The Siamese architecture is used in the design of SBERT to improve its efficiency and effectiveness in semantic similarity tasks."</data>
  <data key="d5">"architecture design, efficiency improvement"&lt;SEP&gt;"architecture design, improvement"&lt;SEP&gt;"architecture design, model composition"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;SOFTMAX LOSS APPROACH&quot;">
  <data key="d3">25.0</data>
  <data key="d4">"The Softmax loss approach is a technique for fine-tuning SBERT on sentence pairs, enhancing its ability to identify relationships between sentences."&lt;SEP&gt;"This method was used to fine-tune SBERT and enhance its performance in semantic similarity tasks."</data>
  <data key="d5">"fine-tuning method, improved performance"&lt;SEP&gt;"fine-tuning, model optimization"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;SUPERVISED LEARNING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SBERT was fine-tuned with the STS benchmark for supervised learning tasks."::</data>
  <data key="d5">"model training, supervised task"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;UNSUPERVISED LEARNING&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Unsupervised learning methods were also used to evaluate SBERT on datasets like SICK-Relatedness and STS benchmark."::&lt;SEP&gt;"Unsupervised learning methods were used with Agirre et al.'s datasets and SICK-Relatedness to evaluate model performance on unsupervised tasks."</data>
  <data key="d5">"evaluation method, unsupervised task"&lt;SEP&gt;"unsupervised training, dataset evaluation"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;OVGU&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"OvGU used Sentence-BERT embeddings in their approach for the third task of COLIEE 2021, focusing on statute law retrieval."</data>
  <data key="d5">"application, task specialization"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;SENTENCE-BERT&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"SBERT is part of the Sentence-BERT family, highlighting their shared lineage or relationship."</data>
  <data key="d5">"family relationship, association"</data>
  <data key="d6">chunk-f66becb00b4d98284bacd25a49e26a3e</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;T5 MODEL&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"SBERT can be fine-tuned using queries generated by T5 models to improve its performance on downstream tasks."|&lt;&gt;"model adaptation, fine-tuning"&lt;SEP&gt;"T5 model is used for generating queries which can then be fine-tuned with SBERT using MNR loss."</data>
  <data key="d5">"query generation, fine-tuning"&lt;SEP&gt;7</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;MNR LOSS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"SBERT is fine-tuned using MNR loss with generated query pairs."|&lt;&gt;"fine-tuning, loss function"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;UKP LAB&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The UKP lab implemented SBERT and is associated with the research behind its development."|&lt;&gt;"research, model implementation"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;STS BENCHMARK (STSB)&quot;">
  <data key="d3">36.0</data>
  <data key="d4">"SBERT was fine-tuned and evaluated using the STS benchmark (STSb), a popular dataset for evaluating supervised Semantic Textual Similarity systems."&lt;SEP&gt;"SBERT was fine-tuned on the STS benchmark dataset, a popular choice for evaluating semantic textual similarity systems."</data>
  <data key="d5">"fine-tuning target, evaluation metric"&lt;SEP&gt;"training, evaluation"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;COSINE SIMILARITY METRIC&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Although SBERT does not require certain pre-processing steps, it still uses cosine similarity for scoring during the search process."</data>
  <data key="d5">"processing techniques, scoring methods"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;STSB (STS BENCHMARK)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SBERT was fine-tuned on the STS benchmark dataset for evaluating supervised semantic textual similarity systems."</data>
  <data key="d5">"model evaluation, benchmarking"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;JNLP&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"JNLP did not use SBERT but instead focused on a self-labeled approach and text chunking for large articles. However, both models belong to the broader category of deep learning approaches in NLP."</data>
  <data key="d5">"model comparison, NLP techniques"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;NIGAM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both SBERT and nigam's approach involve embedding documents' sentences but differ in their specific methods: nigam combined transformer-based and traditional IR techniques with BM25 scores."</data>
  <data key="d5">"embedding techniques, comparison"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;TEAM F1-SCORE PRECISION RECALL MAP&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"SBERT was used in the context of evaluating team performance metrics such as F1-score, Precision, Recall, and MAP. SBERT embeddings were part of the approach that nigam used."&lt;SEP&gt;"The SBERT model is part of the approach that nigam used and its performance is likely evaluated using metrics like F1-score, Precision, Recall, and MAP."</data>
  <data key="d5">&lt;"evaluation metrics, model application"&lt;SEP&gt;&lt;"model evaluation, metric application"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;TASK 3&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SBERT was part of the nigam team’s approach for Task 3 which focused on statute law retrieval using embedding techniques."</data>
  <data key="d5">&lt;"technique application, task focus"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;STS BENCHMARK&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SBERT variants performed better than state-of-the-art multilingual models on the STS task for assin and assin2 datasets."</data>
  <data key="d5">"model performance, evaluation metrics"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;STSB MULTI MT&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"SBERT's performance did not outperform multilingual models on stsb multi mt dataset."</data>
  <data key="d5">"performance comparison, multilingual models"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;EXAMINATION COMMITTEE: PROF. MARIA LUÍSA TORRES RIBEIRO MARQUES DA SILVA COHEUR, PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOSÉ LUÍS BRINQUETE BORBINHA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"SBERT is a technology that the examination committee will consider in their evaluation of the thesis."</data>
  <data key="d5">&lt;technological advancement, evaluation"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;SBERT&quot;" target="&quot;SBERT -NLI&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"SBERT -NLI is an enhanced version of SBERT that was pre-trained on NLI datasets before fine-tuning, indicating an iterative approach to improve model performance."</data>
  <data key="d5">"pre-training, fine-tuning"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SNLI&quot;" target="&quot;STS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both STS and SNLI are part of the broader field of NLP but focus on different aspects; STS measures textual similarity, while SNLI is about natural language inference."</data>
  <data key="d5">"NLP task types, conceptual overlap"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;T5&quot;" target="&quot;GENQ [39]&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"T5 is utilized by the GenQ method to generate queries from original passages, highlighting their interdependence in question-answering scenarios."</data>
  <data key="d5">"methodology, usage"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;T5&quot;" target="&quot;MS MARCO&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The T5 model was fine-tuned on MS MARCO data for two epochs."</data>
  <data key="d5">"training dataset, fine-tuning"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;TF-IDF&quot;" target="&quot;OKAPI BM25&quot;">
  <data key="d3">36.0</data>
  <data key="d4">"Okapi BM25 incorporates Term Frequency-Inverse Document Frequency (TF-IDF) for its calculations and is a widely used algorithm in information retrieval."&lt;SEP&gt;"Okapi BM25 uses TF-IDF for its calculations and is based on term frequency-inverse document frequency principles."</data>
  <data key="d5">"algorithm component, performance"&lt;SEP&gt;&lt;"algorithm component, performance"&lt;SEP&gt;&lt;"algorithm component, theoretical foundation"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;TSDAE&quot;" target="&quot;NILS REIMERS&quot;">
  <data key="d3">41.0</data>
  <data key="d4">"Nils Reimers published TSDAE on April 14th, 2021, as part of his research work."&lt;SEP&gt;"TSDAE was first published by Nils Reimers, indicating his contribution to the method."&lt;SEP&gt;"TSDAE was published by Nils Reimers on April 14th, 2021, making him the author of this method."&lt;SEP&gt;"TSDAE was first published by Nils Reimers on April 14th, 2021."</data>
  <data key="d5">"authorship, publication date"&lt;SEP&gt;"publication, authorship"&lt;SEP&gt;&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;TSDAE&quot;" target="&quot;W ANG, K.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"K. W ANG contributed to the development of TSDAE."</data>
  <data key="d5">&lt;"contribution, paper development"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;TSDAE&quot;" target="&quot;R EIMERS , N.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"N. R EIMERS is involved in developing TSDAE."</data>
  <data key="d5">&lt;"contribution, paper development"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;TSDAE&quot;" target="&quot;GUREVYCH , I.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"I. GUREVYCH contributed to the TSDAE research."</data>
  <data key="d5">&lt;"contribution, paper development"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;TSDAE&quot;" target="&quot;MLM&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Both TSDAE and MLM are used in training some models, showing their interrelation."&lt;SEP&gt;"TSDAE is an advancement over the previously used Masked Language Model (MLM) for language modeling tasks."&lt;SEP&gt;"TSDAE outperforms MLM in sentence embedding tasks by introducing noise through word deletion or swapping rather than using masked tokens like MLM does."</data>
  <data key="d5">"performance comparison, unsupervised learning"&lt;SEP&gt;&lt;"training techniques, overlap"&gt;&lt;SEP&gt;&lt;relationship_keywords&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae&lt;SEP&gt;chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;TSDAE&quot;" target="&quot;TRANSFORMER&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"TSDAE uses modified Transformer architecture."</data>
  <data key="d5">"architecture, transformer"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;TSDAE&quot;" target="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE)&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both methods use Transformer architecture but TSDAE is specifically a denoising autoencoder with different techniques."</data>
  <data key="d5">"architectural similarity, specific technique"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;TSDAE&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"This variant was specifically created using TSDAE to handle unsupervised sentence embedding tasks on legal documents."</data>
  <data key="d5">"unsupervised learning, sentence embedding"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;STJ&quot;">
  <data key="d3">36.0</data>
  <data key="d4">"STJ benefits from the advancements in Information Retrieval (IR) and text summarization provided by the IRIS project to make well-informed decisions."&lt;SEP&gt;"STJ is involved in the IRIS project through its association with INESC-ID Lisboa."&lt;&lt;SEP&gt;"The IRIS project aims to support STJ in their decision-making processes by providing advanced summarization techniques."&lt;SEP&gt;"The IRIS project is aimed at assisting STJ in summarizing court decisions."&lt;SEP&gt;"STJ is involved in the IRIS project by being a target user of its outcomes, particularly for the court decision process."</data>
  <data key="d5">"collaboration, legal institution involvement"&lt;SEP&gt;"judicial support, decision-making aid"&lt;SEP&gt;"support, legal aid"&lt;SEP&gt;"supporting legal institutions, summarization"&lt;SEP&gt;"user involvement"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;HUGGINGFACE PLATFORM&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The HuggingFace platform is used to publish datasets and models developed from the IRIS project."&lt;&lt;SEP&gt;"The datasets and language models developed through the IRIS project are published on HuggingFace, making them publicly accessible."</data>
  <data key="d5">"public sharing, model availability"&lt;SEP&gt;"publication, platform usage"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;NATURAL LANGUAGE (NL)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Natural Language plays a key role in the system's ability to access information based on search queries."&lt;&lt;SEP&gt;"The IRIS project uses Natural Language for querying and information retrieval."</data>
  <data key="d5">"language processing, query formulation"&lt;SEP&gt;"system resilience, language processing"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;SEMANTIC TEXTUAL SIMILARITY (STS)&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"STS models are developed as part of the IRIS project for use in semantic search systems."&lt;&lt;SEP&gt;"The IRIS project utilizes Semantic Textual Similarity models in its language models."</data>
  <data key="d5">"model development, key technology"&lt;SEP&gt;"model development, technology integration"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;DATASET&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The IRIS project involves developing datasets from legal documents."&lt;</data>
  <data key="d5">"project objectives, data generation"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;ARTIFICIAL INTELLIGENCE (AI)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Artificial Intelligence plays an important role in the IRIS project, enhancing its capabilities."</data>
  <data key="d5">"technology integration, project enhancement"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;MACHINE LEARNING (ML)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Machine Learning is utilized by the IRIS project to develop and train models."</data>
  <data key="d5">"model development, technology application"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;LEXICON APPROACHES FOR INFORMATION RETRIEVAL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Lexical approaches contribute to the IRIS project by improving information retrieval methods."</data>
  <data key="d5">"methodology enhancement, information processing"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;NEURAL NETWORKS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Neural Networks are used in the IRIS project for advanced semantic information retrieval."</data>
  <data key="d5">"technological advancement, data analysis"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;IRIS PROJECT&quot;" target="&quot;SEMANTIC STRATEGIES&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The IRIS project aims to develop summarization approaches using semantic strategies for STJ."</data>
  <data key="d5">"project goals"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">40.0</data>
  <data key="d4">"Project IRIS aims to create a representation that is helpful in the court decision process at STJ."&lt;SEP&gt;"Project IRIS developed a Semantic Search System for STJ, indicating its application in specific legal domains or contexts."&lt;SEP&gt;"Project IRIS provided support and development of a Semantic Search System prototype for STJ, indicating its practical application in a real-world setting."|&lt;&gt;"application scope, legal domain"&lt;SEP&gt;"STJ is a part of the larger legal context where Project IRIS's Semantic Search System can have an impact. STJ is targeted by Legal-BERTimbau for summarization tasks."</data>
  <data key="d5">"application, domain relevance"&lt;SEP&gt;"court support, innovation"&lt;SEP&gt;8&lt;SEP&gt;&lt;"development collaboration, targeted deployment"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288&lt;SEP&gt;chunk-fc9187ad3fd00c96d11f9934e91f7051&lt;SEP&gt;chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;LEXICAL TECHNIQUES&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"STJ faces challenges when using lexical techniques due to the complexity of information retrieval needed for judicial decisions."</data>
  <data key="d5">"information retrieval, limitations"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;SEMANTIC TECHNIQUES&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"STJ can benefit from semantic techniques for broader, more accurate information retrieval in judicial decision-making processes."</data>
  <data key="d5">"advanced information retrieval, enhanced decision-making"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;CONSISTENCY IN APPLYING THE LAW&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"STJ's decisions should ensure consistency in applying the law, preventing injustices and maintaining a stable legal framework."</data>
  <data key="d5">"legal integrity, judicial stability"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;INESC-ID LISBOA IRIS PROJECT&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"INESC-ID Lisboa IRIS Project aims to assist STJ in improving its decision-making process through technological advancements."</data>
  <data key="d5">"technology transfer, judicial aid"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;SUPREME COURT OF JUSTICE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"STJ is synonymous with Supreme Court of Justice, both referring to Portugal's highest judiciary court."</data>
  <data key="d5">"synonymous terms"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;LEGAL SEARCH SYSTEM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The need for a Legal Search System arises from the complex task of formulating decisions at STJ due to vast volumes of information."</data>
  <data key="d5">"information retrieval, decision-making"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;INFORMATION RETRIEVAL (IR)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Information Retrieval is crucial for the court decision process at STJ."</data>
  <data key="d5">"court support, technology"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;STJ&quot;" target="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"The Lexical + Semantic Search System is intended for use by STJ (Superior Tribunal de Justiça) in supporting legal decision-making processes."</data>
  <data key="d5">"legal support, system application"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;INFORMATION RETRIEVAL&quot;" target="&quot;OKAPI BM25&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Okapi BM25 is primarily used to enhance the effectiveness of information retrieval systems by ranking documents based on relevance to user queries."</data>
  <data key="d5">&lt;"relevance ranking, algorithm application"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;INFORMATION RETRIEVAL&quot;" target="&quot;STATE-OF-THE-ART TECHNIQUES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Information Retrieval is enhanced by state-of-the-art techniques like TF-IDF and Okapi BM25."</data>
  <data key="d5">&lt;"technological improvement, core methodology"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;INFORMATION RETRIEVAL&quot;" target="&quot;ABSTRACT&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Abstract outlines key methodologies and findings related to Information Retrieval in the development of the Semantic Search System."&lt;SEP&gt;"The Abstract outlines key methodologies and findings related to Information Retrieval in the development of the Semantic Search System." "</data>
  <data key="d5">"methodology summary, research focus"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;INFORMATION RETRIEVAL&quot;" target="&quot;EXAMINATION COMMITTEE: PROF. MARIA LUÍSA TORRES RIBEIRO MARQUES DA SILVA COHEUR, PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOSÉ LUÍS BRINQUETE BORBINHA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Information Retrieval is a concept that the examination committee will evaluate as part of the thesis."</data>
  <data key="d5">&lt;evaluation, information processing"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;INFORMATION RETRIEVAL&quot;" target="&quot;CHAPTER 2&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Chapter 2 provides essential elements for understanding Information Retrieval, particularly state-of-the-art techniques and the developed solution."</data>
  <data key="d5">"content correlation"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;MACHINE LEARNING (ML)&quot;" target="&quot;ARTIFICIAL INTELLIGENCE (AI)&quot;">
  <data key="d3">48.0</data>
  <data key="d4">"AI and ML are interconnected fields with ML often used as an alternative approach within AI."&lt;SEP&gt;"Artificial Intelligence has led to the development of Machine Learning as an alternative solution in various fields."&lt;SEP&gt;"Machine Learning is a part of Artificial Intelligence focusing on specific algorithms like TF-IDF and BM25."</data>
  <data key="d5">"innovation, evolution"&lt;SEP&gt;"subset, focus area"&lt;SEP&gt;"technology interconnection"&lt;SEP&gt;&lt;"subset, focus area"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;MACHINE LEARNING (ML)&quot;" target="&quot;STATE-OF-THE-ART TECHNIQUES&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Machine Learning is also considered a state-of-the-art technique used in various fields as an alternative to traditional solutions."</data>
  <data key="d5">&lt;"advanced technology, alternative approach"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;STATE-OF-THE-ART TECHNIQUES&quot;" target="&quot;ARTIFICIAL INTELLIGENCE (AI)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Artificial Intelligence is a core aspect that includes state-of-the-art techniques in modern society."</data>
  <data key="d5">&lt;"technological advancement, focus area"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;HUGGINGFACE PLATFORM&quot;" target="&quot;DATASETS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Datasets were published on the HuggingFace platform for public use."</data>
  <data key="d5">"data sharing, open access"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;HUGGINGFACE PLATFORM&quot;" target="&quot;LANGUAGE MODELS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Language models developed by the thesis work are available on the HuggingFace platform."</data>
  <data key="d5">"model hosting, technology dissemination"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;OKAPI BM25&quot;" target="&quot;ELASTICSEARCH&quot;">
  <data key="d3">28.0</data>
  <data key="d4">"Elasticsearch supports Okapi BM25 as one of its ranking functions for information retrieval tasks."&lt;SEP&gt;"Elasticsearch uses Okapi BM25 as one of its ranking functions for information retrieval tasks and search engine optimization."</data>
  <data key="d5">&lt;"engine functionality, support function"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF)&quot;" target="&quot;OKAPI BM25 (BM25)&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"BM25 incorporates TF-IDF to rank documents based on query relevance. The algorithm uses factors like term frequency and inverse document frequency for evaluation."</data>
  <data key="d5">"algorithm, ranking function"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;LEXICAL SEARCH&quot;" target="&quot;JACCARD SIMILARITY MEASURE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Jaccard Similarity measure can be used to evaluate the shared information in lexical searches, providing a numerical basis for comparison."</data>
  <data key="d5">"evaluation metric, similarity measure"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;LEXICAL SEARCH&quot;" target="&quot;WORD2VEC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Word2Vec provides an alternative approach to lexical searches by focusing on contextual meaning rather than literal matches of words."</data>
  <data key="d5">"contextual search, word embedding technique"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;LEXICAL SEARCH&quot;" target="&quot;INFORMATION RETRIEVAL (IR)&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Traditional Lexical search is a core technique used in Information Retrieval for searching documents based on exact query words. Okapi BM25 is an extension of this approach."</data>
  <data key="d5">"search techniques, ranking"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;LEXICAL SEARCH&quot;" target="&quot;2.1 LEXICAL APPROACHES FOR INFORMATION RETRIEVAL&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"This section describes the traditional method of searching documents based on exact query words, which is the essence of lexical search."</data>
  <data key="d5">"traditional approach, information retrieval"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">42.0</data>
  <data key="d4">"ElasticSearch is a predefined technology used in Project IRIS for implementing the search system."&lt;SEP&gt;"Elasticsearch is a pre-defined constraint in Project IRIS, used as the engine for the solution."&lt;SEP&gt;"ElasticSearch was a pre-defined constraint since the Project IRIS solution was based on the ElasticSearch engine. To utilize it effectively, Project IRIS members needed to understand its use with embeddings."</data>
  <data key="d5">"constraint, implementation basis"&lt;SEP&gt;"predefined constraint, integration"&lt;SEP&gt;"technology implementation, predefined constraint"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;LEGAL-BERTIMBAU MODEL&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"The Legal-BERTimbau model works with Elasticsearch to contribute to the overall semantic search effectiveness."&lt;SEP&gt;"The Legal-BERTimbau model works within the Elasticsearch environment to contribute to the semantic search system."</data>
  <data key="d5">"integration, semantic search"&lt;SEP&gt;"model integration, contribution"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;SUPREMO TRIBUNAL DE JUSTICA&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Indexed legal documents from the Supremo Tribunal de Justica are stored and managed within Elasticsearch."</data>
  <data key="d5">"document storage, judicial body"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The model uses ElasticSearch to store embeddings of negative passages during the negative mining stage."</data>
  <data key="d5">"embedding storage, negative passage retrieval"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The model uses ElasticSearch to store embeddings of negative passages during the negative mining stage."</data>
  <data key="d5">"embedding storage, negative passage retrieval"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The model uses ElasticSearch to store embeddings of negative passages during the negative mining stage."</data>
  <data key="d5">"embedding storage, negative passage retrieval"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;COSINE SIMILARITY&quot;">
  <data key="d3">22.0</data>
  <data key="d4">"Cosine Similarity can be used with Elasticsearch but requires specific configurations and mappings for implementation."&lt;SEP&gt;"Elasticsearch uses Cosine Similarity as one of the supported search functions to utilize embeddings effectively in semantic search systems. This is crucial for Project IRIS’s Legal-BERTimbau model implementation."</data>
  <data key="d5">"advanced search feature, configuration"&lt;SEP&gt;"functionality, embedding support"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;BI-ENCODER&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"The Bi-Encoder creates embeddings that are stored in Elasticsearch for searching using cosine similarity function."&lt;SEP&gt;"The Bi-Encoder generates embeddings that are stored in Elasticsearch indices for efficient search and retrieval through cosine similarity." "</data>
  <data key="d5">"embedding creation, scalability, fast retrieval"&lt;SEP&gt;"embedding storage, search efficiency"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;PURELY SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Purely Semantic Search System uses Elasticsearch to retrieve relevant search results through cosine similarity evaluations."</data>
  <data key="d5">"embedding retrieval, semantic capabilities"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;SBERT VARIANTS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"ElasticSearch stores sentence embeddings generated by SBERT variants for querying purposes, supporting the evaluation process through efficient retrieval of relevant documents or sentences." "</data>
  <data key="d5">"storage and retrieval, performance support"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;SEMANTIC SEARCH SYSTEM 43&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Semantic Search System 43 might use ElasticSearch as part of its functionalities, though this is not explicitly stated."&lt;</data>
  <data key="d5">"implementation detail, potential usage"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;JO ˜ÃO RODRIGUES ET AL.&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"Although not directly related, Jo ˜ão Rodrigues et al.'s work might integrate Elasticsearch into their system due to pre-defined constraints."</data>
  <data key="d5">"integration potential, technology choice"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;ECLI-INDEXER6&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The ecli-indexer6 tool is used to collect and index legal documents into Elasticsearch for the semantic search system development in Project IRIS."</data>
  <data key="d5">"data collection, indexing"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;NODE CLUSTER&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Node clusters are single instances of Elasticsearch that form part of its architecture and contribute to the scalability and resilience of the system."</data>
  <data key="d5">"architecture component, scalability"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;SHARD&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Shards are subsets of index documents used in Elasticsearch to maintain performance and handle failures through replication."</data>
  <data key="d5">"data partitioning, redundancy"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;HYBRID SEARCH SYSTEMS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Hybrid search systems utilize Elasticsearch for searching similar sentences based on query embeddings."</data>
  <data key="d5">"searching functionality, scalability"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;GPT3.5&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Elasticsearch database provides embeddings that are used by GPT3.5 to generate user-friendly responses based on retrieved results."</data>
  <data key="d5">"embedding retrieval, response generation"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;SYSTEM ID: 9EWRY OMBF LERWH5 W2G&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The System ID refers to a database or system that uses Elasticsearch for embedding and searching."</data>
  <data key="d5">"database, search technology"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;ELASTICSEARCH&quot;" target="&quot;EVALUATION ARCHITECTURE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The Evaluation Architecture involves storing sentence embeddings in ElasticSearch, which is crucial for efficient querying and retrieval."::</data>
  <data key="d5">"evaluation workflow, data storage"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH&quot;" target="&quot;BI-ENCODER&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The concept of semantic search includes Bi-Encoders which generate embeddings and compare them to understand query-text relationships."</data>
  <data key="d5">"embedding generation, comparison"</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH&quot;" target="&quot;CROSS-ENCODER&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Cross-Encoders are part of the methods under semantic search for comparing sentence similarities without generating separate embeddings first."</data>
  <data key="d5">"embedding comparison, similarity measure"</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH&quot;" target="&quot;LEXICAL APPROACHES&quot;">
  <data key="d3">34.0</data>
  <data key="d4">"Lexical approaches are limited compared to semantic search, as they only match words directly without understanding the context or user intent."&lt;SEP&gt;"Lexical approaches struggle with synonyms and exact matches, whereas semantic search aims to overcome these issues by understanding context and meaning."</data>
  <data key="d5">"approach comparison, user intention recognition"&lt;SEP&gt;"context understanding, word matching"&lt;SEP&gt;&lt;"approach comparison, user intention recognition"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH&quot;" target="&quot;SYMMETRIC SEMANTIC SEARCH&quot;">
  <data key="d3">2.0</data>
  <data key="d4">"Symmetric Semantic Search is a part of Semantic Search that involves matching queries with text. It's described in detail within the broader context of Semantic Search."</data>
  <data key="d5">&lt;"search quality improvement, query understanding, embedding comparison"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH&quot;" target="&quot;ASYMMETRIC SEMANTIC SEARCH&quot;">
  <data key="d3">2.0</data>
  <data key="d4">"Asymmetric Semantic Search is also a part of Semantic Search but focuses on providing answers to short queries by returning more substantial paragraphs. This contrasts with Symmetric Semantic Search."</data>
  <data key="d5">&lt;"query answering, paragraph retrieval, embedding comparison"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;SEMANTIC SEARCH&quot;" target="&quot;COSINE SIMILARITY&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Cosine Similarity is used in Semantic Search to compare embeddings of queries and sentences."</data>
  <data key="d5">"embedding comparison, similarity measure"</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;JACCARD SIMILARITY MEASURE&quot;" target="&quot;LEXICAL SEARCHES&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The Jaccard Similarity measure and lexical searches both deal with evaluating shared information, but they differ in how they approach the problem - one through sets and the other through word presence."</data>
  <data key="d5">"evaluation methods, comparison"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;JACCARD SIMILARITY MEASURE&quot;" target="&quot;COSINE SIMILARITY&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Both Jaccard Similarity and Cosine Similarity are measures of similarity but applied in different contexts: Jaccard for set intersection, Cosine for vector angles in multidimensional space."</data>
  <data key="d5">"set theory, vector spaces"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;JACCARD SIMILARITY MEASURE&quot;" target="&quot;SIMILARITY SEARCHES&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Jaccard Similarity measure is a traditional method of similarity search, but it can fail when important documents lack exact matches or contain synonyms."</data>
  <data key="d5">"document retrieval, word match failure"&lt;SEP&gt;&lt;"document retrieval, word match failure"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;WORD EMBEDDING&quot;" target="&quot;DISTRIBUTIONAL HYPOTHESIS (DH)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Word embeddings are based on the Distributional Hypothesis that words appearing in similar contexts have similar meanings, which is fundamental to word embedding techniques."</data>
  <data key="d5">"context-based meaning, hypothesis validation"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;WORD EMBEDDING&quot;" target="&quot;COSINE SIMILARITY&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Cosine Similarity can be used to measure the similarity between word embeddings in a multidimensional space."</data>
  <data key="d5">"similarity metric, vector analysis"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;COSINE SIMILARITY&quot;" target="&quot;PURELY SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"Cosine Similarity is a key metric used by the Purely Semantic Search System for evaluating embedding similarities." "&lt;SEP&gt;"The Purely Semantic Search System uses cosine similarity as a key method for evaluating search results."</data>
  <data key="d5">"core similarity measure, embedding analysis"&lt;SEP&gt;"search evaluation metric"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;COSINE SIMILARITY&quot;" target="&quot;LEXICAL-FIRST SEARCH SYSTEM&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"Cosine Similarity is used in the Lexical-First Search System after BM25 ranking to evaluate the final top results." "&lt;SEP&gt;"The Lexical-First Search System uses cosine similarity as part of its evaluation method after BM25 ranking."</data>
  <data key="d5">"final evaluation, semantic refinement"&lt;SEP&gt;"search system architecture, evaluation metric"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;COSINE SIMILARITY&quot;" target="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">30.0</data>
  <data key="d4">"Cosine Similarity is a key measure in the Lexical + Semantic Search System for determining document similarity."&lt;SEP&gt;"Cosine Similarity is part of the score calculation in the Lexical + Semantic Search System alongside BM25." "&lt;SEP&gt;"In the Lexical + Semantic Search System, cosine similarity is used to rank and verify results after BM25 filtering."</data>
  <data key="d5">"combined scoring, dual method validation"&lt;SEP&gt;"measure, similarity determination"&lt;SEP&gt;"verification, ranking metric"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;COSINE SIMILARITY&quot;" target="&quot;SBERT VARIANTS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Cosine similarity is used to evaluate the different sentences generated by SBERT variants during performance testing." "</data>
  <data key="d5">"performance evaluation, similarity measure"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;COSINE SIMILARITY&quot;" target="&quot;LEXICAL APPROACHES&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Lexical approaches have drawbacks similar to those encountered with cosine similarity, such as missing important passages or failing to recognize synonyms."</data>
  <data key="d5">"retrieval failure, context understanding"&lt;SEP&gt;&lt;"retrieval failure, context understanding"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;TOMAS MIKOLOV&quot;">
  <data key="d3">63.0</data>
  <data key="d4">"Tomas Mikolov is the inventor of Word2Vec, directly contributing to its development and methodology."&lt;SEP&gt;"Tomas Mikolov is the lead developer of Word2Vec, a neural network implementation for word embeddings."&lt;SEP&gt;"Tomas Mikolov led the team that proposed and developed Word2Vec in 2013."&lt;SEP&gt;"Tomas Mikolov led the development of Word2Vec, which was proposed by his Google team."</data>
  <data key="d5">"development leader, creator"&lt;SEP&gt;"invention, authorship"&lt;SEP&gt;"leadership, development"&lt;SEP&gt;&lt;"leadership, development"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07&lt;SEP&gt;chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;STANFORD UNIVERSITY&quot;">
  <data key="d3">64.0</data>
  <data key="d4">"Stanford University developed Word2Vec, a neural network model for creating word vectors through unsupervised training."&lt;SEP&gt;"Stanford University developed Word2Vec, which uses neural networks for natural language processing tasks."&lt;SEP&gt;"Stanford University is the institution where Word2Vec was developed."&lt;SEP&gt;"Stanford University is the institution where Word2Vec was developed, highlighting its academic and research contribution."</data>
  <data key="d5">"development, innovation"&lt;SEP&gt;"research development, unsupervised learning"&lt;SEP&gt;"research institution, development origin"&lt;SEP&gt;"research origin, development"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;COMMON BAG OF WORDS (CBOW)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The CBOW method is part of the Word2Vec implementation, used to predict words based on their context."</data>
  <data key="d5">"model component, neural network usage"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;SKIP-GRAM&quot;">
  <data key="d3">32.0</data>
  <data key="d4">"Both CBOW and Skip-Gram are methods used in implementing Word2Vec for word prediction, though they approach the problem differently."&lt;SEP&gt;"Skip-Gram is another method within Word2Vec that generates context from a single input word, illustrating its methodology diversity."</data>
  <data key="d5">"methodology, alternative approaches"&lt;SEP&gt;"methodology, component"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;CO-OCCURRENCE MATRIX&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The co-occurrence matrix is a key component in the implementation of Word2Vec for deriving word relationships."</data>
  <data key="d5">"key component, unsupervised learning"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;EMBEDDING SPACES&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Embedding spaces provide a framework for representing words as vectors, enabling techniques like Word2Vec to perform arithmetic operations on word embeddings."</data>
  <data key="d5">"representation method, vector space"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;TOMAS MIKOLOV ET AL.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Tomas Mikolov et al. are the creators of Word2Vec."</data>
  <data key="d5">"development, innovation"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;CBOW MODEL ARCHITECTURE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The CBOW model is a component of Word2Vec that uses context words to predict a target word."</data>
  <data key="d5">"model component, functionality"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;SKIP-GRAM MODEL ARCHITECTURE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Skip-Gram model is another component of Word2Vec that predicts context given a target word."</data>
  <data key="d5">"model component, functionality"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;UNSUPERVISED TRAINING&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Word2Vec uses unsupervised training to create its models without labeled data."</data>
  <data key="d5">&lt;"training method, model creation"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;WORD2VEC&quot;" target="&quot;CBOW&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"CBOW is a method within Word2Vec that uses context words to predict the target word, showing its integral part of the model."</data>
  <data key="d5">"methodology, component"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;PUBLIC GARDEN&quot;" target="&quot;STANFORD UNIVERSITY&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"The example sentence references a walk through the Public Garden, which is associated with Stanford University."</data>
  <data key="d5">"location association, context"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;TOMAS MIKOLOV&quot;" target="&quot;GLOBAL VECTORS FOR WORD REPRESENTATION (GLOVE)&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both Word2Vec and GloVe are implementations by Tomas Mikolov's team to solve the downsides of lexical approaches."</data>
  <data key="d5">&lt;"team development, similar goals"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;STANFORD UNIVERSITY&quot;" target="&quot;TOMAS MIKOLOV ET AL.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Stanford University is associated with Tomas Mikolov et al., who developed Word2Vec."</data>
  <data key="d5">"association, research collaboration"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;SKIP-GRAM&quot;" target="&quot;CBOW&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both CBOW and Skip-Gram are methods within Word2Vec, highlighting their complementary roles in the model's architecture."</data>
  <data key="d5">"model architecture, methodology"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;CO-OCCURRENCE MATRIX&quot;" target="&quot;HIGH-DIMENSIONAL SPACE&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The co-occurrence matrix is used to transform words into high-dimensional vector spaces for GloVe's model."</data>
  <data key="d5">"statistical modeling, vectorization"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;CO-OCCURRENCE MATRIX&quot;" target="&quot;GLOVE'S LOSS FUNCTION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The co-occurrence matrix is used as a basis in defining GloVe's loss function, which optimizes word embeddings based on these probabilities."</data>
  <data key="d5">"optimization, basis of model"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;CO-OCCURRENCE MATRIX&quot;" target="&quot;CORPUS OF TEXT&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The co-occurrence matrix is derived from a corpus of text, which provides the statistical basis for GloVe's model."</data>
  <data key="d5">"statistical basis, corpus input"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;CO-OCCURRENCE MATRIX&quot;" target="&quot;SOLID&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Solid has a higher probability of co-occurring with 'ice' than 'gas', indicating a stronger semantic relationship."</data>
  <data key="d5">"semantic similarity, frequency comparison"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;CO-OCCURRENCE MATRIX&quot;" target="&quot;WATER&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Water shows a close relationship to both 'ice' and 'steam', suggesting shared concepts or contexts in the GloVe model."</data>
  <data key="d5">"contextual relation, shared semantics"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;CO-OCCURRENCE MATRIX&quot;" target="&quot;FASHION&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"Fashion serves as an unrelated comparison, showing minimal co-occurrence with words like 'ice' or 'steam'. "</data>
  <data key="d5">"unrelated concept, minimal frequency"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;WORD ICE&quot;" target="&quot;WORD STEAM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The probabilities of 'ice' and 'steam' are compared in the context of their co-occurrences with other words such as 'solid', 'gas', 'water', and 'fashion'. This indicates a relationship based on statistical analysis."</data>
  <data key="d5">"statistical comparison, contextual relationships"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;SOLID&quot;" target="&quot;ICE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The word 'ice' co-occurs more frequently with the word 'solid' than it does with 'gas', indicating a stronger semantic connection."</data>
  <data key="d5">"semantic relation, co-occurrence"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;SOLID&quot;" target="&quot;TARGETS WORD ICE AND STEAM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Solid has a higher probability of co-occurring with 'ice' than 'gas', indicating a stronger semantic relationship."</data>
  <data key="d5">"semantic similarity, frequency comparison"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;GAS&quot;" target="&quot;STEAM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The word 'steam' co-occurs less frequently with the word 'gas' compared to ice's relation with solid, showing contrasting co-occurrence patterns."</data>
  <data key="d5">"contrast, semantic relation"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;WATER&quot;" target="&quot;ICE STEAM&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both words 'ice' and 'steam' are related to water but not strongly with fashion, indicating thematic connections in broader context."</data>
  <data key="d5">"thematic connection, contextual relation"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;WATER&quot;" target="&quot;TARGETS WORD ICE AND STEAM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Water shows a close relationship to both 'ice' and 'steam', suggesting shared concepts or contexts in the GloVe model."</data>
  <data key="d5">"contextual relation, shared semantics"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;HIDDEN VECTOR A&lt;T&gt;&quot;" target="&quot;ACTIVATION FUNCTION G1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The activation function \(g1\) affects the hidden vector \(a_t\) in RNN, determining its state."</data>
  <data key="d5">&lt;"state influence, activation function"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;LOGISTIC FUNCTION (SIGMOID)&quot;" target="&quot;HIDDEN VECTOR&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The logistic function or Sigmoid is an activation function used in RNNs to process the hidden vectors."</data>
  <data key="d5">"activation function, neural network component"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;BACKPROPAGATION THROUGH TIME&quot;" target="&quot;LSTM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both Backpropagation Through Time and LSTMs are techniques aimed at improving the performance of neural networks in sequence tasks."</data>
  <data key="d5">"sequence learning, technique enhancement"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;GRADIENT CLIPPING&quot;" target="&quot;BACKPROPAGATION THROUGH TIME (BPTT)&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Gradient clipping is used in BPTT to prevent exploding gradients that can occur during training of RNNs."&lt;SEP&gt;"Gradient clipping is used in conjunction with BPTT to manage exploding gradients."</data>
  <data key="d5">"gradient management, training stability"&lt;SEP&gt;"gradient management, training technique"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;CELL STATE&quot;" target="&quot;FORGET GATE&quot;">
  <data key="d3">31.0</data>
  <data key="d4">"The Forget Gate decides how much information from the Cell State should be kept or discarded."&lt;SEP&gt;"The Forget Gate decides which information from the previous cell state (Ct-1) to keep or discard using a sigmoid function." &lt;|"gate mechanism, state update"&lt;SEP&gt;"The forget gate decides which information to keep or discard from the cell state, influencing its content over time."</data>
  <data key="d5">"gate control, state management"&lt;SEP&gt;8&lt;SEP&gt;&lt;"cell state management, decision-making"</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f&lt;SEP&gt;chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;CELL STATE&quot;" target="&quot;INPUT GATE&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The input gate updates the cell state with new information, based on the decision made by the forget gate."</data>
  <data key="d5">"information flow, state update"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;CELL STATE&quot;" target="&quot;OUTPUT GATE&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The output gate determines what part of the cell state is used as the final prediction or output, relying on the current state."</data>
  <data key="d5">"state visibility, output determination"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357</data>
</edge>
<edge source="&quot;FORGET GATE&quot;" target="&quot;INPUT GATE&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"Both Forget and Input Gates are components of LSTM networks, both dealing with cell state updates but in different ways."&lt;SEP&gt;"Both gates work together to manage and update the cell state in an LSTM."&lt;SEP&gt;"Both the forget and input gates are components in LSTM designed to manage information flow through the cell state."</data>
  <data key="d5">"gate interaction, state management"&lt;SEP&gt;"information flow management, decision making"&lt;SEP&gt;&lt;"gating mechanisms, processing flow"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;FORGET GATE&quot;" target="&quot;OUTPUT GATE&quot;">
  <data key="d3">28.0</data>
  <data key="d4">"All three gates (Forget, Input, Output) work together to manage the flow and output of information in LSTM models."&lt;SEP&gt;"The forget gate's decision on what information to keep or discard affects the output gate's visibility of the cell state."&lt;SEP&gt;"The forget and output gates both play roles in managing which parts of the cell state are retained or outputted."</data>
  <data key="d5">"cell state management, information selection"&lt;SEP&gt;"state visibility control, decision coordination"&lt;SEP&gt;&lt;"information management, coordination"</data>
  <data key="d6">chunk-3595353bc78e782128ef8148dfaf1357&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;FORGET GATE&quot;" target="&quot;SIGMOID FUNCTION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Forget Gate uses the sigmoid function to make decisions on which information stays or not in the cell state."</data>
  <data key="d5">"gate mechanism, decision making"</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;INPUT GATE&quot;" target="&quot;TANH FUNCTION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Input Gate works with the tanh layer to store new information in the cell state using a combination of sigmoid and tanh functions." &lt;|"gate mechanism, storage"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;INPUT GATE&quot;" target="&quot;˜CT&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Input Gate determines what new values will be updated into the cell state (˜Ct), which is then transformed by a tanh layer." &lt;|"gate determination, candidate vector"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;INPUT GATE&quot;" target="&quot;TANH&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The Input Gate works with the tanh layer to create a new candidate vector for updating the cell state."</data>
  <data key="d5">&lt;"information processing, value normalization"</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;OUTPUT GATE&quot;" target="&quot;TANH FUNCTION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Output Gate combines with the tanh layer to output information passed from the previous cell through a sigmoid function."</data>
  <data key="d5">"gate mechanism, output"</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;OUTPUT GATE&quot;" target="&quot;HIDDEN VECTOR&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Output Gate and Hidden Vector work together to produce the final hidden state that is transmitted to the next cell."&lt;SEP&gt;"The Output Gate decides what information will be outputted and combined with a tanh transformation to form the hidden vector (ht)." &lt;|"gate mechanism, output definition"</data>
  <data key="d5">8&lt;SEP&gt;&lt;"state transformation, sequence continuation"</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;MULTI-HEAD ATTENTION LAYER&quot;" target="&quot;QUERY&quot;">
  <data key="d3">22.0</data>
  <data key="d4">"The Multi-Head Attention Layer uses queries to understand and assign weights to different parts of a sentence based on their context."&lt;SEP&gt;"The query is an input vector used by the Multi-Head Attention Layer to determine which key-value pairs are relevant and important for generating context vectors."</data>
  <data key="d5">"input vector, attention mechanism"&lt;SEP&gt;"query processing, attention mechanism"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;MULTI-HEAD ATTENTION LAYER&quot;" target="&quot;OUTPUT&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The output of the Multi-Head Attention Layer is a set of context vectors derived from weighted sums of key-value pairs, providing rich contextual information."</data>
  <data key="d5">"context vector generation, weighted sum"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;MULTI-HEAD ATTENTION LAYER&quot;" target="&quot;SELF-ATTENTION MECHANISM&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Each layer of the encoder contains a Multi-Head Attention Layer that implements part of the self-attention mechanism to handle long-range dependencies." &lt;|"mechanism implementation, dependency handling"&lt;SEP&gt;"The Multi-Head Attention Layer is a sub-layer that implements part of the self-attention mechanism in each encoder layer for handling long-range dependencies." &lt;|"mechanism implementation"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;MULTI-HEAD ATTENTION LAYER&quot;" target="&quot;SENTENCE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Multi-Head Attention Layer processes sentences by understanding the context and importance of each word in relation to others within the same sentence."</data>
  <data key="d5">"sentence processing, context analysis"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;MULTI-HEAD ATTENTION LAYER&quot;" target="&quot;FEED-FORWARD NEURAL NETWORK&quot;">
  <data key="d3">13.0</data>
  <data key="d4">"Both Multi-Head Attention Layer and Feed-Forward Neural Network are components of the Transformer Model, processing input data through different mechanisms."&lt;SEP&gt;"Both components are used in Transformer models to process input sequences but serve different roles in the architecture."</data>
  <data key="d5">"parallel processing, model architecture"&lt;SEP&gt;&lt;"attention and feed-forward mechanisms, multi-task processing"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663&lt;SEP&gt;chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;MULTI-HEAD ATTENTION LAYER&quot;" target="&quot;ATTENTION FUNCTION&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Multi-Head Attention Layer uses an attention function to assign weights and generate vector representations for words."</data>
  <data key="d5">"mechanism, weight assignment"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;MULTI-HEAD ATTENTION LAYER&quot;" target="&quot;POSITIONAL ENCODING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Positional Encoding influences how Multi-Head Attention Layer processes words in a sentence by providing positional context, enhancing the attention mechanism's effectiveness."</data>
  <data key="d5">"contextual influence, vector space representation"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;POSITIONAL ENCODING&quot;" target="&quot;SINE AND COSINE FUNCTIONS&quot;">
  <data key="d3">31.0</data>
  <data key="d4">"Sine and Cosine Functions are used to generate Positional Encoding vectors, providing positional information for words in a sentence."&lt;SEP&gt;"Sine and Cosine functions form a part of Positional Encoding, used to represent positional context in sentences."&lt;SEP&gt;"Sine and Cosine functions are used in the Positional Encoding process to create context vectors."</data>
  <data key="d5">"function application, context provision"&lt;SEP&gt;"implementation detail, mathematical technique"&lt;SEP&gt;"vector generation, positional information"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;POSITIONAL ENCODING&quot;" target="&quot;INPUT SEQUENCE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The input sequence is used to generate Positional Encoding vectors, which provide positional context to the model."</data>
  <data key="d5">"sequence processing, contextualization"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;POSITIONAL ENCODING&quot;" target="&quot;HIDDEN VECTOR&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Positional Encoding provides context that is incorporated into the Hidden Vector to improve model understanding of sequence order."</data>
  <data key="d5">&lt;"contextual information, vector augmentation"</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;FEED-FORWARD NEURAL NETWORK&quot;" target="&quot;DIMENSIONALITY OUTPUT&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Feed-Forward Neural Network produces outputs with 512 dimensions, crucial for processing and encoding sequence information in the Transformer model." &lt;|"output dimensionality, processing"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;FEED-FORWARD NEURAL NETWORK&quot;" target="&quot;EMBEDDINGS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Feed-Forward Neural Network processes embeddings generated by other layers to further refine or transform them for better accuracy and understanding."</data>
  <data key="d5">"embedding refinement, neural network processing"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;FEED-FORWARD NEURAL NETWORK&quot;" target="&quot;PROJECTION MATRICES WQ I, WK I, WV I&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Projection Matrices are used in the Feed-Forward Neural Network to transform input data into a different representation space."</data>
  <data key="d5">"data transformation, projection"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;MASKED MULTI-HEAD ATTENTION&quot;" target="&quot;DECODER BLOCK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Masked Multi-Head Attention is used specifically in the decoder block to ensure predictions are based on past information only by masking future positions."</data>
  <data key="d5">"attention mechanism, decoder component"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;MASKED MULTI-HEAD ATTENTION&quot;" target="&quot;ATTENTION SCORES&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Attention Scores are calculated during the Masked Multi-Head Attention process to ensure that predictions only consider past information in the input sequence."</data>
  <data key="d5">"prediction context, attention mechanism"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;QUERY&quot;" target="&quot;TERM FREQUENCY ALGORITHM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Queries are used in the Term Frequency algorithm to determine relevance based on term frequency in documents."</data>
  <data key="d5">"query, relevance evaluation"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;KEY&quot;" target="&quot;VALUE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Key and Value vectors are generated from the same set of words but serve different purposes: keys help in determining relevance while values store importance information."</data>
  <data key="d5">"relevance determination, value storage"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;EIGENVECTOR CENTRALITY&quot;" target="&quot;CONNECTIVITY MATRIX&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Eigenvector centrality influences how the connectivity matrix assigns weights to edges in a sentence's graph representation."&lt;SEP&gt;"Eigenvector centrality measures can be applied to the connectivity matrix based on sentence similarity for identifying important nodes within a graph of sentences."</data>
  <data key="d5">"application, importance measure"&lt;SEP&gt;"importance weighting, relevance"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;EIGENVECTOR CENTRALITY&quot;" target="&quot;GRAPH REPRESENTATION OF SENTENCES&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The concept of eigenvector centrality can be applied to the graph representation of sentences where nodes represent importance based on their connections."</data>
  <data key="d5">"graph theory, sentence importance"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING (MLM)&quot;" target="&quot;NEXT SENTENCE PREDICTION (NSP)&quot;">
  <data key="d3">38.0</data>
  <data key="d4">"Both MLM and NSP are techniques used in the BERT training process to enhance its language understanding capabilities."&lt;SEP&gt;"Both Masked Language Modeling and Next Sentence Prediction are techniques used in BERT's pre-training phase."&lt;SEP&gt;"Both Masked Language Modeling and Next Sentence Prediction are techniques used in BERT's pre-training phase." "&lt;SEP&gt;"Both Masked Language Modeling and Next Sentence Prediction are techniques used in BERT's pre-training phase to improve language understanding."</data>
  <data key="d5">"language modeling, comprehensive training"&lt;SEP&gt;"pre-training, techniques"&lt;SEP&gt;"techniques, training phase"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING (MLM)&quot;" target="&quot;BOOKSCORPUS (800M WORDS)&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"BooksCorpus is one of the datasets used in Masked Language Modeling." "</data>
  <data key="d5">"dataset, technique"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING (MLM)&quot;" target="&quot;ENGLISH WIKIPEDIA (2500M WORDS)&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"English Wikipedia is used in Masked Language Modeling." "</data>
  <data key="d5">"dataset, technique"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING (MLM)&quot;" target="&quot;FINE-TUNING PHASE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"During fine-tuning, MLM can be used as part of the training process to adapt the model for different tasks and domains."</data>
  <data key="d5">"training, adaptation"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING (MLM)&quot;" target="&quot;DOMAIN ADAPTATION&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Domain Adaptation can utilize Masked Language Modeling as one technique to re-train models on new datasets for adapting them to different domains." &lt;|"adaptation methods, re-training"</data>
  <data key="d5">6</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;MASKED LANGUAGE MODELING (MLM)&quot;" target="&quot;LEARNING RATE OF 10^-5&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The learning rate was set specifically for the MLM task, impacting its effectiveness on technical language and jargon."</data>
  <data key="d5">"training approach, parameter control"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;TOKEN EMBEDDING LAYER&quot;" target="&quot;INPUT EMBEDDINGS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Token Embedding Layer forms part of the input embeddings along with Segmentation and Position Embedding Layers, contributing to the overall representation of words in BERT."</data>
  <data key="d5">"input processing, embedding composition"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;TOKEN EMBEDDING LAYER&quot;" target="&quot;TRANSFORMER BLOCKS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Transformer blocks include the Token Embedding Layer, which assigns values to each word based on vocabulary IDs."</data>
  <data key="d5">"architecture components, value assignment"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;SEGMENTATION EMBEDDING LAYER&quot;" target="&quot;TRANSFORMER BLOCKS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Transformer blocks incorporate the Segmentation Embedding Layer, which distinguishes between sentence A and B words."</data>
  <data key="d5">"architecture components, context distinction"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;POSITION EMBEDDING LAYER&quot;" target="&quot;TRANSFORMER BLOCKS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Transformer blocks include the Position Embedding Layer, which indicates the position of each word in a sentence."</data>
  <data key="d5">"architecture components, sequence information"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;ADAM OPTIMIZER&quot;" target="&quot;FINE-TUNING&quot;">
  <data key="d3">43.0</data>
  <data key="d4">"Adam Optimizer is used during the fine-tuning phase of BERT to train the model." "&lt;SEP&gt;"Adam Optimizer is utilized in BERT's fine-tuning process."&lt;SEP&gt;"Adam Optimizer is utilized in BERT's fine-tuning process." "&lt;SEP&gt;"Fine-tuning uses the Adam Optimizer for training models after adding an extra layer to the BERT architecture."&lt;SEP&gt;"The Adam Optimizer is used during the fine-tuning phase to train the entire network effectively after pre-training has prepared the model."</data>
  <data key="d5">"optimization, training enhancement"&lt;SEP&gt;"training, optimization"&lt;SEP&gt;6</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95&lt;SEP&gt;chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;ADAM OPTIMIZER&quot;" target="&quot;FINE-TUNING PHASE&quot;">
  <data key="d3">31.0</data>
  <data key="d4">"The Adam Optimizer is crucial during the fine-tuning phase to adjust model weights effectively."&lt;SEP&gt;"The Adam Optimizer is used during the fine-tuning phase to optimize the weights of the neural network."&lt;SEP&gt;"The Adam Optimizer is used during the fine-tuning phase of BERT training to optimize the entire network for a few epochs."</data>
  <data key="d5">"optimization, adjustment"&lt;SEP&gt;"optimization, fine-tuning"&lt;SEP&gt;"training, optimization"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1&lt;SEP&gt;chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;ADAM OPTIMIZER&quot;" target="&quot;DOMAIN ADAPTATION TECHNIQUES&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Domain Adaptation Techniques may sometimes use Adam Optimizer for training purposes, as it is a common optimizer in deep learning models."</data>
  <data key="d5">"optimization, retraining"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;UNIDIRECTIONAL TRAINING&quot;" target="&quot;BIDIRECTIONAL TRAINING&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"Bidirectional training contrasts with unidirectional training by allowing each word to be predicted based on its full context, reducing bias."&lt;SEP&gt;"Unidirectional training is compared against bidirectional training to highlight their differences." "</data>
  <data key="d5">"context awareness, reduction in bias"&lt;SEP&gt;"context consideration, technique comparison"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;UNIDIRECTIONAL TRAINING&quot;" target="&quot;BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS (BERT)&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"BERT differs fundamentally from unidirectional training by processing text bidirectionally to improve context understanding."</data>
  <data key="d5">"training method, model architecture"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;BIDIRECTIONAL TRAINING&quot;" target="&quot;UNSUPERVISED TRAINING&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Bidirectional training addresses issues with unsupervised training where words might be trivially predicted based on their context from only one direction."</data>
  <data key="d5">"context handling, bias reduction"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;TRANSFORMER BLOCKS&quot;" target="&quot;DOMAIN ADAPTATION (DA)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Transformer blocks can be re-trained using domain adaptation techniques for different data distributions."</data>
  <data key="d5">"adaptation, re-training"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;FINE-TUNING PHASE&quot;" target="&quot;PRE-TRAINING PHASE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The pre-training phase prepares BERT to handle a wide range of language understanding tasks, which are further refined during the fine-tuning phase for specific applications."</data>
  <data key="d5">"preparation, specialization"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;BERT BASE&quot;" target="&quot;DOMAIN ADAPTATION (DA)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERT BASE can be adapted using domain adaptation techniques for different domains."</data>
  <data key="d5">"model size, adaptability"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;BERT BASE&quot;" target="&quot;ORIGINAL PAPER AUTHORS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The original paper authors provided specifications for BERT BASE, including its parameters and total parameters."</data>
  <data key="d5">"specification, model size"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;BERT BASE&quot;" target="&quot;FINE-TUNING&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Fine-tuning involves adding an additional layer after BERT BASE for training, indicating a specific phase of BERT BASE's model development."&lt;SEP&gt;"Fine-tuning involves adding an additional layer after BERT BASE for training, indicating a specific phase of BERT BASE's model development." &lt;|"model evolution, training phases"&lt;SEP&gt;"Fine-tuning involves retraining the BERT BASE model for a few epochs."</data>
  <data key="d5">"model evolution, training phases"&lt;SEP&gt;8&lt;SEP&gt;&lt;"retraining process"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;BERT BASE&quot;" target="&quot;DOMAIN ADAPTATION&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"BERT BASE model size is relevant to discussions on Domain Adaptation as it represents a specific configuration within the BERT architecture."</data>
  <data key="d5">5</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;BERT LARGE&quot;" target="&quot;DOMAIN ADAPTATION (DA)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The BERT LARGE model can also be adapted using domain adaptation techniques for different data distributions."</data>
  <data key="d5">"model size, adaptability"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;BERT LARGE&quot;" target="&quot;FINE-TUNING&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Fine-tuning also applies to BERT LARGE, though the context does not specify parameters explicitly."&lt;SEP&gt;"Fine-tuning also applies to the larger BERT LARGE model, showing that it is a general process applicable across different sizes of BERT models."&lt;SEP&gt;"Fine-tuning also applies to the larger BERT LARGE model, showing that it is a general process applicable across different sizes of BERT models." &lt;|"training flexibility, scalability"</data>
  <data key="d5">"training flexibility, scalability"&lt;SEP&gt;7&lt;SEP&gt;&lt;"generalization"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION (DA)&quot;" target="&quot;UNLABELED DATA (UNSUPERVISED LEARNING TASKS)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Domain Adaptation techniques use unsupervised learning tasks to retrain models on different domains without labeled data."</data>
  <data key="d5">"adaptation, training"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION (DA)&quot;" target="&quot;MLM&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"MLM can be used as part of DA techniques for BERT models to adapt their performance across domains."</data>
  <data key="d5">&lt;"adaptation technique"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION (DA)&quot;" target="&quot;3.1.1 DOMAIN ADAPTATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"3.1.1 Domain Adaptation refers to the same technique as Domain Adaptation (DA)."</data>
  <data key="d5">&lt;"same concept"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;TESTING DATASET&quot;" target="&quot;DGSI - INDEXER -STJ&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"dgsi - indexer -STJ created the Testing dataset to assess model performance."</data>
  <data key="d5">"performance evaluation, testing"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;TESTING DATASET&quot;" target="&quot;TRAINING DATASET&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"Both datasets are part of a larger process to develop and validate models, with Testing dataset providing an independent assessment."&lt;SEP&gt;"The datasets are part of a larger corpus, with different splits to ensure model generalization."</data>
  <data key="d5">"corpus split, validation"&lt;SEP&gt;"model validation, data splitting"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE)&quot;" target="&quot;FINE-TUNING&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Fine-tuning and Transformer-based Sequential Denoising Auto-Encoder both involve re-training models on new datasets from different domains but use different methods."</data>
  <data key="d5">"retraining, domain adaptation techniques"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE)&quot;" target="&quot;DOMAIN ADAPTATION&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Domain Adaptation can utilize Transformer-based Sequential Denoising Auto-Encoder as another method for adapting models to different domains." &lt;|"adaptation methods, denoising techniques"</data>
  <data key="d5">6</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;MLM&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"This variant was specifically trained using MLM for handling legal jargon and technical language."</data>
  <data key="d5">"training task, technical language"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;CONTROL&quot;" target="&quot;INTELLIGENCE&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"The concept of Control is challenged by the Intelligence that writes its own rules."</data>
  <data key="d5">"power dynamics, autonomy"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f&lt;SEP&gt;chunk-afd8ce7e1a7d61d34be9bdded6cff755&lt;SEP&gt;chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;T5 MODEL&quot;" target="&quot;GPT3 MODEL PROVIDED BY OPEN AI&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"T5 model generated less biased queries but still relied on some keywords from the summary, indicating a shared challenge in fully anonymizing content."</data>
  <data key="d5">&lt;relationship_description&gt;</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;T5 MODEL&quot;" target="&quot;EVALUATION ARCHITECTURE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The T5 model is integrated into the evaluation architecture to generate queries with reduced bias but still relies on some summary keywords, indicating its role in the overall process."</data>
  <data key="d5">&lt;relationship_description&gt;</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;T5 MODEL&quot;" target="&quot;CROSS-ENCODER&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Cross-Encoder receives triplets from the Pseudo Labeling step to compare positive and negative passages generated by T5 models."|&lt;&gt;"model comparison, passage similarity"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;T5 MODEL&quot;" target="&quot;GENQ APPROACH&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"T5 model uses the GenQ approach to generate queries which are then used for fine-tuning."|&lt;&gt;"query generation, fine-tuning"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;T5 MODEL&quot;" target="&quot;GPT3 MODEL PROVIDED BY OPENAI&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both T5 and GPT3 were used to generate meaningful queries from summaries, with T5 providing a more balanced approach without exact keyword repetition." "&lt;SEP&gt;"Both T5 and GPT3 were used to generate queries from summaries, with T5 providing a more balanced approach in generating meaningful queries without exact keyword repetition."</data>
  <data key="d5">"model selection, query generation"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;T5 MODEL&quot;" target="&quot;LEXRANK SUMMARIZATION TECHNIQUE&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"T5 Model replaced LexRank in the second phase of query generation, aiming to reduce bias and improve query diversity."</data>
  <data key="d5">"process evolution, semantic search enhancement"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;BERT MODEL&quot;" target="&quot;SENTENCE EMBEDDING GENERATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERT model is used in generating sentence embeddings through mean pooling of all tokens present in a sentence."|&lt;&gt;"embedding generation, token processing"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;SIAMESE ARCHITECTURE&quot;" target="&quot;SENTENCE-BERT&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"SBERT utilizes the siamese architecture for generating sentence embeddings."|&lt;&gt;"architecture usage, embedding generation"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;SIAMESE ARCHITECTURE&quot;" target="&quot;NLI&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Siamese architecture is used in NLI tasks to compare premise and hypothesis sentence embeddings."</data>
  <data key="d5">"architecture application, task relevance"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;MARGIN MEAN SQUARED ERROR LOSS&quot;" target="&quot;TRIPLETS (POSITIVE PASSAGE, NEGATIVE PASSAGE AND MARGIN SCORE)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Margin Mean Squared Error Loss was applied during the final training step using created triplets."</data>
  <data key="d5">"training process, loss function application"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;SNLI (STANFORD NATURAL LANGUAGE INFERENCE)&quot;" target="&quot;ASSIN AND ASSIN2 DATASETS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both datasets provide entailment labels similar to SNLI, indicating a shared purpose in natural language processing research."</data>
  <data key="d5">"dataset comparison, task similarity"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;SICK-RELATEDNESS DATASET&quot;" target="&quot;SBERT -NLI&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"SBERT -NLI could be fine-tuned using the SICK-Relatedness dataset to improve its performance in handling sentence-relatedness tasks."</data>
  <data key="d5">"fine-tuning, relatedness evaluation"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SICK-RELATEDNESS DATASET&quot;" target="&quot;STS BENCHMARK (STSB)&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The SICK-Relatedness dataset is one of the resources used for evaluating models on STS benchmark tasks, contributing to model performance."</data>
  <data key="d5">"benchmark diversity, resource integration"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SUPERVISED LEARNING&quot;" target="&quot;STS BENCHMARK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The STS benchmark is a popular dataset used for supervised learning tasks in evaluating models like BERT and SBERT."::</data>
  <data key="d5">"dataset use, evaluation criterion"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SUPERVISED LEARNING&quot;" target="&quot;STSBENCHMARK (STSB)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"STSBenchmark (STSb) was used for supervised learning, training SBERT on labeled data."</data>
  <data key="d5">"training dataset, supervised approach"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SUPERVISED LEARNING&quot;" target="&quot;STSB (STS BENCHMARK)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The STS benchmark was used for fine-tuning models in supervised learning scenarios."</data>
  <data key="d5">"supervised learning, model validation"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;UNSUPERVISED LEARNING&quot;" target="&quot;STS BENCHMARK&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"STSB was also trained using the STS benchmark for unsupervised learning evaluation."::</data>
  <data key="d5">"evaluation method, training dataset"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;UNSUPERVISED LEARNING&quot;" target="&quot;SNLI AND MULTI-GENRE NLI&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The researchers used SNLI and Multi-Genre NLI datasets for unsupervised learning tasks."</data>
  <data key="d5">"unsupervised training, diverse data sources"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;SBERT -STSB-BASE&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"The SBERT -STSb-base model is trained on the STSb dataset, indicating its use as a benchmark for evaluation and training."&lt;SEP&gt;"The STSb dataset is a benchmark for SBERT -STSb-base models, providing context for their performance measures in evaluations." "</data>
  <data key="d5">"evaluation, benchmark"&lt;SEP&gt;&lt;"training, benchmarking"&gt;</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;BERT -STSB-LARGE&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"The BERT -STSb-large model is also trained on the STSb dataset, highlighting its role in sentence embeddings for evaluating models against this standard."&lt;SEP&gt;"The STSb dataset is relevant to BERT -STSb-large as it is used in evaluations of this model's performance on sentence embedding tasks." "</data>
  <data key="d5">"benchmark, evaluation"&lt;SEP&gt;&lt;"training, benchmarking"&gt;</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;INFERSENT - GLOVE&quot;">
  <data key="d3">22.0</data>
  <data key="d4">"The InferSent - GloVe model was evaluated on the STSb dataset, indicating its use as a benchmark for performance evaluation."&lt;SEP&gt;"The STSb dataset is used to evaluate InferSent - GloVe, providing context for their performance measures."</data>
  <data key="d5">"evaluation, benchmark"&lt;SEP&gt;&lt;"benchmarking, evaluation"&gt;</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;UNIVERSAL SENTENCE ENCODER&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"The STSb dataset serves as a benchmark for models like Universal Sentence Encoder when assessing their effectiveness in similarity judgments between sentences."&lt;SEP&gt;"The Universal Sentence Encoder also performed evaluations on the STSb dataset, highlighting its effectiveness in sentence embeddings tasks."</data>
  <data key="d5">"evaluation, benchmark"&lt;SEP&gt;&lt;"performance, evaluation"&gt;</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;AVG. GLOVE EMBEDDINGS&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"The STSb dataset is used to evaluate models like avg. GloVe embeddings, providing context for their performance measures."&lt;SEP&gt;"The STSb dataset is used to evaluate the performance of avg. GloVe embeddings."</data>
  <data key="d5">"evaluation, benchmark"&lt;SEP&gt;&lt;"evaluation, benchmarking"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;SBERT -NLI-BASE&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"The STSb dataset is relevant to SBERT -NLI-base as it is used in evaluations of this model's performance on sentence embedding tasks."&lt;SEP&gt;"The STSb dataset is used by SBERT - NLI-base for fine-tuning and evaluation."</data>
  <data key="d5">"benchmark, evaluation"&lt;SEP&gt;&lt;"fine-tuning, benchmarking"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;BERT CLS-VECTOR&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"STSb dataset serves as a benchmark for models like BERT CLS-vector when assessing their effectiveness in similarity judgments between sentences."</data>
  <data key="d5">"evaluation, benchmark"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;BERT -STSB-BASE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The STSb dataset is relevant to BERT -STSb-base as it is used in evaluations of this model's performance on sentence embedding tasks." "</data>
  <data key="d5">"benchmark, evaluation"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;SROBERTA-STSB-BASE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The STSb dataset serves as a benchmark for SRoBERTa-STSb-base when assessing its effectiveness in similarity judgments between sentences." "</data>
  <data key="d5">"evaluation, benchmark"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;SBERT -STSB-LARGE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The STSb dataset is a benchmark for SBERT -STSb-large models, providing context for their performance measures in evaluations." "</data>
  <data key="d5">"evaluation, benchmark"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;SROBERTA-STSB-LARGE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The STSb dataset serves as a benchmark for SRoBERTa-STSb-large when assessing its effectiveness in similarity judgments between sentences." "</data>
  <data key="d5">"evaluation, benchmark"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;STSB DATASET&quot;" target="&quot;SBERT -NLI-LARGE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"SBERT - NLI-large utilizes the STSb dataset for training and performance assessment."</data>
  <data key="d5">&lt;"training, benchmarking"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;BERT -STSB-BASE&quot;" target="&quot;STSB&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"BERT -STSb-base uses STSb for fine-tuning on base size models, enhancing its capabilities in complex NLP tasks."</data>
  <data key="d5">"evaluation, fine-tuning"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;SBERT -STSB-BASE&quot;" target="&quot;STSB EVALUATION&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"SBERT -STSb-base performs well on the STS benchmark test set, indicating its effectiveness in tasks like sentence similarity and paraphrase identification."</data>
  <data key="d5">&lt;"performance, evaluation"&gt;</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;SBERT -STSB-BASE&quot;" target="&quot;STSB&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"STSb is the primary evaluation dataset used to assess SBERT -STSb-base's performance on sentence embedding tasks."</data>
  <data key="d5">"evaluation, fine-tuning"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;SBERT -STSB-BASE&quot;" target="&quot;SBERT -NLI-BASE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both SBERT -NLI-base and SBERT -STSb-base are variants of SBERT fine-tuned on the STS benchmark dataset."</data>
  <data key="d5">&lt;"benchmark, variant"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;SROBERTA-STSB-BASE&quot;" target="&quot;STSB&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SRoBERTa-STSb-base is a RoBERTa model fine-tuned on the STS benchmark dataset for base size models and complex NLP tasks."</data>
  <data key="d5">"evaluation, fine-tuning"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;BERT -STSB-LARGE&quot;" target="&quot;SBERT -STSB-LARGE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SBERT -STSb-large is an enhanced version of BERT -STSb-large, both trained on larger datasets for sentence similarity tasks."</data>
  <data key="d5">&lt;"enhancement, model evolution"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;INFERSENT - GLOVE&quot;" target="&quot;SBERT -NLI-STSB-BASE&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both InferSent and SBERT models were evaluated using various training datasets, indicating a comparative analysis of different model architectures on the same benchmarks."</data>
  <data key="d5">&lt;"model comparison, evaluation"&gt;</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;SBERT -NLI-STSB-BASE&quot;" target="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Multilingual Knowledge Distillation (MKD) technique was introduced to address limitations in monolingual models like SBERT, aiming to improve their performance across multiple languages."</data>
  <data key="d5">&lt;"technique improvement, multilingual training"&gt;</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;" target="&quot;NEIL REIMERS&quot;">
  <data key="d3">52.0</data>
  <data key="d4">"Neil Reimers developed Multilingual Knowledge Distillation, which was used in this work."&lt;SEP&gt;"Reimers is the developer of MKD, which is central to the work described."</data>
  <data key="d5">"development, creator"&lt;SEP&gt;"technique development, knowledge transfer"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;" target="&quot;TEACHER MODEL M&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"MKD relies on a teacher model that produces vectors for both source and target sentences."</data>
  <data key="d5">"teacher-student, embedding learning"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;" target="&quot;STUDENT MODEL ˆM&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The student model learns from the teacher model by producing embeddings close to those of the teacher model during the MKD process."</data>
  <data key="d5">"embedding similarity, error backpropagation"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;" target="&quot;PARAPHRASE-MULTILINGUAL-MPNET-BASE (768 DIMENSIONS)&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both the MKD process and Paraphrase-multilingual-mpnet-base are related to creating multilingual embedding models."</data>
  <data key="d5">"model creation, multilingualism"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;" target="&quot;LEAD RESEARCHER FROM THE TEAM THAT PUBLISHED SBERT&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The lead researcher from the team that published SBERT is responsible for introducing MKD."</data>
  <data key="d5">"innovation, leadership"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;" target="&quot;TEACHER MODEL M AND STUDENT MODEL ˆM&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both teacher and student models are key components in the MKD process."</data>
  <data key="d5">"model interaction, learning process"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;" target="&quot;MEAN-SQUARED LOSS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Mean-squared loss is a critical part of the optimization in the MKD process."</data>
  <data key="d5">"optimization, error minimization"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION (MKD)&quot;" target="&quot;SENTENCE-TRANSFORMERS/STSB-ROBERTA-LARGE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The MKD technique was applied to train sentence-transformers/stsb-roberta-large as the teacher model for Portuguese language tasks."</data>
  <data key="d5">"technique application, knowledge transfer"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;SBERT EVALUATION&quot;" target="&quot;LEAD RESEARCHER FROM THE TEAM THAT PUBLISHED SBERT&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The lead researcher's work involves evaluating models like SBERT using benchmarks such as the STS test set."</data>
  <data key="d5">"evaluation, research"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;NIGAM [27] &quot;" target="&quot;JNLP &quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both JNLP and nigam used pre-trained models and developed unique approaches for tasks in COLIEE 2021, indicating a competitive relationship."</data>
  <data key="d5">"approach comparison, competition"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;JNLP&quot; &quot;" target="&quot;NIGAM [27]&quot; &quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both JNLP and nigam developed unique approaches for tasks in COLIEE 2021, indicating a competitive relationship."</data>
  <data key="d5">"approach comparison, competition"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;MI-YOUNG KIM ET AL.&quot; &quot;" target="&quot;UNIVERSITY OF ALBERTA&quot; &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Mi-Young Kim et al. discussed the University of Alberta's participation in the COLIEE 2021 competition, focusing on their involvement with deep learning techniques."</data>
  <data key="d5">"research collaboration, participation discussion"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;TEAM F1-SCORE PRECISION RECALL MAP&quot; &quot;" target="&quot;COLIEE 2021 - TASK 3 RESULTS&quot; &quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The COLIEE 2021 - Task 3 results show specific metrics for Team OvGU and nigam, indicating performance evaluation in statute law retrieval."&lt;SEP&gt;"The COLIEE 2021 - Task 3 results show specific metrics for Team OvGU and nigam, indicating performance in statute law retrieval."</data>
  <data key="d5">"evaluation, performance metric"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;TEAM F1-SCORE PRECISION RECALL MAP&quot; &quot;" target="&quot;COLIEE 2021 - TASK 1 RESULTS&quot; &quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The COLIEE 2021 - Task 1 results provide specific metrics for teams JNLP and nigam, showing performance in case law retrieval."&lt;SEP&gt;"The COLIEE 2021 - Task 1 results provide specific metrics for teams JNLP and nigam, showing their performance in case law retrieval."</data>
  <data key="d5">"evaluation, performance metric"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;NUNO CORDEIRO&quot;" target="&quot;LEGAL SEMANTIC SEARCH ENGINE (LESSE)&quot;">
  <data key="d3">37.0</data>
  <data key="d4">"Nuno Cordeiro created LeSSE as part of his master's thesis, focusing on legal document retrieval and semantic search for Portuguese consumer law."&lt;SEP&gt;"Nuno Cordeiro developed LeSSE for his master's thesis on Portuguese consumer law."&lt;SEP&gt;"Nuno Cordeiro is the creator of Legal Semantic Search Engine (LeSSE), a system that was developed for Portuguese consumer law."&lt;SEP&gt;"Nuno Cordeiro created LeSSE as part of his master’s thesis work in 2022."</data>
  <data key="d5">"creation, development"&lt;SEP&gt;"creation, research focus"&lt;SEP&gt;"creativity, innovation"&lt;SEP&gt;"development, thesis work"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;NUNO CORDEIRO&quot;" target="&quot;2022 MASTER’S THESIS&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Nuno Cordeiro is the author of this specific master's thesis which led to LeSSE."</data>
  <data key="d5">"thesis, development"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;NUNO CORDEIRO&quot;" target="&quot;LESSE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Nuno Cordeiro created the LeSSE system for his master’s thesis focusing on combining document retrieval with semantic search."</data>
  <data key="d5">&lt;"creation, development"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;NUNO CORDEIRO&quot;" target="&quot;PORTUGUESE CONSUMER LAW&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Nuno Cordeiro created a system aimed at making Portuguese Consumer Law more accessible and understandable"</data>
  <data key="d5">"goal, focus"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;NUNO CORDEIRO&quot;" target="&quot;2022&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The year 2022 marks when Nuno Cordeiro created the Legal Semantic Search Engine (LeSSE)."</data>
  <data key="d5">"creation timeline"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;NUNO CORDEIRO&quot;" target="&quot;2022 THESIS WORK&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"The 2022 Thesis Work of Nuno Cordeiro refers to the creation of Legal Semantic Search Engine (LeSSE)."</data>
  <data key="d5">"thesis work, creation"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;LEGAL SEMANTIC SEARCH ENGINE (LESSE)&quot;" target="&quot;INESC-ID, IMPRENSA NACIONAL-CASA DA MOEDA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"INESC-ID and Imprensa Nacional-Casa da Moeda partnered to develop LeSSE as a system for legal document retrieval and semantic search."</data>
  <data key="d5">"partnership, development"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;LEGAL SEMANTIC SEARCH ENGINE (LESSE)&quot;" target="&quot;BERTIMBAU BASE (BERT -BASE)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERTimbau Base is used in the Legal Semantic Search Engine to generate embeddings from segments and queries for similarity search."</data>
  <data key="d5">"usage, integration"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;LEGAL SEMANTIC SEARCH ENGINE (LESSE)&quot;" target="&quot;INESC-ID&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"INESC-ID partnered with Nuno Cordeiro to develop LeSSE, contributing towards the accessibility of Portuguese consumer law."&lt;SEP&gt;"INESC-ID was a partner with Nuno Cordeiro in the development of LeSSE."</data>
  <data key="d5">"collaboration, partnership"&lt;SEP&gt;"partnership, collaboration"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;LEGAL SEMANTIC SEARCH ENGINE (LESSE)&quot;" target="&quot;IMPRENSA NACIONAL-CASA DA MOEDA&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Imprensa Nacional-Casa da Moeda partnered with Nuno Cordeiro to develop LeSSE, contributing towards the accessibility of Portuguese consumer law."&lt;SEP&gt;"Imprensa Nacional-Casa da Moeda was also a partner with INESC-ID in the development of LeSSE."</data>
  <data key="d5">"collaboration, partnership"&lt;SEP&gt;"partnership, collaboration"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;INESC-ID&quot;" target="&quot;LESSE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"INESC-ID is involved in the development of LeSSE along with Nuno Cordeiro and Imprensa Nacional-Casa da Moeda."</data>
  <data key="d5">"collaboration, development"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;IMPRENSA NACIONAL-CASA DA MOEDA&quot;" target="&quot;LESSE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Imprensa Nacional-Casa da Moeda is a partner in the development of LeSSE for Portuguese consumer law."</data>
  <data key="d5">"partnership, development"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;BERTIMBAU BASE&quot;" target="&quot;LESSE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"LeSSE uses BERTimbau Base as part of its semantic pipeline for processing Portuguese legal documents."</data>
  <data key="d5">"technology, usage"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;JO ˜ÃO RODRIGUES ET AL.&quot;" target="&quot;ALBERTINA [35] MODEL&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Jo ˜ão Rodrigues et al. shared Albertina as a new BERT model in May 2023."</data>
  <data key="d5">"model sharing, development"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;JO ˜ÃO RODRIGUES ET AL.&quot;" target="&quot;ALBERTINA PT-BR&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Jo ˜ão Rodrigues et al. developed Albertina PT-BR."</data>
  <data key="d5">"creators, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;JO ˜ÃO RODRIGUES ET AL.&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Jo ˜ão Rodrigues et al.'s work is part of Project IRIS."</data>
  <data key="d5">"project membership, contribution"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;JO ˜ÃO RODRIGUES ET AL.&quot;" target="&quot;CHAPTER 4&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Jo ˜ão Rodrigues et al. wrote Chapter 4 as part of their work."</data>
  <data key="d5">"writing, contribution"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;JO ˜ÃO RODRIGUES ET AL.&quot;" target="&quot;CHAPTER 5&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Jo ˜ão Rodrigues et al. wrote Chapter 5 as part of their work."</data>
  <data key="d5">"writing, contribution"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;TABLE 3.4&quot;" target="&quot;TASK 1 RESULTS&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"Table 3.4 is a reference to Task 1 results, linking these tables with the research tasks."&lt;SEP&gt;"Table 3.4 shows the results of Task 1."</data>
  <data key="d5">"table, task result"&lt;SEP&gt;"task results, documentation"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;TABLE 3.4&quot;" target="&quot;LESSE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Task 1 results are shown in Table 3.4, which relates to the development of LeSSE."</data>
  <data key="d5">"results, development"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;TABLE 3.5&quot;" target="&quot;TASK 3 RESULTS&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"Table 3.5 is a reference to Task 3 results, linking these tables with the research tasks."&lt;SEP&gt;"Table 3.5 shows the results of Task 3."</data>
  <data key="d5">"table, task result"&lt;SEP&gt;"task results, documentation"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;TABLE 3.5&quot;" target="&quot;LESSE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Task 3 results are shown in Table 3.5, which might relate to the broader impact or application of LeSSE."</data>
  <data key="d5">"results, application"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;COSINE SIMILARITY MEASURE&quot;" target="&quot;BM25 ALGORITHM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both cosine similarity and BM25 are used in the system to score segments."</data>
  <data key="d5">"search algorithms, scoring"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;COSINE SIMILARITY MEASURE&quot;" target="&quot;LESSE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The cosine similarity measure is used as part of the search index creation in the system developed by Nuno Cordeiro for LeSSE."</data>
  <data key="d5">"search algorithms, usage"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;BM25 ALGORITHM&quot;" target="&quot;COSINE SIMILARITY METRIC&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both cosine similarity and BM25 are used in the reordering model to produce final results, showing a relationship through their use in scoring segments."</data>
  <data key="d5">"scoring methods, similarity measures"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;JOÃO RODRIGUES ET AL.&quot;" target="&quot;PROJECT IRIS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"As part of Project IRIS, João Rodrigues and his team are contributing to the development of semantic search systems."</data>
  <data key="d5">"project contribution, collaboration"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;JOÃO RODRIGUES ET AL.&quot;" target="&quot;ALBERTINA&quot;">
  <data key="d3">19.0</data>
  <data key="d4">"João Rodrigues et al. are the creators of Albertina, a BERT model that represents the new state-of-the-art for European and Brazilian Portuguese."&lt;SEP&gt;"João Rodrigues et al. created Albertina, sharing it in May 2023 as a new state-of-the-art model."</data>
  <data key="d5">"creation, development"&lt;SEP&gt;&lt;"innovation, introduction"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;DEBERTA&quot;" target="&quot;CALLAN, J.&quot;">
  <data key="d3">11.0</data>
  <data key="d4">"Both are involved in advancements in language models and neural processing.&lt;"|&gt;power dynamics, indirect connection via advancements"&lt;SEP&gt;"Callan, J. is not directly related to DeBERTa, but the text mentions it as a recent advancement."</data>
  <data key="d5">"not directly related, indirect connection via advancements"&lt;SEP&gt;6</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;DEBERTA&quot;" target="&quot;SIGIR CONFERENCE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The SIGIR Conference could be relevant to the development and presentation of DeBERTa due to its focus on information retrieval and natural language processing."</data>
  <data key="d5">"presentation, relevance, research community"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;BRWAC CORPUS&quot;" target="&quot;ALBERTINA PT-BR&quot;">
  <data key="d3">27.0</data>
  <data key="d4">"The BrWaC corpus is used in the pre-training of the Albertina model for Brazilian Portuguese (PT-BR), providing relevant data for training."&lt;SEP&gt;"The BrWaC corpus was used as one of the data sets for pre-training the PT-BR version of Albertina."</data>
  <data key="d5">"data set usage"&lt;SEP&gt;"data source, pre-training"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;CHAPTER 1 AND CHAPTER 5&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Chapter 1 introduces Project IRIS and its constraints, while Chapter 5 provides detailed information about the Legal-BERTimbau model within the project."</data>
  <data key="d5">"context introduction, content detail"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;ECLI-INDEXER6&quot;">
  <data key="d3">30.0</data>
  <data key="d4">"The ecli-indexer6 tool is used for collecting legal documents from dgsi.pt as part of the Project IRIS data collection process."&lt;SEP&gt;"ecli-indexer6 is part of the tools used in Project IRIS for collecting legal documents."</data>
  <data key="d5">"tool usage, document retrieval"&lt;SEP&gt;"tool usage, project integration"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;ORGANIZATION&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"Project IRIS is a larger project that includes this research work, allowing for skill development and collaboration with professionals and doctoral students."&lt;SEP&gt;"Project IRIS is an organization where the research and development of the semantic search system were conducted."</data>
  <data key="d5">"project context, skill development"&lt;SEP&gt;&lt;"development context, organizational affiliation"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;CHAPTER 1&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Chapter 1 is part of Project IRIS and provides foundational information about pre-defined constraints."&lt;SEP&gt;"Chapter 1 mentions Project IRIS as part of the ongoing work, providing context for the constraints mentioned."</data>
  <data key="d5">"context, documentation"&lt;SEP&gt;"document reference, project description"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;CHAPTER 5&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Chapter 5 is part of Project IRIS and delves deeper into specific technical details, such as how the Legal-BERTimbau model works within the system."</data>
  <data key="d5">"technical documentation, integration"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;WORK PRESENTED&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The work is a segment of the larger Project IRIS, emphasizing its contribution to the development and exploration of advanced search systems."|&lt;&gt;"project scope, research focus"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;ELASTIC-SEARCH&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Project IRIS utilized Elastic-Search to develop the Semantic Search System, highlighting its importance in the project's infrastructure."|&lt;&gt;"technological choice, system development"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;SOFT SKILLS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The development of soft skills was part of the broader benefits derived from working within Project IRIS."|&lt;&gt;"skill development, personal growth"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;PROJECT IRIS&quot;" target="&quot;SUPERVISED LEARNING TECHNIQUES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Supervised learning techniques are used as part of Project IRIS for developing language models and other technologies."</data>
  <data key="d5">"technique application"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;ECLI-INDEXER6&quot;" target="&quot;DGSI.PT&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"ecli-indexer6 retrieves content from dgsi.pt to collect and index legal documents."</data>
  <data key="d5">"document retrieval, indexing tool"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ECLI-INDEXER6&quot;" target="&quot;IRIS MEMBERS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The IRIS team uses ecli-indexer6 for data collection, highlighting its importance in the document gathering process."</data>
  <data key="d5">"data collection, tool usage"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ECLI-INDEXER6&quot;" target="&quot;HTML CONTENT&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ecli-indexer6 tool processes HTML content from web pages for indexing into Elasticsearch."</data>
  <data key="d5">"data extraction, processing"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;ECLI-INDEXER6&quot;" target="&quot;31690 DOCUMENTS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ecli-indexer6 tool collected 31,690 legal documents for indexing into Elasticsearch."</data>
  <data key="d5">"data collection, quantity"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;DGSI.PT&quot;" target="&quot;IRIS MEMBERS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"IRIS team retrieves data from dgsi.pt, making it a critical source of information."</data>
  <data key="d5">"data source, collaboration"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;RELATOR 1&quot;" target="&quot;SUPREMO TRIBUNAL DE JUSTICA&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Relator 1 is associated with a process in the Supremo Tribunal de Justica as one of the indexed documents mentions it."</data>
  <data key="d5">"tribunal association, relator involvement"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;DGSI - INDEXER -STJ&quot;" target="&quot;TRAINING DATASET&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"dgsi - indexer -STJ created the Training dataset for model training purposes."</data>
  <data key="d5">"dataset creation, training"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;DGSI - INDEXER -STJ&quot;" target="&quot;VALIDATION DATASET&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"dgsi - indexer -STJ developed the Validation dataset for ensuring robustness and generalization of models."</data>
  <data key="d5">"robustness, validation"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;TRAINING DATASET&quot;" target="&quot;VALIDATION DATASET&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Similar to testing dataset, training dataset is used in the initial development phase of model building."</data>
  <data key="d5">"data preparation, machine learning"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;ACORDAO&quot;" target="&quot;UNANIMIDADE&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Acordao resulted in a unanimous voting outcome."</data>
  <data key="d5">"unanimous decision, judgement"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;ACORDAO&quot;" target="&quot;REVISTA&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The Revista process is associated with the Acordao, possibly involving reviews or revisions."</data>
  <data key="d5">"legal review, document revision"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;ACORDAO&quot;" target="&quot;TEXTO&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Texto contains detailed descriptions and rulings from an Acordao, making it relevant for legal searches."</data>
  <data key="d5">"document content, legal information"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;REVISTA&quot;" target="&quot;RE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Re can request Revista to challenge the original ruling, seeking reconsideration of their claim."</data>
  <data key="d5">"legal dispute, appeal"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;REVISTA&quot;" target="&quot;AUTOR&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"Autor's case could be reviewed through Revista if they believe the original decision was incorrect or unfair."</data>
  <data key="d5">"claim review, legal process"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;BI-ENCODER&quot;" target="&quot;CROSS-ENCODER&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"While Bi-Encoders generate separate embeddings for sentences, Cross-Encoders compare the similarities of sentences directly without generating individual embeddings first."</data>
  <data key="d5">"embedding generation, direct comparison"</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;CROSS-ENCODER&quot;" target="&quot;PSEUDO LABELING STEP&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Cross-Encoder receives triplets from the Pseudo Labeling step to compare positive and negative passages."|&lt;&gt;"model training, triplet comparison"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;DOCUMENT PRE-PROCESSING&quot;" target="&quot;SUBSECTION 4.2.1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Subsection 4.2.1 provides insights on the analysis of documents during pre-processing to ensure they are not too long or short and do not raise other concerns."</data>
  <data key="d5">&lt;"analysis, detail verification"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;SUBSECTION 4.2.1&quot;" target="&quot;PASSAGE RELEVANTE PARA A QUEST ˜AO&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Subsection 4.2.1 was referenced in the document to analyze phrases and sentences for potential issues."</data>
  <data key="d5">"analysis, reference"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;GENERATIVE LANGUAGE MODEL (GLM)&quot;" target="&quot;SEARCH SYSTEM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Generative Language Model (GLM), specifically GPT3.5, is used to generate user-friendly responses based on the retrieved results from the search system."</data>
  <data key="d5">&lt;"response generation, output refinement"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;PURELY SEMANTIC SEARCH SYSTEM&quot;" target="&quot;ARCHITECTURE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Architecture includes the design of the Purely Semantic Search System as part of its overall structure."</data>
  <data key="d5">"system integration, architecture design"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;PURELY SEMANTIC SEARCH SYSTEM&quot;" target="&quot;LEXICAL-FIRST SEARCH SYSTEM&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both systems are described in the document and seem to be different approaches within the broader Semantic Search System topic."&lt;</data>
  <data key="d5">"architecture comparison, alternative approach"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;HUGGINGFACE&quot;" target="&quot;STJIRIS/IRIS STS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"HuggingFace provided a platform where the custom STS dataset stjiris/IRIS sts is publicly available for further training of models."</data>
  <data key="d5">"platform provision, dataset availability"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;HUGGINGFACE&quot;" target="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The datasets are available on HuggingFace, an online platform where researchers share models and data sets."</data>
  <data key="d5">"data sharing, collaboration"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;HUGGINGFACE&quot;" target="&quot;MACHINE TRANSLATED MULTILINGUAL STS BENCHMARK DATASET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"HuggingFace is the creator of the Machine translated multilingual STS benchmark dataset."</data>
  <data key="d5">"creation, data provider"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;BRWAC&quot;" target="&quot;LEGAL-BERTIMBAU TRAINING&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Training Legal-BERTimbau involves using BrWaC as a large corpus for pre-training and adaptation."</data>
  <data key="d5">"corpus usage, model training"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LEGAL LANGUAGE MODEL&quot;" target="&quot;DOMAIN ADAPTATION&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"Domain Adaptation is applied within the Legal Language Model to tailor it for legal-specific tasks."&lt;SEP&gt;"Legal Language Model is involved in Domain Adaptation processes as it's discussed under this domain."&lt;</data>
  <data key="d5">"domain expertise, adaptation process"&lt;SEP&gt;"task adaptation, domain specialization"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;TRANSFER LEARNING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Transfer learning is facilitated by neural networks, which are used to fine-tune pre-trained models like BERTimbau for specific tasks."</data>
  <data key="d5">&lt;"technique_relationship"&gt;</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;SBERT MODEL&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Neural Networks are the foundation for creating SBERT models that can process and understand natural language sentences effectively."</data>
  <data key="d5">"technological basis, advanced processing"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;TRANSFER LEARNING&quot;" target="&quot;LARGE CORPUS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The concept of Large Corpus is integral to the technique of Transfer Learning, providing substantial data for fine-tuning models."</data>
  <data key="d5">"data source, transfer learning"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">32.0</data>
  <data key="d4">"Domain adaptation is crucial for improving the models' ability to perform well on tasks like STS, ensuring they can effectively handle unseen data and contexts."&lt;SEP&gt;"The STS task's evaluation relies on the effectiveness of domain adaptation techniques, especially in adapting models for new tasks and domains."&lt;SEP&gt;"The STS task's evaluation relies on the effectiveness of domain adaptation techniques, especially in adapting models for new tasks and domains." "&lt;SEP&gt;"Domain adaptation techniques like MLM and TSDAE are evaluated through tasks such as STS to determine their effectiveness."</data>
  <data key="d5">"evaluation, adaptation"&lt;SEP&gt;"evaluation, technique comparison"&lt;SEP&gt;"task relevance, effectiveness assessment"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f&lt;SEP&gt;chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;STS TASK.&quot; &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both Domain Adaptation and STS task are crucial for evaluating model performance, with Domain Adaptation improving generalization for new data, while STS measures semantic similarity."&lt;/|&gt;"evaluation, technique enhancement"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Transformer-based Sequential Denoising Auto-Encoder is utilized within the process of Domain Adaptation for specific tasks."</data>
  <data key="d5">"technology application, task implementation"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;TRAINING TASKS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Multiple training tasks were combined to achieve different versions through domain adaptation techniques."</data>
  <data key="d5">"adaptation, model versioning"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;FINE-TUNING ON DOWNSTREAM TASKS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Domain Adaptation is a technique that can be applied to Fine-tuning on Downstream Tasks to adapt models to new domains or tasks."</data>
  <data key="d5">"adaptation techniques, task-specific adaptation"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;FINE-TUNING&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Fine-tuning is part of the process that can be used in Domain Adaptation techniques, where models trained for one task or domain are adjusted to perform better in another."</data>
  <data key="d5">6</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;TSDAE TECHNIQUE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The TSDAE technique was specifically used to adapt models for new domains, directly related to the concept of domain adaptation in STS tasks." "</data>
  <data key="d5">"technique, adaptation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;EVALUATION OF MODELS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Domain Adaptation technique is crucial for the evaluation of models, especially in addressing the problem of model generalization." "</data>
  <data key="d5">"evaluation, adaptation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;DOMAIN ADAPTATION&quot;" target="&quot;NEGATIVE LOG-LIKELIHOOD LOSS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Negative log-likelihood loss is used to measure the effectiveness of domain adaptation techniques like TSDAE and MLM."</data>
  <data key="d5">"performance evaluation, loss function"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;BERTIMBAU LARGE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both models were subjected to different domain adaptation techniques but performed better than BERTimbau in the STS task."&lt;/|&gt;"model performance, adaptation technique"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;STS TASK.&quot; &quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The evaluation of stjiris/bert-large-portuguese-cased-legal-tsdae is done through the STS task to assess its semantic textual similarity performance."&lt;/|&gt;"evaluation, model validation"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;STSB MULTI MT PORTUGUESE SUB-DATASET&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The STS evaluation task variant of the Legal-BERTimbau-large model was fine-tuned using this dataset."</data>
  <data key="d5">"fine-tuning, dataset usage"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;ASSIN DATASET&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The model variant used a training set from the assin dataset for STS evaluation."</data>
  <data key="d5">"dataset usage, training set"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;ASSIN2 DATASET&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"This dataset was also utilized to train the model variant used in the STS task."</data>
  <data key="d5">"training set, dataset usage"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;STJIRIS/IRIS STS&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The IRIS organization provided a custom dataset for further training models like stjiris/bert-large-portuguese-cased-legal-tsdae."</data>
  <data key="d5">"dataset provision, model variant"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;SENTENCETRANSFORMER LIBRARY&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The SentenceTransformer library was used to define and fine-tune the SBERT variant stjiris/bert-large-portuguese-cased-legal-tsdae."</data>
  <data key="d5">"library usage, model definition"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;ADAM OPTIMIZATION ALGORITHM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Adam optimization algorithm was employed during the training of stjiris/bert-large-portuguese-cased-legal-tsdae with a learning rate of \(10^{-5}\)."</data>
  <data key="d5">"optimization, training process"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;LEARNING RATE&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The specific parameter \(10^{-5}\) was used for the learning rate during the training of stjiris/bert-large-portuguese-cased-legal-tsdae."</data>
  <data key="d5">"parameter, training process"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;BATCH SIZE&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The batch size of 8 was utilized during the training of stjiris/bert-large-portuguese-cased-legal-tsdae."</data>
  <data key="d5">"parameter, training process"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;EPOCHS&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The model was trained over five epochs during the fine-tuning process."</data>
  <data key="d5">"training process, parameter"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The stjiris/bert-large-portuguese-cased-legal-tsdae performed better in the STS task compared to BERTimbau large, validating its adaptation through TSDAE technique."&lt;SEP&gt;"The stjiris/bert-large-portuguese-cased-legal-tsdae performed better in the STS task compared to BERTimbau large, validating its adaptation through TSDAE technique." "</data>
  <data key="d5">"performance comparison, validation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;MLM TASK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The stjiris/bert-large-portuguese-cased-legal-tsdae model performed better in the MLM task, which is an important part of its evaluation." "</data>
  <data key="d5">"performance comparison, validation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;BATCH SIZE OF 2&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"A batch size of 2 was also used in the fine-tuning stage for this variant, affecting its training process and loss calculation."</data>
  <data key="d5">"training parameter, model adaptation"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;EVALUATION SPLIT&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The evaluation split was used to select the best model performance for this variant during the TSDAE fine-tuning stage."</data>
  <data key="d5">"model selection, performance evaluation"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE&quot;" target="&quot;TABLE 6.1: MLM AVERAGE LOSS FOR LEGAL DOCUMENTS IN THE TEST SET&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The table shows how stjiris/bert-large-portuguese-cased-legal-tsdae performed compared to other models on a specific task."</data>
  <data key="d5">"performance comparison, evaluation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;RUFIMELO/LEGAL-BERTIMBAU-LARGE&quot;" target="&quot;LEGAL-BERTIMBAU-LARGE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"rufimelo/Legal-BERTimbau-large is a variant derived from Legal-BERTimbau-large." "</data>
  <data key="d5">"model derivation, variation"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;HUGGINGFACE STJIRIS/IRIS STS DATASET&quot;">
  <data key="d3">22.0</data>
  <data key="d4">"The STS dataset is hosted by HuggingFace, making it publicly available for further training models."&lt;SEP&gt;"The datasets are available at HuggingFace repository 'stjiris/IRIS sts', indicating a relation of hosting or availability."</data>
  <data key="d5">"hosting, availability"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;OPENAI’S GPT3 TEXT-DAVINCI-003 MODEL API&quot;">
  <data key="d3">25.0</data>
  <data key="d4">"The GPT3 model was used to generate sentence pairs for the custom STS dataset."&lt;SEP&gt;"The datasets were used to generate sentence pairs using the GPT3 API, indicating a tool or process relation."</data>
  <data key="d5">"sentence generation, customization"&lt;SEP&gt;"tool usage, generation"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;BERTIMBAU’S PAPER&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"The custom STS dataset was used in the BERTimbau's fine-tuning stage for model training."&lt;SEP&gt;"The paper discusses similar fine-tuning strategies used in the creation of STS datasets, indicating a theoretical or methodological relation."</data>
  <data key="d5">"dataset usage, research contribution"&lt;SEP&gt;"methodology, inspiration"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-STS-V1&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The datasets were used to train the model 'stjiris/bert-large-portuguese-cased-legal-mlm-sts-v1', indicating a training relation."</data>
  <data key="d5">"training, development"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-STS-V1&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The datasets were also used to train the model 'stjiris/bert-large-portuguese-cased-legal-tsdae-sts-v1', indicating a similar training relation."</data>
  <data key="d5">"training, development"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;NLI ANNOTATIONS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The STS datasets include NLI annotations for sentence pairs, indicating a relation of incorporating or using these annotations in the dataset creation process."</data>
  <data key="d5">"dataset integration, annotation inclusion"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;RELATEDNESS SCORES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The STS datasets include relatedness scores for sentence pairs, indicating a relation of using these scores in the dataset creation process."</data>
  <data key="d5">"score inclusion, semantic similarity"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;LEARNING RATE OF 10^-5&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The datasets were used to train models with a specific learning rate (10^-5), indicating a relation of training context."</data>
  <data key="d5">"training context, learning rate"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;ADAM OPTIMIZATION ALGORITHM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The datasets were used to train models with the Adam optimization algorithm, indicating a relation of training method."</data>
  <data key="d5">"training method, optimization algorithm"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;SBERT VARIANTS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The datasets were used to train various SBERT model variants, indicating a relation of dataset usage in model development."</data>
  <data key="d5">"model training, dataset use"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;STJIRIS/IRIS STS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both datasets are used for the STS task, indicating a collaborative effort in model development."</data>
  <data key="d5">"task collaboration, dataset usage"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The STS dataset is directly used in the STS task to compare sentence pairs."</data>
  <data key="d5">"task relevance, data utility"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;HUGGINGFACE: STJIRIS/IRIS STS DATASET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The STS datasets are hosted on HuggingFace where the IRIS dataset is publicly available for further training."</data>
  <data key="d5">"dataset availability, hosting service"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;GPT3 TEXT-DAVINCI-003 MODEL API&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The GPT3 text-davinci-003 is used to generate sentence pairs for STS tasks based on the provided dataset."</data>
  <data key="d5">"sentence generation, relatedness scoring"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS DATASETS: ASSIN, ASSIN2, ANDSTSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;BERTIMBAU'S PAPER&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERTimbau's paper discusses similar fine-tuning processes used for the STS dataset described here."</data>
  <data key="d5">"methodology, performance improvement"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;HUGGINGFACE STJIRIS/IRIS STS DATASET&quot;" target="&quot;SBERT VARIANTS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The HuggingFace dataset is where some of the SBERT variants are hosted or available for use, indicating a relation of hosting or availability."</data>
  <data key="d5">"dataset hosting, model availability"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-STS-V1&quot;" target="&quot;GPT3 TEXT-DAVINCI-003 MODEL API&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The GPT3 text-davinci-003 model API was used to generate sentence pairs for the STS datasets, which are also utilized by the stjiris/bert-large-portuguese-cased-legal-mlm-sts-v1 model."</data>
  <data key="d5">"model training, dataset generation"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-STS-V1&quot;" target="&quot;STS FINE-TUNING STAGE DESCRIBED IN BERTIMBAU’S PAPER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The models trained on STS datasets as per the BERTimbau's method include the stjiris/bert-large-portuguese-cased-legal-tsdae-sts-v1 variant."</data>
  <data key="d5">"model training, dataset utilization"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;LEARNING RATE OF 10^-5&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"A specific learning rate (10^-5) was used during the training of models for the STS task to control how quickly or slowly the model learns."</data>
  <data key="d5">"learning control, fine-tuning process"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;ADAM OPTIMIZATION ALGORITHM&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-V0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Adam optimization algorithm with a batch size of 8 was used for training over five epochs."</data>
  <data key="d5">"training method, optimization technique"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;ADAM OPTIMIZATION ALGORITHM&quot;" target="&quot;STS FINE-TUNING STAGE DESCRIBED IN BERTIMBAU’S PAPER&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Adam optimization algorithm was part of the configuration used in the STS fine-tuning process as per BERTimbau's method.".</data>
  <data key="d5">"model training, algorithm usage"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;ADAM OPTIMIZATION ALGORITHM&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Adam optimization algorithm was used in the training process of models for the STS task to adjust learning rates effectively."</data>
  <data key="d5">"optimization method, fine-tuning process"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;ADAM OPTIMIZATION ALGORITHM&quot;" target="&quot;LARGE MODELS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Adam optimization algorithm was used to train the large models."</data>
  <data key="d5">"training process, optimization technique"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;ASSIN1 AND ASSIN2 DATASETS&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"SBERT variants performed better than state-of-the-art multilingual models on these Portuguese datasets."&lt;SEP&gt;"SBERT variants showed superior performance compared to state-of-the-art multilingual models when evaluated on these specific Portuguese datasets."</data>
  <data key="d5">"performance comparison, language model evaluation"&lt;SEP&gt;"performance improvement, specific dataset evaluation"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;ASSIN1&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"SBERT variants performed better than state-of-the-art multilingual models on ASSIN1."</data>
  <data key="d5">"performance comparison, language model evaluation"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;ASSIN2&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SBERT variants performed better than state-of-the-art multilingual models on ASSIN2."</data>
  <data key="d5">"performance comparison, language model evaluation"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;MULTILINGUAL MODELS&quot;">
  <data key="d3">33.0</data>
  <data key="d4">"SBERT Variants outperformed some but not all Multilingual Models on the STS task for Portuguese datasets."&lt;SEP&gt;"SBERT variants outperformed multilingual models on specific Portuguese datasets but not on STSB multi MT, showing varying effectiveness across different tasks."&lt;SEP&gt;"SBERT variants performed better than state-of-the-art multilingual models on some datasets but not others, indicating their effectiveness in certain tasks."</data>
  <data key="d5">"performance comparison, domain adaptation"&lt;SEP&gt;"performance comparison, evaluation"&lt;SEP&gt;&lt;relationship_description&gt;</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;STATE-OF-THE-ART MULTILINGUAL MODELS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SBERT variants outperform state-of-the-art multilingual models in performance on the STS task for Portuguese datasets."</data>
  <data key="d5">"performance, multilingual models"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;STSB MULTI MT DATASET&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SBERT variants did not outperform state-of-the-art multilingual models on the stsb multi mt dataset."&lt;SEP&gt;"While SBERT variants performed well on ASSIN1 and ASSIN2, their performance did not outshine state-of-the-art multilingual models when tested on the STSB multi MT dataset, which includes multiple translations from different languages."</data>
  <data key="d5">"dataset diversity, mixed results"&lt;SEP&gt;"dataset evaluation, performance comparison"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;ASSIM BENCHMARK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SBERT variants outperformed state-of-the-art models on the Assin and Assin2 benchmark tasks."</data>
  <data key="d5">"performance, benchmarks"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;PORTUGUESE DATASETS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SBERT variants were evaluated on Portuguese datasets, demonstrating superior performance compared to state-of-the-art multilingual models on some tasks."</data>
  <data key="d5">"dataset evaluation, model performance"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;STS FINE-TUNING STAGE DESCRIBED IN BERTIMBAU’S PAPER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The SBERT variants were generated through the fine-tuning process as per the method described in BERTimbau's research paper, particularly those trained on STS datasets.".</data>
  <data key="d5">"model training, variant generation"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;STS TASK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"SBERT variants were trained on STS tasks to improve their performance, using the STS dataset as a basis."</data>
  <data key="d5">"task relevance, performance improvement"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both models are part of the multilingual models evaluated, with SBERT variants performing better on certain datasets."</data>
  <data key="d5">"model comparison, performance improvement"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;ALL-MPNET-BASE-V2&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"While both models are mentioned, all-mpnet-base-v2 did not outperform the SBERT variants on specific Portuguese datasets."</data>
  <data key="d5">&lt;"model comparison, mixed results"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;MLM-STS-V0&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both models were evaluated and compared, with SBERT variants showing better performance on certain tasks."</data>
  <data key="d5">&lt;"model evaluation, performance improvement"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;MLM-NLI-STS-V0&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"These models were part of the comparison, with SBERT variants outperforming these on specific Portuguese datasets."</data>
  <data key="d5">&lt;"model comparison, performance difference"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;MLM-GPL-NLI-STS-V0&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both models were evaluated and compared, showing that SBERT variants performed better in some scenarios."</data>
  <data key="d5">&lt;"model evaluation, comparative analysis"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;MLM-MKD-NLI-STS-V0&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"These models were part of the comparison, with SBERT variants outperforming these on specific Portuguese datasets."</data>
  <data key="d5">&lt;"model comparison, performance difference"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;TSDAE-STS-V0&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both models were evaluated and compared, with SBERT variants showing better performance in certain tasks."</data>
  <data key="d5">&lt;"model evaluation, performance improvement"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;TSDAE-NLI-STS-V0&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"These models were part of the comparison, with SBERT variants outperforming these on specific Portuguese datasets."</data>
  <data key="d5">&lt;"model comparison, performance difference"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;TSDAE-GPL-NLI-STS-V0&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both models were evaluated and compared, showing that SBERT variants performed better in some scenarios."</data>
  <data key="d5">&lt;"model evaluation, comparative analysis"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;SBERT VARIANTS&quot;" target="&quot;TSDAE-MKD-NLI-STS-V0&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"These models were part of the comparison, with SBERT variants outperforming these on specific Portuguese datasets."</data>
  <data key="d5">&lt;"model comparison, performance difference"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V0&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"These are different model variants trained with similar data but using different techniques."&lt;SEP&gt;"These models are variants of each other, differing possibly in training data or epochs."</data>
  <data key="d5">"model variant, NLI/STS training"&lt;SEP&gt;&lt;"training variations"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V0&quot;" target="&quot;NLIIN&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The 'nliin' suffix denotes the model was trained with NLI data, distinguishing it from other models."</data>
  <data key="d5">&lt;"model differentiation"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0&quot;" target="&quot;T5-BASE-QA-SQUAD-V1.1-PORTUGUESE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The T5 model was used to generate queries, which influenced the GPL technique."</data>
  <data key="d5">&lt;"query generation"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-NLI-STS-V0&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-NLI-STS-V1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both are more advanced versions of the initial model variants."</data>
  <data key="d5">"model evolution, NLI/STS training"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;PIERREGUILLOU/T5-BASE-QA-SQUAD-V1.1-PORTUGUESE&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The BERT variant was likely influenced by or trained with the T5 model during its development process."</data>
  <data key="d5">"model influence, language training"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;" target="&quot;NEIL REIMERS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Neil Reimers developed the MKD technique which was applied to create the stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1 model."</data>
  <data key="d5">"technique development, model creation"&lt;SEP&gt;&lt;"technique development, model creation"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;" target="&quot;TED 2020 – PARALLEL SENTENCES CORPUS&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1 model was trained using the TED 2020 dataset, which provided the necessary language data."</data>
  <data key="d5">&lt;"dataset utilization, training process"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;" target="&quot;SENTENCE-TRANSFORMERS/STSB-ROBERTA-LARGE&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The sentence-transformers/stsb-roberta-large model served as the teacher for the stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1 student model, guiding its learning process."</data>
  <data key="d5">&lt;"teacher-student relationship, knowledge transfer"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;" target="&quot;METAKD TECHNIQUE&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1 model incorporates the MetaKD technique, enhancing its performance on specific tasks."</data>
  <data key="d5">&lt;"technique integration, model enhancement"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION (METAKD)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The MetaKD technique was applied to the stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v1 model, enhancing its performance."</data>
  <data key="d5">&lt;"technique application, model enhancement"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;" target="&quot;SUBSECTION 3.1.2.B&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The BERT variant was developed based on the techniques introduced in this subsection."</data>
  <data key="d5">"technique foundation, model development"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;" target="&quot;ENGLISH&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The BERT variant is designed to work with and improve knowledge of the English language as well as Portuguese."</data>
  <data key="d5">"language integration, model versatility"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V1&quot;" target="&quot;TRAINING TASKS SUCH AS THIS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The BERT variant was trained using a specific set of tasks and datasets to improve its performance."</data>
  <data key="d5">"model training, task execution"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;NEIL REIMERS&quot;" target="&quot;MKD (MULTILINGUAL KNOWLEDGE DISTILLATION)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Neil Reimers developed this technique, making it directly related to him as its creator."</data>
  <data key="d5">&lt;"creator-technique relation"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU-LARGE&quot;" target="&quot;MLM AND TSDAE DOMAIN ADAPTATION&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"These techniques were performed on Legal-BERTimbau-large before fine-tuning the large version of the model."</data>
  <data key="d5">"technique application, model preparation"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU-LARGE&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1&quot;">
  <data key="d3">50.0</data>
  <data key="d4">"The student model stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1 was trained using the MKD technique to learn Portuguese from Legal-BERTimbau-large as the teacher model."&lt;SEP&gt;"This model is derived from Legal-BERTimbau-large as the student for Portuguese learning."</data>
  <data key="d5">"base model, evolution"&lt;SEP&gt;"model training, language learning"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU-LARGE&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-V0&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"The SBERT version is derived from Legal-BERTimbau-large, incorporating specific layers and training data."&lt;SEP&gt;"The latter is a variant of the former after fine-tuning and additional layers are added."</data>
  <data key="d5">"model derivation, fine-tuning"&lt;SEP&gt;"variant development, adaptation"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;SENTENCE-TRANSFORMERS/STSB-ROBERTA-LARGE&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"sentence-transformers/stsb-roberta-large was the teacher model used to train stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1 in MKD technique."</data>
  <data key="d5">"teacher-student relationship, knowledge transfer"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;METADATA KNOWLEDGE DISTILLATION (METAKD)&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-MKD-NLI-STS-V0&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The MetaKD technique was applied to the stjiris/bert-large-portuguese-cased-legal-tsdae-mkd-nli-sts-v0 model, enhancing its performance."</data>
  <data key="d5">&lt;"technique application, model enhancement"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;METADATA KNOWLEDGE DISTILLATION (METAKD)&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V1&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The MetaKD technique was applied to the stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v1 model, enhancing its performance."</data>
  <data key="d5">&lt;"technique application, model enhancement"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;METADATA KNOWLEDGE DISTILLATION (METAKD)&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-MKD-NLI-STS-V0&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The MetaKD technique was applied to the stjiris/bert-large-portuguese-cased-legal-mlm-mkd-nli-sts-v0 model, enhancing its performance."</data>
  <data key="d5">&lt;"technique application, model enhancement"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;DOCUMENTS'&lt; RELATIONSHIP&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The process of embedding adjustment is based on the tags and centroids derived from these documents."</data>
  <data key="d5">"data source, model training"</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;DOCUMENTS'&lt; RELATIONSHIP&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"The generated models are directly linked to the processing of these documents and their embeddings."&lt;SEP&gt;"The process of embedding adjustment is based on the tags and centroids derived from these documents."</data>
  <data key="d5">"data source, model training"&lt;SEP&gt;"model generation, embeddings"</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;DOCUMENTS'&lt; RELATIONSHIP&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"The generated models are directly linked to the processing of these documents and their embeddings."&lt;SEP&gt;"The process of embedding adjustment is based on the tags and centroids derived from these documents."</data>
  <data key="d5">"data source, model training"&lt;SEP&gt;"model generation, embeddings"</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;EMBEDDING ADJUSTMENT PROCESS&quot;" target="&quot;MODEL TRAINING&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The embedding adjustment process is part of the broader model training framework."</data>
  <data key="d5">"process integration, sub-process relationship"</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;INFORMATION RETRIEVAL PROCESS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The STS (Semantic Textual Similarity) task is part of evaluating the performance of language models used in the information retrieval process."'</data>
  <data key="d5">"task integration, model evaluation"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;MULTILINGUAL MODELS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The STS task evaluates the performance of multilingual models on various datasets like assin, assin2, and stsb multi mt."</data>
  <data key="d5">"performance evaluation, cross-lingual tasks"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;LEXRANK SUMMARIZATION TECHNIQUE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The LexRank summarization technique was initially used in the STS task to generate sentence summaries, but it didn't fully explore the semantic search capabilities."</data>
  <data key="d5">"summarization technique, initial approach"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;BERTIMBAU LARGE&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"BERTimbau large was used as a reference model to compare with other adaptations, particularly in the STS task evaluation."&lt;SEP&gt;"BERTimbau large was used as a reference model to compare with other adaptations, particularly in the STS task evaluation." "</data>
  <data key="d5">"reference model, comparison"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The stjiris/bert-large-portuguese-cased-legal-mlm showed better performance in the MLM task but was evaluated for its STS task performance, which is important for model adaptation evaluation."&lt;SEP&gt;"The stjiris/bert-large-portuguese-cased-legal-mlm showed better performance in the MLM task but was evaluated for its STS task performance, which is important for model adaptation evaluation." "</data>
  <data key="d5">"performance, evaluation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;ASSIN AND ASSIN2 DATASETS&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"ASSIN and ASSIN2 datasets are used to evaluate the performance of SBERT variants on the STS task."&lt;SEP&gt;"The ASSIN datasets are specifically designed for the STS task, providing a domain-relevant testbed for evaluating sentence similarity models."</data>
  <data key="d5">"dataset evaluation, task relevance"&lt;SEP&gt;"dataset relevance, task specificity"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;TSDAE-MKD-NLI-STS-V1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The tsdae-mkd-nli-sts-v1 variant is evaluated on the STS task to ensure it performs well in assessing text similarity within the Portuguese legal context."</data>
  <data key="d5">"model evaluation, task relevance"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;MLM-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The mlm-gpl-nli-sts-MetaKD-v0 variant is evaluated on the STS task to ensure it performs well in assessing text similarity within the Portuguese legal context."</data>
  <data key="d5">"model evaluation, task relevance"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;MLM-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The mlm-gpl-nli-sts-MetaKD-v1 variant is evaluated on the STS task to ensure it performs well in assessing text similarity within the Portuguese legal context."</data>
  <data key="d5">"model evaluation, task relevance"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;TSDAE-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The tsdae-gpl-nli-sts-MetaKD-v0 variant is evaluated on the STS task to ensure it performs well in assessing text similarity within the Portuguese legal context."</data>
  <data key="d5">"model evaluation, task relevance"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;TSDAE-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The tsdae-gpl-nli-sts-MetaKD-v1 variant is evaluated on the STS task to ensure it performs well in assessing text similarity within the Portuguese legal context."</data>
  <data key="d5">"model evaluation, task relevance"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;STS TASK&quot;" target="&quot;ASSIN2 DATASET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"ASSIN2 Dataset is a specific dataset used for evaluating sentence similarity tasks in Portuguese."::</data>
  <data key="d5">"dataset evaluation, task relevance"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;COURT PROFESSIONALS&quot;" target="&quot;INFORMATION RETRIEVAL PROCESS&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"Court professionals use the information retrieval process to gather relevant documents for their decision-making tasks."&lt;SEP&gt;"The information retrieval process is designed to assist court professionals, particularly judges, in their work."</data>
  <data key="d5">"assistance, efficiency improvement"&lt;SEP&gt;"document retrieval, decision support"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;JUDGES&quot;" target="&quot;RELEVANT DOCUMENTS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Judges need access to relevant documents to formulate decisions, which the information retrieval process aims to provide effectively."</data>
  <data key="d5">"document retrieval, decision-making support"</data>
  <data key="d6">chunk-3eda103fe58f7ddc99d8921614d8db3f</data>
</edge>
<edge source="&quot;JUDGES&quot;" target="&quot;FORMULATING A DECISION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Judges use the search system to formulate decisions in the legal domain."|&lt;&gt;"decision-making process, user application"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;STS TASK.&quot; &quot;" target="&quot;PARAPHRASE-MULTILINGUAL-MPNET-BASE-V2&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Paraphrase-multilingual-mpnet-base-v2 serves as a baseline for the STS task evaluation against state-of-the-art multilingual models."&lt;/|&gt;"baseline comparison, model validation"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STS TASK.&quot; &quot;" target="&quot;ALL-MPNET-BASE-V2&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"All-mpnet-base-v2 is another baseline used in comparing the performance of different versions of Legal-BERTimbau against state-of-the-art multilingual models."&lt;/|&gt;"baseline comparison, model validation"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STS TASK.&quot; &quot;" target="&quot;FINE-TUNING&quot; &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Fine-tuning is also used for the STS task to enhance a model's ability to provide accurate similarity scores between sentences." "</data>
  <data key="d5">"process enhancement, technique refinement"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;MLM TASK&quot; &quot;" target="&quot;FINE-TUNING&quot; &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Fine-tuning is applied to models during the MLM task to improve their performance on predicting masked tokens." "</data>
  <data key="d5">"process enhancement, technique refinement"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;MLM TASK&quot; &quot;" target="&quot;MASKING&quot; &quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Masking is integral to the MLM task as it involves replacing tokens with [MASK] for training purposes." "</data>
  <data key="d5">"core component, technique basis"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;MASKING&quot; &quot;" target="&quot;FINE-TUNING&quot; &quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The masking process in the MLM task influences fine-tuning by providing specific training data that enhances model performance." "</data>
  <data key="d5">"training data, process interaction"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STATE-OF-THE-ART MULTILINGUAL MODELS&quot;" target="&quot;STSB MULTI MT DATASET&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"State-of-the-art multilingual models did not outperform SBERT variants in the STSB Multi MT dataset evaluation."</data>
  <data key="d5">"evaluation, performance comparison"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;STSB MULTI MT DATASET&quot;" target="&quot;ORIGINAL STSBENCHMARK DATASET&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"stsb multi mt dataset contains multilingual translations from the original STSbenchmark dataset."</data>
  <data key="d5">"translation, benchmark"</data>
  <data key="d6">chunk-67e58f1a8dccff23d808e5fa663753db</data>
</edge>
<edge source="&quot;STSB MULTI MT DATASET&quot;" target="&quot;MULTILINGUAL MODELS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Multilingual models engaged with multiple translations during training, potentially improving their performance on the STSB multi mt dataset through exposure to varied linguistic contexts."</data>
  <data key="d5">"training process, model adaptation"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;STSB MULTI MT DATASET&quot;" target="&quot;ORIGINAL STS BENCHMARK DATASET&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Original STS Benchmark Dataset is the source dataset from which STSB Multi MT derives, used for training multilingual models."::</data>
  <data key="d5">"data derivation, benchmarking"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;MULTILINGUAL MODELS&quot;" target="&quot;SEARCH METRIC&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Multilingual models such as 'sentence-transformers/all-mpnet-base-v2' and 'paraphrase-multilingual-mpnet-base-v2' are used for comparison against Legal-BERTimbau in the Search metric."</data>
  <data key="d5">"model evaluation, performance comparison"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;STSB MULTI MT&quot;" target="&quot;STS BENCHMARK&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The STS benchmark and STSB multi MT are related in that the latter is derived from the former, indicating a connection between different evaluation datasets."</data>
  <data key="d5">&lt;relationship_description&gt;</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;LEXRANK SUMMARIZATION TECHNIQUE&quot;" target="&quot;GPT3 MODEL PROVIDED BY OPEN AI&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both techniques were used for query generation but yielded different results, with GPT3 providing more varied and potentially useful queries."</data>
  <data key="d5">&lt;relationship_description&gt;</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;LEXRANK SUMMARIZATION TECHNIQUE&quot;" target="&quot;EVALUATION ARCHITECTURE&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The evaluation architecture includes steps involving the LexRank summarization technique for generating queries."</data>
  <data key="d5">&lt;relationship_description&gt;</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;LEXRANK SUMMARIZATION TECHNIQUE&quot;" target="&quot;ELASTICSEARCH INDEX&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The LexRank summarization technique generated sentence summaries that were embedded using ElasticSearch for performance evaluation."</data>
  <data key="d5">"embedding storage, summarization technique"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;LEXRANK SUMMARIZATION TECHNIQUE&quot;" target="&quot;GPT3 MODEL PROVIDED BY OPENAI&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"LexRank is used initially but later replaced by GPT3 for generating more meaningful queries, indicating a shift in approach based on effectiveness and results obtained."</data>
  <data key="d5">"technique comparison, performance improvement"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;LEXRANK SUMMARIZATION TECHNIQUE&quot;" target="&quot;GPT3 MODEL&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Initially LexRank was used to extract important sentences from summaries, whereas GPT3 was later employed for rewriting sentences with similar meaning but different wording."</data>
  <data key="d5">"technique complementarity, semantic preservation"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;GPT3 MODEL PROVIDED BY OPEN AI&quot;" target="&quot;EVALUATION ARCHITECTURE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The evaluation architecture incorporates the GPT3 model for rewriting sentences while maintaining meaning, part of the overall query generation process."</data>
  <data key="d5">&lt;relationship_description&gt;</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;EVALUATION ARCHITECTURE&quot;" target="&quot;SYNONYMS AND SIMILAR EXPRESSIONS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The evaluation architecture uses synonyms and similar expressions to replace top 20 keywords from the summary, as part of a strategy to generate meaningful queries without exact keyword reuse."</data>
  <data key="d5">&lt;relationship_description&gt;</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;EVALUATION ARCHITECTURE&quot;" target="&quot;QUERY GENERATION TECHNIQUES&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Evaluation Architecture includes multiple techniques such as LexRank, GPT3, and T5 models to generate queries from document summaries."::</data>
  <data key="d5">"technique integration, evaluation process"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;MULTILINGUAL MODEL&quot;" target="&quot;MODELS V0 AND V1&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"A multilingual model performs worse than models fine-tuned on pre-existing and manually annotated datasets (V0 models)."</data>
  <data key="d5">"performance discrepancy, inferior model"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;MULTILINGUAL MODEL&quot;" target="&quot;LEXICAL-FIRST APPROACH&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The Multilingual model performs worse than Legal-BERTimbau, indicating that the Lexical-First approach might be more effective when using specific models like Legal-BERTimbau."</data>
  <data key="d5">"model comparison, effectiveness"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;MULTILINGUAL MODEL&quot;" target="&quot;LEXICAL+SEMANTIC APPROACH&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The Multilingual model performs worse than both Lexical-First and Lexical+Semantic approaches, highlighting their superior performance in certain metrics."</data>
  <data key="d5">"approach comparison, performance gap"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;LEXICAL-FIRST APPROACH&quot;" target="&quot;LEXICAL+SEMANTIC APPROACH&quot;">
  <data key="d3">36.0</data>
  <data key="d4">"Both the Lexical-First and Lexical+Semantic approaches outperform BM25 in certain scenarios, indicating their effectiveness."&lt;SEP&gt;"The Lexical-First and Lexical+Semantic approaches maintain similar performance to BM25 but can outperform it occasionally, showing their effectiveness in different scenarios."&lt;SEP&gt;"While Lexical-First performs closer to BM25, Lexical+Semantic significantly outperforms both BM25 and general Semantic Search Systems."::</data>
  <data key="d5">"approach comparison, performance superiority"&lt;SEP&gt;"approach comparison, performance variability"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;LEXICAL+SEMANTIC APPROACH&quot;" target="&quot;SEMANTICALLY-BASED SYSTEM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Lexical+Semantic approach outperforms the purely semantic system but does not match or exceed BM25 in terms of performance."&lt;</data>
  <data key="d5">"superiority over semantically-based, relative performance"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;V0 MODELS&quot;" target="&quot;V1 MODELS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"V0 models show better performance than V1 models, which were fine-tuned on a custom STS dataset but perform marginally worse in some metrics."&lt;</data>
  <data key="d5">"performance improvement, model version comparison"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;V0 MODELS&quot;" target="&quot;TSDAE-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The tsdae-gpl-nli-sts-MetaKD-v0 model performs better than V1 models in both Search and Discovery metrics."::</data>
  <data key="d5">"model comparison, performance contrast"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;V1 MODELS&quot;" target="&quot;SEARCH METRIC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"V1 models perform marginally worse than V0 models in both the Search and Discovery metrics but still outperform other semantic systems."::</data>
  <data key="d5">"model comparison, performance contrast"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;PURELY SEMANTIC&quot;" target="&quot;TOP 1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Purely Semantic's performance is significantly lower than BM25 in the Top 1 result, showing the limitations of relying solely on semantic analysis."|&lt;&gt;"performance gap, baseline comparison"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;PURELY SEMANTIC&quot;" target="&quot;SEARCH SYSTEM EVALUATION – DISCOVERY METRIC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Purely Semantic's performance is evaluated for its discovery capabilities in the Discovery metric."|&lt;&gt;"discovery capability evaluation, baseline comparison"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;PURELY SEMANTIC&quot;" target="&quot;SEARCH METRIC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Purely Semantic is also tested with the Search metric to compare its performance against BM25 and other models."</data>
  <data key="d5">"performance comparison, baseline technique"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;PURELY SEMANTIC&quot;" target="&quot;TABLE 6.4&quot;">
  <data key="d3">27.0</data>
  <data key="d4">"Purely Semantic is also evaluated in Table 6.4, showing significant improvements over BM25 in the Discovery metric."</data>
  <data key="d5">"performance improvement, metric comparison"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;LEXICAL-FIRST&quot;" target="&quot;SEARCH METRIC&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Lexical-First demonstrates specific patterns of performance in the Search metric, showing how initial lexical analysis affects overall results."</data>
  <data key="d5">&lt;"performance metrics, comparative analysis"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;LEXICAL-FIRST&quot;" target="&quot;TOP 3&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Lexical-First outperforms Purely Semantic but underperforms BM25 at Top 3 results."|&lt;&gt;"performance balance, intermediate comparison"&lt;SEP&gt;"Lexical-First outperforms Purely Semantic but underperforms BM25 in Top 3 results, indicating a balanced approach between lexical and semantic features."|&lt;&gt;"performance balance, intermediate comparison"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;LEXICAL-FIRST&quot;" target="&quot;TABLE 6.3&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Lexical-First is included in the evaluation of Table 6.3 but does not perform as well as Lexical + Semantic or BM25."</data>
  <data key="d5">"evaluation performance, comparative analysis"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;LEXICAL + SEMANTIC&quot;" target="&quot;ELASTIC-SEARCH&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Elastic-Search was used to host the Lexical + Semantic Search System, indicating its role in system deployment."</data>
  <data key="d5">&lt;"deployment platform, technology integration"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;LEXICAL + SEMANTIC&quot;" target="&quot;TOP 10&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Lexical + Semantic shows the best performance across all Top N results, especially at Top 10, demonstrating its comprehensive approach."|&lt;&gt;"comprehensive approach, superior performance"&lt;SEP&gt;"Lexical + Semantic shows the best performance across all Top N results, especially at Top 10."|&lt;&gt;"comprehensive approach, superior performance"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;LEXICAL + SEMANTIC&quot;" target="&quot;TABLE 6.4&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Lexical + Semantic is evaluated in both Tables but performs particularly well in the Discovery metric of Table 6.4."</data>
  <data key="d5">"performance evaluation, significant improvement"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;SEARCH METRIC&quot;" target="&quot;CUSTOM STS DATASET&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"The Custom STS dataset is used for fine-tuning models to improve their performance in the Search metric evaluation process."&lt;SEP&gt;"The Custom STS dataset is used for fine-tuning the Legal-BERTimbau model and thus impacts its performance in the Search metric."</data>
  <data key="d5">"model tuning, evaluation method"&lt;SEP&gt;&lt;"fine-tuning, performance improvement"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;SEARCH METRIC&quot;" target="&quot;TOP RESULTS SIZES&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Top results sizes are evaluated using the Search metric to assess the system’s ability to find relevant documents within a specified number of results."</data>
  <data key="d5">&lt;"evaluation criteria, metric use"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;SEARCH METRIC&quot;" target="&quot;ELASTICSEARCH INDEX&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The ElasticSearch index stores sentence embeddings generated by Legal-BERTimbau and is crucial for performing searches according to the Search metric."</data>
  <data key="d5">"data storage, retrieval system"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;SEARCH METRIC&quot;" target="&quot;SENTENCE-TRANSFORMERS/ALL-MPNET-BASE-V2&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Sentence-transformers/all-mpnet-base-v2 is used as a baseline for comparison against Legal-BERTimbau in the Search metric."</data>
  <data key="d5">"baseline model, performance comparison"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;OUR WORK&quot;" target="&quot;ALEX (MENTIONED IN CONCLUSION) &quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Alex contributed to 'our work' as a researcher involved in developing the Semantic Search System."</data>
  <data key="d5">&lt;"researcher contribution, project involvement"</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;BM25 TECHNIQUE&quot;" target="&quot;ROUGE-1 SCORE, ROUGE-2 SCORE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"ROUGE-1 and ROUGE-2 scores are used to compare the performance of the Semantic Search Systems against BM25 as a baseline."</data>
  <data key="d5">"performance evaluation, comparison"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;BM25 TECHNIQUE&quot;" target="&quot;LEGAL SEARCH SYSTEM&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"The Legal Search System was evaluated against BM25 technique during its performance assessment using different queries and multilingual models as baselines."</data>
  <data key="d5">"performance evaluation, comparison metric"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU MODELS&quot;" target="&quot;ROUGE-1 SCORE OF 47.92 AND A ROUGE-2 SCORE OF 22.50&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The scores indicate the performance metrics of Legal-BERTimbau models."</data>
  <data key="d5">"performance, evaluation"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU MODELS&quot;" target="&quot;EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Legal-BERTimbau models were submitted to and accepted by EPIA Conference on Artificial Intelligence, showcasing their significance in the field of AI."</data>
  <data key="d5">"submission, recognition"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU MODELS&quot;" target="&quot;ROUGE-1 SCORE 47.92, ROUGE-2 SCORE 22.50&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The model's performance is measured by these specific metrics."</data>
  <data key="d5">"performance evaluation, benchmarking"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU MODELS&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION TECHNIQUE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The technique was applied to train Legal-BERTimbau and produced good results."</data>
  <data key="d5">"training, performance improvement"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION TECHNIQUE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Metadata Knowledge Distillation technique was highlighted in an article submitted to EPIA Conference on Artificial Intelligence."</data>
  <data key="d5">"publication, recognition"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;" target="&quot;LEGAL-BBERTIMBAU MODELS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Legal-BBERTimbau models were submitted to and accepted by EPIA Conference on Artificial Intelligence, showcasing their significance in the field of AI."</data>
  <data key="d5">"submission, recognition"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;EPIA CONFERENCE ON ARTIFICIAL INTELLIGENCE&quot;" target="&quot;ALBERTINA PT-PT&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Albertina PT-PT model was published in May 2023, potentially influencing future submissions to EPIA conferences in AI research."</data>
  <data key="d5">"publication timing, influence on future submissions"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;ALBERTINA PT -PT&quot;" target="&quot;PORTUGUESE JURISPRUDENCE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Albertina PT -PT is specifically trained on Portuguese jurisprudence, indicating its relevance in the legal domain."</data>
  <data key="d5">"training, specialization"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;THESIS&quot;" target="&quot;PAPER&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The paper is part of the thesis that covers the main aspects of this research and its impact."</data>
  <data key="d5">"content, submission"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;THESIS&quot;" target="&quot;SEARCH SYSTEM&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The search system developed as part of the thesis aims to address the outlined motivations and challenges."&lt;</data>
  <data key="d5">"motivation fulfillment, research objective"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;PAPER&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION TECHNIQUE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Metadata Knowledge Distillation technique was mentioned in the paper submitted to EPIA Conference on Artificial Intelligence."</data>
  <data key="d5">"publication, recognition"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;HYBRID SEARCH SYSTEM&quot;" target="&quot;KNOWLEDGE GRAPH EMBEDDINGS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Knowledge Graph Embeddings can be incorporated into the Hybrid Search System for better query expansion and document retrieval."</data>
  <data key="d5">"integration, improvement"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;KNOWLEDGE GRAPH EMBEDDINGS&quot;" target="&quot;SEARCH SYSTEM&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Knowledge Graph Embeddings enhance the Search System by incorporating structured knowledge, leading to more accurate search results."&lt;SEP&gt;"Knowledge graph embeddings could significantly improve the Search System by providing more sophisticated and accurate information retrieval capabilities."</data>
  <data key="d5">"knowledge enhancement, accuracy improvement"&lt;SEP&gt;"knowledge representation, improved accuracy"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;KNOWLEDGE GRAPH EMBEDDINGS&quot;" target="&quot;DOCUMENT RETRIEVAL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Knowledge graph embeddings can significantly enhance document retrieval by providing more sophisticated and contextually relevant information."</data>
  <data key="d5">"contextual relevance, improved accuracy"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;KNOWLEDGE GRAPH EMBEDDINGS&quot;" target="&quot;ENTITY RECOGNITION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Entity Recognition and Knowledge Graph Embeddings are techniques that can be used to improve the accuracy of a Search System, with Entity Recognition identifying named entities and Knowledge Graph Embeddings providing more sophisticated information."</data>
  <data key="d5">"technique integration, entity-based enhancement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ACTIVE LEARNING&quot;" target="&quot;SEARCH SYSTEM&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Active learning can be applied to the Search System to improve its performance over time through user feedback."&lt;SEP&gt;"Active learning can be applied within the Search System to continuously improve its performance based on user feedback."&lt;SEP&gt;"Active learning is a method that the Search System could employ to continuously improve its performance based on user feedback."</data>
  <data key="d5">"feedback loop, continuous improvement"&lt;SEP&gt;"user feedback, continuous improvement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ACTIVE LEARNING&quot;" target="&quot;ENTITY RECOGNITION&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Both Entity Recognition and Active Learning are techniques used for improving the accuracy of document retrieval systems, with Active Learning enabling continuous learning from user feedback and Entity Recognition helping in query expansion."&lt;SEP&gt;"Entity Recognition can be combined with Active Learning to improve the system's ability to learn from interactions with users."&lt;SEP&gt;"Entity Recognition can be integrated with Active Learning to improve the system's ability to learn from user interactions."</data>
  <data key="d5">"data augmentation, user interaction improvement"&lt;SEP&gt;"technique enhancement, user interaction"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ACTIVE LEARNING&quot;" target="&quot;LEGAL DOMAIN&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"In the legal domain, active learning is particularly useful due to its ability to incorporate user feedback for improving performance over time."</data>
  <data key="d5">"domain-specific application, continuous improvement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ACTIVE LEARNING&quot;" target="&quot;EMBEDDING GENERATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Active learning can be used in embedding generation processes to improve model performance through iterative feedback."</data>
  <data key="d5">"feedback-driven improvement, continuous learning"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ACTIVE LEARNING&quot;" target="&quot;USER’S FEEDBACK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Active learning relies on user's feedback to continuously improve model performance and relevance in search tasks."</data>
  <data key="d5">"continuous improvement, feedback-driven"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ACTIVE LEARNING&quot;" target="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Active learning can be applied to improve models in tasks related to semantic textual similarity by continuously refining model performance based on user feedback."</data>
  <data key="d5">"model refinement, continuous improvement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;GLM (GENERAL LANGUAGE MODEL)&quot;" target="&quot;ENTITY RECOGNITION&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Entity Recognition can be applied within GLMs like GPT-3 to enrich queries and responses, making the interaction more seamless for users."&lt;SEP&gt;"Entity Recognition techniques can be integrated with GLMs like GPT-3 for better query handling and response generation."</data>
  <data key="d5">"query enrichment, response improvement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;GLM (GENERAL LANGUAGE MODEL)&quot;" target="&quot;SEARCH SYSTEM&quot;">
  <data key="d3">27.0</data>
  <data key="d4">"GLM is a technology that could enhance the Search System's capabilities in understanding and generating text for better query responses."&lt;SEP&gt;"GLM is used within the context of the Search System to generate responses and improve user interactions with the system."</data>
  <data key="d5">"model integration, response generation"&lt;SEP&gt;"text generation, model response"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;GLM (GENERAL LANGUAGE MODEL)&quot;" target="&quot;GPT-3&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"GPT-3 is an advanced model within the broader category of GLMs, highlighting advancements in this field."</data>
  <data key="d5">"model advancement, specific instance"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;GLM (GENERAL LANGUAGE MODEL)&quot;" target="&quot;EMBEDDING GENERATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Embedding generation can be an integral part of GLMs to improve their performance in various NLP tasks."</data>
  <data key="d5">"model enhancement, task improvement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;GPT-3&quot;" target="&quot;SEARCH SYSTEM&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The Search System can utilize advanced models like GPT-3 for improved query handling and document retrieval."&lt;SEP&gt;"The Search System utilizes advanced models like GPT-3 to enhance document retrieval and user interaction."</data>
  <data key="d5">"model integration, performance enhancement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;SEARCH SYSTEM&quot;" target="&quot;LANGUAGE MODEL&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Language models are integrated into the search system to enhance its functionality."&lt;</data>
  <data key="d5">"integration, technological support"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;SEARCH SYSTEM&quot;" target="&quot;ENTITY RECOGNITION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Entity recognition can be used to expand the query by including relevant entities, aiding in more accurate document retrieval."</data>
  <data key="d5">"query expansion, entity identification"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;SEARCH SYSTEM&quot;" target="&quot;LANGUAGE MODELS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The search system utilizes developed language models for improved performance in court decision processes."</data>
  <data key="d5">"system integration, technology application"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;SEARCH SYSTEM&quot;" target="&quot;DATASETS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Datasets are part of the resources supporting the development and testing of the search system."</data>
  <data key="d5">"resource provision, system validation"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;ENTITY RECOGNITION&quot;" target="&quot;USER’S QUERY&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Entity recognition helps in understanding and expanding the user’s query, thereby improving the relevance of the search results."</data>
  <data key="d5">"query expansion, entity identification"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ENTITY RECOGNITION&quot;" target="&quot;SEARCH SYSTEM’S RESPONSE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"By recognizing entities in the user’s query, entity recognition can guide the search process to provide more relevant and precise responses."</data>
  <data key="d5">"relevance improvement, precision enhancement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ENTITY RECOGNITION&quot;" target="&quot;DOCUMENT RETRIEVAL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Entity recognition can enhance document retrieval by identifying and expanding the user’s query with relevant entities."</data>
  <data key="d5">"query expansion, relevance improvement"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ENTITY RECOGNITION&quot;" target="&quot;USER’S INTERACTION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Entity recognition can improve the quality and relevance of user interactions by better understanding and expanding user queries."</data>
  <data key="d5">"interaction improvement, query expansion"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ENTITY RECOGNITION&quot;" target="&quot;ACTIVE LEARNING TECHNIQUE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both techniques are used to improve the system's performance by learning from user feedback and interactions with text."</data>
  <data key="d5">"technique integration, feedback loop"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;SEMEVAL-2015 TASK 2: SEMANTIC TEXTUAL SIMILARITY, ENGLISH, SPANISH AND PILOT ON INTERPRETABILITY&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The event focused on tasks related to semantic textual similarity across different languages, including the one in SemEval 2015."</data>
  <data key="d5">"task evaluation, cross-lingual performance"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;SEMEVAL-2014 TASK 10: MULTILINGUAL SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The event included tasks related to multilingual semantic textual similarity, which is relevant to the Semantic Textual Similarity task."</data>
  <data key="d5">"multilingual evaluation, task relevance"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;SEMEVAL-2016 TASK 1: SEMANTIC TEXTUAL SIMILARITY, MONOLINGUAL AND CROSS-LINGUAL EVALUATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The event included tasks for both monolingual and cross-lingual semantic textual similarity, which is related to the main task of semantic textual similarity."</data>
  <data key="d5">"evaluation scope, task breadth"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;*SEM 2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The *SEM 2013 shared task focused on semantic textual similarity, providing a platform for evaluating systems in this area."</data>
  <data key="d5">"task focus, evaluation platform"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;SEMEVAL-2016&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The 10th International Workshop on Semantic Evaluation (SemEval-2016) focused on semantic textual similarity tasks."</data>
  <data key="d5">"event, task focus"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;SEMANTIC TEXTUAL SIMILARITY CUSTOM DATASET&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Semantic Textual Similarity Custom Dataset is specifically used for the task of Semantic Textual Similarity, providing tailored data for model training."</data>
  <data key="d5">"data source, custom dataset"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;NATURAL LANGUAGE INFERENCE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Natural Language Inference can be applied in Semantic Textual Similarity tasks to better understand textual relationships and similarities."</data>
  <data key="d5">"process integration, task overlap"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;SEMEVAL-2016&quot;" target="&quot;BRWAC: A WACKY CORPUS FOR BRAZILIAN PORTUGUESE&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Corpus brWaC is mentioned in relation to computational processing tasks, which might have been discussed or used at the SemEval-2016 event."</data>
  <data key="d5">"corpus usage, related field"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2016&quot;" target="&quot;SAN DIEGO, CALIFORNIA&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"The event SemEval-2016 was held in San Diego, California."&lt;SEP&gt;"The workshop SemEval-2016 was held in San Diego, California."</data>
  <data key="d5">"location of event"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2016&quot;" target="&quot;CONNEAU, A.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Aurko Conneau participated in SemEval-2016 and is an active contributor to related tasks and workshops."</data>
  <data key="d5">"contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2016&quot;" target="&quot;ACL&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The SemEval workshops including SemEval-2016 are hosted by ACL and involve multiple research communities in natural language processing."</data>
  <data key="d5">"conference affiliation, research community"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;ATLANTA, GEORGIA, USA&quot;" target="&quot;*SEM 2013 SHARED TASK&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The *SEM 2013 conference in Atlanta, Georgia hosted the shared task on Semantic Textual Similarity."</data>
  <data key="d5">"conference, shared task"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;ATLANTA, GEORGIA, USA&quot;" target="&quot;SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"The shared task on semantic textual similarity took place at the SEM conference in Atlanta, Georgia, USA."&lt;SEP&gt;"The shared task on semantic textual similarity was part of SemEval-2013 held in Atlanta, Georgia, USA."</data>
  <data key="d5">"conference location"&lt;SEP&gt;"task location"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;BRWAC: A WACKY CORPUS FOR BRAZILIAN PORTUGUESE&quot;" target="&quot;CHAM, 2014&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The corpus brWaC for Brazilian Portuguese was presented in Cham, 2014."</data>
  <data key="d5">"corpus presentation location and year"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1&quot;" target="&quot;MULTILINGUAL AND CROSSLINGUAL EVALUATION&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The SemEval-2017 task 1 was focused specifically on semantic textual similarity multilingual and crosslingual evaluation."</data>
  <data key="d5">"evaluation, shared task"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1&quot;" target="&quot;VANCOUVER, CANADA&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The SemEval-2017 task 1 was held in Vancouver, Canada and focused on semantic textual similarity evaluation."</data>
  <data key="d5">"evaluation location, event"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1&quot;" target="&quot;CONNEAU, A.&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"A Conneau is involved in the SemEval-2017 Task 1 focusing on semantic textual similarity evaluation."&lt;SEP&gt;"Aurko Conneau contributed to SemEval-2017 Task 1 on semantic textual similarity multilingual and crosslingual focused evaluation."</data>
  <data key="d5">"research contribution, semantic evaluation"&lt;SEP&gt;"task contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1&quot;" target="&quot;SCHWENK, H.&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"H Schwenk is a co-author involved in the SemEval-2017 Task 1 semantic textual similarity evaluation."</data>
  <data key="d5">"task participation, research collaboration"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1&quot;" target="&quot;BORDES, A.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"A Bordes is involved in the SemEval-2017 Task 1 focusing on semantic textual similarity evaluation."</data>
  <data key="d5">"research contribution, evaluation task"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1&quot;" target="&quot;KIELA, D.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Daniele Kiela contributed to the task on semantic textual similarity at SemEval-2017."</data>
  <data key="d5">"task contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1&quot;" target="&quot;BOWMAN, S. R.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Sam Bowman is an author involved in SemEval-2017 Task 1 related to semantic textual similarity multilingual and crosslingual focused evaluation."</data>
  <data key="d5">"task contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1&quot;" target="&quot;POTTS, C.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Chris Potts is an author involved in SemEval-2017 Task 1 related to semantic textual similarity multilingual and crosslingual focused evaluation."</data>
  <data key="d5">"task contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;A LARGE ANNOTATED CORPUS FOR LEARNING NATURAL LANGUAGE INFERENCE&quot;" target="&quot;COPENHAGEN, DENMARK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The 42nd International ACM SIGIR Conference in Copenhagen, Denmark likely involved the use of a significant annotated corpus such as the one used for learning natural language inference."</data>
  <data key="d5">"corpus usage, conference"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;A LARGE ANNOTATED CORPUS FOR LEARNING NATURAL LANGUAGE INFERENCE&quot;" target="&quot;CONNEAU, A.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Aurma Conneau is involved in research using significant annotated corpora like the one used for natural language inference tasks."</data>
  <data key="d5">"researcher, dataset"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;A LARGE ANNOTATED CORPUS FOR LEARNING NATURAL LANGUAGE INFERENCE&quot;" target="&quot;ANGELI, G.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"G Angeli is involved in creating a large annotated corpus for NLI tasks."</data>
  <data key="d5">"corpus creation, task participation"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;CONNEAU, A.&quot;" target="&quot;CORDEIRO, N.&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"N Cordeiro's work on Portuguese consumer law could be relevant in the broader context of natural language processing and evaluation tasks."</data>
  <data key="d5">"domain knowledge, cross-disciplinary impact"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;DAI, Z.&quot;" target="&quot;SUPERVISED LEARNING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Z Dai's work contributed to deeper text understanding using contextual neural language modeling."</data>
  <data key="d5">"model development, research contribution"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;CALLAN, J.&quot;" target="&quot;DEVLIN, J.&quot;">
  <data key="d3">40.0</data>
  <data key="d4">"Both authors are involved in advancing text understanding techniques through neural language modeling."&lt;SEP&gt;"Both authors are prominent figures in the field of deep learning and NLP."&lt;SEP&gt;"Both authors contribute to advancements in natural language processing."</data>
  <data key="d5">"collaboration, NLP advancement"&lt;SEP&gt;"collaboration, research"&lt;SEP&gt;"influence, research community"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;CALLAN, J.&quot;" target="&quot;FONSECA, E.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The work by Callan is related to broader advancements in text understanding and neural modeling that are contemporaneous with Fonseca's work on ASSIN."</data>
  <data key="d5">"field advancement, contemporaneity"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;CALLAN, J.&quot;" target="&quot;KINGMA, D. P., BA, J.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both authors contribute to the field of deep learning and neural modeling, indirectly influencing each other through shared advancements in optimization methods."</data>
  <data key="d5">"influence, research community"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;CALLAN, J.&quot;" target="&quot;MARELLI, M., MENINI, S., BARONIS, M., BENTIVOGLI, L., BERNARDI, R., ZAMPARELLI, R.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both groups are part of the broader academic community working on NLP and neural modeling."</data>
  <data key="d5">"field advancement, contemporaneity"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;CALLAN, J.&quot;" target="&quot;ERGUN, G.&quot;">
  <data key="d3">0.0</data>
  <data key="d4">"No direct relationship identified between Callan and Ergun from the provided text."</data>
  <data key="d5">"none"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;DEVLIN, J.&quot;" target="&quot;BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Jacob Devlin authored BERT, a pre-trained model for language understanding."</data>
  <data key="d5">"author, model"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;DEVLIN, J.&quot;" target="&quot;HENDERSON, P.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both authors have contributed significantly to the field of deep learning and neural modeling."</data>
  <data key="d5">"influence, research community"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;DEVLIN, J.&quot;" target="&quot;KIM, M., RABELO, J., GOEBEL, R.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The work by Devlin is part of the broader advancements in NLP that influenced Kim's research on legal information extraction."</data>
  <data key="d5">"influence, research community"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;DEVLIN, J.&quot;" target="&quot;CHANG, M.-W.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Co-authors and contributors to BERT model development."</data>
  <data key="d5">"collaboration, NLP advancement"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;DEVLIN, J.&quot;" target="&quot;LEE, K.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Co-authors and contributors to BERT model development."</data>
  <data key="d5">"collaboration, NLP advancement"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;DEVLIN, J.&quot;" target="&quot;TOUTANOVA, K.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Co-authors and contributors to BERT model development."</data>
  <data key="d5">"collaboration, NLP advancement"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;GURURARANG, S., MARASOVIC, A., SWAYAMDIPATA, S., LO, K., BELTAGY, I., DOWNEY, D., SMITH, N. A.&quot;" target="&quot;KINGMA, D. P., BA, J.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The work by Gururarang et al. and Kingma et al. both contribute to the broader field of deep learning and neural modeling."</data>
  <data key="d5">"influence, research community"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;KIM, S.-W., GIL, J.-M.&quot;" target="&quot;FONSECA, E.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both authors are part of the broader academic community contributing to text understanding and NLP advancements."</data>
  <data key="d5">"field advancement, contemporaneity"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;MARELLI, M., MENINI, S., BARONIS, M., BENTIVOGLI, L., BERNARDI, R., ZAMPARELLI, R.&quot;" target="&quot;HENDERSON, P.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both groups are part of the broader academic community working on NLP and neural modeling."</data>
  <data key="d5">"field advancement, contemporaneity"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;SAN DIEGO, CA, USA&quot;">
  <data key="d3">49.0</data>
  <data key="d4">"ICLR 2015 took place in San Diego, CA, USA, indicating the location."&lt;SEP&gt;"The event ICLR 2015 took place in San Diego, CA, USA."&lt;SEP&gt;"The event ICLR 2015 was held in San Diego, CA, USA, indicating a specific location for the conference."&lt;SEP&gt;"The ICLR 2015 event took place in San Diego, CA, USA, making it the location of significant discussions and presentations."</data>
  <data key="d5">"conference location"&lt;SEP&gt;"conference venue, research location"&lt;SEP&gt;"location, event occurrence"&lt;SEP&gt;"location, organization"&lt;SEP&gt;&lt;"location, organization"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;Y. BENGIO AND Y. LECUN&quot;">
  <data key="d3">53.0</data>
  <data key="d4">"Y. Bengio and Y. LeCun are editors of proceedings from ICLR 2015, showing their involvement in organizing the event."&lt;SEP&gt;"Y. Bengio and Y. LeCun edited the ICLR 2015 proceedings, showing their role in organizing the event."&lt;SEP&gt;"Y. Bengio and Y. LeCun edited the proceedings of ICLR 2015."&lt;SEP&gt;"Y. Bengio and Y. LeCun edited the proceedings for ICLR 2015, indicating their involvement in organizing the event."</data>
  <data key="d5">"editorial roles, organization"&lt;SEP&gt;"editorial work, organization"&lt;SEP&gt;"editorship, organization involvement"&lt;SEP&gt;"organization leadership"&lt;SEP&gt;&lt;"editorial roles, organization"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;GLOVE: GLOBAL VECTORS FOR WORD REPRESENTATION&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The methodologies or discussions presented at ICLR 2015 might reference this work as part of ongoing research and development."&lt;SEP&gt;"The technology GloVe was presented at ICLR 2015, indicating its relevance in the research community."</data>
  <data key="d5">&lt;"technology presentation, event participation"&lt;SEP&gt;research relevance, word vector representations</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;MS MARCO: A HUMAN GENERATED MACHINE READING COMPREHENSION DATASET&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The methodologies or discussions presented at ICLR 2015 might reference this dataset as part of ongoing research and development."&lt;SEP&gt;"The technology MS MARCO was presented at ICLR 2015, indicating its relevance in the research community."</data>
  <data key="d5">&lt;"technology presentation, event participation"&lt;SEP&gt;research relevance, machine reading comprehension</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;MACHINE TRANSLATED MULTILINGUAL STS BENCHMARK DATASET&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Although not directly related to ICLR 2015, this dataset may be discussed or referenced in relation to other research presented at the conference."</data>
  <data key="d5">"research relevance, benchmarking"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;EFFICIENT ESTIMATION OF WORD REPRESENTATIONS IN VECTOR SPACE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The research presented in this paper might be discussed or referenced at the ICLR 2015 conference as part of machine learning advancements."</data>
  <data key="d5">"research relevance, vector space models"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;JNLP TEAM: DEEP LEARNING FOR LEGAL PROCESSING IN COLIEE 2020&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"While not directly related, the advancements and methodologies discussed by JNLP might be relevant to research presented at ICLR 2015."</data>
  <data key="d5">research relevance, legal processing</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;NIGAM@COLIEE-22: LEGAL CASE RETRIEVAL AND ENTAILMENT USING CASCADING OF LEXICAL AND SEMANTIC-BASED MODELS&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The advancements or methodologies discussed in this work might be relevant to research presented at ICLR 2015."</data>
  <data key="d5">research relevance, legal case retrieval</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;EXPLORING THE LIMITS OF TRANSFER LEARNING WITH A UNIFIED TEXT-TO-TEXT TRANSFORMER&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The methodologies or discussions presented at ICLR 2015 might reference this work as part of ongoing research and development."</data>
  <data key="d5">research relevance, transfer learning</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;THE ASSIN 2 SHARED TASK: A QUICK OVERVIEW&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Although not directly related, the advancements or methodologies discussed in this shared task might be relevant to research presented at ICLR 2015."</data>
  <data key="d5">research relevance, sentence similarity</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ICLR 2015&quot;" target="&quot;MAY 7-9, 2015&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"ICLR 2015 specifically took place on May 7-9, 2015."</data>
  <data key="d5">"date of conference, timing"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;MACHINE TRANSLATED MULTILINGUAL STS BENCHMARK DATASET&quot;" target="&quot;P. MAY&quot;">
  <data key="d3">50.0</data>
  <data key="d4">"P. May created the Machine translated multilingual STS benchmark dataset."&lt;SEP&gt;"P. May's work involves the use of datasets like the multilingual STS benchmark, indicating his research focus in this area."&lt;SEP&gt;"The machine translated multilingual STS benchmark dataset is associated with research by P. May."</data>
  <data key="d5">"dataset creation"&lt;SEP&gt;"dataset creator, research association"&lt;SEP&gt;&lt;"research focus, technology usage"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;T. MIKOLOV AND CO-AUTHORS&quot;" target="&quot;ICLR 2013&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"T. Mikolov and co-authors presented their work at ICLR 2013."</data>
  <data key="d5">"presentation at event"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;T. MIKOLOV AND CO-AUTHORS&quot;" target="&quot;GLOVE: GLOBAL VECTORS FOR WORD REPRESENTATION&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"GloVe was published by T. Mikolov and co-authors."</data>
  <data key="d5">"publication attribution"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;M. ARELLI ET AL.&quot;" target="&quot;SICK&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"M. Arelli et al. are researchers associated with the SICK project, which evaluates compositional distributional semantic models."</data>
  <data key="d5">"research association"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;P. MAY&quot;" target="&quot;M ARELLI ET AL.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"P. May is part of the research team that presented work at LREC'14, indicating his involvement and contribution to the field of computational linguistics."</data>
  <data key="d5">"team member, research collaboration"&lt;SEP&gt;&lt;"team member, research collaboration"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;H. NGUYEN ET AL.&quot;" target="&quot;JNLP TEAM&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"H. Nguyen et al. are authors of the JNLP team's paper."&lt;SEP&gt;"H. Nguyen et al. are part of the JNLP team."</data>
  <data key="d5">"member, organization"&lt;SEP&gt;"paper authorship"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;JNLP TEAM&quot;" target="&quot;TASK 1&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"JNLP focused on large articles and used text chunking for Task 1."&lt;SEP&gt;"JNLP focused on large articles using text chunking and a self-labeled approach for Task 1 of COLIEE 2021."</data>
  <data key="d5">&lt;"task focus, data handling techniques"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;JNLP TEAM&quot;" target="&quot;COLIEE 2020&quot;">
  <data key="d3">27.0</data>
  <data key="d4">"The JNLP team participated in COLIEE 2020 to showcase their research."&lt;SEP&gt;"The JNLP team participated in the COLIEE 2020 competition with their work."</data>
  <data key="d5">"participation, event involvement"&lt;SEP&gt;"participation, research contribution"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;P. ROSENBERG, X. SONG, J. GAO, S. TIWARY, R. MAJUMDER, AND L. DENG&quot;" target="&quot;MS MARCO&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The authors of MS MARCO are P. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng."</data>
  <data key="d5">"dataset creation"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;MS MARCO&quot;" target="&quot;REAL ET AL.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Real et al. created the MS MARCO dataset."</data>
  <data key="d5">"creation, data provider"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;S. K. NIGAM ET AL.&quot;" target="&quot;NIGAM@COLIEE-22&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"S. K. Nigam et al. are authors of the nigam@coliee-22 paper."</data>
  <data key="d5">"paper authorship"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;NIGAM@COLIEE-22&quot;" target="&quot;COLIEE 2022&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The nigam@coliee-22 group participated in COLIEE 2022 with their research."</data>
  <data key="d5">"participation, research contribution"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;C. RAFFEL ET AL.&quot;" target="&quot;EXPLORING THE LIMITS OF TRANSFER LEARNING WITH A UNIFIED TEXT-TO-TEXT TRANSFORMER&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"C. Raffel et al. are authors of this paper."</data>
  <data key="d5">"paper authorship"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;C. RAFFEL ET AL.&quot;" target="&quot;JOURNAL OF MACHINE LEARNING RESEARCH&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The authors contributed their work to the Journal of Machine Learning Research."</data>
  <data key="d5">"publication, academic contribution"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;C. RAFFEL ET AL.&quot;" target="&quot;PENGCHENG CENTER FOR ARTIFICIAL INTELLIGENCE RESEARCH&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"C. Raffel's research was conducted at or affiliated with Pengcheng Center for AI."</data>
  <data key="d5">"affiliation, research location"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ANDLIU, P. J.&quot;" target="&quot;REIMERS, N.&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"ANDLIU, P. J. and REIMERS, N. both contribute to the development of BERT-based language models and transfer learning."&lt;SEP&gt;"ANDLIU, P. J. and REIMERS, N., both are authors of several machine learning papers which involve deep learning techniques."</data>
  <data key="d5">"collaboration, research contribution"&lt;SEP&gt;&lt;"collaboration, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ANDLIU, P. J.&quot;" target="&quot;SOUZA, F.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"ANDLIU, P. J. and SOUZA, F. both contribute to the development of BERTimbau models for Brazilian Portuguese."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ANDLIU, P. J.&quot;" target="&quot;Y.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Y., ANDLIU, P. J., L I, and W. are authors contributing to research papers on transfer learning with a unified text-to-text transformer."</data>
  <data key="d5">"authorship, collaboration"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ANDLIU, P. J.&quot;" target="&quot;L I&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Y., ANDLIU, P. J., L I, and W. are authors contributing to research papers on transfer learning with a unified text-to-text transformer."</data>
  <data key="d5">"authorship, collaboration"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ANDLIU, P. J.&quot;" target="&quot;W.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Y., ANDLIU, P. J., L I, and W. are authors contributing to research papers on transfer learning with a unified text-to-text transformer."</data>
  <data key="d5">"authorship, collaboration"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ANDLIU, P. J.&quot;" target="&quot;JOURNAL OF MACHINE LEARNING RESEARCH&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"ANDLIU, P. J.'s paper 'Exploring the limits of transfer learning with a unified text-to-text transformer' is published in this journal."&lt;SEP&gt;"ANDLIU, P. J.'s paper is published in Journal of Machine Learning Research indicating a formal connection to the organization through publication."</data>
  <data key="d5">"publication, academic contribution"&lt;SEP&gt;&lt;"publication, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ANDLIU, P. J.&quot;" target="&quot;RODRIGUES , J.&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both RODRIGUES, J. and ANDLIU, P. J.'s work is related to the field of machine learning, indicating a potential professional connection in research areas."</data>
  <data key="d5">"field overlap"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REAL, L.&quot;" target="&quot;SOUZA, F.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"REAL, L. and Souza are involved in computational processing tasks related to Portuguese language but from different perspectives."</data>
  <data key="d5">"language processing, expertise overlap"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REAL, L.&quot;" target="&quot;FONSECA , E.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both REAL, L. and FONSECA, E. are authors of a paper together indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REAL, L.&quot;" target="&quot;OLIVEIRA , H. G.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both REAL, L. and OLIVEIRA, H. G. are authors of a paper together indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;FONSECA, E.&quot;" target="&quot;GUREVYCH, I.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"FONSECA, E. collaborates with GUREVYCH, I. on BERT-based language models and zero-shot evaluation benchmarks."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;FONSECA, E.&quot;" target="&quot;SANTOS, L.&quot;">
  <data key="d3">30.0</data>
  <data key="d4">"Collaborators on the ASSIN project for semantic similarity and textual inference."&lt;SEP&gt;"They are co-authors of ASSIN, working together on semantic similarity and textual inference systems."</data>
  <data key="d5">"co-authorship, evaluation"&lt;SEP&gt;"project collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;FONSECA, E.&quot;" target="&quot;INTERNATIONAL CONFERENCE ON COMPUTATIONAL PROCESSING OF THE PORTUGUESE LANGUAGE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"E. FONSECA and H. G. OLIVEIRA presented 'The assin 2 shared task: a quick overview' at this conference."</data>
  <data key="d5">&lt;"presentation, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;FONSECA, E.&quot;" target="&quot;CRISCUOLO, M.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators on the ASSIN project for semantic similarity and textual inference."</data>
  <data key="d5">"project collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;FONSECA, E.&quot;" target="&quot;ALUISIO, S.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators on the ASSIN project for semantic similarity and textual inference."</data>
  <data key="d5">"project collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;OLIVEIRA, H. G.&quot;" target="&quot;THAKUR, N.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"OLIVEIRA, H. G. and THAKUR, N. both contribute to the field of information retrieval through their respective projects."</data>
  <data key="d5">"research collaboration, information retrieval"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REIMERS, N.&quot;" target="&quot;REDBEDDY, T.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"REIMERS, N. and REDDY, T. collaborate on BERT-based language models and knowledge distillation techniques."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REIMERS, N.&quot;" target="&quot;REDDY, T.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"REIMERS, N. and REDDY, T. collaborate on BERT-based language models and knowledge distillation techniques."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REIMERS, N.&quot;" target="&quot;GUREVYCH, I.&quot;">
  <data key="d3">26.0</data>
  <data key="d4">"Both Reimers and Gurevych are authors on 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models' indicating collaboration in research."&lt;SEP&gt;"Both Reimers and Gurevych have multiple co-authorships together, indicating a strong collaborative relationship in research."&lt;SEP&gt;"REIMERS, N. and GUREVYCH, I., have co-authored multiple papers including 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks' and 'Making monolingual sentence embeddings multilingual using knowledge distillation', indicating strong collaboration."</data>
  <data key="d5">"collaboration, co-authorship"&lt;SEP&gt;&lt;"collaboration, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REIMERS, N.&quot;" target="&quot;ROBERTSON, S.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both are active contributors to the field of information retrieval and machine learning, indicating potential collaboration or interaction based on their research topics."</data>
  <data key="d5">&lt;"field contribution, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REIMERS, N.&quot;" target="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"N. REIMERS and I. GUREVYCH published 'Making monolingual sentence embeddings multilingual using knowledge distillation' in this association."</data>
  <data key="d5">&lt;"publication, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REIMERS, N.&quot;" target="&quot;THAKUR, N.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Thakur and Reimers are co-authors on 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REIMERS, N.&quot;" target="&quot;RÜCKLÉ, A.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Reimers and Rücklé are authors on 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REIMERS, N.&quot;" target="&quot;SRIVASTAVA, A.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Reimers and Srivastava are authors on 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;GUREVYCH, I.&quot;" target="&quot;THAKUR, N.&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"Both Thakur and Gurevych are authors on 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models' indicating collaboration in research."&lt;SEP&gt;"THAKUR, N. and GUREVYCH, I. both contribute to the BEIR benchmark for zero-shot evaluation of information retrieval models."</data>
  <data key="d5">"collaboration, co-authorship"&lt;SEP&gt;"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;GUREVYCH, I.&quot;" target="&quot;SRIVASTAVA, A.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"SRIVASTAVA, A. and GUREVYCH, I. collaborate on BERT-based language models and zero-shot evaluation benchmarks like BEIR."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ROBERTSON, S.&quot;" target="&quot;ZARAGOZA, H.&quot;">
  <data key="d3">25.0</data>
  <data key="d4">"ROBERTSON, S. and ZARAGOZA, H. provide foundational insights into information retrieval through their work on the Probabilistic Relevance Framework."&lt;SEP&gt;"ROBERTSON, S. and ZARAGOZA, H., co-authored 'The Probabilistic Relevance Framework: BM25 and Beyond', indicating strong collaboration on information retrieval research."</data>
  <data key="d5">"foundational research, collaboration"&lt;SEP&gt;&lt;"collaboration, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ROBERTSON, S.&quot;" target="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"S. ROBERTSON and H. ZARAGOZA published 'The Probabilistic Relevance Framework: BM25 and Beyond' in this association."</data>
  <data key="d5">&lt;"publication, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;RODRIGUES, J.&quot;" target="&quot;GOMES, L.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"RODRIGUES, J. and GOMES, L. collaborate on linguistic advancements for the Portuguese language."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;RODRIGUES, J.&quot;" target="&quot;CARDOSO, H. L.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"RODRIGUES, J. and CARDOSO, H. L., along with other contributors, worked on 'Advancing Neural Encoding of Portuguese with Transformer Albertina PT -*', suggesting a collaborative effort in advancing deep learning for Portuguese."</data>
  <data key="d5">&lt;"collaboration, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;SILVA, J.&quot;" target="&quot;BRANCO, A.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SILVA, J. and BRANCO, A. both contribute to neural encoding of Portuguese with Transformer Albertina PT."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;SANTOS, R.&quot;" target="&quot;DE ALENCAR LOTUFO, R.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SANTOS, R. and DE ALENCAR LOTUFO, R. collaborate on Portuguese NER tasks using BERT-CRF methods."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;SANTOS, R.&quot;" target="&quot;CARDOSO, H. L.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"SANTOS, R. and CARDOSO, H. L. collaborate on Portuguese NER tasks using BERT-CRF methods."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;CARDOSO, H. L.&quot;" target="&quot;SOUZA, F.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"CARDOSO, H. L. and SOUZA, F. both contribute to Portuguese language models but from different aspects of the task."</data>
  <data key="d5">"language modeling, expertise overlap"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;OS´ORIO, T.&quot;" target="&quot;DE ALENCAR LOTUFO, R.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"OS´ORIO, T. and DE ALENCAR LOTUFO, R. collaborate on Portuguese language models and neural encodings for the Portuguese language."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;RONG, X.&quot;" target="&quot;SOUZA, F.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"RONG, X. and SOUZA, F. both contribute to creating BERTimbau models for Brazilian Portuguese."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;SOUZA, F.&quot;" target="&quot;LOTUFO, R.&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"Both Souza and Lotufo are co-authors on multiple papers indicating collaboration."&lt;SEP&gt;"SOUZA, F. and LOTUFO, R., along with N OGUEIRA, contributed to multiple papers involving BERT models for Portuguese."</data>
  <data key="d5">"collaboration, co-authorship"&lt;SEP&gt;&lt;"collaboration, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;SOUZA, F.&quot;" target="&quot;NOGUEIRA, R.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Souza and Nogueira are co-authors on multiple papers indicating collaboration."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;THAKUR, N.&quot;" target="&quot;RUCKLE, A.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"RUCKLE, A. collaborates with THAKUR, N. on the BEIR benchmark for zero-shot evaluation of information retrieval models."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;THAKUR, N.&quot;" target="&quot;REDBEDDY, T.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"THAKUR, N. and REDDY, T. collaborate on BERT-based language models and knowledge distillation techniques."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;THAKUR, N.&quot;" target="&quot;REDDY, T.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"THAKUR, N. and REDDY, T. collaborate on BERT-based language models and knowledge distillation techniques."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;THAKUR, N.&quot;" target="&quot;THIRTY-FIFTH CONFERENCE ON NEURAL INFORMATION PROCESSING SYSTEMS DATASETS AND BENCHMARKS TRACK (ROUND 2)&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"N. THAKUR, A. R ¨UCKL ´E, A. SRIVASTAVA, and I. GUREVYCH presented 'BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models' at this conference."</data>
  <data key="d5">&lt;"presentation, authorship"&gt;</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VASWANI, A.&quot;" target="&quot;HAZEEER, N.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Vaswani and Hazeeer are authors on 'Attention is all you need' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VASWANI, A.&quot;" target="&quot;PARMA, N.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Vaswani and Parma are authors on 'Attention is all you need' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VASWANI, A.&quot;" target="&quot;UZUKI, J.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Vaswani and Uzuki are authors on 'Attention is all you need' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VASWANI, A.&quot;" target="&quot;JONES, L.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Vaswani and Jones are authors on 'Attention is all you need' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VASWANI, A.&quot;" target="&quot;GOMEZ, A. N.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Vaswani and Gomez are authors on 'Attention is all you need' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VASWANI, A.&quot;" target="&quot;KAISER, L.U.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Vaswani and Kaiser are authors on 'Attention is all you need' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VASWANI, A.&quot;" target="&quot;POLOSUKHIN, I.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both Vaswani and Polosukhin are authors on 'Attention is all you need' indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;HAZEER, N.&quot;" target="&quot;OLIPHANT, T. E.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"OLIPHANT, T. E. and HAZEER, N. collaborate on developing numerical and array programming tools for Python libraries like NumPy."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VIRTANEN, P.&quot;" target="&quot;HAZEER, R.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"VIRTANEN, P. and HAZEER, R. collaborate on developing numerical and array programming tools for Python libraries like NumPy."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VIRTANEN, P.&quot;" target="&quot;HABERLAND, M.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"HABERLAND, M. collaborates with Virtanen on developing scientific computing libraries like NumPy."</data>
  <data key="d5">"collaboration, research contribution"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;VIRTANEN, P.&quot;" target="&quot;GOMMERS, R.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both Virtanen and Gommers work closely together on multiple projects related to scientific computing in Python indicating collaboration."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;OLIPHANT, T. E.&quot;" target="&quot;REDDY, T.&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both Oliphant and Reddy are creators of key libraries for scientific computing (NumPy and SciPy), indicating a professional connection in the field."</data>
  <data key="d5">"field overlap"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;REDDY, T.&quot;" target="&quot;COURNAPEAU, D.&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both Cournapeau and Reddy are creators of key libraries for scientific computing (Scikit-learn and SciPy), indicating a professional connection in the field."</data>
  <data key="d5">"field overlap"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;N., K AISER &quot;" target="&quot;ATTENTION IS ALL YOU NEED PAPER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N., K AISER contributed to the development of the 'Attention is all you need' paper."</data>
  <data key="d5">&lt;"contribution, paper development"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;N., K AISER &quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N., K AISER is part of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;N., K AISER &quot;" target="&quot;CURRAN ASSOCIATES, INC.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N. Kaiser's work, 'Attention is all you need,' was published by Curran Associates, Inc."</data>
  <data key="d5">"publication, contribution"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;L.U. ANDPOLOSUKHIN , I.&quot;" target="&quot;ATTENTION IS ALL YOU NEED PAPER&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"L.U. ANDPOLOSUKHIN, I. contributed to the development of the 'Attention is all you need' paper."</data>
  <data key="d5">&lt;"contribution, paper development"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;L.U. ANDPOLOSUKHIN , I.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"I. POLAT and L. U. ANDPOLOSUKHIN are part of the SciPy 1.0 contributors group."&lt;SEP&gt;"L.U. ANDPOLOSUKHIN and I. are part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;V IRTANEN , P.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"P. V IRTANEN is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;V IRTANEN , P.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">15.0</data>
  <data key="d4">"P. V IRTANEN is part of the broader community contributing to Python libraries like SciPy and NumPy."&lt;SEP&gt;"P. V IRTANEN is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"community contribution, shared research field"&lt;SEP&gt;&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;V IRTANEN , P.&quot;" target="&quot;W ANG, K.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"P. V IRTANEN and K. W ANG are contributors to different projects but share a field of research in Python libraries."</data>
  <data key="d5">&lt;"research collaboration, cross-field interaction"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;V IRTANEN , P.&quot;" target="&quot;SCIPY1.0 C ONTRIBUTORS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"P. Virtanen is a member of the SciPy 1.0 contributors group."</data>
  <data key="d5">"association, membership"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;G OMMERS , R.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"R. G OMMERS is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;G OMMERS , R.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"R. G OMMERS is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;O LIPHANT , T. E.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"T. E. O LIPHANT is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;O LIPHANT , T. E.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"T. E. O LIPHANT is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;O LIPHANT , T. E.&quot;" target="&quot;W ANG, K.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"T. E. O LIPHANT and K. W ANG are part of the broader scientific community contributing to Python libraries."</data>
  <data key="d5">&lt;"community contribution, shared research field"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;H ABERLAND , M.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"M. H ABERLAND is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;H ABERLAND , M.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"M. H ABERLAND is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;R EDDY , T.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"T. R EDDY is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;R EDDY , T.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"T. R EDDY is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;C OURNAPEAU , D.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"D. C OURNAPEAU is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;C OURNAPEAU , D.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"D. C OURNAPEAU is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;BUROVSKI , E.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. B UROVSKI is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;P ETERSON , P.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"P. P ETERSON is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;P ETERSON , P.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"P. P ETERSON is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;W ECKESSER , W.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"W. W ECKESSER is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;W ECKESSER , W.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"W. W ECKESSER is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;B RIGHT , J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"J. BRIGHT is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;B RETT , M.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"M. B RETT is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;B RETT , M.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"M. B RETT is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;WILSON , J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"J. W ILSON is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;M ILLMAN , K. J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"K. J. M ILLMAN is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;M ILLMAN , K. J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"K. J. M ILLMAN is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;M AYOROV , N.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N. M AYOROV is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;M AYOROV , N.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N. M AYOROV is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;N ELSON , A. R. J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"A. R. J. N ELSON is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;N ELSON , A. R. J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"A. R. J. N ELSON is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;J ONES , E.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. J ONES is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;J ONES , E.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. J ONES is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;K ERN, R.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"R. K ERN is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;K ERN, R.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"R. K ERN is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;L ARSON , E.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. L ARSON is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;L ARSON , E.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. L ARSON is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;C AREY , C. J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"C. J. C AREY is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;C AREY , C. J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"C. J. C AREY is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;P OLAT,˙I.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"I. P OLAT is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;P OLAT,˙I.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"I. P OLAT is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;F ENG, Y.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Y. F ENG is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;F ENG, Y.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Y. F ENG is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;M OORE , E. W.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. W. M OORE is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;M OORE , E. W.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. W. M OORE is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;V ANDER PLAS, J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"J. V ANDER PLAS is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;V ANDER PLAS, J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"J. V ANDER PLAS is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;L AXALDE , D.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"D. L AXALDE is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;L AXALDE , D.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"D. L AXALDE is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;C IMRMAN , R.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"R. C IMRMAN is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;C IMRMAN , R.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"R. C IMRMAN is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;Q UINTERO , E. A.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. A. QUINTERO is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;Q UINTERO , E. A.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. A. Q UINTERO is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;RIBEIRO , A. H.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"A. H. RIBEIRO is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;RIBEIRO , A. H.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"A. H. RIBEIRO is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;V ANMULBREGT , P.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"P. V ANMULBREGT is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;V ANMULBREGT , P.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"P. V ANMULBREGT is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;SCIPY 1.0&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The SCIPY1.0 Contributors are involved in developing and maintaining SciPy 1.0."</data>
  <data key="d5">&lt;"development, maintenance"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;B UROVSKI , E.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"E. B UROVSKI is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;V AN DER WALT, S. J.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"S. J. V AN DER W ALT is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;W ILSON , J.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"J. W ILSON is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;P ERKTOLD , J.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"J. P ERKTOLD is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;H ENRIKSEN , I.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"I. H ENRIKSEN is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;H ARRIS , C. R.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"C. R. H ARRIS is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;A RCHIBALD , A. M.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"A. M. A RCHIBALD is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;P EDREGOSA , F.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"F. P EDREGOSA is a member of the larger group that contributed to SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;W ANG, K.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"K. W ANG and others are part of a broader team contributing to various projects including SciPy 1.0."</data>
  <data key="d5">"collaboration, contribution"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;R EIMERS , N.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"N. R EIMERS and others are part of a broader team contributing to various projects including SciPy 1.0."</data>
  <data key="d5">"collaboration, contribution"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;GUREVYCH , I.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"I. GUREVYCH and others are part of a broader team contributing to various projects including SciPy 1.0."</data>
  <data key="d5">"collaboration, contribution"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;K. WANG &quot;">
  <data key="d3">16.0</data>
  <data key="d4">"K. WANG is part of the larger group that contributed to various projects including SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;N. R EIMERS &quot;">
  <data key="d3">16.0</data>
  <data key="d4">"N. R EIMERS is part of the larger group that contributed to various projects including SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;AND SCIPY1.0 C ONTRIBUTORS &quot;" target="&quot;I. GUREVYCH &quot;">
  <data key="d3">16.0</data>
  <data key="d4">"I. GUREVYCH is part of the larger group that contributed to various projects including SciPy 1.0."</data>
  <data key="d5">"contribution, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;CURRAN ASSOCIATES, INC.&quot;" target="&quot;ATTENTION IS ALL YOU NEED PAPER&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Curran Associates, Inc. published the 'Attention is all you need' paper."</data>
  <data key="d5">&lt;"publication, distribution"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;CURRAN ASSOCIATES, INC.&quot;" target="&quot;SCIPY1.0 C ONTRIBUTORS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The SciPy 1.0 contributors' work, 'SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python,' was published by Curran Associates, Inc."</data>
  <data key="d5">"publication, contribution"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;W ANG, K.&quot;" target="&quot;GPL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"K. W ANG is a co-author of the GPL paper."</data>
  <data key="d5">&lt;"co-authorship, paper development"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;R EIMERS , N.&quot;" target="&quot;GPL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N. R EIMERS contributed to the development of the GPL paper."</data>
  <data key="d5">&lt;"contribution, paper development"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;GUREVYCH , I.&quot;" target="&quot;GPL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"I. GUREVYCH is involved in developing the GPL for unsupervised domain adaptation."</data>
  <data key="d5">&lt;"co-authorship, research involvement"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;GUREVYCH , I.&quot;" target="&quot;REIMERS , N.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Both REIMERS, N. and GUREVYCH, I. are co-authors of multiple papers indicating a strong collaborative relationship in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;THAKUR , N.&quot;" target="&quot;GPL&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"N. THAKUR contributed to the development of the GPL paper for unsupervised domain adaptation."</data>
  <data key="d5">&lt;"contribution, research involvement"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;TSDAE ARCHITECTURE&quot;" target="&quot;SYSTEM ARCHITECTURE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both TSDAE Architecture and System Architecture are part of a series of training tasks and system development, indicating they are interrelated processes."</data>
  <data key="d5">&lt;"training, architecture"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;TSDAE ARCHITECTURE&quot;" target="&quot;FIGURE 3.4 AND FIGURE 3.5&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The TSDAE architecture is illustrated in Figures 3.4 and 3.5, showing its structure for sequential data processing." "</data>
  <data key="d5">"tsdae architecture, denoising autoencoder"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;FINE-TUNING SBERT&quot;" target="&quot;METADATA KNOWLEDGE DISTILLATION IDEOLOGY&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both Fine-Tuning SBERT and Metadata Knowledge Distillation Ideology are related to model training processes, suggesting a link in the context of fine-tuning and knowledge distillation techniques."&lt;SEP&gt;"These techniques are both involved in model fine-tuning and knowledge distillation processes, indicating a relationship in the document."&lt;</data>
  <data key="d5">"fine-tuning, knowledge transfer"&lt;SEP&gt;"training technique, knowledge transfer"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;FINE-TUNING SBERT&quot;" target="&quot;SYSTEM ARCHITECTURE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Fine-Tuning SBERT is a part of the System Architecture described in section 4.1, showing their direct relationship as components within the same system."</data>
  <data key="d5">&lt;"architecture, fine-tuning"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;METADATA KNOWLEDGE DISTILLATION IDEOLOGY&quot;" target="&quot;MULTILINGUAL KNOWLEDGE DISTILLATION PROCESS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both involve knowledge distillation but on different scales (multilingual vs single language), suggesting a conceptual link in their methodologies or principles."</data>
  <data key="d5">&lt;"knowledge transfer, multilingualism"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;SEARCH SYSTEM EVALUATION – SEARCH METRIC - MODELS V0&quot;" target="&quot;SEARCH SYSTEM EVALUATION – SEARCH METRIC - MODELS V1&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Both evaluations are related to search metrics for different model versions, indicating a relationship in the context of comparing model performance."&lt;</data>
  <data key="d5">"model comparison, evaluation"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;3.4 TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE&quot;" target="&quot;3.5 T5 DIAGRAM&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Both are parts of the series of training tasks and system architectures described in the text, indicating their interrelated roles."&lt;SEP&gt;"Both sections refer to related techniques in the document, possibly discussing similar models or architectures."&lt;</data>
  <data key="d5">"related techniques, architecture description"&lt;SEP&gt;&lt;"architecture, model training"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;3.4 TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE&quot;" target="&quot;3.6 GENQ&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both are parts of a series of training tasks and techniques used in the development process, indicating their interrelated roles."</data>
  <data key="d5">&lt;"training tasks, model development"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;3.4 TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE&quot;" target="&quot;3.8 FINE-TUNING SBERT&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both are parts of a series of training tasks and techniques used in the development process, indicating their interrelated roles."</data>
  <data key="d5">&lt;"training tasks, model development"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;3.4 TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSDAE) ARCHITECTURE&quot;" target="&quot;3.9 MULTILINGUAL KNOWLEDGE DISTILLATION PROCESS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both are parts of the series of training tasks and techniques used in the development process, indicating their interrelated roles."</data>
  <data key="d5">&lt;"training tasks, model development"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;STS&quot;" target="&quot;NLI&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"Both NLI and STS are tasks that involve analyzing the relationship between sentences, with NLI focusing on entailment and contradiction while STS measures similarity."::&lt;SEP&gt;"Natural Language Inference (NLI) and Semantic Textual Similarity (STS) are both NLP challenges that models must solve to understand sentence relationships."</data>
  <data key="d5">"task interrelation, model applicability"&lt;SEP&gt;"task interrelation, semantic analysis"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;FINE-TUNING&quot;" target="&quot;PRE-TRAINING&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Pre-training and fine-tuning are sequential phases in the BERT training process, with pre-training laying the groundwork for fine-tuning."</data>
  <data key="d5">"training sequence, phase progression"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;FINE-TUNING&quot;" target="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Fine-tuning, similar to the Transformer-based Sequential Denoising Auto-Encoder, involves re-training a model with data from a different domain but focuses on adding layers and training epochs rather than unsupervised learning."</data>
  <data key="d5">7</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;PASSAGES&quot;" target="&quot;QUERY GENERATION STEP&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Query Generation step generates queries based on multiple passages."</data>
  <data key="d5">&lt;"query generation, passage source"</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;QUERY GENERATION STEP&quot;" target="&quot;NEGATIVE MINING STEP&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Query Generation step generates positive and negative passages which are then mined in the Negative Mining step."|&lt;&gt;"query generation, passage mining"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;OVGU&quot;" target="&quot;JNLP&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both teams participated in COLIEE 2021 and used different approaches for task completion: JNLP focused on text chunking while OvGU used TF-IDF vectorization combined with Sentence-BERT embeddings."</data>
  <data key="d5">"comparative approach, competition dynamics"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;OVGU&quot;" target="&quot;NIGAM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both teams participated in the same competition and used different approaches but both contributed to advancements in NLP techniques for legal information retrieval."</data>
  <data key="d5">"competition dynamics, model comparison"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;OVGU&quot;" target="&quot;TASK 1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Team OvGU participated in Task 1 of COLIEE 2021 with a two-stage TF-IDF vectorization combined with Sentence-BERT embeddings."</data>
  <data key="d5">&lt;"task participation, approach comparison"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;LESSE&quot;" target="&quot;TASK 3&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Legal Semantic Search Engine (LeSSE) could be relevant to the tasks in COLIEE 2021 focusing on legal information retrieval and entailment."</data>
  <data key="d5">&lt;"task relevance, system application"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;LESSE&quot;" target="&quot;JNLP&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both JNLP and LeSSE are part of the broader advancements in NLP techniques for legal information retrieval but operate on different systems."</data>
  <data key="d5">&lt;"system comparison, NLP research"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;LESSE&quot;" target="&quot;PORTUGUESE CONSUMER LAW&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"LeSSE aims to make Portuguese Consumer Law more accessible and understandable to citizens through semantic search."</data>
  <data key="d5">"goal, accessibility"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;LESSE&quot;" target="&quot;BERTIMBAU BASE (BERT -BASE)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERTimbau Base is used in the Legal Semantic Search Engine for processing Brazilian Portuguese text."</data>
  <data key="d5">&lt;"text processing, BERT model"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;ALBERTINA PT-PT&quot;" target="&quot;FCUL&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"FCUL developed the PT-PT version of Albertina, contributing to the research in natural language and speech through the NLX group."</data>
  <data key="d5">"development collaboration, research focus"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT-PT&quot;" target="&quot;FCUL - NLX – NATURAL LANGUAGE AND SPEECH GROUP&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Albertina PT-PT is a model developed by FCUL - NLX – Natural Language and Speech Group."</data>
  <data key="d5">&lt;"development, collaboration"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT-PT&quot;" target="&quot;ALBERTINA PT-BR&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Albertina PT-PT and Albertina PT-BR are related as they share the same core technology but cater to different language variants of Portuguese."</data>
  <data key="d5">&lt;"model relationship, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;ALBERTINA PT-PT&quot;" target="&quot;PORTUGUESE JURISPRUDENCE&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Albertina is a model specifically designed for Portuguese jurisprudence and European Portuguese language."</data>
  <data key="d5">"specialization, adaptation"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;ALBERTINA PT-BR&quot;" target="&quot;FEUP - LABORATÓRIO DE INTELIGÊNCIA ARTIFICIAL E CIÊNCIA DE COMPUTADORES&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Albertina PT-BR is a model developed by FEUP - Laboratório de Inteligência Artificial e Ciência de Computadores."</data>
  <data key="d5">&lt;"development, collaboration"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-METAKD-V0&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"These two models are part of the same research project and likely share some components or methodologies."</data>
  <data key="d5">"same project, shared methodology"</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-METAKD-V0&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"These two models are part of the same research project and likely share some components or methodologies."</data>
  <data key="d5">"same project, shared methodology"</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V0&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-METAKD-V1&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"These models are variations of each other, indicating ongoing development within the same project."</data>
  <data key="d5">"versioning, research evolution"</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;MODEL TRAINING&quot;" target="&quot;EMBEDDINGS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Embeddings are adjusted during the training process to optimize model performance."</data>
  <data key="d5">&lt;"training, embedding adjustment"&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;BERTIMBAU LARGE&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both models were fine-tuned but on different techniques (MLM vs. MLM variant), comparing their performance in STS task."&lt;/|&gt;"model comparison, technique variation"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;BERTIMBAU LARGE&quot;" target="&quot;ELASTICSEARCH INDEX&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"BERTimbau large is used to generate sentence embeddings that are stored in the ElasticSearch index for semantic search operations."</data>
  <data key="d5">"embedding generation, storage mechanism"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;BERTIMBAU LARGE&quot;" target="&quot;TABLE 6.1: MLM AVERAGE LOSS FOR LEGAL DOCUMENTS IN THE TEST SET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The table includes BERTimbau large as a baseline for performance comparison."</data>
  <data key="d5">"baseline comparison, evaluation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;" target="&quot;BATCH SIZE OF 2&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"A batch size of 2 was used in the fine-tuning stage for this variant, affecting its training process and loss calculation."</data>
  <data key="d5">"training parameter, model adaptation"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;" target="&quot;EVALUATION SPLIT&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The evaluation split was used to select the best model performance for this variant during the fine-tuning stage."</data>
  <data key="d5">"model selection, performance evaluation"</data>
  <data key="d6">chunk-baa53bb86fb07d800e53012d99a5e681</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM&quot;" target="&quot;TABLE 6.1: MLM AVERAGE LOSS FOR LEGAL DOCUMENTS IN THE TEST SET&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The table also includes stjiris/bert-large-portuguese-cased-legal-mlm, another model variant evaluated on the same task."</data>
  <data key="d5">"performance comparison, evaluation"</data>
  <data key="d6">chunk-41c849f637d2cf0401100a6cc855d2d2</data>
</edge>
<edge source="&quot;BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;" target="&quot;SUPERVISED LEARNING OF UNIVERSAL SENTENCE REPRESENTATIONS FROM NATURAL LANGUAGE INFERENCE DATA&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both papers focus on using NLP and machine learning techniques to improve language understanding, indicating a shared research interest."</data>
  <data key="d5">"research interest, methodology"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;HENDERSON, P.&quot;" target="&quot;KRAS, M. S.&quot;">
  <data key="d3">32.0</data>
  <data key="d4">"Collaborators in learning responsible data filtering from law and a 256GB open-source legal dataset."&lt;SEP&gt;"They collaborate on projects like Pile of Law to learn from legal data using responsible data filtering techniques."</data>
  <data key="d5">"collaboration, data filtering"&lt;SEP&gt;"collaboration, learning"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;HENDERSON, P.&quot;" target="&quot;ZHENG, L.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators in learning responsible data filtering from law and a 256GB open-source legal dataset."</data>
  <data key="d5">"collaboration, data filtering"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;HENDERSON, P.&quot;" target="&quot;GUHA, N.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators in learning responsible data filtering from law and a 256GB open-source legal dataset."&lt;SEP&gt;"Collaborators in learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d5">"collaboration, data filtering"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;ICLR 2013&quot;" target="&quot;T. MIKOLOV AND OTHERS&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"T. Mikolov and colleagues presented their work at ICLR 2013."</data>
  <data key="d5">"presentation, conference attendance"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ARTIFICIAL INTELLIGENCE&quot;" target="&quot;SUPERVISORS: PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOÃO DIAS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Artificial Intelligence is the core concept supervised by the thesis advisors."</data>
  <data key="d5">&lt;supervision, guidance, technology development"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;JURISPRUDENCE&quot;" target="&quot;SUPERVISORS: PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOÃO DIAS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Jurisprudence is a key domain of study supervised by the thesis advisors."</data>
  <data key="d5">&lt;legal expertise, guidance"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;THE CORPUS&quot;" target="&quot;DATA PROCESSING&quot;">
  <data key="d3">23.0</data>
  <data key="d4">"Data Processing is used to prepare the The Corpus before it can be utilized by the Semantic Search System."&lt;SEP&gt;"The Corpus is processed using Data Processing techniques before being used in Semantic Search System tasks."&lt;SEP&gt;"The Corpus is a location where Data Processing occurs, as it's mentioned that data processing happens to create or maintain this corpus."&lt;</data>
  <data key="d5">"data preparation, preprocessing"&lt;SEP&gt;"data source, process step"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;THE CORPUS&quot;" target="&quot;DATA PROCESSING 47&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Corpus and Data Processing are directly related, as the former serves as input for the latter process."&lt;</data>
  <data key="d5">"data source, processing step"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;THE CORPUS&quot;" target="&quot;PURELY SEMANTIC SEARCH SYSTEM 50&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Corpus is used as input for the Purely Semantic Search System, which processes it to generate search results."&lt;</data>
  <data key="d5">"input data, processing step"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;LEXICAL-FIRST SEARCH SYSTEM&quot;" target="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"Both search systems use similar underlying technologies but with different integration methods for improved search performance."&lt;SEP&gt;"The Lexical-First Search System and Lexical + Semantic Search System are related in that the latter combines approaches from both systems."&lt;SEP&gt;"The Lexical-First Search System and the Lexical + Semantic Search System are related in that they both use similar components but with different integration approaches to improve search results."</data>
  <data key="d5">"approach combination, system enhancement"&lt;SEP&gt;"search system comparison, technology evolution"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8&lt;SEP&gt;chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;CONTRIBUTIONS&quot;" target="&quot;FUTURE WORK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Future Work outlines areas of further research based on existing contributions from the contributions section."</data>
  <data key="d5">&lt;"research, future work"</data>
  <data key="d6">chunk-c905a9c26d22cc769d6639180aabeda9</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL EXPLANATION&quot;" target="&quot;SCALED DOT-PRODUCT ATTENTION MECHANISM&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The Transformer model uses these attention mechanisms to process sequential data efficiently, enabling parallelization of tasks and handling long-range dependencies effectively."</data>
  <data key="d5">"attention mechanisms, efficient processing"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;TRANSFORMER MODEL EXPLANATION&quot;" target="&quot;MULTI-HEAD ATTENTION MECHANISM&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The Transformer model uses this mechanism to handle long-range dependencies and parallelize tasks effectively, similar to how multi-head attention works in the context of Transformers."</data>
  <data key="d5">&lt;"transformer mechanisms, multi-head attention"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;MULTI-HEAD ATTENTION MECHANISM&quot;" target="&quot;LEXRANK ALGORITHM&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"While LexRank is a graph-based ranking algorithm for summarization, attention mechanisms in models like Transformer provide similar functionalities by focusing on different parts of the input."</data>
  <data key="d5">&lt;"ranking algorithms, attention mechanisms"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;VECTOR SPACE WITH QUERY AND SENTENCE EMBEDDINGS&quot;" target="&quot;BI-ENCODER METHOD&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both involve vector space representations for comparison or analysis, with the Bi-Encoder specifically comparing pairs of sentences using their embeddings."</data>
  <data key="d5">&lt;"vector spaces, sentence comparisons"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;T5 MODEL DIAGRAM&quot;" target="&quot;GENQ SYSTEM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both are complex systems with intricate architectures designed for specific NLP tasks; T5 is a sequence-to-sequence model while GenQ might involve similar architectural considerations."</data>
  <data key="d5">&lt;"sequence models, generation systems"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;LEXICAL + SEMANTIC SEARCH SYSTEM RETRIEVAL METHOD&quot;" target="&quot;COSINE SIMILARITY CALCULATION&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Cosine Similarity Calculation is part of the Lexical + Semantic Search System Retrieval Method, which uses it for measuring similarity between vectors."</data>
  <data key="d5">"method integration, similarity measurement"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;4.1 SYSTEM ARCHITECTURE&quot;" target="&quot;5.1 TRAINING TASKS OVERVIEW&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both provide overviews of important aspects of the system and its development, indicating their interrelated roles."</data>
  <data key="d5">&lt;"system architecture, training overview"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;4.2 LEXICAL-FIRST SEARCH SYSTEM RETRIEVAL METHOD&quot;" target="&quot;4.3 LEXICAL + SEMANTIC SEARCH SYSTEM RETRIEVAL METHOD&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both are specific retrieval methods used in search systems, indicating their interrelated roles."</data>
  <data key="d5">&lt;"search system, retrieval methods"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;5.1 TRAINING TASKS OVERVIEW&quot;" target="&quot;6.1 EVALUATION ARCHITECTURE&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both provide overviews of important aspects of the development process and evaluation, indicating their interrelated roles."</data>
  <data key="d5">&lt;"development overview, evaluation"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;5.2 MASKED LANGUAGE MODELING (MLM) TRAINING LOSS&quot;" target="&quot;5.3 TSDAE TRAINING LOSS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both are specific training loss functions used in the training process, indicating their interrelated roles."</data>
  <data key="d5">&lt;"training losses, model development"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;LEXICAL TECHNIQUES&quot;" target="&quot;SEMANTIC TECHNIQUES&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Lexical techniques and semantic techniques both aim to retrieve relevant information but differ in their approach."&lt;SEP&gt;"Lexical techniques often contrast with semantic techniques in terms of their approach to query processing but can complement each other."</data>
  <data key="d5">"query processing methods"&lt;SEP&gt;"technique comparison, retrieval methods"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;INESC-ID LISBOA IRIS PROJECT&quot;" target="&quot;NL NATURAL LANGUAGE PROCESSING (NLP)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The INESC-ID Lisboa IRIS project is aimed at improving NLP techniques for court decisions."</data>
  <data key="d5">"technological advancement, legal support"</data>
  <data key="d6">chunk-4ae3d638fab35f846eff55a5b4f9d288</data>
</edge>
<edge source="&quot;OKAPI BM25 (BM25)&quot;" target="&quot;INFORMATION RETRIEVAL (IR)&quot;">
  <data key="d3">36.0</data>
  <data key="d4">"BM25 is a key algorithm used in Information Retrieval to estimate the relevance of documents given a query. It's widely used and performs well in various benchmarks."&lt;SEP&gt;"BM25 is a specific approach in the field of Information Retrieval used by search engines like Elasticsearch for estimating document relevance given a query."</data>
  <data key="d5">"application, methodology"&lt;SEP&gt;"relevance, document search"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;OKAPI BM25 (BM25)&quot;" target="&quot;TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF) &quot;">
  <data key="d3">18.0</data>
  <data key="d4">"BM25 relies on TF-IDF for its ranking function, integrating term frequency and document frequency to estimate relevance."</data>
  <data key="d5">"algorithm interdependence"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;INFORMATION RETRIEVAL (IR)&quot;" target="&quot;TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF) &quot;">
  <data key="d3">14.0</data>
  <data key="d4">"TF-IDF is a key component in the Information Retrieval process, particularly in lexical search approaches."</data>
  <data key="d5">"relevance evaluation"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;SYNONYMS&quot;" target="&quot;LEXICAL APPROACHES&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Synonyms pose challenges for lexical approaches as they might not match exact terms, leading to missed documents or passages."</data>
  <data key="d5">"matching issues, semantic limitations"</data>
  <data key="d6">chunk-8dc9bc0243bc2842874f8fe5a91e0b07</data>
</edge>
<edge source="&quot;HIDDEN VECTOR&quot;" target="&quot;ACTIVATION FUNCTION&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Activation functions are applied to the hidden vector in an RNN to introduce non-linearity and enable it to process sequential data effectively."</data>
  <data key="d5">"non-linearity, processing sequence"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;HIDDEN VECTOR&quot;" target="&quot;OUTPUT OF OUTPUT GATE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The hidden vector is derived from the combination of the Output Gate's sigmoid output and the tanh layer, representing the processed data ready for transmission to the next cell." &lt;|"data processing, transmission"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;HIDDEN VECTOR&quot;" target="&quot;RNN CELL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Hidden vectors in RNN cells store information from previous iterations, maintaining context over time."</data>
  <data key="d5">"context maintenance, iteration"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;TANH FUNCTION&quot;" target="&quot;˜CT&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The tanh function transforms ˜Ct into a range between -1 and 1 for use in updating the cell state." &lt;|"function transformation, update process"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;SELF-ATTENTION MECHANISM&quot;" target="&quot;ENCODER&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Self-Attention mechanism is a key component in the Encoder of Transformers, allowing it to process input sequences more effectively than traditional LSTM architectures."</data>
  <data key="d5">&lt;"attention mechanisms, efficient processing"</data>
  <data key="d6">chunk-d7a96db6c2522c5e8ce8fe400d66902f</data>
</edge>
<edge source="&quot;EMBEDDINGS&quot;" target="&quot;CENTROID CALCULATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The embeddings are adjusted based on centroid calculation to ensure related sentences are closer in space."</data>
  <data key="d5">&lt;"adjustment, clustering"&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;EMBEDDINGS&quot;" target="&quot;LEARNING RATE, BATCH SIZE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The learning rate and batch size settings are crucial in adjusting embeddings during training."</data>
  <data key="d5">&lt;"training, hyperparameters"&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;EMBEDDINGS&quot;" target="&quot;STS TASK EVALUATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The STS task evaluation is directly related to the quality and performance of embeddings."</data>
  <data key="d5">&lt;"embedding quality, evaluation"&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;EMBEDDINGS&quot;" target="&quot;DOCUMENTS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The documents contain information that is processed to generate embeddings."</data>
  <data key="d5">&lt;"data source, processing"&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;SYMMETRIC SEMANTIC SEARCH&quot;" target="&quot;BI-ENCODERS&quot;">
  <data key="d3">2.0</data>
  <data key="d4">"Bi-Encoders are a method used in Symmetric Semantic Search for generating sentence embeddings and comparing them using cosine similarity. They are mentioned as part of the process."</data>
  <data key="d5">&lt;"embedding generation, cosine similarity comparison"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;SYMMETRIC SEMANTIC SEARCH&quot;" target="&quot;CROSS-ENCODERS&quot;">
  <data key="d3">2.0</data>
  <data key="d4">"Cross-Encoders provide an alternative method in Symmetric Semantic Search for comparing the similarity between sentences without generating individual embeddings. They are described as part of the system."</data>
  <data key="d5">&lt;"sentence comparison, similarity score output"&gt;&lt;7&gt;</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;QUERY EMBEDDING&quot;" target="&quot;SENTENCE EMBEDDING&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both query and sentence embeddings are used in semantic search to find similar sentences or match queries with text using vector spaces."</data>
  <data key="d5">"embedding comparison, similarity measure"</data>
  <data key="d6">chunk-5d200aadc0a3370985a8b9824fa2c738</data>
</edge>
<edge source="&quot;CONNECTIVITY MATRIX&quot;" target="&quot;PAGERANK METHOD&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"The PageRank method is applied to the connectivity matrix to calculate the LexRank score of each phrase."&lt;SEP&gt;"The connectivity matrix serves as input for PageRank which uses it to calculate LexRank scores and determine the importance of sentences in a document."</data>
  <data key="d5">"input data, algorithm application"&lt;SEP&gt;"relevance calculation, iterative algorithm"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;CONNECTIVITY MATRIX&quot;" target="&quot;INTRA-SENTENCE COSINE SIMILARITY&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Intra-sentence cosine similarity is used to derive the connectivity matrix which serves as input for PageRank and LexRank."</data>
  <data key="d5">"similarity measure, graph construction"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;PAGERANK METHOD&quot;" target="&quot;LEXRANK SCORE&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"The PageRank method generates the LexRank score for sentences based on their similarity and importance within the graph."&lt;SEP&gt;"The PageRank method is used as the basis for calculating LexRank scores, which assess the relevance and connectivity of phrases in sentences through a similar algorithm."</data>
  <data key="d5">"basis, similarity assessment"&lt;SEP&gt;"score generation, centrality measure"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;LEXRANK SCORE&quot;" target="&quot;GRAPH REPRESENTATION OF SENTENCES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Graph representation forms the basis for calculating LexRank scores using PageRank on each sentence's node."</data>
  <data key="d5">"graph theory, ranking method"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;TRANSFORMER-BASED SEQUENTIAL DENOISING AUTO-ENCODER (TSAE)&quot;" target="&quot;SUBSECTION 3.1.1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Subsection 3.1.1 discusses TSAE as a technique for domain adaptation."</data>
  <data key="d5">&lt;"discussion"</data>
  <data key="d6">chunk-d6b6e12e9e8f1eb9b5e8c5e2e6168f95</data>
</edge>
<edge source="&quot;STS BENCHMARK (STSB)&quot;" target="&quot;MULTI-GENRE NLI&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Multi-Genre NLI dataset was used as part of the STS benchmark, contributing diverse sentence pairs for evaluation and training."</data>
  <data key="d5">"dataset diversity, evaluation"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;MULTI-GENRE NLI&quot;" target="&quot;STNLI (STANFORD NATURAL LANGUAGE INFERENCE)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"STNLI is part of the multi-genre approach to natural language inference tasks, contributing diverse sentence pairs for training models."</data>
  <data key="d5">"dataset diversity, resource integration"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;ROBUST04&quot;" target="&quot;BERT -FIRSTP&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The dataset Robust04 was used in testing the performance with scores related to the score of the first passage (BERT -FirstP), evaluating BERT's understanding in well-written text contexts."</data>
  <data key="d5">"dataset, evaluation context"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;ROBUST04&quot;" target="&quot;BERT -SUMP&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The dataset Robust04 was also used in testing the performance with scores related to the sum of all passage scores (BERT -SumP), evaluating BERT's comprehensive understanding across multiple passages."</data>
  <data key="d5">"dataset, evaluation context"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;CLUEWEB09-B&quot;" target="&quot;BERT -MAXP&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The dataset ClueWeb09-B was used in testing the performance with scores related to the best passage (BERT -MaxP), assessing BERT's ability to understand the highest-scoring passages."</data>
  <data key="d5">"dataset, evaluation context"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;JNLP&quot;" target="&quot;MI-Y OUNG KIM ET AL.&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both JNLP and Mi-Y oung Kim et al. were involved in the COLIEE 2021 competition, though their approaches differed."</data>
  <data key="d5">&lt;"approach comparison, involvement in research"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;JNLP&quot;" target="&quot;TASK 1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Team JNLP participated in Task 1 of COLIEE 2021 with a focus on dealing with large articles using text chunking and self-labeled approaches."</data>
  <data key="d5">&lt;"task participation, approach comparison"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;JNLP&quot;" target="&quot;TASK 3&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Team JNLP did not participate in Task 3 but their methods and models might be related to the broader NLP advancements discussed in COLIEE 2021."</data>
  <data key="d5">&lt;"task involvement, model comparison"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;JNLP&quot;" target="&quot;COLIEE 2021 &quot;">
  <data key="d3">7.0</data>
  <data key="d4">"JNLP participated in the COLIEE 2021 event and focused on using self-labeled approaches for task 1."</data>
  <data key="d5">"participation, approach focus"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;NIGAM&quot;" target="&quot;TASK 1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Team nigam also participated in Task 1 of COLIEE 2021 but used a different approach combining transformer-based and traditional IR techniques with BM25 scores."</data>
  <data key="d5">&lt;"task participation, approach comparison"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;COLIEE 2021&quot;" target="&quot;TASK 1&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The COLIEE 2021 event included multiple tasks such as case law retrieval and statute law retrieval."</data>
  <data key="d5">&lt;"event scope, task diversity"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;TASK 1&quot;" target="&quot;NIGAM TEAM&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The nigam team proposed an approach combining transformer-based and traditional IR techniques for Task 1 using SBERT and Sent2Vec combined with BM25."</data>
  <data key="d5">&lt;"technique integration, model combination"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;TASK 3&quot;" target="&quot;OVGU TEAM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"OvGU used two-stage TF-IDF vectorization combined with Sentence-BERT embeddings for the statute law retrieval task of COLIEE 2021."</data>
  <data key="d5">&lt;"task focus, embedding techniques"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;TASK 3&quot;" target="&quot;TEAM F1-SCORE PRECISION RECALL MAP&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Task 3 results are shown in the table with team performance metrics such as F1-score, Precision, Recall, and MAP."</data>
  <data key="d5">&lt;"task results, metric correlation"</data>
  <data key="d6">chunk-4178cfa608054c267be41d058b830af4</data>
</edge>
<edge source="&quot;JO ˜AO RODRIGUES ET AL.&quot;" target="&quot;ALBERTINA [35]&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Jo ˜ao Rodrigues et al. are the creators of Albertina, a state-of-the-art model that represents new advancements for European and Brazilian Portuguese language processing."</data>
  <data key="d5">"creation, innovation"</data>
  <data key="d6">chunk-afd8ce7e1a7d61d34be9bdded6cff755</data>
</edge>
<edge source="&quot;JO ˜AO RODRIGUES ET AL.&quot;" target="&quot;FCUL - NLX – NATURAL LANGUAGE AND SPEECH GROUP&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Jo ˜ao Rodrigues et al. are the authors who developed Albertina PT with FCUL - NLX – Natural Language and Speech Group."</data>
  <data key="d5">&lt;"authorship, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;JO ˜AO RODRIGUES ET AL.&quot;" target="&quot;FEUP - LABORATÓRIO DE INTELIGÊNCIA ARTIFICIAL E CIÊNCIA DE COMPUTADORES&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Jo ˜ao Rodrigues et al. are the authors who developed Albertina PT with FEUP - Laboratório de Inteligência Artificial e Ciência de Computadores."</data>
  <data key="d5">&lt;"authorship, development"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;FCUL - NLX – NATURAL LANGUAGE AND SPEECH GROUP&quot;" target="&quot;FEUP - LABORATÓRIO DE INTELIGÊNCIA ARTIFICIAL E CIÊNCIA DE COMPUTADORES&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Both organizations are part of the partnership that developed Albertina PT."</data>
  <data key="d5">&lt;"partnership, collaboration"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;SENTENCETRANSFORMERS PYTHON LIBRARY&quot;" target="&quot;LEGAL-BERTIMBAU MODEL HOSTED ON THE HUGGING FACE HUB8&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Legal-BERTimbau model is hosted on the Hugging Face Hub and managed through the SentenceTransformers Python library." "</data>
  <data key="d5">"model management, embedding creation"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;PRE-PROCESSING&quot;" target="&quot;DOCUMENTS IN THE ORIGINAL DATASET&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The pre-processing step involves splitting documents into smaller units to make Legal-BERTimbau more effective for processing large sentences." "</data>
  <data key="d5">"data processing, document segmentation"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;PRE-PROCESSING&quot;" target="&quot;LEGAL-BERTIMBAU MODEL HOSTED ON THE HUGGING FACE HUB8&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The pre-processing step involves ensuring that Legal-BERTimbau is effective with smaller sentences." "</data>
  <data key="d5">"effectiveness improvement, sentence size consideration"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;LEGAL-BERTIMBAU MODEL HOSTED ON THE HUGGING FACE HUB8&quot;" target="&quot;HUGGING FACE HUB&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Legal-BERTimbau model is hosted on the Hugging Face Hub and managed through the SentenceTransformers Python library." "</data>
  <data key="d5">"model hosting, management"</data>
  <data key="d6">chunk-486e9fdbc67025b64b42032778600c9c</data>
</edge>
<edge source="&quot;STSB MULTI MT PORTUGUESE SUB-DATASET&quot;" target="&quot;STJIRIS/IRIS STS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The IRIS organization's custom dataset is also used in the fine-tuning of STS evaluation tasks like those performed by the STSB multi mt Portuguese sub-dataset."</data>
  <data key="d5">"dataset interaction, training set"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;SENTENCETRANSFORMER LIBRARY&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-V0&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The SentenceTransformer library is used to create the SBERT model from Legal-BERTimbau-large."</data>
  <data key="d5">"model creation, library usage"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;CENTROID CALCULATION&quot;" target="&quot;DOCUMENT SAMPLE&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The document sample is used to calculate centroids for embedding adjustment."</data>
  <data key="d5">&lt;"data, calculation"&gt;</data>
  <data key="d6">chunk-427843b4c7ba44f1dcc8f571081e36ae</data>
</edge>
<edge source="&quot;ELASTICSEARCH INDEX&quot;" target="&quot;QUERY GENERATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The ElasticSearch index stores sentence embeddings generated from 1000 legal documents to evaluate query generation techniques in the search system."</data>
  <data key="d5">"embedding storage, evaluation architecture"</data>
  <data key="d6">chunk-dc1fa210fa8cf3125e9c46996f5dbd40</data>
</edge>
<edge source="&quot;ELASTICSEARCH INDEX&quot;" target="&quot;LEGAL SEARCH SYSTEM&quot;">
  <data key="d3">27.0</data>
  <data key="d4">"The Legal Search System relies on ElasticSearch index to store and retrieve sentence embeddings created by Legal-BERTimbau."</data>
  <data key="d5">"data storage, retrieval process"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;ELASTICSEARCH INDEX&quot;" target="&quot;ORIGINAL BERTIMBAU LARGE FINE-TUNED FOR STS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ElasticSearch index stored embeddings of summaries used by the original BERTimbau model in the Negative Mining stage."</data>
  <data key="d5">"indexing, embedding storage"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;STS DATASET&quot;" target="&quot;LEGAL SEARCH SYSTEM&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The Semantic Search System was evaluated using both fine-tuned models without and with the custom STS dataset."</data>
  <data key="d5">"dataset usage, evaluation metric"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;TOP RESULTS SIZES&quot;" target="&quot;LEGAL SEARCH SYSTEM&quot;">
  <data key="d3">21.0</data>
  <data key="d4">"The Legal Search System's performance was assessed based on different top result sizes (e.g., Top 1, Top 2, etc.) during the evaluation process."</data>
  <data key="d5">"performance evaluation, result size"</data>
  <data key="d6">chunk-437fca5e9e86e40c8ca3cf7fc41a8c65</data>
</edge>
<edge source="&quot;CUSTOM STS DATASET (V1 MODELS)&quot;" target="&quot;PRE-EXISTING AND MANUALLY ANNOTATED DATASETS (V0 MODELS)&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Models using V1 dataset perform marginally worse than those using pre-existing and manually annotated data."</data>
  <data key="d5">"performance discrepancy, inferior model"</data>
  <data key="d6">chunk-23944cdf583b2c2b5fcf537fea3f8421</data>
</edge>
<edge source="&quot;LEXRANK TECHNIQUE&quot;" target="&quot;ROUGE-1 SCORE&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"The LexRank technique is a method used to generate summaries, which are then evaluated using the ROUGE-1 score."</data>
  <data key="d5">"evaluation metric"</data>
  <data key="d6">chunk-fc9187ad3fd00c96d11f9934e91f7051</data>
</edge>
<edge source="&quot;KNOWLEDGE GRAPH&quot;" target="&quot;QUERY EXPANSION SYSTEM&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The query expansion system incorporates knowledge graphs for better context understanding and document retrieval."</data>
  <data key="d5">"contextual understanding, retrieval enhancement"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;BOWMAN, S. R.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"S.R. Bowman contributed to the 2013 SemEval shared task on semantic textual similarity."</data>
  <data key="d5">"task contribution, research collaboration"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;KIELA, D.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Daniele Kiela participated in the 2013 shared task on semantic textual similarity at SEM conference."</data>
  <data key="d5">"task contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;ANGELIS, G.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Giuseppe Angelis contributed to the shared task on semantic textual similarity at the 2013 SEM conference."</data>
  <data key="d5">"task contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2013 SHARED TASK: SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;MANNING, C. D.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Christopher Manning contributed to the shared task on semantic textual similarity at the 2013 SEM conference."</data>
  <data key="d5">"task contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2012 TASK 6: A PILOT ON SEMANTIC TEXTUAL SIMILARITY&quot;" target="&quot;USA, 2012&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The task focused on a pilot study for semantic textual similarity was held in the USA in 2012."</data>
  <data key="d5">"task location and year"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1: SEMANTIC TEXTUAL SIMILARITY MULTILINGUAL AND CROSSLINGUAL FOCUSED EVALUATION&quot;" target="&quot;MANNING, C. D.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"C.D. Manning's work contributed to the SemEval-2017 Task 1 on semantic textual similarity."</data>
  <data key="d5">"evaluation task contribution, research collaboration"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1: SEMANTIC TEXTUAL SIMILARITY MULTILINGUAL AND CROSSLINGUAL FOCUSED EVALUATION&quot;" target="&quot;DIAZ-GAZPIO, I.&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"I Diaz-Gazpio is a co-author involved in the SemEval-2017 Task 1 focusing on semantic textual similarity evaluation."</data>
  <data key="d5">"task participation, research collaboration"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SEMEVAL-2017 TASK 1: SEMANTIC TEXTUAL SIMILARITY MULTILINGUAL AND CROSSLINGUAL FOCUSED EVALUATION&quot;" target="&quot;VANCOUVER, CANADA, 2017&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The task on semantic textual similarity was part of SemEval-2017 held in Vancouver, Canada, 2017."</data>
  <data key="d5">"task location and year"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;KIELA, D.&quot;" target="&quot;SUPERVISED LEARNING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"D Kiela's work contributed to the development and application of BERT in natural language processing."</data>
  <data key="d5">"research collaboration, model development"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;BARRAULT, L.&quot;" target="&quot;SUPERVISED LEARNING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"L Barrault contributed to the development and training of BERT models."</data>
  <data key="d5">"model training, collaboration"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;POTTS, C.&quot;" target="&quot;SUPERVISED LEARNING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"C Potts has contributed to the development and training of BERT models."</data>
  <data key="d5">"model development, research collaboration"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;MANNING, C. D.&quot;" target="&quot;JURAFSKY, D.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators involved in learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d5">"collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;MANNING, C. D.&quot;" target="&quot;HO, D. E.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators involved in learning responsible data filtering from the law and a 256GB open-source legal dataset."</data>
  <data key="d5">"collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;SPECIA, L.&quot;" target="&quot;SUPERVISED LEARNING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"L Specia contributed to multilingual and crosslingual focused evaluations using BERT models."</data>
  <data key="d5">"evaluation task contribution, research collaboration"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;ERKAN, G.&quot;" target="&quot;RAEV, D. R.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"They co-authored LexRank and worked on graph-based lexical centrality measures for summarization."</data>
  <data key="d5">"co-authorship, summarization"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;GURURARANG, S.&quot;" target="&quot;MARASOVIC, A.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"They developed Don't Stop Pretraining in collaboration with other authors to enhance language model training."</data>
  <data key="d5">"collaboration, pre-training"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;LO, K.&quot;" target="&quot;GURURANGAN, S.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Co-authors and contributors to adapting language models to domains through pretraining."</data>
  <data key="d5">"collaboration, model adaptation"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;BELTAGY, I.&quot;" target="&quot;GURURANGAN, S.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Co-authors and contributors to adapting language models to domains through pretraining."</data>
  <data key="d5">"collaboration, model adaptation"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;DOWNEY, D.&quot;" target="&quot;GURURANGAN, S.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Co-authors and contributors to adapting language models to domains through pretraining."</data>
  <data key="d5">"collaboration, model adaptation"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;SMITH, N. A. A.&quot;" target="&quot;GURURANGAN, S.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Co-authors and contributors to adapting language models to domains through pretraining."</data>
  <data key="d5">"collaboration, model adaptation"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;HE, P.&quot;" target="&quot;LIU, X.&quot;">
  <data key="d3">34.0</data>
  <data key="d4">"Authors of DeBERTa: Decoding-enhanced BERT with disentangled attention."&lt;SEP&gt;"Both authors are involved in the creation of DeBERTA for advanced language understanding models."</data>
  <data key="d5">"co-authorship, development"&lt;SEP&gt;"collaboration, NLP model development"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;HE, P.&quot;" target="&quot;GAO, J.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Authors of DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d5">"collaboration, NLP model development"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;HE, P.&quot;" target="&quot;CHEN, W.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Authors of DeBERTa: Decoding-enhanced BERT with disentangled attention."</data>
  <data key="d5">"collaboration, NLP model development"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;IJITEE&quot;" target="&quot;VAISHNAV, V.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Publication venue for an AI-based analysis in the legal domain by Vaishnav and Deepalakshmi."</data>
  <data key="d5">"publication venue, AI applications"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;IJITEE&quot;" target="&quot;DEEPALAKSHMI&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Publication venue for an AI-based analysis in the legal domain by Vaishnav and Deepalakshmi."</data>
  <data key="d5">"publication venue, AI applications"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;KIM, M.&quot;" target="&quot;RABELO, J.&quot;">
  <data key="d3">30.0</data>
  <data key="d4">"Co-authors involved in BM25 and transformer-based legal information extraction and entailment research."&lt;SEP&gt;"They co-authored a paper on BM25 and transformer-based legal information extraction for entailment tasks."</data>
  <data key="d5">"co-authorship, research"&lt;SEP&gt;"collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;KIM, M.&quot;" target="&quot;GOEBEL, R.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Co-authors involved in BM25 and transformer-based legal information extraction and entailment research."</data>
  <data key="d5">"collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;KINGMA, D. P.&quot;" target="&quot;BA, J.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Developers of Adam: A method for stochastic optimization."</data>
  <data key="d5">"collaboration, optimization methods"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;REAL, E. FONSECA, AND H. G. OLIVEIRA&quot;" target="&quot;INTERNATIONAL CONFERENCE ON COMPUTATIONAL PROCESSING OF THE PORTUGUESE LANGUAGE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Their work was presented at this conference."</data>
  <data key="d5">"presentation, research contribution"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;JOURNAL OF MACHINE LEARNING RESEARCH&quot;" target="&quot;ACM, JMLR&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"N. Robertson's work on the probabilistic relevance framework is published in the Journal of Machine Learning Research, indicating a relationship with ACM and JMLR."</data>
  <data key="d5">"publication, association"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;P.J.&quot;" target="&quot;L.I.W.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Both authors have contributed to the same research topic on transfer learning and unified text-to-text transformer, indicating a potential collaborative relationship."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;V AN DER WALT, S. J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"S. J. V AN DER WALT is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;P ERKTOLD , J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"J. P ERKTOLD is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;H ENRIKSEN , I.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"I. H ENRIKSEN is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;H ARRIS , C. R.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"C. R. H ARRIS is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;A RCHIBALD , A. M.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"A. M. A RCHIBALD is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;P EDREGOSA , F.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"F. P EDREGOSA is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;K. WANG &quot;" target="&quot;N. R EIMERS &quot;">
  <data key="d3">9.0</data>
  <data key="d4">"K. Wang and N. Reimers are co-authors of the 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;K. WANG &quot;" target="&quot;I. GUREVYCH &quot;">
  <data key="d3">9.0</data>
  <data key="d4">"K. Wang and I. Gurevych are co-authors of the 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;K. WANG &quot;" target="&quot;I. W ANG &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"K. Wang and I. Wang are co-authors of the 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;K. WANG &quot;" target="&quot;N. T HAKUR &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"K. Wang and N. Thakur are co-authors of the 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;N. R EIMERS &quot;" target="&quot;I. GUREVYCH &quot;">
  <data key="d3">9.0</data>
  <data key="d4">"N. Reimers and I. Gurevych are co-authors of the 'TSDAE: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;N. R EIMERS &quot;" target="&quot;I. W ANG &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N. Reimers and I. Wang are co-authors of the 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;N. R EIMERS &quot;" target="&quot;N. T HAKUR &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N. Reimers and N. Thakur are co-authors of the 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;I. GUREVYCH &quot;" target="&quot;I. W ANG &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"I. Gurevych and I. Wang are co-authors of the 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;I. GUREVYCH &quot;" target="&quot;N. T HAKUR &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"I. Gurevych and N. Thakur are co-authors of the 'GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval' paper."</data>
  <data key="d5">"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;NEGATIVE MINING STEP&quot;" target="&quot;PSEUDO LABELING STEP&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Negative passages retrieved by the Negative Mining step are used as inputs for the Pseudo Labeling step to train a Cross-Encoder model."|&lt;&gt;"passage retrieval, model training"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-6eeb6febf5ce46ec96655d84dc54cd2f</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1&quot;" target="&quot;METAKD (METADATA KNOWLEDGE DISTILLATION)&quot;">
  <data key="d3">44.0</data>
  <data key="d4">"MetaKD is applied to further train the model for better performance in specific tasks."&lt;SEP&gt;"The model stjiris/bert-large-portuguese-cased-legal-tsdae-gpl-nli-sts-v1 was trained using the MetaKD technique for improved information retrieval."</data>
  <data key="d5">"enhancement, application"&lt;SEP&gt;"model training, metadata enhancement"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V1&quot;" target="&quot;TED 2020 – PARALLEL SENTENCES CORPUS&quot;[33]">
  <data key="d3">32.0</data>
  <data key="d4">"This dataset is used as part of the training for the model, contributing to its knowledge base."</data>
  <data key="d5">"training data, resource"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;METAKD (METADATA KNOWLEDGE DISTILLATION)&quot;" target="&quot;DESCRITORES&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The MetaKD technique utilizes Descritores to encode and cluster documents based on their content tags."</data>
  <data key="d5">&lt;"metadata encoding, document clustering"</data>
  <data key="d6">chunk-ce4847f54b29367988561206721bdbb7</data>
</edge>
<edge source="&quot;QUERY GENERATION&quot;" target="&quot;NEGATIVE MINING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Query Generation step creates queries that are then used to mine negative passages during the Negative Mining stage."</data>
  <data key="d5">&lt;"query-negative relationship"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;FORMULATING A DECISION BASED ON INCOMPLETE INFORMATION LIMITATION AND RISKS&quot;" target="&quot;INCOMPLETE INFORMATION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Incomplete information poses limitations and risks when formulating judicial decisions."|&lt;&gt;"information quality impact, risk assessment"</data>
  <data key="d5">7</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;FORMULATING A DECISION BASED ON INCOMPLETE INFORMATION LIMITATION AND RISKS&quot;" target="&quot;IRRELEVANT INFORMATION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Irrelevant information contributes to the limitations and risks when formulating decisions based on incomplete data."|&lt;&gt;"relevance impact, risk assessment"</data>
  <data key="d5">8</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;JUDICIAL WORK EASE AND ENRICHMENT&quot;" target="&quot;RELEVANT AND INSIGHTFUL INFORMATION&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Relevant and insightful information helps ease and enrich the work of judges in the legal domain."|&lt;&gt;"benefit realization, application effectiveness"</data>
  <data key="d5">9</data>
  <data key="d6">chunk-87a104ccfd8b71d9a4e7ad0343692cac</data>
</edge>
<edge source="&quot;ACL&quot;" target="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The ACL is the organization that hosts events like ICCPPL where N. Reimers and I. Gurevych present their work."</data>
  <data key="d5">"association, venue"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;ACL&quot;" target="&quot;SEMEVAL-2015 TASK 2: SEMANTIC TEXTUAL SIMILARITY&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The task was part of the work done by ACL."</data>
  <data key="d5">"collaboration, research"</data>
  <data key="d6">chunk-d23192c04e35b99777b833e26dafed9f</data>
</edge>
<edge source="&quot;ACL&quot;" target="&quot;Z. DAI AND J. CALLAN&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"The authors of the BERT paper are affiliated with ACL, which is the organization that hosts such conferences and publishes papers like theirs."</data>
  <data key="d5">"conference affiliation, research community"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;F. SOUZA, R. NOGUEIRA, R. LOTUFO&quot;" target="&quot;N. REIMERS, I. GUREVYCH&quot;">
  <data key="d3">28.0</data>
  <data key="d4">"These researchers have contributed to named entity recognition using BERT-CRF techniques in Brazilian Portuguese, which aligns with the focus of N. Reimers and I. Gurevych's work."&lt;SEP&gt;"These researchers have contributed to named entity recognition using BERT-CRF techniques in Brazilian Portuguese, which aligns with the focus of Reimers and Gurevych's work."</data>
  <data key="d5">"technological alignment, research focus"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;N. REIMERS, I. GUREVYCH&quot;" target="&quot;A. VASWANI, N. SHAZEER, N. PARMAR, J. USZKOREIT, L. JONES, A.N. GOMEZ, L.U. KAISER, I. POLOSUKHIN&quot;">
  <data key="d3">24.0</data>
  <data key="d4">"Both researchers have contributed to multiple research areas in natural language processing, showing a relationship through shared interests and potential collaboration."</data>
  <data key="d5">"research collaboration, shared interest"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;N. REIMERS, I. GUREVYCH&quot;" target="&quot;J. RODRIGUES, L. GOMES, J. SILVA, A. BRANCO, R. SANTOS, H.L. CARDOSO, T. OSÓRIO&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Portuguese researchers have contributed to named entity recognition using BERT-CRF techniques in Brazilian Portuguese, which aligns with the focus of N. Reimers and I. Gurevych's work on language processing models."</data>
  <data key="d5">"technological alignment, research focus"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;N. REIMERS, I. GUREVYCH&quot;" target="&quot;N. THAKUR, A. RÜCKLÉ, A. SRIVASTAVA, I. GUREVYCH&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Both groups of researchers have contributed to multiple research areas in natural language processing, showing a relationship through shared interests and potential collaboration."</data>
  <data key="d5">"research collaboration, shared interest"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;P.J., L.I.W.&quot;" target="&quot;X. RONG&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"X. Rong has written about Word2Vec parameter learning, which could be relevant to the research by P.J. and L.I.W. on transfer learning."</data>
  <data key="d5">"research relatedness, complementary work"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;SUPERVISORS&quot;" target="&quot;EXAMINATION COMMITTEE&quot;">
  <data key="d3">1.0</data>
  <data key="d4">"The supervisors are part of the examination committee that oversees Rui's thesis, ensuring its quality and validity."</data>
  <data key="d5">&lt;source_entity&gt;</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;SUPERVISORS&quot;" target="&quot;EXAMINATION COMMITTEE CHAIRPERSON&quot;">
  <data key="d3">2.0</data>
  <data key="d4">"Both supervisors are involved in the examination process and are also part of the committee chair's team."&lt;SEP&gt;"The examination committee chair, Prof. Maria Luís Torres Ribeiro Marques da Silva Coheur, is part of the supervisory team."</data>
  <data key="d5">&lt;source_entity&gt;</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;TERM FREQUENCY ALGORITHM&quot;" target="&quot;BEST MATCHING ALGORITHM&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both Term Frequency and Best Matching (BM25) are ranking functions in information retrieval, with BM25 being an extension of the former."</data>
  <data key="d5">"methodology relationship"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;BEST MATCHING ALGORITHM&quot;" target="&quot;DOCUMENT&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Documents are ranked by the Best Matching (BM25) algorithm based on query terms and their frequency within each document."</data>
  <data key="d5">"document ranking, relevance estimation"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;RECURRENT NEURAL NETWORK&quot;" target="&quot;TECHNOLOGIES&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"RNNs are foundational in understanding sequence data, which includes a lot of tasks that GloVe and other NLP techniques aim to solve."</data>
  <data key="d5">"sequence processing, foundational technology"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;LONG SHORT-TERM MEMORY&quot;" target="&quot;TECHNOLOGY&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"LSTM is an improvement over RNN in handling long-term dependencies, often used in conjunction with other NLP models like BERT and GloVe."</data>
  <data key="d5">"improvement, sequence processing"</data>
  <data key="d6">chunk-cb545b2bab1e85e24c22c3bd238a556f</data>
</edge>
<edge source="&quot;ARCHITECTURE IMPROVEMENTS&quot;" target="&quot;BIBLIOGRAPHY&quot;">
  <data key="d3">5.0</data>
  <data key="d4">"The Architecture Improvements section might reference the bibliography for sources or further reading."</data>
  <data key="d5">"citations, documentation"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;DATASETS&quot;" target="&quot;LANGUAGE MODELS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Datasets were used to develop and train language models for the IRIS project."</data>
  <data key="d5">"data-driven development, model training"</data>
  <data key="d6">chunk-ae090822f3d4769354cc463665e2df89</data>
</edge>
<edge source="&quot;LEXICAL APPROACHES FOR INFORMATION RETRIEVAL&quot;" target="&quot;DOCUMENT&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Lexical approaches focus on searching within documents based on exact query words."</data>
  <data key="d5">"query, document search"</data>
  <data key="d6">chunk-469ff73d10b043e9b3515dc8baa15494</data>
</edge>
<edge source="&quot;GLOVE'S LOSS FUNCTION&quot;" target="&quot;WORD ICE AND STEAM&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The GloVe model uses the probabilities of 'ice' and 'steam' to define its loss function, optimizing embeddings based on these co-occurrences."</data>
  <data key="d5">"embedding optimization, specific examples"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;WORD ICE AND STEAM&quot;" target="&quot;SOLID, GAS, WATER, FASHION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The terms 'ice' and 'steam' co-occur more frequently with specific concepts (solid and water), while showing lesser or no co-occurrence with others (gas and fashion)."</data>
  <data key="d5">"co-occurrence, context relationship"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;FEED-FORWARD NEURAL NETWORKS&quot;" target="&quot;RNN CELL&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"RNN cells are structured as feed-forward neural networks that process sequential data by iterating over themselves."</data>
  <data key="d5">"sequential processing, iteration mechanism"</data>
  <data key="d6">chunk-c22c3b4107fedf31ff541c2373a8f1a6</data>
</edge>
<edge source="&quot;PROJECTION MATRICES WQ I, WK I, WV I&quot;" target="&quot;PARALLEL ATTENTION LAYERS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Parallel Attention Layers use projection matrices to run the attention function in parallel for each head."</data>
  <data key="d5">"parallel processing, multiple heads"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;MASKED ATTENTION COMPONENT&quot;" target="&quot;INPUT SEQUENCE CURRENT POSITION&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Masked Attention Component masks positions after the current one, ensuring attention is only on past information in the input sequence."</data>
  <data key="d5">"mask application, historical dependency"</data>
  <data key="d6">chunk-e27d93fef9843514dd8dbc524160d663</data>
</edge>
<edge source="&quot;JACCARD SIMILARITY&quot;" target="&quot;ADJACENCY MATRIX&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Jaccard similarity can also be used alongside or in conjunction with cosine similarity to form the adjacency matrix in the sentence graph representation."</data>
  <data key="d5">"alternative measurement, graph theory"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;GRAPH REPRESENTATION OF SENTENCES&quot;" target="&quot;ADJACENCY MATRIX&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The adjacency matrix derived from intra-sentence cosine or Jaccard similarity forms the basis for the graph representation of sentences."</data>
  <data key="d5">"input data, graph construction"</data>
  <data key="d6">chunk-abe3791c0e495b4a72310f8a36c50056</data>
</edge>
<edge source="&quot;GLOVE EMBEDDINGS&quot;" target="&quot;BERT EMBEDDINGS&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"BERT and GloVe are both techniques used for word and sentence embedding, with BERT outperforming GloVe in certain tasks."</data>
  <data key="d5">"embedding comparison, model performance"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;SBERT -NLI-BASE&quot;" target="&quot;SBERT -NLI-LARGE&quot;">
  <data key="d3">25.0</data>
  <data key="d4">"Both SBERT -NLI-base and SBERT -NLI-large are variations of the same model fine-tuned on NLI tasks, with different sizes indicating varying levels of complexity and performance measures provided."&lt;SEP&gt;"SBERT -NLI-large is an enhanced version of SBERT -NLI-base, both using BERT for sentence embeddings."</data>
  <data key="d5">"model variant, size comparison"&lt;SEP&gt;&lt;"enhancement, model evolution"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;SBERT -NLI-BASE&quot;" target="&quot;SROBERTA-NLI-BASE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both SRoBERTa-NLI-base and SBERT -NLI-base are part of the NLI fine-tuned models but use different underlying architectures (RoBERTa vs BERT), showing similar performance measures in evaluations."</data>
  <data key="d5">"model comparison, architecture difference"</data>
  <data key="d6">chunk-53db1ddb7040c30dab02f6b0bd355f81</data>
</edge>
<edge source="&quot;NEURALMIND/BERT-BASE-PORTUGUESE-CASED BERT -BASE 12 110M&quot;" target="&quot;NEURALMIND/BERT-LARGE-PORTUGUESE-CASED BERT -LARGE 24 335M&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Both models are pre-trained for the Portuguese language and have similar architectures but differ in size."</data>
  <data key="d5">"model comparison, architecture similarity"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;NEURALMIND/BERT-BASE-PORTUGUESE-CASED BERT -BASE 12 110M&quot;" target="&quot;BRAZILIAN WEB AS CORPUS (BRWAC)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The neuralmind/bert-base-portuguese-cased model was trained using the Brazilian Web as Corpus."</data>
  <data key="d5">"corpus utilization, pretraining"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;NEURALMIND/BERT-LARGE-PORTUGUESE-CASED BERT -LARGE 24 335M&quot;" target="&quot;BRAZILIAN WEB AS CORPUS (BRWAC)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The neuralmind/bert-large-portuguese-cased model was trained using the Brazilian Web as Corpus."</data>
  <data key="d5">"corpus utilization, pretraining"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;BRAZILIAN WEB AS CORPUS (BRWAC)&quot;" target="&quot;HTML BODY AND TITLES/FOOTNOTES&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The Brazilian Web as Corpus uses the HTML body while ignoring titles and footnotes."</data>
  <data key="d5">"data processing, corpus creation"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;1E−4 LEARNING RATE&quot;" target="&quot;BERTIMBAU TRAINING PROCESS&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The 1e-4 learning rate is a key parameter in the training process of BERTimbau models."</data>
  <data key="d5">"training, parameter tuning"</data>
  <data key="d6">chunk-493aab369fd44529a838be55e938c506</data>
</edge>
<edge source="&quot;JO˜ÃO RODRIGUES ET AL.&quot;" target="&quot;STS-B DATASET&quot;">
  <data key="d3">3.0</data>
  <data key="d4">"Jo˜ão Rodrigues et al. evaluated Albertina PT on the STS-B dataset, indicating a performance comparison."</data>
  <data key="d5">"evaluation, model testing"</data>
  <data key="d6">chunk-a24ce5ad9bdfbbf32c946ee404c32fd6</data>
</edge>
<edge source="&quot;RE&quot;" target="&quot;AUTOR&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Re is responsible for paying Autor the difference in wages as determined by court ruling."</data>
  <data key="d5">"legal responsibility, compensation"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;DGSI - INDEXER -STJ &quot;" target="&quot;ACORDAO &quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The legal document corpus managed by dgsi - indexer -STJ includes Acordao as part of its content."</data>
  <data key="d5">"data management, legal documents"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;JURISPRUDENCIA &quot;" target="&quot;ACORDAO &quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Acordao is a specific instance that contributes to the established jurisprudence."</data>
  <data key="d5">"legal rulings, case law"</data>
  <data key="d6">chunk-a8661700ca5b769e561d0c13fe452ae7</data>
</edge>
<edge source="&quot;DE ARMAS&quot;" target="&quot;PERSON&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"De Armas refers to a specific person whose name is mentioned in the document." "</data>
  <data key="d5">"mentioned entity, individual reference"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;PASSAGEM RELEVANTE PARA A QUEST&quot;" target="&quot;EVENT&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The passagem relevante para a quest is an event or piece of text relevant to the query, providing legal information." "</data>
  <data key="d5">"query relevance, legal information"</data>
  <data key="d6">chunk-338cf6d446307fa476c0b025098bcd87</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-V0&quot;" target="&quot;LEARNING RATE (1E-5)&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The learning rate of \(10^{-5}\) was used during the training process to fine-tune the model."</data>
  <data key="d5">"training parameter, optimization"</data>
  <data key="d6">chunk-2abe609e17efe1ef0aa4e13a6b7ccddc</data>
</edge>
<edge source="&quot;GPT3 TEXT-DAVINCI-003 MODEL API&quot;" target="&quot;STS FINE-TUNING STAGE DESCRIBED IN BERTIMBAU’S PAPER&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The fine-tuning process on STS datasets, as described by BERTimbau, uses the GPT3 text-davinci-003 model API for generating sentence pairs."</data>
  <data key="d5">"model usage, data generation"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STS FINE-TUNING STAGE DESCRIBED IN BERTIMBAU’S PAPER&quot;" target="&quot;BATCH SIZE OF 8 FOR FIVE EPOCHS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The batch size of 8 and 5 epochs was part of the configuration used in the STS fine-tuning process as per BERTimbau's method.".</data>
  <data key="d5">"model training, configuration"</data>
  <data key="d6">chunk-de764363085fa944c487f5f0db894d4d</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V0&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V0&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"These are different model variants trained with similar data but using different techniques."&lt;SEP&gt;"These models were both trained using the GPL technique and NLI data, making them similar in structure."</data>
  <data key="d5">"model variant, GPL/STS/NLI training"&lt;SEP&gt;&lt;"GPL application"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-TSDAE-GPL-NLI-STS-V0&quot;" target="&quot;STJIRIS/BERT-LARGE-PORTUGUESE-CASED-LEGAL-MLM-GPL-NLI-STS-V1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Both are more advanced versions of the initial model variants."</data>
  <data key="d5">"model evolution, GPL/STS/NLI training"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;BRWAC CORPUS FOR BRAZILIAN PORTUGUESE&quot;" target="&quot;BOOS, R.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Rafael Boos contributed to the creation of brWaC Corpus."</data>
  <data key="d5">"corpus creator"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;BRWAC CORPUS FOR BRAZILIAN PORTUGUESE&quot;" target="&quot;PRESENTES, K.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Katharina Presentes is part of the team that created brWaC Corpus."</data>
  <data key="d5">"corpus contributor"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;BRWAC CORPUS FOR BRAZILIAN PORTUGUESE&quot;" target="&quot;VILLAVICENCIO, A.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Andrea Villavicencio contributed to creating brWaC Corpus."</data>
  <data key="d5">"corpus creator"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;BRWAC CORPUS FOR BRAZILIAN PORTUGUESE&quot;" target="&quot;PADRÓ, M.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Manuel Padró is involved in the creation of brWaC Corpus."</data>
  <data key="d5">"corpus creator"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;LREC 2014&quot;" target="&quot;REYKJAVIK, ICELAND&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The event LREC 2014 was held in Reykjavik, Iceland, indicating the location of another important research conference."</data>
  <data key="d5">&lt;"location, organization"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;REYKJAVIK, ICELAND&quot;" target="&quot;LREC'14&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The LREC'14 conference was held in Reykjavik, Iceland."</data>
  <data key="d5">"conference venue, location"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;N., K AISER&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"N., K AISER is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;N., K AISER&quot;" target="&quot;ANDPOLOSUKHIN , L.U.&quot;">
  <data key="d3">18.0</data>
  <data key="d4">"Both are co-authors on the 'Attention is all you need' paper."</data>
  <data key="d5">&lt;"co-authorship, collaboration"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;BRIGHT , J.&quot;" target="&quot;AND SCIPY1.0 C ONTRIBUTORS.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"J. BRIGHT is part of the group contributing to the SciPy project."</data>
  <data key="d5">&lt;"group membership, contribution to SciPy"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
<edge source="&quot;EVENT&quot;" target="&quot;FINE-TUNING ON MS MARCO FOR TWO EPOCHS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The event of fine-tuning the T5 model on MS MARCO for two epochs is directly linked with using it to generate queries. This process improves the model’s performance on specific tasks."</data>
  <data key="d5">"training, dataset usage"</data>
  <data key="d6">chunk-dece8100a2db817f460c5edaaa852208</data>
</edge>
<edge source="&quot;EVENT&quot;" target="&quot;MARELLI ET AL.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"MARELLI et al.'s work was presented at LREC'14, which is another important event for researchers in language and computational linguistics, indicating their active participation in the research community."</data>
  <data key="d5">"research contribution, community involvement"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;EVENT&quot;" target="&quot;MAY 7-9, 2015&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"This date marks the duration of ICLR 2015, highlighting its timing and duration as a significant event for researchers in natural language processing."</data>
  <data key="d5">"timeframe, organization"&lt;SEP&gt;&lt;"timeframe, organization"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;EVENT&quot;" target="&quot;M ARELLI ET AL.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"MARELLI et al.'s work was presented at LREC'14, which is another important event for researchers in language and computational linguistics, indicating their active participation in the research community."</data>
  <data key="d5">&lt;"research contribution, community involvement"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;SUPERVISORS: PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOÃO DIAS&quot;" target="&quot;EXAMINATION COMMITTEE: PROF. MARIA LUÍSA TORRES RIBEIRO MARQUES DA SILVA COHEUR, PROF. PEDRO ALEXANDRE SIMÕES DOS SANTOS &amp; PROF. JOSÉ LUÍS BRINQUETE BORBINHA&quot;">
  <data key="d3">17.0</data>
  <data key="d4">"Professors Pedro Alexandre Simões dos Santos and João Dias are also members of the examination committee, overseeing the thesis work and providing final evaluations."&lt;SEP&gt;"Professors Pedro Alexandre Simões dos Santos and João Dias are key figures in both the supervision and examination committee processes."</data>
  <data key="d5">"supervision, evaluation"</data>
  <data key="d6">chunk-61af9c4cbc05bef525950862cfae5a86</data>
</edge>
<edge source="&quot;LEXICAL-FIRST SEARCH SYSTEM 50&quot;" target="&quot;PURELY SEMANTIC SEARCH SYSTEM 50&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both systems are described in close relation, as they both deal with search functionalities within the broader Semantic Search System."&lt;</data>
  <data key="d5">"architecture comparison, alternative approach"</data>
  <data key="d6">chunk-1721070d8e5848c74ff9604584ac59f8</data>
</edge>
<edge source="&quot;VECTOR SPACE WITH QUERY EMBEDDING AND SENTENCE EMBEDDINGS&quot;" target="&quot;FIGURE 2.10&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The vector space is illustrated in Figure 2.10, showing its application in information retrieval tasks." "</data>
  <data key="d5">"vector space, information retrieval"</data>
  <data key="d6">chunk-4438e56d1dbc938d09784326b42337ca</data>
</edge>
<edge source="&quot;30-34&quot;" target="&quot;56-76&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The pages 30-34 introduce models and their descriptions, while pages 56-76 provide detailed evaluations and metrics, showing a logical progression of information."</data>
  <data key="d5">"information flow, evaluation process"</data>
  <data key="d6">chunk-86921910d0aba929bef9ae955431a4fa</data>
</edge>
<edge source="&quot;CBOW&quot;" target="&quot;MULTIPLE WORDS CONTEXT CBOW&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Multiple Words Context CBOW extends the standard CBOW model by increasing the input layer size for more complex context inputs."</data>
  <data key="d5">"extension, complexity"</data>
  <data key="d6">chunk-073485a6071a38818c47ff7188ec860b</data>
</edge>
<edge source="&quot;TOKEN EMBEDDINGS&quot;" target="&quot;SEGMENTATION EMBEDDINGS&quot;">
  <data key="d3">12.0</data>
  <data key="d4">"Token Embeddings and Segmentation Embeddings are both components in BERT's input representation process, working together to create meaningful word vectors."</data>
  <data key="d5">"word vector creation, input representation"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;TOKEN EMBEDDINGS&quot;" target="&quot;POSITION EMBEDDINGS&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Position Embeddings work alongside Token Embeddings to provide context and order information for words within a sentence, enhancing the overall language understanding of BERT."</data>
  <data key="d5">"context awareness, word vectors"</data>
  <data key="d6">chunk-03104590ddb41a3cfeb2c96dccb5c2f1</data>
</edge>
<edge source="&quot;SBERT -NLI&quot;" target="&quot;STNLI (STANFORD NATURAL LANGUAGE INFERENCE)&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"SBERT -NLI was pre-trained on the STNLI dataset, which contributes to its ability to handle diverse natural language inference tasks."</data>
  <data key="d5">"pre-training, multi-genre support"</data>
  <data key="d6">chunk-b6270162d82d1fef624d494a11c5caca</data>
</edge>
<edge source="&quot;50% ADJACENT SEQUENCE SELECTION&quot;" target="&quot;15% TOKEN REPLACEMENT&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Both events are part of the example generation process in pretraining, complementing each other to form diverse training examples."</data>
  <data key="d5">"complementary processes, diversity"</data>
  <data key="d6">chunk-813c546217d2864adf1fc0789841ad36</data>
</edge>
<edge source="&quot;INDEX&quot;" target="&quot;JURISPRUDENCIA.1.0&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"jurisprudencia.1.0 is a specific index used to store and retrieve indexed legal documents in Elasticsearch."</data>
  <data key="d5">"document storage, indexing"</data>
  <data key="d6">chunk-9c13271d2c9dfd85cce22bb863dd2aa7</data>
</edge>
<edge source="&quot;COSINE SIMILARITY CALCULATION&quot;" target="&quot;FIGURE 4.3&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"The process of Cosine Similarity Calculation is illustrated in Figure 4.3 as part of the retrieval method."</data>
  <data key="d5">"method illustration, similarity measurement"</data>
  <data key="d6">chunk-ca0f485e268dd70b104be9c53d4b68fd</data>
</edge>
<edge source="&quot;LARGE MODELS&quot;" target="&quot;8 BATCH SIZE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"A specific batch size of 8 was used for training the large models."</data>
  <data key="d5">"batch size, model training"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;LARGE MODELS&quot;" target="&quot;5 EPOCHS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The training process involved a total of 5 epochs for the large models."</data>
  <data key="d5">"training process, number of epochs"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;LARGE MODELS&quot;" target="&quot;10−5 LEARNING RATE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"A specific learning rate value (10−5) was used in the training of the large models."</data>
  <data key="d5">"learning rate, model training"</data>
  <data key="d6">chunk-20dc031e50b0aef84979653c6aeb3a8d</data>
</edge>
<edge source="&quot;PILE OF LAW: LEARNING RESPONSIBLE DATA FILTERING FROM THE LAW&quot;" target="&quot;ETHICAL CHALLENGES&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"This publication discusses and addresses ethical issues faced during dataset creation and usage."</data>
  <data key="d5">"ethical considerations, data quality"</data>
  <data key="d6">chunk-6c7ea4dc0b9ee1c35294eedffacbeb38</data>
</edge>
<edge source="&quot;SUPERVISED LEARNING OF UNIVERSAL SENTENCE REPRESENTATIONS FROM NATURAL LANGUAGE INFERENCE DATA&quot;" target="&quot;COPENHAGEN, DENMARK, 2017&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The supervised learning task was presented at the conference in Copenhagen, Denmark, 2017."</data>
  <data key="d5">"task location and year"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;SUPERVISED LEARNING OF UNIVERSAL SENTENCE REPRESENTATIONS FROM NATURAL LANGUAGE INFERENCE DATA&quot;" target="&quot;Z. DAI AND J. CALLAN&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The authors Z. Dai and J. Callan are associated with the supervised learning task on NLI data."</data>
  <data key="d5">"authorship"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;Z. DAI AND J. CALLAN&quot;" target="&quot;J. DEVLIN&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"The authors of the BERT paper are likely part of the same community as J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, who are also associated with ACL."</data>
  <data key="d5">"community membership, collaboration"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;J. DEVLIN&quot;" target="&quot;M.-W. CHANG&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"Co-authors J. Devlin and M.-W. Chang are working together on the BERT model paper."</data>
  <data key="d5">"collaboration, authorship"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;J. DEVLIN&quot;" target="&quot;K. LEE&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"Co-authors J. Devlin and K. Lee are collaborating on the BERT model paper."</data>
  <data key="d5">"collaboration, authorship"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;J. DEVLIN&quot;" target="&quot;K. TOUTANOVA&quot;">
  <data key="d3">20.0</data>
  <data key="d4">"Co-authors J. Devlin and K. Toutanova are working together on the BERT model paper."</data>
  <data key="d5">"collaboration, authorship"</data>
  <data key="d6">chunk-914c2300c13f4d0b3d2cd59e1e8f3bce</data>
</edge>
<edge source="&quot;ERGUN, G.&quot;" target="&quot;RADEV, D. R.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Authors of LexRank: Graph-based lexical centrality as salience in text summarization."</data>
  <data key="d5">"collaboration, summarization techniques"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;GURURANGAN, S.&quot;" target="&quot;MARASOVIĆ, A.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Co-authors and contributors to adapting language models to domains through pretraining."</data>
  <data key="d5">"collaboration, model adaptation"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;GURURANGAN, S.&quot;" target="&quot;SWAYAMB DIPSTA, S.&quot;">
  <data key="d3">16.0</data>
  <data key="d4">"Co-authors and contributors to adapting language models to domains through pretraining."</data>
  <data key="d5">"collaboration, model adaptation"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;MARELLI, M.&quot;" target="&quot;MENINI, S.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators on the SICK cure project."</data>
  <data key="d5">"project collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;MARELLI, M.&quot;" target="&quot;BARONI, M.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators on the SICK cure project."</data>
  <data key="d5">"project collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;MARELLI, M.&quot;" target="&quot;BENTIVOGLI, L.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators on the SICK cure project."</data>
  <data key="d5">"project collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;MARELLI, M.&quot;" target="&quot;BERNARDI, R.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators on the SICK cure project."</data>
  <data key="d5">"project collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;MARELLI, M.&quot;" target="&quot;ZAMPARELLI, R.&quot;">
  <data key="d3">14.0</data>
  <data key="d4">"Collaborators on the SICK cure project."</data>
  <data key="d5">"project collaboration, NLP research"</data>
  <data key="d6">chunk-2016e530048d82125b70492619ae1cd8</data>
</edge>
<edge source="&quot;MARELLI, M.&quot;" target="&quot;ASICK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"MARELLI, M. and co-authors are working on the ASICK project for evaluating compositional distributional semantic models."</data>
  <data key="d5">"research collaboration, evaluation project"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;EFFICIENT ESTIMATION OF WORD REPRESENTATIONS IN VECTOR SPACE&quot;" target="&quot;T. MIKOLOV&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The research on efficient estimation of word representations was conducted by T. Mikolov."</data>
  <data key="d5">"research collaboration, methodology development"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;EFFICIENT ESTIMATION OF WORD REPRESENTATIONS IN VECTOR SPACE&quot;" target="&quot;K. CHEN&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The research on efficient estimation of word representations was conducted by K. Chen."</data>
  <data key="d5">"research collaboration, methodology development"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;EFFICIENT ESTIMATION OF WORD REPRESENTATIONS IN VECTOR SPACE&quot;" target="&quot;G. CORRADO&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The research on efficient estimation of word representations was conducted by G. Corrado."</data>
  <data key="d5">"research collaboration, methodology development"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;EFFICIENT ESTIMATION OF WORD REPRESENTATIONS IN VECTOR SPACE&quot;" target="&quot;J. DEAN&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The research on efficient estimation of word representations was conducted by J. Dean."</data>
  <data key="d5">"research collaboration, methodology development"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;JNLP TEAM: DEEP LEARNING FOR LEGAL PROCESSING IN COLIEE 2020&quot;" target="&quot;H. NGUYEN, P. M. NGUYEN, T. B. DANG, Q. M. BU , V. T. SINH, C. M. NGUYEN, V. D. TRAN, K. SATOH, AND M. L. NGUYEN&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The JNLP team includes these individuals who are working on deep learning for legal processing."</data>
  <data key="d5">"team membership, research collaboration"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;THE ASSIN 2 SHARED TASK: A QUICK OVERVIEW&quot;" target="&quot;L. REAL, E. FONSECA, H. OLIVEIRA&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The ASSIN 2 shared task was worked on by these individuals."</data>
  <data key="d5">"team membership, research collaboration"</data>
  <data key="d6">chunk-9b1e1352f338d948b4876b635d24d01c</data>
</edge>
<edge source="&quot;ROBERTSON , S.&quot;" target="&quot;ZARAGOZA , H.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both ROBERTSON, S. and ZARAGOZA, H. are authors of a paper together indicating collaboration in research."</data>
  <data key="d5">"collaboration, co-authorship"</data>
  <data key="d6">chunk-dc84a2863207e7e3c024c8d2254064a2</data>
</edge>
<edge source="&quot;SCIPY1.0 C ONTRIBUTORS&quot;" target="&quot;NATURE METHODS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The SciPy 1.0 contributors' work, 'SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python,' was published in Nature Methods."</data>
  <data key="d5">"publication, contribution"</data>
  <data key="d6">chunk-90c5c9d551d02ddad6bd6b872760fc23</data>
</edge>
</graph></graphml>