{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5756bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d091a1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b6519bdc894bc08c5ac30bede8b6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff20ed90f274098a0dfedfb8c0f776d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:  37%|###7      | 2.98G/7.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ed53428cba43a0ae14dea31599d868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:  40%|####      | 3.32G/8.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1370669635f6402598acc3a15bcaea47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:  30%|##9       | 2.09G/7.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Francisco Azeredo\\.cache\\huggingface\\hub\\models--PORTULAN--gervasio-8b-portuguese-ptpt-decoder. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fc2c64ffc54ed3b64556cd73bde2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2945ee89ab4569ba062750e868ec0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe27ffdc9264908bc7b582925a3775f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dfaa4b675d46d781b788a6edbfa9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5b3e77e0b34763b9e85e194f2d79dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A comida portuguesa é uma das mais ricas e variadas do mundo'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(model='PORTULAN/gervasio-8b-portuguese-ptpt-decoder')\n",
    "generator(\"A comida portuguesa é\", max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abccbaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A comida portuguesa é uma das mais ricas e variadas do mundo, com uma rica herança gastronómica que reflete a influência de diferentes culturas. A culinária portuguesa é conhecida por seus pratos típicos, como o bacalhau à brás, o frango grelhado e o arroz de tamboril, que são apreciados em todo o mundo.\\nA comida portuguesa é uma combinação de sabores, texturas e temperos, com uma ênfase na utilização de ingredientes frescos e locais. Os pratos portugueses são frequentemente servidos em conjunto, com uma variedade de acompanhamentos, como saladas, legumes e panes.\\nA culinária portuguesa é também conhecida por suas sobremesas, como o pastel de nata, que é uma especialidade portuguesa feita com ovos, açúcar e leite. Outra sobremesa popular é o arroz doce, que é uma mistura de arroz, açúcar e frutas.\\nAlém disso, a culinária portuguesa é influenciada pela sua história'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"A comida portuguesa é\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d824f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from minirag.utils import calculate_similarity_score  # legacy helper (returns indices) – not used now\n",
    "from nltk.metrics import edit_distance\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# (Optionally create and reuse these globally to avoid re-loading each call)\n",
    "_BERT_MODEL = None\n",
    "_ROUGE = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def calculate_best_similarity(sentences: list[str], target: str, method=\"levenshtein\", n=1):\n",
    "    \"\"\"\n",
    "    Returns the highest similarity score (float) between any sentence in `sentences` and `target`.\n",
    "    Methods: jaccard | levenshtein | rouge | bert | overlap | bleu\n",
    "    For rouge, n=1 or 2 selects rouge-1 or rouge-2 F.\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    tgt_tokens = target.lower().split()\n",
    "    scores = []\n",
    "\n",
    "    if method == \"jaccard\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_tokens = set(s.lower().split())\n",
    "            inter = set(s_tokens).intersection(set(tgt_set))\n",
    "            union = set(s_tokens).union(set(tgt_set))\n",
    "            scores.append(len(inter) / len(union) if union else 0.0)\n",
    "\n",
    "    elif method == \"levenshtein\":\n",
    "        tgt_len = max(len(tgt_tokens), 1)\n",
    "        for s in sentences:\n",
    "            dist = edit_distance(tgt_tokens, s.lower().split())\n",
    "            norm = max(tgt_len, len(s.split()))\n",
    "            scores.append(1 - dist / norm if norm else 0.0)\n",
    "\n",
    "    elif method == \"rouge\":\n",
    "        global _ROUGE\n",
    "        if _ROUGE is None:\n",
    "            _ROUGE = Rouge()\n",
    "        key = f\"rouge-{n}\"\n",
    "        for s in sentences:\n",
    "            r = _ROUGE.get_scores(s, target)\n",
    "            scores.append(r[0].get(key, {}).get(\"f\", 0.0))\n",
    "\n",
    "    elif method == \"bert\":\n",
    "        global _BERT_MODEL\n",
    "        if _BERT_MODEL is None:\n",
    "            _BERT_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = _BERT_MODEL.encode(sentences + [target])\n",
    "        tgt_vec = embeddings[-1]\n",
    "        tgt_norm = np.linalg.norm(tgt_vec)\n",
    "        for i in range(len(sentences)):\n",
    "            v = embeddings[i]\n",
    "            denom = (np.linalg.norm(v) * tgt_norm)\n",
    "            scores.append(float(np.dot(v, tgt_vec) / denom) if denom else 0.0)\n",
    "\n",
    "    elif method == \"overlap\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_set = set(s.lower().split())\n",
    "            inter = s_set & tgt_set\n",
    "            denom = min(len(s_set), len(tgt_set))\n",
    "            scores.append(len(inter) / denom if denom else 0.0)\n",
    "\n",
    "    elif method == \"bleu\":\n",
    "        tgt_bleu = word_tokenize(target.lower())\n",
    "        for s in sentences:\n",
    "            s_bleu = word_tokenize(s.lower())\n",
    "            scores.append(sentence_bleu([tgt_bleu], s_bleu, smoothing_function=_SMOOTH))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method.\")\n",
    "\n",
    "    return max(scores) if scores else 0.0\n",
    "\n",
    "answer = calculate_best_similarity([\"quick fox\"], \"the quick brown fox\", method=\"jaccard\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9f7eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\\results_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os, random, string, time\n",
    "from csv import DictWriter\n",
    "from datetime import datetime\n",
    "\n",
    "def rand_text(n=8):\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for _ in range(n))\n",
    "\n",
    "def make_random_row(i):\n",
    "    return {\n",
    "        \"question\": f\"Q{i} {rand_text(5)}?\",\n",
    "        \"gold\": f\"Gold answer {rand_text(6)}\",\n",
    "        \"answer\": f\"Model answer {rand_text(7)}\",\n",
    "        \"latency_s\": round(random.uniform(0.01, 0.5), 4),\n",
    "        \"exact\": random.choice([0,1]),\n",
    "        \"substring\": random.choice([0,1]),\n",
    "        \"token_recall\": round(random.random(), 3),\n",
    "        \"jaccard\": round(random.random(), 3),\n",
    "        \"levenshtein\": round(random.random(), 3),\n",
    "        \"rouge1_f\": round(random.random(), 3),\n",
    "        \"rouge2_f\": round(random.random(), 3),\n",
    "        \"overlap\": round(random.random(), 3),\n",
    "        \"bleu\": round(random.random(), 3),\n",
    "        \"bert_cos\": round(random.uniform(0.3, 0.95), 3),\n",
    "    }\n",
    "\n",
    "# create N random rows\n",
    "N = 5\n",
    "rows = [make_random_row(i+1) for i in range(N)]\n",
    "OUTPUT_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\"  # set to None to skip saving\n",
    "mode = \"test\"\n",
    "os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_CSV_PATH, f\"results_{mode}.csv\")\n",
    "# Optional CSV\n",
    "if OUTPUT_CSV and rows:\n",
    "    write_header = not os.path.exists(OUTPUT_CSV)\n",
    "    with open(OUTPUT_CSV, 'x', encoding='utf-8', newline='') as f:\n",
    "        writer = DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        if write_header: writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"Saved results to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a52024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from minirag.llm.openai import openai_complete\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set in environment. Set it before running this cell.\")\n",
    "\n",
    "# Correct usage: pass api_key directly (don't wrap inside a kwargs dict)\n",
    "# Choose a currently supported model name. (gpt-4o-mini shown; adjust if needed.)\n",
    "response = await openai_complete(\n",
    "    \"Hello, world!\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    api_key=api_key,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2f1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init device: cuda\n",
      "Loading embedding tokenizer/model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Logger initialized for working directory: C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\n",
      "INFO:minirag:Load KV json_doc_status_storage with 0 data\n",
      "INFO:minirag:Load KV llm_response_cache with 0 data\n",
      "INFO:minirag:Load KV full_docs with 28 data\n",
      "INFO:minirag:Load KV text_chunks with 34 data\n",
      "INFO:minirag:Load KV json_doc_status_storage with 0 data\n",
      "INFO:minirag:Load KV llm_response_cache with 0 data\n",
      "INFO:minirag:Load KV full_docs with 28 data\n",
      "INFO:minirag:Load KV text_chunks with 34 data\n",
      "INFO:minirag:Loaded graph from C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\\graph_chunk_entity_relation.graphml with 600 nodes, 718 edges\n",
      "INFO:nano-vectordb:Load (591, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_entities.json'} 591 data\n",
      "INFO:nano-vectordb:Load (591, 384) data\n",
      "INFO:minirag:Loaded graph from C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\\graph_chunk_entity_relation.graphml with 600 nodes, 718 edges\n",
      "INFO:nano-vectordb:Load (591, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_entities.json'} 591 data\n",
      "INFO:nano-vectordb:Load (591, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_entities_name.json'} 591 data\n",
      "INFO:nano-vectordb:Load (718, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_relationships.json'} 718 data\n",
      "INFO:nano-vectordb:Load (34, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_chunks.json'} 34 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_entities_name.json'} 591 data\n",
      "INFO:nano-vectordb:Load (718, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_relationships.json'} 718 data\n",
      "INFO:nano-vectordb:Load (34, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\Código\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage\\\\vdb_chunks.json'} 34 data\n",
      "INFO:minirag:Loaded document status storage with 33 records\n",
      "INFO:minirag:Loaded document status storage with 33 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Loads embedding model, builds embedding_func, and instantiates a MiniRAG object.\n",
    "# Does NOT ingest documents. Use the next cell to index.\n",
    "\n",
    "import os, torch, sys\n",
    "import minirag\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.llm import ollama\n",
    "from minirag.llm.openai import openai_complete\n",
    "from minirag import MiniRAG\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set in environment. Set it before running this cell.\")\n",
    "\n",
    "sys.path.append(r'c:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\Chatbot\\lightrag')\n",
    "\n",
    "# Core configuration (shared by later cells)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\"\n",
    "LLM_MODEL_NAME = \"qwen2m:latest\"  # set to None if no local Ollama model\n",
    "LOG_LEVEL = \"INFO\"\n",
    "\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Init device:\", device)\n",
    "\n",
    "print(\"Loading embedding tokenizer/model...\")\n",
    "_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "_embed_model = AutoModel.from_pretrained(EMBEDDING_MODEL).to(device)\n",
    "_embed_model.eval()\n",
    "\n",
    "async def _embed_batch(texts: list[str]):\n",
    "    return await hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model)\n",
    "\n",
    "async def _embed_dispatch(input_text):\n",
    "    if isinstance(input_text, str):\n",
    "        return (await _embed_batch([input_text]))[0]\n",
    "\n",
    "\n",
    "        \n",
    "    if isinstance(input_text, (list, tuple)) and all(isinstance(t, str) for t in input_text):\n",
    "        return await _embed_batch(list(input_text))\n",
    "    raise TypeError(f\"Unsupported input type for embedding_func: {type(input_text)}\")\n",
    "\n",
    "_embedding_func = EmbeddingFunc(\n",
    "    embedding_dim=_embed_model.config.hidden_size,\n",
    "    max_token_size=_tokenizer.model_max_length,\n",
    "    func = lambda texts: hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model),\n",
    ")\n",
    "rag = minirag.MiniRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=ollama.ollama_model_complete if LLM_MODEL_NAME else None,\n",
    "    llm_model_name=LLM_MODEL_NAME,\n",
    "    embedding_func=_embedding_func,\n",
    "    log_level=LOG_LEVEL,\n",
    "    suppress_httpx_logging=True\n",
    ")\n",
    "print(\"RAG initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdced5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFrancisco Azeredo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtecnico\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m5 ano\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtese\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCódigo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMiniRAG\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnotebooks\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mO Acesso a documentos administrativos.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Run indexing (sync now; no await needed)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m index_pdf(file, rag)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPDF indexed into RAG.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m, in \u001b[0;36mindex_pdf\u001b[1;34m(file, rag)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mindex_pdf\u001b[39m(file, rag: MiniRAG):\n\u001b[0;32m     35\u001b[0m     text \u001b[38;5;241m=\u001b[39m extract_pdf_text(file)\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\minirag\\minirag.py:363\u001b[0m, in \u001b[0;36mMiniRAG.insert\u001b[1;34m(self, string_or_strings, metadata)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minsert\u001b[39m(\u001b[38;5;28mself\u001b[39m, string_or_strings, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    362\u001b[0m     loop \u001b[38;5;241m=\u001b[39m always_get_an_event_loop()\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mainsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring_or_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\asyncio\\base_events.py:667\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    669\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[0;32m    670\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\asyncio\\base_events.py:626\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 626\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from minirag import MiniRAG\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "# NOTE:\n",
    "# The previous version treated the *string path* as if it were an async file object (await file.read()).\n",
    "# A str has no .read(), hence AttributeError. We don't need async here; just open the path.\n",
    "\n",
    "def extract_pdf_text(file):\n",
    "    \"\"\"Return full extracted text from a PDF.\n",
    "\n",
    "    Accepts either:\n",
    "      - a path string / PathLike pointing to a PDF file\n",
    "      - a file-like object with .read() returning bytes or str\n",
    "    \"\"\"\n",
    "    # Case 1: path provided\n",
    "    if isinstance(file, (str, os.PathLike)):\n",
    "        with open(file, 'rb') as f:\n",
    "            reader = PdfReader(f)\n",
    "            return \"\\n\".join((page.extract_text() or \"\") for page in reader.pages)\n",
    "\n",
    "    # Case 2: file-like object\n",
    "    if hasattr(file, 'read'):\n",
    "        data = file.read()\n",
    "        if isinstance(data, str):  # if someone passed text, re-encode\n",
    "            data = data.encode('utf-8')\n",
    "        reader = PdfReader(BytesIO(data))\n",
    "        return \"\\n\".join((page.extract_text() or \"\") for page in reader.pages)\n",
    "\n",
    "    raise TypeError(f\"Unsupported file parameter type: {type(file)}. Provide a path or file-like object.\")\n",
    "\n",
    "\n",
    "async def index_pdf(file, rag: MiniRAG):\n",
    "    text = extract_pdf_text(file)\n",
    "    rag.insert(text)\n",
    "\n",
    "file = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\O Acesso a documentos administrativos.pdf\"\n",
    "\n",
    "# Run indexing (sync now; no await needed)\n",
    "await index_pdf(file, rag)\n",
    "print(\"PDF indexed into RAG.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
